{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-02-17T12:25:37.771976Z",
     "start_time": "2025-02-17T12:25:37.634842Z"
    }
   },
   "source": [
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "import random\n",
    "import imageio\n",
    "import os\n",
    "import tqdm\n",
    "\n",
    "#import pickle5 as pickle\n",
    "from tqdm import tqdm  # Instead of tqdm.notebook"
   ],
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-17T12:25:39.846124Z",
     "start_time": "2025-02-17T12:25:39.837697Z"
    }
   },
   "cell_type": "code",
   "source": [
    "env = gym.make(\"FrozenLake-v1\", map_name=\"8x8\", is_slippery=False, render_mode=\"rgb_array\")\n",
    "#env = gym.make(\"FrozenLake-v1\", map_name=\"4x4\", is_slippery=False, render_mode=\"human\")\n"
   ],
   "id": "7320b7bfe952ae67",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-17T12:25:41.745358Z",
     "start_time": "2025-02-17T12:25:41.740763Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# We create our environment with gym.make(\"<name_of_the_environment>\")- `is_slippery=False`: The agent always moves in the intended direction due to the non-slippery nature of the frozen lake (deterministic).\n",
    "print(\"_____OBSERVATION SPACE_____ \\n\")\n",
    "print(\"Observation Space\", env.observation_space)\n",
    "print(\"Sample observation\", env.observation_space.sample()) # Get a random observation"
   ],
   "id": "45f39f3bbc2a277b",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_____OBSERVATION SPACE_____ \n",
      "\n",
      "Observation Space Discrete(64)\n",
      "Sample observation 34\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-17T12:25:44.784154Z",
     "start_time": "2025-02-17T12:25:44.780667Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(\"\\n _____ACTION SPACE_____ \\n\")\n",
    "print(\"Action Space Shape\", env.action_space.n)\n",
    "print(\"Action Space Sample\", env.action_space.sample()) # Take a random action"
   ],
   "id": "4aa44fe2f85c1e67",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " _____ACTION SPACE_____ \n",
      "\n",
      "Action Space Shape 4\n",
      "Action Space Sample 1\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-17T12:25:47.453157Z",
     "start_time": "2025-02-17T12:25:47.450235Z"
    }
   },
   "cell_type": "code",
   "source": [
    "state_space = env.observation_space.n\n",
    "print(\"There are \", state_space, \" possible states\")\n",
    "\n",
    "action_space = env.action_space.n\n",
    "print(\"There are \", action_space, \" possible actions\")"
   ],
   "id": "ee1fae8192f16ea0",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are  64  possible states\n",
      "There are  4  possible actions\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-17T12:25:51.302301Z",
     "start_time": "2025-02-17T12:25:51.298781Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Let's create our Qtable of size (state_space, action_space) and initialized each values at 0 using np.zeros\n",
    "def initialize_q_table(state_space, action_space):\n",
    "    Qtable = np.zeros((state_space, action_space))\n",
    "    return Qtable"
   ],
   "id": "4f45c46a54195f58",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-17T12:25:53.665523Z",
     "start_time": "2025-02-17T12:25:53.662709Z"
    }
   },
   "cell_type": "code",
   "source": "Qtable_frozenlake = initialize_q_table(state_space, action_space)",
   "id": "e00953f907094d5f",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-17T12:25:56.471938Z",
     "start_time": "2025-02-17T12:25:56.469698Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def greedy_policy(Qtable, state):\n",
    "    # Exploitation: take the action with the highest state, action value\n",
    "    action = np.argmax(Qtable[state][:])\n",
    "\n",
    "    return action"
   ],
   "id": "8eea2f872946e299",
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-17T12:25:59.919187Z",
     "start_time": "2025-02-17T12:25:59.914922Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def epsilon_greedy_policy(Qtable, state, epsilon):\n",
    "    # Randomly generate a number between 0 and 1\n",
    "    random_num = random.uniform(0,1)\n",
    "    # if random_num > greater than epsilon --> exploitation\n",
    "    if random_num > epsilon:\n",
    "        # Take the action with the highest value given a state\n",
    "        # np.argmax can be useful here\n",
    "        action = greedy_policy(Qtable, state)\n",
    "    # else --> exploration\n",
    "    else:\n",
    "        action = env.action_space.sample()\n",
    "\n",
    "    return action"
   ],
   "id": "9facfdd1249c5fc6",
   "outputs": [],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-17T12:26:03.736532Z",
     "start_time": "2025-02-17T12:26:03.732018Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Training parameters\n",
    "n_training_episodes = 100000  # Total training episodes\n",
    "learning_rate = 0.1          # Learning rate\n",
    "\n",
    "# Evaluation parameters\n",
    "n_eval_episodes = 100        # Total number of test episodes\n",
    "\n",
    "# Environment parameters\n",
    "env_id = \"FrozenLake-v1\"     # Name of the environment\n",
    "max_steps = 300               # Max steps per episode\n",
    "gamma = 0.95                 # Discounting rate\n",
    "eval_seed = []               # The evaluation seed of the environment\n",
    "\n",
    "# Exploration parameters\n",
    "max_epsilon = 1.0             # Exploration probability at start\n",
    "min_epsilon = 0.01            # Minimum exploration probability\n",
    "decay_rate = 0.001            # Exponential decay rate for exploration prob"
   ],
   "id": "68cee9045a9424b4",
   "outputs": [],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-17T12:29:51.379656Z",
     "start_time": "2025-02-17T12:29:50.896873Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def debug_train(n_training_episodes, min_epsilon, max_epsilon, decay_rate, env, max_steps, Qtable):\n",
    "    # Lists to keep track of rewards\n",
    "    episode_rewards = []\n",
    "    successful_episodes = []\n",
    "\n",
    "    for episode in range(min(100, n_training_episodes)):  # Let's just look at first 100 episodes for debugging\n",
    "        epsilon = min_epsilon + (max_epsilon - min_epsilon)*np.exp(-decay_rate*episode)\n",
    "        state, info = env.reset()\n",
    "\n",
    "        print(f\"\\nEpisode {episode}\")\n",
    "        print(f\"Starting state: {state}\")\n",
    "\n",
    "        for step in range(max_steps):\n",
    "            # Print current state and Q-values\n",
    "            print(f\"\\nStep {step}\")\n",
    "            print(f\"Current state: {state}\")\n",
    "            print(f\"Q-values for current state: {Qtable[state]}\")\n",
    "\n",
    "            # Choose and print action\n",
    "            action = epsilon_greedy_policy(Qtable, state, epsilon)\n",
    "            print(f\"Chosen action: {action}\")\n",
    "\n",
    "            # Take action and print results\n",
    "            old_q = Qtable[state][action]\n",
    "            new_state, reward, terminated, truncated, info = env.step(action)\n",
    "\n",
    "            print(f\"New state: {new_state}\")\n",
    "            print(f\"Reward: {reward}\")\n",
    "            print(f\"Terminated: {terminated}\")\n",
    "\n",
    "            # Update Q-value\n",
    "            Qtable[state][action] = Qtable[state][action] + learning_rate * (\n",
    "                    reward + gamma * np.max(Qtable[new_state]) - Qtable[state][action]\n",
    "            )\n",
    "\n",
    "            print(f\"Old Q-value: {old_q}\")\n",
    "            print(f\"New Q-value: {Qtable[state][action]}\")\n",
    "\n",
    "            if reward > 0:\n",
    "                print(\"GOAL REACHED!\")\n",
    "                successful_episodes.append(episode)\n",
    "\n",
    "            if terminated or truncated:\n",
    "                break\n",
    "\n",
    "            state = new_state\n",
    "\n",
    "        if episode % 10 == 0:\n",
    "            print(f\"\\nSuccessful episodes so far: {successful_episodes}\")\n",
    "\n",
    "    return Qtable, successful_episodes\n",
    "\n",
    "# Test environment layout\n",
    "print(\"Environment Layout:\")\n",
    "env.reset()\n",
    "desc = env.render()\n",
    "print(desc)\n",
    "\n",
    "# Run debug training\n",
    "print(\"\\nStarting debug training...\")\n",
    "Qtable_frozenlake, successful_episodes = debug_train(\n",
    "    n_training_episodes=100,\n",
    "    min_epsilon=max_epsilon,\n",
    "    max_epsilon=max_epsilon,\n",
    "    decay_rate=decay_rate,\n",
    "    env=env,\n",
    "    max_steps=max_steps,\n",
    "    Qtable=initialize_q_table(state_space, action_space)\n",
    ")\n",
    "\n",
    "print(\"\\nDebug Summary:\")\n",
    "print(f\"Total successful episodes: {len(successful_episodes)}\")\n",
    "print(f\"Successful episodes: {successful_episodes}\")"
   ],
   "id": "a40f2719859579fc",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Environment Layout:\n",
      "[[[180 200 230]\n",
      "  [180 200 230]\n",
      "  [180 200 230]\n",
      "  ...\n",
      "  [180 200 230]\n",
      "  [180 200 230]\n",
      "  [180 200 230]]\n",
      "\n",
      " [[180 200 230]\n",
      "  [204 230 255]\n",
      "  [204 230 255]\n",
      "  ...\n",
      "  [204 230 255]\n",
      "  [204 230 255]\n",
      "  [180 200 230]]\n",
      "\n",
      " [[180 200 230]\n",
      "  [235 245 249]\n",
      "  [204 230 255]\n",
      "  ...\n",
      "  [204 230 255]\n",
      "  [204 230 255]\n",
      "  [180 200 230]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[180 200 230]\n",
      "  [235 245 249]\n",
      "  [235 245 249]\n",
      "  ...\n",
      "  [204 230 255]\n",
      "  [235 245 249]\n",
      "  [180 200 230]]\n",
      "\n",
      " [[180 200 230]\n",
      "  [235 245 249]\n",
      "  [235 245 249]\n",
      "  ...\n",
      "  [204 230 255]\n",
      "  [204 230 255]\n",
      "  [180 200 230]]\n",
      "\n",
      " [[180 200 230]\n",
      "  [180 200 230]\n",
      "  [180 200 230]\n",
      "  ...\n",
      "  [180 200 230]\n",
      "  [180 200 230]\n",
      "  [180 200 230]]]\n",
      "\n",
      "Starting debug training...\n",
      "\n",
      "Episode 0\n",
      "Starting state: 0\n",
      "\n",
      "Step 0\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 1\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 1\n",
      "Current state: 1\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 2\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 2\n",
      "Current state: 2\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 10\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 3\n",
      "Current state: 10\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 2\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 4\n",
      "Current state: 2\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 3\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 5\n",
      "Current state: 3\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 11\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 6\n",
      "Current state: 11\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 3\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 7\n",
      "Current state: 3\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 4\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 8\n",
      "Current state: 4\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 5\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 9\n",
      "Current state: 5\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 4\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 10\n",
      "Current state: 4\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 4\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 11\n",
      "Current state: 4\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 5\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 12\n",
      "Current state: 5\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 6\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 13\n",
      "Current state: 6\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 6\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 14\n",
      "Current state: 6\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 7\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 15\n",
      "Current state: 7\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 7\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 16\n",
      "Current state: 7\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 6\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 17\n",
      "Current state: 6\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 14\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 18\n",
      "Current state: 14\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 22\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 19\n",
      "Current state: 22\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 23\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 20\n",
      "Current state: 23\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 31\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 21\n",
      "Current state: 31\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 39\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 22\n",
      "Current state: 39\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 39\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 23\n",
      "Current state: 39\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 47\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 24\n",
      "Current state: 47\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 46\n",
      "Reward: 0.0\n",
      "Terminated: True\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Successful episodes so far: []\n",
      "\n",
      "Episode 1\n",
      "Starting state: 0\n",
      "\n",
      "Step 0\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 1\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 2\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 3\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 4\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 5\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 6\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 7\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 8\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 9\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 10\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 11\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 12\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 1\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 13\n",
      "Current state: 1\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 14\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 8\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 15\n",
      "Current state: 8\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 16\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 16\n",
      "Current state: 16\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 24\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 17\n",
      "Current state: 24\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 32\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 18\n",
      "Current state: 32\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 40\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 19\n",
      "Current state: 40\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 40\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 20\n",
      "Current state: 40\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 32\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 21\n",
      "Current state: 32\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 40\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 22\n",
      "Current state: 40\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 41\n",
      "Reward: 0.0\n",
      "Terminated: True\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Episode 2\n",
      "Starting state: 0\n",
      "\n",
      "Step 0\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 1\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 1\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 2\n",
      "Current state: 1\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 2\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 3\n",
      "Current state: 2\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 10\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 4\n",
      "Current state: 10\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 11\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 5\n",
      "Current state: 11\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 3\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 6\n",
      "Current state: 3\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 2\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 7\n",
      "Current state: 2\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 1\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 8\n",
      "Current state: 1\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 2\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 9\n",
      "Current state: 2\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 10\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 10\n",
      "Current state: 10\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 2\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 11\n",
      "Current state: 2\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 3\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 12\n",
      "Current state: 3\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 4\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 13\n",
      "Current state: 4\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 5\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 14\n",
      "Current state: 5\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 6\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 15\n",
      "Current state: 6\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 6\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 16\n",
      "Current state: 6\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 7\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 17\n",
      "Current state: 7\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 15\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 18\n",
      "Current state: 15\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 15\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 19\n",
      "Current state: 15\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 15\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 20\n",
      "Current state: 15\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 23\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 21\n",
      "Current state: 23\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 22\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 22\n",
      "Current state: 22\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 14\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 23\n",
      "Current state: 14\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 15\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 24\n",
      "Current state: 15\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 14\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 25\n",
      "Current state: 14\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 6\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 26\n",
      "Current state: 6\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 14\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 27\n",
      "Current state: 14\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 6\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 28\n",
      "Current state: 6\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 7\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 29\n",
      "Current state: 7\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 7\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 30\n",
      "Current state: 7\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 15\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 31\n",
      "Current state: 15\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 15\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 32\n",
      "Current state: 15\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 15\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 33\n",
      "Current state: 15\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 14\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 34\n",
      "Current state: 14\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 6\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 35\n",
      "Current state: 6\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 7\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 36\n",
      "Current state: 7\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 6\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 37\n",
      "Current state: 6\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 7\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 38\n",
      "Current state: 7\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 6\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 39\n",
      "Current state: 6\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 5\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 40\n",
      "Current state: 5\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 5\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 41\n",
      "Current state: 5\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 6\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 42\n",
      "Current state: 6\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 6\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 43\n",
      "Current state: 6\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 6\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 44\n",
      "Current state: 6\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 6\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 45\n",
      "Current state: 6\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 7\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 46\n",
      "Current state: 7\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 15\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 47\n",
      "Current state: 15\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 15\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 48\n",
      "Current state: 15\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 23\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 49\n",
      "Current state: 23\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 31\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 50\n",
      "Current state: 31\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 23\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 51\n",
      "Current state: 23\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 22\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 52\n",
      "Current state: 22\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 30\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 53\n",
      "Current state: 30\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 31\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 54\n",
      "Current state: 31\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 23\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 55\n",
      "Current state: 23\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 23\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 56\n",
      "Current state: 23\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 22\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 57\n",
      "Current state: 22\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 23\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 58\n",
      "Current state: 23\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 31\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 59\n",
      "Current state: 31\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 30\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 60\n",
      "Current state: 30\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 29\n",
      "Reward: 0.0\n",
      "Terminated: True\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Episode 3\n",
      "Starting state: 0\n",
      "\n",
      "Step 0\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 1\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 2\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 3\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 1\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 4\n",
      "Current state: 1\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 5\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 6\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 7\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 8\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 9\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 10\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 11\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 8\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 12\n",
      "Current state: 8\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 16\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 13\n",
      "Current state: 16\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 17\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 14\n",
      "Current state: 17\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 9\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 15\n",
      "Current state: 9\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 8\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 16\n",
      "Current state: 8\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 8\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 17\n",
      "Current state: 8\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 16\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 18\n",
      "Current state: 16\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 8\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 19\n",
      "Current state: 8\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 16\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 20\n",
      "Current state: 16\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 8\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 21\n",
      "Current state: 8\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 9\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 22\n",
      "Current state: 9\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 1\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 23\n",
      "Current state: 1\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 24\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 8\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 25\n",
      "Current state: 8\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 16\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 26\n",
      "Current state: 16\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 24\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 27\n",
      "Current state: 24\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 32\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 28\n",
      "Current state: 32\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 32\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 29\n",
      "Current state: 32\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 24\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 30\n",
      "Current state: 24\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 16\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 31\n",
      "Current state: 16\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 16\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 32\n",
      "Current state: 16\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 24\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 33\n",
      "Current state: 24\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 32\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 34\n",
      "Current state: 32\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 32\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 35\n",
      "Current state: 32\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 40\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 36\n",
      "Current state: 40\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 40\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 37\n",
      "Current state: 40\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 48\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 38\n",
      "Current state: 48\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 56\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 39\n",
      "Current state: 56\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 56\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 40\n",
      "Current state: 56\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 56\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 41\n",
      "Current state: 56\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 56\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 42\n",
      "Current state: 56\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 56\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 43\n",
      "Current state: 56\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 57\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 44\n",
      "Current state: 57\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 49\n",
      "Reward: 0.0\n",
      "Terminated: True\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Episode 4\n",
      "Starting state: 0\n",
      "\n",
      "Step 0\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 1\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 8\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 2\n",
      "Current state: 8\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 3\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 4\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 5\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 6\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 7\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 8\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 9\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 10\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 1\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 11\n",
      "Current state: 1\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 2\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 12\n",
      "Current state: 2\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 3\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 13\n",
      "Current state: 3\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 11\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 14\n",
      "Current state: 11\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 12\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 15\n",
      "Current state: 12\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 13\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 16\n",
      "Current state: 13\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 12\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 17\n",
      "Current state: 12\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 11\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 18\n",
      "Current state: 11\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 12\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 19\n",
      "Current state: 12\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 13\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 20\n",
      "Current state: 13\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 14\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 21\n",
      "Current state: 14\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 13\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 22\n",
      "Current state: 13\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 21\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 23\n",
      "Current state: 21\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 29\n",
      "Reward: 0.0\n",
      "Terminated: True\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Episode 5\n",
      "Starting state: 0\n",
      "\n",
      "Step 0\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 1\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 2\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 8\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 3\n",
      "Current state: 8\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 16\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 4\n",
      "Current state: 16\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 24\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 5\n",
      "Current state: 24\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 25\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 6\n",
      "Current state: 25\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 17\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 7\n",
      "Current state: 17\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 25\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 8\n",
      "Current state: 25\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 17\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 9\n",
      "Current state: 17\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 9\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 10\n",
      "Current state: 9\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 10\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 11\n",
      "Current state: 10\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 2\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 12\n",
      "Current state: 2\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 3\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 13\n",
      "Current state: 3\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 2\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 14\n",
      "Current state: 2\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 2\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 15\n",
      "Current state: 2\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 1\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 16\n",
      "Current state: 1\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 1\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 17\n",
      "Current state: 1\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 2\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 18\n",
      "Current state: 2\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 10\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 19\n",
      "Current state: 10\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 11\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 20\n",
      "Current state: 11\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 10\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 21\n",
      "Current state: 10\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 18\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 22\n",
      "Current state: 18\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 26\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 23\n",
      "Current state: 26\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 18\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 24\n",
      "Current state: 18\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 10\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 25\n",
      "Current state: 10\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 11\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 26\n",
      "Current state: 11\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 10\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 27\n",
      "Current state: 10\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 2\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 28\n",
      "Current state: 2\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 10\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 29\n",
      "Current state: 10\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 9\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 30\n",
      "Current state: 9\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 17\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 31\n",
      "Current state: 17\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 16\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 32\n",
      "Current state: 16\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 17\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 33\n",
      "Current state: 17\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 18\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 34\n",
      "Current state: 18\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 26\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 35\n",
      "Current state: 26\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 18\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 36\n",
      "Current state: 18\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 17\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 37\n",
      "Current state: 17\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 16\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 38\n",
      "Current state: 16\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 8\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 39\n",
      "Current state: 8\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 16\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 40\n",
      "Current state: 16\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 16\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 41\n",
      "Current state: 16\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 8\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 42\n",
      "Current state: 8\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 9\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 43\n",
      "Current state: 9\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 10\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 44\n",
      "Current state: 10\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 9\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 45\n",
      "Current state: 9\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 8\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 46\n",
      "Current state: 8\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 16\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 47\n",
      "Current state: 16\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 16\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 48\n",
      "Current state: 16\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 17\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 49\n",
      "Current state: 17\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 16\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 50\n",
      "Current state: 16\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 16\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 51\n",
      "Current state: 16\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 24\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 52\n",
      "Current state: 24\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 25\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 53\n",
      "Current state: 25\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 33\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 54\n",
      "Current state: 33\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 25\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 55\n",
      "Current state: 25\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 24\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 56\n",
      "Current state: 24\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 16\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 57\n",
      "Current state: 16\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 8\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 58\n",
      "Current state: 8\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 8\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 59\n",
      "Current state: 8\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 16\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 60\n",
      "Current state: 16\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 8\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 61\n",
      "Current state: 8\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 8\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 62\n",
      "Current state: 8\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 9\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 63\n",
      "Current state: 9\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 17\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 64\n",
      "Current state: 17\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 18\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 65\n",
      "Current state: 18\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 10\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 66\n",
      "Current state: 10\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 18\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 67\n",
      "Current state: 18\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 17\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 68\n",
      "Current state: 17\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 16\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 69\n",
      "Current state: 16\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 17\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 70\n",
      "Current state: 17\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 9\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 71\n",
      "Current state: 9\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 1\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 72\n",
      "Current state: 1\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 73\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 74\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 1\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 75\n",
      "Current state: 1\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 1\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 76\n",
      "Current state: 1\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 9\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 77\n",
      "Current state: 9\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 8\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 78\n",
      "Current state: 8\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 8\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 79\n",
      "Current state: 8\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 80\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 1\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 81\n",
      "Current state: 1\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 1\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 82\n",
      "Current state: 1\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 1\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 83\n",
      "Current state: 1\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 84\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 85\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 8\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 86\n",
      "Current state: 8\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 16\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 87\n",
      "Current state: 16\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 17\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 88\n",
      "Current state: 17\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 9\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 89\n",
      "Current state: 9\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 1\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 90\n",
      "Current state: 1\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 91\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 92\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 1\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 93\n",
      "Current state: 1\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 9\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 94\n",
      "Current state: 9\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 10\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 95\n",
      "Current state: 10\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 9\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 96\n",
      "Current state: 9\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 10\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 97\n",
      "Current state: 10\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 18\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 98\n",
      "Current state: 18\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 10\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 99\n",
      "Current state: 10\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 2\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Episode 6\n",
      "Starting state: 0\n",
      "\n",
      "Step 0\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 8\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 1\n",
      "Current state: 8\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 8\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 2\n",
      "Current state: 8\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 9\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 3\n",
      "Current state: 9\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 1\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 4\n",
      "Current state: 1\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 5\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 6\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 7\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 8\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 8\n",
      "Current state: 8\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 9\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 9\n",
      "Current state: 9\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 17\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 10\n",
      "Current state: 17\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 16\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 11\n",
      "Current state: 16\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 17\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 12\n",
      "Current state: 17\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 16\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 13\n",
      "Current state: 16\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 8\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 14\n",
      "Current state: 8\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 16\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 15\n",
      "Current state: 16\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 8\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 16\n",
      "Current state: 8\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 8\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 17\n",
      "Current state: 8\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 9\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 18\n",
      "Current state: 9\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 8\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 19\n",
      "Current state: 8\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 20\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 1\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 21\n",
      "Current state: 1\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 1\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 22\n",
      "Current state: 1\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 23\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 8\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 24\n",
      "Current state: 8\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 25\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 8\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 26\n",
      "Current state: 8\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 9\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 27\n",
      "Current state: 9\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 17\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 28\n",
      "Current state: 17\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 18\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 29\n",
      "Current state: 18\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 19\n",
      "Reward: 0.0\n",
      "Terminated: True\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Episode 7\n",
      "Starting state: 0\n",
      "\n",
      "Step 0\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 8\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 1\n",
      "Current state: 8\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 9\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 2\n",
      "Current state: 9\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 8\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 3\n",
      "Current state: 8\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 4\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 5\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 6\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 8\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 7\n",
      "Current state: 8\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 16\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 8\n",
      "Current state: 16\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 16\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 9\n",
      "Current state: 16\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 24\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 10\n",
      "Current state: 24\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 32\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 11\n",
      "Current state: 32\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 40\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 12\n",
      "Current state: 40\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 32\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 13\n",
      "Current state: 32\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 33\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 14\n",
      "Current state: 33\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 41\n",
      "Reward: 0.0\n",
      "Terminated: True\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Episode 8\n",
      "Starting state: 0\n",
      "\n",
      "Step 0\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 1\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 1\n",
      "Current state: 1\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 1\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 2\n",
      "Current state: 1\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 9\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 3\n",
      "Current state: 9\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 1\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 4\n",
      "Current state: 1\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 1\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 5\n",
      "Current state: 1\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 9\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 6\n",
      "Current state: 9\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 10\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 7\n",
      "Current state: 10\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 18\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 8\n",
      "Current state: 18\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 17\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 9\n",
      "Current state: 17\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 18\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 10\n",
      "Current state: 18\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 26\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 11\n",
      "Current state: 26\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 25\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 12\n",
      "Current state: 25\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 17\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 13\n",
      "Current state: 17\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 18\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 14\n",
      "Current state: 18\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 17\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 15\n",
      "Current state: 17\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 16\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 16\n",
      "Current state: 16\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 24\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 17\n",
      "Current state: 24\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 32\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 18\n",
      "Current state: 32\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 32\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 19\n",
      "Current state: 32\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 24\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 20\n",
      "Current state: 24\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 32\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 21\n",
      "Current state: 32\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 24\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 22\n",
      "Current state: 24\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 24\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 23\n",
      "Current state: 24\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 16\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 24\n",
      "Current state: 16\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 24\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 25\n",
      "Current state: 24\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 24\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 26\n",
      "Current state: 24\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 16\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 27\n",
      "Current state: 16\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 17\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 28\n",
      "Current state: 17\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 9\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 29\n",
      "Current state: 9\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 10\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 30\n",
      "Current state: 10\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 18\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 31\n",
      "Current state: 18\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 10\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 32\n",
      "Current state: 10\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 9\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 33\n",
      "Current state: 9\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 10\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 34\n",
      "Current state: 10\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 2\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 35\n",
      "Current state: 2\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 1\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 36\n",
      "Current state: 1\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 1\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 37\n",
      "Current state: 1\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 2\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 38\n",
      "Current state: 2\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 2\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 39\n",
      "Current state: 2\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 1\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 40\n",
      "Current state: 1\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 9\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 41\n",
      "Current state: 9\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 17\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 42\n",
      "Current state: 17\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 16\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 43\n",
      "Current state: 16\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 17\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 44\n",
      "Current state: 17\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 18\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 45\n",
      "Current state: 18\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 10\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 46\n",
      "Current state: 10\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 11\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 47\n",
      "Current state: 11\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 19\n",
      "Reward: 0.0\n",
      "Terminated: True\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Episode 9\n",
      "Starting state: 0\n",
      "\n",
      "Step 0\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 1\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 8\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 2\n",
      "Current state: 8\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 16\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 3\n",
      "Current state: 16\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 16\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 4\n",
      "Current state: 16\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 16\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 5\n",
      "Current state: 16\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 17\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 6\n",
      "Current state: 17\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 18\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 7\n",
      "Current state: 18\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 26\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 8\n",
      "Current state: 26\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 25\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 9\n",
      "Current state: 25\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 17\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 10\n",
      "Current state: 17\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 18\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 11\n",
      "Current state: 18\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 10\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 12\n",
      "Current state: 10\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 2\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 13\n",
      "Current state: 2\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 3\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 14\n",
      "Current state: 3\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 4\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 15\n",
      "Current state: 4\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 4\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 16\n",
      "Current state: 4\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 4\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 17\n",
      "Current state: 4\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 4\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 18\n",
      "Current state: 4\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 12\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 19\n",
      "Current state: 12\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 11\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 20\n",
      "Current state: 11\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 3\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 21\n",
      "Current state: 3\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 2\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 22\n",
      "Current state: 2\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 1\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 23\n",
      "Current state: 1\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 1\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 24\n",
      "Current state: 1\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 25\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 1\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 26\n",
      "Current state: 1\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 1\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 27\n",
      "Current state: 1\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 9\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 28\n",
      "Current state: 9\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 10\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 29\n",
      "Current state: 10\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 11\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 30\n",
      "Current state: 11\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 19\n",
      "Reward: 0.0\n",
      "Terminated: True\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Episode 10\n",
      "Starting state: 0\n",
      "\n",
      "Step 0\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 8\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 1\n",
      "Current state: 8\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 2\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 8\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 3\n",
      "Current state: 8\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 4\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 8\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 5\n",
      "Current state: 8\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 16\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 6\n",
      "Current state: 16\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 17\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 7\n",
      "Current state: 17\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 25\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 8\n",
      "Current state: 25\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 24\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 9\n",
      "Current state: 24\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 25\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 10\n",
      "Current state: 25\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 17\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 11\n",
      "Current state: 17\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 18\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 12\n",
      "Current state: 18\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 17\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 13\n",
      "Current state: 17\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 18\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 14\n",
      "Current state: 18\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 19\n",
      "Reward: 0.0\n",
      "Terminated: True\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Successful episodes so far: []\n",
      "\n",
      "Episode 11\n",
      "Starting state: 0\n",
      "\n",
      "Step 0\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 1\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 1\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 2\n",
      "Current state: 1\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 3\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 8\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 4\n",
      "Current state: 8\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 16\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 5\n",
      "Current state: 16\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 24\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 6\n",
      "Current state: 24\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 25\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 7\n",
      "Current state: 25\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 17\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 8\n",
      "Current state: 17\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 16\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 9\n",
      "Current state: 16\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 17\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 10\n",
      "Current state: 17\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 18\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 11\n",
      "Current state: 18\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 26\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 12\n",
      "Current state: 26\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 18\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 13\n",
      "Current state: 18\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 17\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 14\n",
      "Current state: 17\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 16\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 15\n",
      "Current state: 16\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 16\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 16\n",
      "Current state: 16\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 8\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 17\n",
      "Current state: 8\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 8\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 18\n",
      "Current state: 8\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 8\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 19\n",
      "Current state: 8\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 16\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 20\n",
      "Current state: 16\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 8\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 21\n",
      "Current state: 8\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 22\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 1\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 23\n",
      "Current state: 1\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 1\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 24\n",
      "Current state: 1\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 1\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 25\n",
      "Current state: 1\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 26\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 27\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 28\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 29\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 30\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 8\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 31\n",
      "Current state: 8\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 32\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 8\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 33\n",
      "Current state: 8\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 34\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 35\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 1\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 36\n",
      "Current state: 1\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 37\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 8\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 38\n",
      "Current state: 8\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 8\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 39\n",
      "Current state: 8\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 9\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 40\n",
      "Current state: 9\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 1\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 41\n",
      "Current state: 1\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 42\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 8\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 43\n",
      "Current state: 8\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 9\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 44\n",
      "Current state: 9\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 10\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 45\n",
      "Current state: 10\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 18\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 46\n",
      "Current state: 18\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 19\n",
      "Reward: 0.0\n",
      "Terminated: True\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Episode 12\n",
      "Starting state: 0\n",
      "\n",
      "Step 0\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 1\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 2\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 3\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 4\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 5\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 8\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 6\n",
      "Current state: 8\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 8\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 7\n",
      "Current state: 8\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 8\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 8\n",
      "Current state: 8\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 9\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 9\n",
      "Current state: 9\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 8\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 10\n",
      "Current state: 8\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 16\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 11\n",
      "Current state: 16\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 24\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 12\n",
      "Current state: 24\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 32\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 13\n",
      "Current state: 32\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 33\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 14\n",
      "Current state: 33\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 25\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 15\n",
      "Current state: 25\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 24\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 16\n",
      "Current state: 24\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 24\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 17\n",
      "Current state: 24\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 24\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 18\n",
      "Current state: 24\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 24\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 19\n",
      "Current state: 24\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 16\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 20\n",
      "Current state: 16\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 17\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 21\n",
      "Current state: 17\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 16\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 22\n",
      "Current state: 16\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 17\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 23\n",
      "Current state: 17\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 18\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 24\n",
      "Current state: 18\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 19\n",
      "Reward: 0.0\n",
      "Terminated: True\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Episode 13\n",
      "Starting state: 0\n",
      "\n",
      "Step 0\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 1\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 1\n",
      "Current state: 1\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 1\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 2\n",
      "Current state: 1\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 9\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 3\n",
      "Current state: 9\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 8\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 4\n",
      "Current state: 8\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 5\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 1\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 6\n",
      "Current state: 1\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 7\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 8\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 8\n",
      "Current state: 8\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 9\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 9\n",
      "Current state: 9\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 10\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 10\n",
      "Current state: 10\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 11\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 11\n",
      "Current state: 11\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 19\n",
      "Reward: 0.0\n",
      "Terminated: True\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Episode 14\n",
      "Starting state: 0\n",
      "\n",
      "Step 0\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 8\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 1\n",
      "Current state: 8\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 2\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 1\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 3\n",
      "Current state: 1\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 1\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 4\n",
      "Current state: 1\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 9\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 5\n",
      "Current state: 9\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 17\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 6\n",
      "Current state: 17\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 16\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 7\n",
      "Current state: 16\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 8\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 8\n",
      "Current state: 8\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 16\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 9\n",
      "Current state: 16\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 17\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 10\n",
      "Current state: 17\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 25\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 11\n",
      "Current state: 25\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 24\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 12\n",
      "Current state: 24\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 25\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 13\n",
      "Current state: 25\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 26\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 14\n",
      "Current state: 26\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 25\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 15\n",
      "Current state: 25\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 17\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 16\n",
      "Current state: 17\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 25\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 17\n",
      "Current state: 25\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 33\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 18\n",
      "Current state: 33\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 41\n",
      "Reward: 0.0\n",
      "Terminated: True\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Episode 15\n",
      "Starting state: 0\n",
      "\n",
      "Step 0\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 1\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 1\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 2\n",
      "Current state: 1\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 2\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 3\n",
      "Current state: 2\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 10\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 4\n",
      "Current state: 10\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 9\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 5\n",
      "Current state: 9\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 10\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 6\n",
      "Current state: 10\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 9\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 7\n",
      "Current state: 9\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 17\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 8\n",
      "Current state: 17\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 9\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 9\n",
      "Current state: 9\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 1\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 10\n",
      "Current state: 1\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 11\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 8\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 12\n",
      "Current state: 8\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 16\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 13\n",
      "Current state: 16\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 8\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 14\n",
      "Current state: 8\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 8\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 15\n",
      "Current state: 8\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 9\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 16\n",
      "Current state: 9\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 1\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 17\n",
      "Current state: 1\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 1\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 18\n",
      "Current state: 1\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 19\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 1\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 20\n",
      "Current state: 1\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 9\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 21\n",
      "Current state: 9\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 1\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 22\n",
      "Current state: 1\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 9\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 23\n",
      "Current state: 9\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 1\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 24\n",
      "Current state: 1\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 1\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 25\n",
      "Current state: 1\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 1\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 26\n",
      "Current state: 1\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 27\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 8\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 28\n",
      "Current state: 8\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 29\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 8\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 30\n",
      "Current state: 8\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 9\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 31\n",
      "Current state: 9\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 10\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 32\n",
      "Current state: 10\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 9\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 33\n",
      "Current state: 9\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 1\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 34\n",
      "Current state: 1\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 9\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 35\n",
      "Current state: 9\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 10\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 36\n",
      "Current state: 10\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 2\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 37\n",
      "Current state: 2\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 3\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 38\n",
      "Current state: 3\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 3\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 39\n",
      "Current state: 3\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 11\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 40\n",
      "Current state: 11\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 19\n",
      "Reward: 0.0\n",
      "Terminated: True\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Episode 16\n",
      "Starting state: 0\n",
      "\n",
      "Step 0\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 1\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 1\n",
      "Current state: 1\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 2\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 3\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 4\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 8\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 5\n",
      "Current state: 8\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 9\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 6\n",
      "Current state: 9\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 1\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 7\n",
      "Current state: 1\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 2\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 8\n",
      "Current state: 2\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 10\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 9\n",
      "Current state: 10\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 11\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 10\n",
      "Current state: 11\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 10\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 11\n",
      "Current state: 10\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 2\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 12\n",
      "Current state: 2\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 3\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 13\n",
      "Current state: 3\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 3\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 14\n",
      "Current state: 3\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 11\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 15\n",
      "Current state: 11\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 19\n",
      "Reward: 0.0\n",
      "Terminated: True\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Episode 17\n",
      "Starting state: 0\n",
      "\n",
      "Step 0\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 8\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 1\n",
      "Current state: 8\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 2\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 1\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 3\n",
      "Current state: 1\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 4\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 5\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 8\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 6\n",
      "Current state: 8\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 16\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 7\n",
      "Current state: 16\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 16\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 8\n",
      "Current state: 16\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 16\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 9\n",
      "Current state: 16\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 17\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 10\n",
      "Current state: 17\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 18\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 11\n",
      "Current state: 18\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 26\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 12\n",
      "Current state: 26\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 25\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 13\n",
      "Current state: 25\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 33\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 14\n",
      "Current state: 33\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 41\n",
      "Reward: 0.0\n",
      "Terminated: True\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Episode 18\n",
      "Starting state: 0\n",
      "\n",
      "Step 0\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 8\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 1\n",
      "Current state: 8\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 16\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 2\n",
      "Current state: 16\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 16\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 3\n",
      "Current state: 16\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 16\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 4\n",
      "Current state: 16\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 17\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 5\n",
      "Current state: 17\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 9\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 6\n",
      "Current state: 9\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 8\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 7\n",
      "Current state: 8\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 8\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 8\n",
      "Current state: 8\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 16\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 9\n",
      "Current state: 16\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 24\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 10\n",
      "Current state: 24\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 16\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 11\n",
      "Current state: 16\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 16\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 12\n",
      "Current state: 16\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 24\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 13\n",
      "Current state: 24\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 16\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 14\n",
      "Current state: 16\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 24\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 15\n",
      "Current state: 24\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 32\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 16\n",
      "Current state: 32\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 33\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 17\n",
      "Current state: 33\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 34\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 18\n",
      "Current state: 34\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 26\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 19\n",
      "Current state: 26\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 27\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 20\n",
      "Current state: 27\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 35\n",
      "Reward: 0.0\n",
      "Terminated: True\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Episode 19\n",
      "Starting state: 0\n",
      "\n",
      "Step 0\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 8\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 1\n",
      "Current state: 8\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 8\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 2\n",
      "Current state: 8\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 3\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 8\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 4\n",
      "Current state: 8\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 16\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 5\n",
      "Current state: 16\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 17\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 6\n",
      "Current state: 17\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 9\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 7\n",
      "Current state: 9\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 8\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 8\n",
      "Current state: 8\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 9\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 9\n",
      "Current state: 9\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 8\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 10\n",
      "Current state: 8\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 11\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 1\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 12\n",
      "Current state: 1\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 13\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 14\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 15\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 8\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 16\n",
      "Current state: 8\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 8\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 17\n",
      "Current state: 8\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 18\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 8\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 19\n",
      "Current state: 8\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 20\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 21\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 1\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 22\n",
      "Current state: 1\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 2\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 23\n",
      "Current state: 2\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 3\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 24\n",
      "Current state: 3\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 2\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 25\n",
      "Current state: 2\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 10\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 26\n",
      "Current state: 10\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 2\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 27\n",
      "Current state: 2\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 2\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 28\n",
      "Current state: 2\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 3\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 29\n",
      "Current state: 3\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 4\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 30\n",
      "Current state: 4\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 3\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 31\n",
      "Current state: 3\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 3\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 32\n",
      "Current state: 3\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 2\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 33\n",
      "Current state: 2\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 2\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 34\n",
      "Current state: 2\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 10\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 35\n",
      "Current state: 10\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 2\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 36\n",
      "Current state: 2\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 2\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 37\n",
      "Current state: 2\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 10\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 38\n",
      "Current state: 10\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 11\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 39\n",
      "Current state: 11\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 3\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 40\n",
      "Current state: 3\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 4\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 41\n",
      "Current state: 4\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 12\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 42\n",
      "Current state: 12\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 13\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 43\n",
      "Current state: 13\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 21\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 44\n",
      "Current state: 21\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 20\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 45\n",
      "Current state: 20\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 12\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 46\n",
      "Current state: 12\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 4\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 47\n",
      "Current state: 4\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 3\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 48\n",
      "Current state: 3\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 2\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 49\n",
      "Current state: 2\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 2\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 50\n",
      "Current state: 2\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 3\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 51\n",
      "Current state: 3\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 11\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 52\n",
      "Current state: 11\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 12\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 53\n",
      "Current state: 12\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 20\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 54\n",
      "Current state: 20\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 28\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 55\n",
      "Current state: 28\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 29\n",
      "Reward: 0.0\n",
      "Terminated: True\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Episode 20\n",
      "Starting state: 0\n",
      "\n",
      "Step 0\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 1\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 1\n",
      "Current state: 1\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 2\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 2\n",
      "Current state: 2\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 1\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 3\n",
      "Current state: 1\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 4\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 1\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 5\n",
      "Current state: 1\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 9\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 6\n",
      "Current state: 9\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 17\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 7\n",
      "Current state: 17\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 18\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 8\n",
      "Current state: 18\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 26\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 9\n",
      "Current state: 26\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 27\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 10\n",
      "Current state: 27\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 26\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 11\n",
      "Current state: 26\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 27\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 12\n",
      "Current state: 27\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 26\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 13\n",
      "Current state: 26\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 18\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 14\n",
      "Current state: 18\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 10\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 15\n",
      "Current state: 10\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 2\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 16\n",
      "Current state: 2\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 10\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 17\n",
      "Current state: 10\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 9\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 18\n",
      "Current state: 9\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 1\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 19\n",
      "Current state: 1\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 9\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 20\n",
      "Current state: 9\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 8\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 21\n",
      "Current state: 8\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 9\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 22\n",
      "Current state: 9\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 8\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 23\n",
      "Current state: 8\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 16\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 24\n",
      "Current state: 16\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 24\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 25\n",
      "Current state: 24\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 16\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 26\n",
      "Current state: 16\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 16\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 27\n",
      "Current state: 16\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 8\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 28\n",
      "Current state: 8\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 29\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 8\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 30\n",
      "Current state: 8\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 16\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 31\n",
      "Current state: 16\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 16\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 32\n",
      "Current state: 16\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 16\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 33\n",
      "Current state: 16\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 17\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 34\n",
      "Current state: 17\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 16\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 35\n",
      "Current state: 16\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 24\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 36\n",
      "Current state: 24\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 32\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 37\n",
      "Current state: 32\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 33\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 38\n",
      "Current state: 33\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 34\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 39\n",
      "Current state: 34\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 26\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 40\n",
      "Current state: 26\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 25\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 41\n",
      "Current state: 25\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 17\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 42\n",
      "Current state: 17\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 25\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 43\n",
      "Current state: 25\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 17\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 44\n",
      "Current state: 17\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 16\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 45\n",
      "Current state: 16\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 8\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 46\n",
      "Current state: 8\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 9\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 47\n",
      "Current state: 9\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 10\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 48\n",
      "Current state: 10\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 11\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 49\n",
      "Current state: 11\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 10\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 50\n",
      "Current state: 10\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 2\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 51\n",
      "Current state: 2\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 3\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 52\n",
      "Current state: 3\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 11\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 53\n",
      "Current state: 11\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 10\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 54\n",
      "Current state: 10\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 2\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 55\n",
      "Current state: 2\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 2\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 56\n",
      "Current state: 2\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 2\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 57\n",
      "Current state: 2\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 3\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 58\n",
      "Current state: 3\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 4\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 59\n",
      "Current state: 4\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 3\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 60\n",
      "Current state: 3\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 2\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 61\n",
      "Current state: 2\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 3\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 62\n",
      "Current state: 3\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 3\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 63\n",
      "Current state: 3\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 4\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 64\n",
      "Current state: 4\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 12\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 65\n",
      "Current state: 12\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 4\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 66\n",
      "Current state: 4\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 12\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 67\n",
      "Current state: 12\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 13\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 68\n",
      "Current state: 13\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 14\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 69\n",
      "Current state: 14\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 15\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 70\n",
      "Current state: 15\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 7\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 71\n",
      "Current state: 7\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 6\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 72\n",
      "Current state: 6\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 5\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 73\n",
      "Current state: 5\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 4\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 74\n",
      "Current state: 4\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 4\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 75\n",
      "Current state: 4\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 12\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 76\n",
      "Current state: 12\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 13\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 77\n",
      "Current state: 13\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 12\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 78\n",
      "Current state: 12\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 11\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 79\n",
      "Current state: 11\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 10\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 80\n",
      "Current state: 10\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 11\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 81\n",
      "Current state: 11\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 19\n",
      "Reward: 0.0\n",
      "Terminated: True\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Successful episodes so far: []\n",
      "\n",
      "Episode 21\n",
      "Starting state: 0\n",
      "\n",
      "Step 0\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 1\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 8\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 2\n",
      "Current state: 8\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 3\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 1\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 4\n",
      "Current state: 1\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 9\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 5\n",
      "Current state: 9\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 1\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 6\n",
      "Current state: 1\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 9\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 7\n",
      "Current state: 9\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 10\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 8\n",
      "Current state: 10\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 18\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 9\n",
      "Current state: 18\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 17\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 10\n",
      "Current state: 17\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 25\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 11\n",
      "Current state: 25\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 24\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 12\n",
      "Current state: 24\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 16\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 13\n",
      "Current state: 16\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 8\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 14\n",
      "Current state: 8\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 16\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 15\n",
      "Current state: 16\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 24\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 16\n",
      "Current state: 24\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 25\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 17\n",
      "Current state: 25\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 24\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 18\n",
      "Current state: 24\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 16\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 19\n",
      "Current state: 16\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 24\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 20\n",
      "Current state: 24\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 24\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 21\n",
      "Current state: 24\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 32\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 22\n",
      "Current state: 32\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 24\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 23\n",
      "Current state: 24\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 32\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 24\n",
      "Current state: 32\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 24\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 25\n",
      "Current state: 24\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 32\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 26\n",
      "Current state: 32\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 33\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 27\n",
      "Current state: 33\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 34\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 28\n",
      "Current state: 34\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 33\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 29\n",
      "Current state: 33\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 34\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 30\n",
      "Current state: 34\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 33\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 31\n",
      "Current state: 33\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 41\n",
      "Reward: 0.0\n",
      "Terminated: True\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Episode 22\n",
      "Starting state: 0\n",
      "\n",
      "Step 0\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 8\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 1\n",
      "Current state: 8\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 16\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 2\n",
      "Current state: 16\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 8\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 3\n",
      "Current state: 8\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 4\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 1\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 5\n",
      "Current state: 1\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 9\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 6\n",
      "Current state: 9\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 10\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 7\n",
      "Current state: 10\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 11\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 8\n",
      "Current state: 11\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 3\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 9\n",
      "Current state: 3\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 2\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 10\n",
      "Current state: 2\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 1\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 11\n",
      "Current state: 1\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 1\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 12\n",
      "Current state: 1\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 13\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 1\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 14\n",
      "Current state: 1\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 2\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 15\n",
      "Current state: 2\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 10\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 16\n",
      "Current state: 10\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 2\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 17\n",
      "Current state: 2\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 3\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 18\n",
      "Current state: 3\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 4\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 19\n",
      "Current state: 4\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 5\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 20\n",
      "Current state: 5\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 5\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 21\n",
      "Current state: 5\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 6\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 22\n",
      "Current state: 6\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 5\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 23\n",
      "Current state: 5\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 13\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 24\n",
      "Current state: 13\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 14\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 25\n",
      "Current state: 14\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 15\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 26\n",
      "Current state: 15\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 14\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 27\n",
      "Current state: 14\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 22\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 28\n",
      "Current state: 22\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 21\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 29\n",
      "Current state: 21\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 20\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 30\n",
      "Current state: 20\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 28\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 31\n",
      "Current state: 28\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 20\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 32\n",
      "Current state: 20\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 21\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 33\n",
      "Current state: 21\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 13\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 34\n",
      "Current state: 13\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 12\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 35\n",
      "Current state: 12\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 13\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 36\n",
      "Current state: 13\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 14\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 37\n",
      "Current state: 14\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 22\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 38\n",
      "Current state: 22\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 30\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 39\n",
      "Current state: 30\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 31\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 40\n",
      "Current state: 31\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 31\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 41\n",
      "Current state: 31\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 39\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 42\n",
      "Current state: 39\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 47\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 43\n",
      "Current state: 47\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 46\n",
      "Reward: 0.0\n",
      "Terminated: True\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Episode 23\n",
      "Starting state: 0\n",
      "\n",
      "Step 0\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 8\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 1\n",
      "Current state: 8\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 2\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 3\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 4\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 5\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 1\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 6\n",
      "Current state: 1\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 9\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 7\n",
      "Current state: 9\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 1\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 8\n",
      "Current state: 1\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 2\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 9\n",
      "Current state: 2\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 2\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 10\n",
      "Current state: 2\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 1\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 11\n",
      "Current state: 1\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 12\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 8\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 13\n",
      "Current state: 8\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 14\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 1\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 15\n",
      "Current state: 1\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 16\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 8\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 17\n",
      "Current state: 8\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 8\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 18\n",
      "Current state: 8\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 8\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 19\n",
      "Current state: 8\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 16\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 20\n",
      "Current state: 16\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 16\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 21\n",
      "Current state: 16\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 17\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 22\n",
      "Current state: 17\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 18\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 23\n",
      "Current state: 18\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 26\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 24\n",
      "Current state: 26\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 25\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 25\n",
      "Current state: 25\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 24\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 26\n",
      "Current state: 24\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 32\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 27\n",
      "Current state: 32\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 32\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 28\n",
      "Current state: 32\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 24\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 29\n",
      "Current state: 24\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 25\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 30\n",
      "Current state: 25\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 33\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 31\n",
      "Current state: 33\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 41\n",
      "Reward: 0.0\n",
      "Terminated: True\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Episode 24\n",
      "Starting state: 0\n",
      "\n",
      "Step 0\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 1\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 2\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 1\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 3\n",
      "Current state: 1\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 9\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 4\n",
      "Current state: 9\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 10\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 5\n",
      "Current state: 10\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 2\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 6\n",
      "Current state: 2\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 2\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 7\n",
      "Current state: 2\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 3\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 8\n",
      "Current state: 3\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 11\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 9\n",
      "Current state: 11\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 10\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 10\n",
      "Current state: 10\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 18\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 11\n",
      "Current state: 18\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 17\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 12\n",
      "Current state: 17\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 16\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 13\n",
      "Current state: 16\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 24\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 14\n",
      "Current state: 24\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 32\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 15\n",
      "Current state: 32\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 33\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 16\n",
      "Current state: 33\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 41\n",
      "Reward: 0.0\n",
      "Terminated: True\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Episode 25\n",
      "Starting state: 0\n",
      "\n",
      "Step 0\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 1\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 2\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 3\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 1\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 4\n",
      "Current state: 1\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 2\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 5\n",
      "Current state: 2\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 10\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 6\n",
      "Current state: 10\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 11\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 7\n",
      "Current state: 11\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 10\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 8\n",
      "Current state: 10\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 11\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 9\n",
      "Current state: 11\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 12\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 10\n",
      "Current state: 12\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 13\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 11\n",
      "Current state: 13\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 12\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 12\n",
      "Current state: 12\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 4\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 13\n",
      "Current state: 4\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 4\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 14\n",
      "Current state: 4\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 4\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 15\n",
      "Current state: 4\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 3\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 16\n",
      "Current state: 3\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 4\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 17\n",
      "Current state: 4\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 5\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 18\n",
      "Current state: 5\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 4\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 19\n",
      "Current state: 4\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 3\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 20\n",
      "Current state: 3\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 11\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 21\n",
      "Current state: 11\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 12\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 22\n",
      "Current state: 12\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 4\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 23\n",
      "Current state: 4\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 4\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 24\n",
      "Current state: 4\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 4\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 25\n",
      "Current state: 4\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 5\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 26\n",
      "Current state: 5\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 6\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 27\n",
      "Current state: 6\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 5\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 28\n",
      "Current state: 5\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 13\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 29\n",
      "Current state: 13\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 5\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 30\n",
      "Current state: 5\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 4\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 31\n",
      "Current state: 4\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 5\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 32\n",
      "Current state: 5\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 6\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 33\n",
      "Current state: 6\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 5\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 34\n",
      "Current state: 5\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 6\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 35\n",
      "Current state: 6\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 7\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 36\n",
      "Current state: 7\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 15\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 37\n",
      "Current state: 15\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 7\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 38\n",
      "Current state: 7\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 7\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 39\n",
      "Current state: 7\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 7\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 40\n",
      "Current state: 7\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 15\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 41\n",
      "Current state: 15\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 14\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 42\n",
      "Current state: 14\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 6\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 43\n",
      "Current state: 6\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 7\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 44\n",
      "Current state: 7\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 7\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 45\n",
      "Current state: 7\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 15\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 46\n",
      "Current state: 15\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 14\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 47\n",
      "Current state: 14\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 13\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 48\n",
      "Current state: 13\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 5\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 49\n",
      "Current state: 5\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 5\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 50\n",
      "Current state: 5\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 4\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 51\n",
      "Current state: 4\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 12\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 52\n",
      "Current state: 12\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 13\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 53\n",
      "Current state: 13\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 12\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 54\n",
      "Current state: 12\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 20\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 55\n",
      "Current state: 20\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 28\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 56\n",
      "Current state: 28\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 27\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 57\n",
      "Current state: 27\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 28\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 58\n",
      "Current state: 28\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 36\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 59\n",
      "Current state: 36\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 28\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 60\n",
      "Current state: 28\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 36\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 61\n",
      "Current state: 36\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 28\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 62\n",
      "Current state: 28\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 20\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 63\n",
      "Current state: 20\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 19\n",
      "Reward: 0.0\n",
      "Terminated: True\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Episode 26\n",
      "Starting state: 0\n",
      "\n",
      "Step 0\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 8\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 1\n",
      "Current state: 8\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 2\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 1\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 3\n",
      "Current state: 1\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 1\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 4\n",
      "Current state: 1\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 2\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 5\n",
      "Current state: 2\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 2\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 6\n",
      "Current state: 2\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 10\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 7\n",
      "Current state: 10\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 9\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 8\n",
      "Current state: 9\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 8\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 9\n",
      "Current state: 8\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 8\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 10\n",
      "Current state: 8\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 16\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 11\n",
      "Current state: 16\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 24\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 12\n",
      "Current state: 24\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 25\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 13\n",
      "Current state: 25\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 24\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 14\n",
      "Current state: 24\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 24\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 15\n",
      "Current state: 24\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 16\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 16\n",
      "Current state: 16\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 8\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 17\n",
      "Current state: 8\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 16\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 18\n",
      "Current state: 16\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 17\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 19\n",
      "Current state: 17\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 9\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 20\n",
      "Current state: 9\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 10\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 21\n",
      "Current state: 10\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 11\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 22\n",
      "Current state: 11\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 10\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 23\n",
      "Current state: 10\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 11\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 24\n",
      "Current state: 11\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 19\n",
      "Reward: 0.0\n",
      "Terminated: True\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Episode 27\n",
      "Starting state: 0\n",
      "\n",
      "Step 0\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 8\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 1\n",
      "Current state: 8\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 9\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 2\n",
      "Current state: 9\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 10\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 3\n",
      "Current state: 10\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 11\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 4\n",
      "Current state: 11\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 19\n",
      "Reward: 0.0\n",
      "Terminated: True\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Episode 28\n",
      "Starting state: 0\n",
      "\n",
      "Step 0\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 8\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 1\n",
      "Current state: 8\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 2\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 1\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 3\n",
      "Current state: 1\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 9\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 4\n",
      "Current state: 9\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 1\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 5\n",
      "Current state: 1\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 1\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 6\n",
      "Current state: 1\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 1\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 7\n",
      "Current state: 1\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 8\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 9\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 10\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 11\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 8\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 12\n",
      "Current state: 8\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 16\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 13\n",
      "Current state: 16\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 17\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 14\n",
      "Current state: 17\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 18\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 15\n",
      "Current state: 18\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 10\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 16\n",
      "Current state: 10\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 2\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 17\n",
      "Current state: 2\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 1\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 18\n",
      "Current state: 1\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 1\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 19\n",
      "Current state: 1\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 20\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 21\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 22\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 8\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 23\n",
      "Current state: 8\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 16\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 24\n",
      "Current state: 16\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 24\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 25\n",
      "Current state: 24\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 24\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 26\n",
      "Current state: 24\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 32\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 27\n",
      "Current state: 32\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 24\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 28\n",
      "Current state: 24\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 25\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 29\n",
      "Current state: 25\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 24\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 30\n",
      "Current state: 24\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 32\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 31\n",
      "Current state: 32\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 24\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 32\n",
      "Current state: 24\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 32\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 33\n",
      "Current state: 32\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 32\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 34\n",
      "Current state: 32\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 24\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 35\n",
      "Current state: 24\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 24\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 36\n",
      "Current state: 24\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 25\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 37\n",
      "Current state: 25\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 17\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 38\n",
      "Current state: 17\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 16\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 39\n",
      "Current state: 16\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 16\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 40\n",
      "Current state: 16\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 16\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 41\n",
      "Current state: 16\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 16\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 42\n",
      "Current state: 16\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 24\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 43\n",
      "Current state: 24\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 24\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 44\n",
      "Current state: 24\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 25\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 45\n",
      "Current state: 25\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 26\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 46\n",
      "Current state: 26\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 25\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 47\n",
      "Current state: 25\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 33\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 48\n",
      "Current state: 33\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 25\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 49\n",
      "Current state: 25\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 24\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 50\n",
      "Current state: 24\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 24\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 51\n",
      "Current state: 24\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 25\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 52\n",
      "Current state: 25\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 26\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 53\n",
      "Current state: 26\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 18\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 54\n",
      "Current state: 18\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 19\n",
      "Reward: 0.0\n",
      "Terminated: True\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Episode 29\n",
      "Starting state: 0\n",
      "\n",
      "Step 0\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 1\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 2\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 3\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 8\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 4\n",
      "Current state: 8\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 5\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 8\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 6\n",
      "Current state: 8\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 8\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 7\n",
      "Current state: 8\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 8\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 9\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 10\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 11\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 12\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 1\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 13\n",
      "Current state: 1\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 2\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 14\n",
      "Current state: 2\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 1\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 15\n",
      "Current state: 1\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 1\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 16\n",
      "Current state: 1\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 9\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 17\n",
      "Current state: 9\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 1\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 18\n",
      "Current state: 1\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 9\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 19\n",
      "Current state: 9\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 10\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 20\n",
      "Current state: 10\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 11\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 21\n",
      "Current state: 11\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 19\n",
      "Reward: 0.0\n",
      "Terminated: True\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Episode 30\n",
      "Starting state: 0\n",
      "\n",
      "Step 0\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 1\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 1\n",
      "Current state: 1\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 9\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 2\n",
      "Current state: 9\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 17\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 3\n",
      "Current state: 17\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 9\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 4\n",
      "Current state: 9\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 17\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 5\n",
      "Current state: 17\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 9\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 6\n",
      "Current state: 9\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 10\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 7\n",
      "Current state: 10\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 11\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 8\n",
      "Current state: 11\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 19\n",
      "Reward: 0.0\n",
      "Terminated: True\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Successful episodes so far: []\n",
      "\n",
      "Episode 31\n",
      "Starting state: 0\n",
      "\n",
      "Step 0\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 1\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 2\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 8\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 3\n",
      "Current state: 8\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 4\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 1\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 5\n",
      "Current state: 1\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 9\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 6\n",
      "Current state: 9\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 10\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 7\n",
      "Current state: 10\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 9\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 8\n",
      "Current state: 9\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 17\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 9\n",
      "Current state: 17\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 18\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 10\n",
      "Current state: 18\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 10\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 11\n",
      "Current state: 10\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 11\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 12\n",
      "Current state: 11\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 12\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 13\n",
      "Current state: 12\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 11\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 14\n",
      "Current state: 11\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 3\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 15\n",
      "Current state: 3\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 3\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 16\n",
      "Current state: 3\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 11\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 17\n",
      "Current state: 11\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 3\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 18\n",
      "Current state: 3\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 11\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 19\n",
      "Current state: 11\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 10\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 20\n",
      "Current state: 10\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 2\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 21\n",
      "Current state: 2\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 3\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 22\n",
      "Current state: 3\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 11\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 23\n",
      "Current state: 11\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 10\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 24\n",
      "Current state: 10\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 11\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 25\n",
      "Current state: 11\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 3\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 26\n",
      "Current state: 3\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 3\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 27\n",
      "Current state: 3\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 2\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 28\n",
      "Current state: 2\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 10\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 29\n",
      "Current state: 10\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 2\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 30\n",
      "Current state: 2\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 3\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 31\n",
      "Current state: 3\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 3\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 32\n",
      "Current state: 3\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 3\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 33\n",
      "Current state: 3\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 11\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 34\n",
      "Current state: 11\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 12\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 35\n",
      "Current state: 12\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 13\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 36\n",
      "Current state: 13\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 12\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 37\n",
      "Current state: 12\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 11\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 38\n",
      "Current state: 11\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 19\n",
      "Reward: 0.0\n",
      "Terminated: True\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Episode 32\n",
      "Starting state: 0\n",
      "\n",
      "Step 0\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 8\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 1\n",
      "Current state: 8\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 8\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 2\n",
      "Current state: 8\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 8\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 3\n",
      "Current state: 8\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 4\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 5\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 6\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 7\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 8\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 1\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 9\n",
      "Current state: 1\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 1\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 10\n",
      "Current state: 1\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 11\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 12\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 1\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 13\n",
      "Current state: 1\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 14\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 8\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 15\n",
      "Current state: 8\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 16\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 17\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 18\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 19\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 20\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 21\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 22\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 8\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 23\n",
      "Current state: 8\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 24\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 8\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 25\n",
      "Current state: 8\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 8\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 26\n",
      "Current state: 8\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 9\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 27\n",
      "Current state: 9\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 8\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 28\n",
      "Current state: 8\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 8\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 29\n",
      "Current state: 8\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 30\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 31\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 8\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 32\n",
      "Current state: 8\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 33\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 8\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 34\n",
      "Current state: 8\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 9\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 35\n",
      "Current state: 9\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 1\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 36\n",
      "Current state: 1\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 9\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 37\n",
      "Current state: 9\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 17\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 38\n",
      "Current state: 17\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 9\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 39\n",
      "Current state: 9\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 10\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 40\n",
      "Current state: 10\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 9\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 41\n",
      "Current state: 9\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 1\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 42\n",
      "Current state: 1\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 1\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 43\n",
      "Current state: 1\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 1\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 44\n",
      "Current state: 1\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 45\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 46\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 1\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 47\n",
      "Current state: 1\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 1\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 48\n",
      "Current state: 1\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 2\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 49\n",
      "Current state: 2\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 3\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 50\n",
      "Current state: 3\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 11\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 51\n",
      "Current state: 11\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 19\n",
      "Reward: 0.0\n",
      "Terminated: True\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Episode 33\n",
      "Starting state: 0\n",
      "\n",
      "Step 0\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 1\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 1\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 2\n",
      "Current state: 1\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 1\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 3\n",
      "Current state: 1\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 2\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 4\n",
      "Current state: 2\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 3\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 5\n",
      "Current state: 3\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 4\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 6\n",
      "Current state: 4\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 12\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 7\n",
      "Current state: 12\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 20\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 8\n",
      "Current state: 20\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 19\n",
      "Reward: 0.0\n",
      "Terminated: True\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Episode 34\n",
      "Starting state: 0\n",
      "\n",
      "Step 0\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 1\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 2\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 1\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 3\n",
      "Current state: 1\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 2\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 4\n",
      "Current state: 2\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 2\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 5\n",
      "Current state: 2\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 10\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 6\n",
      "Current state: 10\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 11\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 7\n",
      "Current state: 11\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 10\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 8\n",
      "Current state: 10\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 2\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 9\n",
      "Current state: 2\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 10\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 10\n",
      "Current state: 10\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 9\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 11\n",
      "Current state: 9\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 8\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 12\n",
      "Current state: 8\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 16\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 13\n",
      "Current state: 16\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 24\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 14\n",
      "Current state: 24\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 25\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 15\n",
      "Current state: 25\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 26\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 16\n",
      "Current state: 26\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 18\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 17\n",
      "Current state: 18\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 26\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 18\n",
      "Current state: 26\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 25\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 19\n",
      "Current state: 25\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 24\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 20\n",
      "Current state: 24\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 24\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 21\n",
      "Current state: 24\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 25\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 22\n",
      "Current state: 25\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 17\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 23\n",
      "Current state: 17\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 18\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 24\n",
      "Current state: 18\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 10\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 25\n",
      "Current state: 10\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 2\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 26\n",
      "Current state: 2\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 1\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 27\n",
      "Current state: 1\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 1\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 28\n",
      "Current state: 1\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 29\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 8\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 30\n",
      "Current state: 8\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 8\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 31\n",
      "Current state: 8\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 9\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 32\n",
      "Current state: 9\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 10\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 33\n",
      "Current state: 10\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 2\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 34\n",
      "Current state: 2\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 3\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 35\n",
      "Current state: 3\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 11\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 36\n",
      "Current state: 11\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 19\n",
      "Reward: 0.0\n",
      "Terminated: True\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Episode 35\n",
      "Starting state: 0\n",
      "\n",
      "Step 0\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 1\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 8\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 2\n",
      "Current state: 8\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 9\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 3\n",
      "Current state: 9\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 8\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 4\n",
      "Current state: 8\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 8\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 5\n",
      "Current state: 8\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 6\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 1\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 7\n",
      "Current state: 1\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 9\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 8\n",
      "Current state: 9\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 10\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 9\n",
      "Current state: 10\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 9\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 10\n",
      "Current state: 9\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 8\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 11\n",
      "Current state: 8\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 8\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 12\n",
      "Current state: 8\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 8\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 13\n",
      "Current state: 8\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 14\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 15\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 8\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 16\n",
      "Current state: 8\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 9\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 17\n",
      "Current state: 9\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 1\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 18\n",
      "Current state: 1\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 19\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 20\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 8\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 21\n",
      "Current state: 8\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 9\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 22\n",
      "Current state: 9\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 1\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 23\n",
      "Current state: 1\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 1\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 24\n",
      "Current state: 1\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 1\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 25\n",
      "Current state: 1\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 26\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 27\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 8\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 28\n",
      "Current state: 8\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 9\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 29\n",
      "Current state: 9\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 1\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 30\n",
      "Current state: 1\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 31\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 8\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 32\n",
      "Current state: 8\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 33\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 34\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 35\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 8\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 36\n",
      "Current state: 8\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 16\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 37\n",
      "Current state: 16\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 8\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 38\n",
      "Current state: 8\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 39\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 8\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 40\n",
      "Current state: 8\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 41\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 42\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 8\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 43\n",
      "Current state: 8\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 44\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 8\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 45\n",
      "Current state: 8\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 8\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 46\n",
      "Current state: 8\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 47\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 8\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 48\n",
      "Current state: 8\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 16\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 49\n",
      "Current state: 16\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 17\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 50\n",
      "Current state: 17\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 18\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 51\n",
      "Current state: 18\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 26\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 52\n",
      "Current state: 26\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 27\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 53\n",
      "Current state: 27\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 19\n",
      "Reward: 0.0\n",
      "Terminated: True\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Episode 36\n",
      "Starting state: 0\n",
      "\n",
      "Step 0\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 8\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 1\n",
      "Current state: 8\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 2\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 3\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 1\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 4\n",
      "Current state: 1\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 9\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 5\n",
      "Current state: 9\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 8\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 6\n",
      "Current state: 8\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 8\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 7\n",
      "Current state: 8\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 8\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 8\n",
      "Current state: 8\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 9\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 9\n",
      "Current state: 9\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 10\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 10\n",
      "Current state: 10\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 11\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 11\n",
      "Current state: 11\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 10\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 12\n",
      "Current state: 10\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 9\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 13\n",
      "Current state: 9\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 10\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 14\n",
      "Current state: 10\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 2\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 15\n",
      "Current state: 2\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 2\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 16\n",
      "Current state: 2\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 3\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 17\n",
      "Current state: 3\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 3\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 18\n",
      "Current state: 3\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 11\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 19\n",
      "Current state: 11\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 19\n",
      "Reward: 0.0\n",
      "Terminated: True\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Episode 37\n",
      "Starting state: 0\n",
      "\n",
      "Step 0\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 1\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 1\n",
      "Current state: 1\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 2\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 8\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 3\n",
      "Current state: 8\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 9\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 4\n",
      "Current state: 9\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 17\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 5\n",
      "Current state: 17\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 18\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 6\n",
      "Current state: 18\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 19\n",
      "Reward: 0.0\n",
      "Terminated: True\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Episode 38\n",
      "Starting state: 0\n",
      "\n",
      "Step 0\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 1\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 2\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 8\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 3\n",
      "Current state: 8\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 9\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 4\n",
      "Current state: 9\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 1\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 5\n",
      "Current state: 1\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 6\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 8\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 7\n",
      "Current state: 8\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 8\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 8\n",
      "Current state: 8\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 9\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 9\n",
      "Current state: 9\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 17\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 10\n",
      "Current state: 17\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 18\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 11\n",
      "Current state: 18\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 19\n",
      "Reward: 0.0\n",
      "Terminated: True\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Episode 39\n",
      "Starting state: 0\n",
      "\n",
      "Step 0\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 1\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 1\n",
      "Current state: 1\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 1\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 2\n",
      "Current state: 1\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 1\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 3\n",
      "Current state: 1\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 1\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 4\n",
      "Current state: 1\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 5\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 6\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 1\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 7\n",
      "Current state: 1\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 2\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 8\n",
      "Current state: 2\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 2\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 9\n",
      "Current state: 2\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 2\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 10\n",
      "Current state: 2\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 10\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 11\n",
      "Current state: 10\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 2\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 12\n",
      "Current state: 2\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 10\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 13\n",
      "Current state: 10\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 11\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 14\n",
      "Current state: 11\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 12\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 15\n",
      "Current state: 12\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 20\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 16\n",
      "Current state: 20\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 21\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 17\n",
      "Current state: 21\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 22\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 18\n",
      "Current state: 22\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 23\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 19\n",
      "Current state: 23\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 23\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 20\n",
      "Current state: 23\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 15\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 21\n",
      "Current state: 15\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 14\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 22\n",
      "Current state: 14\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 22\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 23\n",
      "Current state: 22\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 30\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 24\n",
      "Current state: 30\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 31\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 25\n",
      "Current state: 31\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 23\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 26\n",
      "Current state: 23\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 31\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 27\n",
      "Current state: 31\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 39\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 28\n",
      "Current state: 39\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 39\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 29\n",
      "Current state: 39\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 39\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 30\n",
      "Current state: 39\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 47\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 31\n",
      "Current state: 47\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 55\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 32\n",
      "Current state: 55\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 55\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 33\n",
      "Current state: 55\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 55\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 34\n",
      "Current state: 55\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 54\n",
      "Reward: 0.0\n",
      "Terminated: True\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Episode 40\n",
      "Starting state: 0\n",
      "\n",
      "Step 0\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 1\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 8\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 2\n",
      "Current state: 8\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 9\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 3\n",
      "Current state: 9\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 10\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 4\n",
      "Current state: 10\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 11\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 5\n",
      "Current state: 11\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 19\n",
      "Reward: 0.0\n",
      "Terminated: True\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Successful episodes so far: []\n",
      "\n",
      "Episode 41\n",
      "Starting state: 0\n",
      "\n",
      "Step 0\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 1\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 1\n",
      "Current state: 1\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 2\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 2\n",
      "Current state: 2\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 10\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 3\n",
      "Current state: 10\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 9\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 4\n",
      "Current state: 9\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 10\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 5\n",
      "Current state: 10\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 9\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 6\n",
      "Current state: 9\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 17\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 7\n",
      "Current state: 17\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 25\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 8\n",
      "Current state: 25\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 33\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 9\n",
      "Current state: 33\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 25\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 10\n",
      "Current state: 25\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 24\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 11\n",
      "Current state: 24\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 24\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 12\n",
      "Current state: 24\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 25\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 13\n",
      "Current state: 25\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 26\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 14\n",
      "Current state: 26\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 27\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 15\n",
      "Current state: 27\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 19\n",
      "Reward: 0.0\n",
      "Terminated: True\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Episode 42\n",
      "Starting state: 0\n",
      "\n",
      "Step 0\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 1\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 2\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 8\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 3\n",
      "Current state: 8\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 4\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 1\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 5\n",
      "Current state: 1\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 9\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 6\n",
      "Current state: 9\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 17\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 7\n",
      "Current state: 17\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 25\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 8\n",
      "Current state: 25\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 33\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 9\n",
      "Current state: 33\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 34\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 10\n",
      "Current state: 34\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 26\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 11\n",
      "Current state: 26\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 34\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 12\n",
      "Current state: 34\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 35\n",
      "Reward: 0.0\n",
      "Terminated: True\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Episode 43\n",
      "Starting state: 0\n",
      "\n",
      "Step 0\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 1\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 1\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 2\n",
      "Current state: 1\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 2\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 3\n",
      "Current state: 2\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 1\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 4\n",
      "Current state: 1\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 1\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 5\n",
      "Current state: 1\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 6\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 7\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 8\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 9\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 1\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 10\n",
      "Current state: 1\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 11\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 12\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 8\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 13\n",
      "Current state: 8\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 16\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 14\n",
      "Current state: 16\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 17\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 15\n",
      "Current state: 17\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 18\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 16\n",
      "Current state: 18\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 10\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 17\n",
      "Current state: 10\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 9\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 18\n",
      "Current state: 9\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 1\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 19\n",
      "Current state: 1\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 1\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 20\n",
      "Current state: 1\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 9\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 21\n",
      "Current state: 9\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 1\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 22\n",
      "Current state: 1\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 2\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 23\n",
      "Current state: 2\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 1\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 24\n",
      "Current state: 1\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 1\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 25\n",
      "Current state: 1\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 26\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 27\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 1\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 28\n",
      "Current state: 1\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 1\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 29\n",
      "Current state: 1\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 2\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 30\n",
      "Current state: 2\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 2\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 31\n",
      "Current state: 2\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 1\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 32\n",
      "Current state: 1\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 2\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 33\n",
      "Current state: 2\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 1\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 34\n",
      "Current state: 1\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 35\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 8\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 36\n",
      "Current state: 8\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 37\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 1\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 38\n",
      "Current state: 1\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 9\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 39\n",
      "Current state: 9\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 8\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 40\n",
      "Current state: 8\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 41\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 42\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 8\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 43\n",
      "Current state: 8\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 8\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 44\n",
      "Current state: 8\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 8\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 45\n",
      "Current state: 8\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 16\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 46\n",
      "Current state: 16\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 16\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 47\n",
      "Current state: 16\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 8\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 48\n",
      "Current state: 8\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 8\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 49\n",
      "Current state: 8\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 16\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 50\n",
      "Current state: 16\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 24\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 51\n",
      "Current state: 24\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 25\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 52\n",
      "Current state: 25\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 24\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 53\n",
      "Current state: 24\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 24\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 54\n",
      "Current state: 24\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 32\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 55\n",
      "Current state: 32\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 33\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 56\n",
      "Current state: 33\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 32\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 57\n",
      "Current state: 32\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 32\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 58\n",
      "Current state: 32\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 40\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 59\n",
      "Current state: 40\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 32\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 60\n",
      "Current state: 32\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 24\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 61\n",
      "Current state: 24\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 24\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 62\n",
      "Current state: 24\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 24\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 63\n",
      "Current state: 24\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 25\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 64\n",
      "Current state: 25\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 26\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 65\n",
      "Current state: 26\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 34\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 66\n",
      "Current state: 34\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 42\n",
      "Reward: 0.0\n",
      "Terminated: True\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Episode 44\n",
      "Starting state: 0\n",
      "\n",
      "Step 0\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 1\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 8\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 2\n",
      "Current state: 8\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 3\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 8\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 4\n",
      "Current state: 8\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 5\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 8\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 6\n",
      "Current state: 8\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 8\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 7\n",
      "Current state: 8\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 8\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 8\n",
      "Current state: 8\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 9\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 9\n",
      "Current state: 9\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 1\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 10\n",
      "Current state: 1\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 11\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 8\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 12\n",
      "Current state: 8\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 9\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 13\n",
      "Current state: 9\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 8\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 14\n",
      "Current state: 8\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 16\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 15\n",
      "Current state: 16\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 24\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 16\n",
      "Current state: 24\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 16\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 17\n",
      "Current state: 16\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 8\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 18\n",
      "Current state: 8\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 8\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 19\n",
      "Current state: 8\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 8\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 20\n",
      "Current state: 8\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 21\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 8\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 22\n",
      "Current state: 8\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 8\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 23\n",
      "Current state: 8\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 16\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 24\n",
      "Current state: 16\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 16\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 25\n",
      "Current state: 16\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 8\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 26\n",
      "Current state: 8\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 8\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 27\n",
      "Current state: 8\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 28\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 29\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 30\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 1\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 31\n",
      "Current state: 1\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 2\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 32\n",
      "Current state: 2\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 3\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 33\n",
      "Current state: 3\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 2\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 34\n",
      "Current state: 2\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 2\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 35\n",
      "Current state: 2\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 2\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 36\n",
      "Current state: 2\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 10\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 37\n",
      "Current state: 10\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 9\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 38\n",
      "Current state: 9\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 17\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 39\n",
      "Current state: 17\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 25\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 40\n",
      "Current state: 25\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 17\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 41\n",
      "Current state: 17\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 18\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 42\n",
      "Current state: 18\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 19\n",
      "Reward: 0.0\n",
      "Terminated: True\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Episode 45\n",
      "Starting state: 0\n",
      "\n",
      "Step 0\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 1\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 1\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 2\n",
      "Current state: 1\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 9\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 3\n",
      "Current state: 9\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 17\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 4\n",
      "Current state: 17\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 16\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 5\n",
      "Current state: 16\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 16\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 6\n",
      "Current state: 16\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 17\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 7\n",
      "Current state: 17\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 16\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 8\n",
      "Current state: 16\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 17\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 9\n",
      "Current state: 17\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 16\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 10\n",
      "Current state: 16\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 8\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 11\n",
      "Current state: 8\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 9\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 12\n",
      "Current state: 9\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 1\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 13\n",
      "Current state: 1\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 1\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 14\n",
      "Current state: 1\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 15\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 16\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 17\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 8\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 18\n",
      "Current state: 8\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 9\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 19\n",
      "Current state: 9\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 1\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 20\n",
      "Current state: 1\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 1\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 21\n",
      "Current state: 1\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 22\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 23\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 1\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 24\n",
      "Current state: 1\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 9\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 25\n",
      "Current state: 9\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 10\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 26\n",
      "Current state: 10\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 9\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 27\n",
      "Current state: 9\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 8\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 28\n",
      "Current state: 8\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 29\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 30\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 31\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 1\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 32\n",
      "Current state: 1\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 2\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 33\n",
      "Current state: 2\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 1\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 34\n",
      "Current state: 1\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 2\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 35\n",
      "Current state: 2\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 10\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 36\n",
      "Current state: 10\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 9\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 37\n",
      "Current state: 9\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 8\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 38\n",
      "Current state: 8\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 9\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 39\n",
      "Current state: 9\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 8\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 40\n",
      "Current state: 8\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 9\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 41\n",
      "Current state: 9\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 17\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 42\n",
      "Current state: 17\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 9\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 43\n",
      "Current state: 9\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 1\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 44\n",
      "Current state: 1\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 9\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 45\n",
      "Current state: 9\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 10\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 46\n",
      "Current state: 10\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 11\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 47\n",
      "Current state: 11\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 10\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 48\n",
      "Current state: 10\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 11\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 49\n",
      "Current state: 11\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 10\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 50\n",
      "Current state: 10\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 2\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 51\n",
      "Current state: 2\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 2\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 52\n",
      "Current state: 2\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 10\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 53\n",
      "Current state: 10\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 9\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 54\n",
      "Current state: 9\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 1\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 55\n",
      "Current state: 1\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 2\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 56\n",
      "Current state: 2\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 3\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 57\n",
      "Current state: 3\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 4\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 58\n",
      "Current state: 4\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 5\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 59\n",
      "Current state: 5\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 6\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 60\n",
      "Current state: 6\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 14\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 61\n",
      "Current state: 14\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 6\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 62\n",
      "Current state: 6\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 6\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 63\n",
      "Current state: 6\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 14\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 64\n",
      "Current state: 14\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 6\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 65\n",
      "Current state: 6\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 7\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 66\n",
      "Current state: 7\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 15\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 67\n",
      "Current state: 15\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 15\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 68\n",
      "Current state: 15\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 15\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 69\n",
      "Current state: 15\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 23\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 70\n",
      "Current state: 23\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 15\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 71\n",
      "Current state: 15\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 15\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 72\n",
      "Current state: 15\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 7\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 73\n",
      "Current state: 7\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 15\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 74\n",
      "Current state: 15\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 7\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 75\n",
      "Current state: 7\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 7\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 76\n",
      "Current state: 7\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 15\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 77\n",
      "Current state: 15\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 14\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 78\n",
      "Current state: 14\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 15\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 79\n",
      "Current state: 15\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 7\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 80\n",
      "Current state: 7\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 7\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 81\n",
      "Current state: 7\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 7\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 82\n",
      "Current state: 7\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 6\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 83\n",
      "Current state: 6\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 7\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 84\n",
      "Current state: 7\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 15\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 85\n",
      "Current state: 15\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 23\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 86\n",
      "Current state: 23\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 31\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 87\n",
      "Current state: 31\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 39\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 88\n",
      "Current state: 39\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 38\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 89\n",
      "Current state: 38\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 39\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 90\n",
      "Current state: 39\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 38\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 91\n",
      "Current state: 38\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 39\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 92\n",
      "Current state: 39\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 39\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 93\n",
      "Current state: 39\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 31\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 94\n",
      "Current state: 31\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 23\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 95\n",
      "Current state: 23\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 22\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 96\n",
      "Current state: 22\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 23\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 97\n",
      "Current state: 23\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 22\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 98\n",
      "Current state: 22\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 30\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 99\n",
      "Current state: 30\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 29\n",
      "Reward: 0.0\n",
      "Terminated: True\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Episode 46\n",
      "Starting state: 0\n",
      "\n",
      "Step 0\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 1\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 1\n",
      "Current state: 1\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 9\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 2\n",
      "Current state: 9\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 17\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 3\n",
      "Current state: 17\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 25\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 4\n",
      "Current state: 25\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 17\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 5\n",
      "Current state: 17\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 9\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 6\n",
      "Current state: 9\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 10\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 7\n",
      "Current state: 10\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 2\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 8\n",
      "Current state: 2\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 3\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 9\n",
      "Current state: 3\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 4\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 10\n",
      "Current state: 4\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 12\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 11\n",
      "Current state: 12\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 11\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 12\n",
      "Current state: 11\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 12\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 13\n",
      "Current state: 12\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 20\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 14\n",
      "Current state: 20\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 19\n",
      "Reward: 0.0\n",
      "Terminated: True\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Episode 47\n",
      "Starting state: 0\n",
      "\n",
      "Step 0\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 8\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 1\n",
      "Current state: 8\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 2\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 3\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 1\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 4\n",
      "Current state: 1\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 5\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 8\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 6\n",
      "Current state: 8\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 7\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 8\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 1\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 9\n",
      "Current state: 1\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 10\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 11\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 8\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 12\n",
      "Current state: 8\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 16\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 13\n",
      "Current state: 16\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 24\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 14\n",
      "Current state: 24\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 25\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 15\n",
      "Current state: 25\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 33\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 16\n",
      "Current state: 33\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 41\n",
      "Reward: 0.0\n",
      "Terminated: True\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Episode 48\n",
      "Starting state: 0\n",
      "\n",
      "Step 0\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 1\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 2\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 1\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 3\n",
      "Current state: 1\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 2\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 4\n",
      "Current state: 2\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 1\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 5\n",
      "Current state: 1\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 9\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 6\n",
      "Current state: 9\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 8\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 7\n",
      "Current state: 8\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 8\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 8\n",
      "Current state: 8\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 8\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 9\n",
      "Current state: 8\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 16\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 10\n",
      "Current state: 16\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 24\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 11\n",
      "Current state: 24\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 24\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 12\n",
      "Current state: 24\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 24\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 13\n",
      "Current state: 24\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 25\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 14\n",
      "Current state: 25\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 24\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 15\n",
      "Current state: 24\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 25\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 16\n",
      "Current state: 25\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 26\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 17\n",
      "Current state: 26\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 25\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 18\n",
      "Current state: 25\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 24\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 19\n",
      "Current state: 24\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 32\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 20\n",
      "Current state: 32\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 40\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 21\n",
      "Current state: 40\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 48\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 22\n",
      "Current state: 48\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 48\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 23\n",
      "Current state: 48\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 49\n",
      "Reward: 0.0\n",
      "Terminated: True\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Episode 49\n",
      "Starting state: 0\n",
      "\n",
      "Step 0\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 8\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 1\n",
      "Current state: 8\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 16\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 2\n",
      "Current state: 16\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 16\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 3\n",
      "Current state: 16\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 16\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 4\n",
      "Current state: 16\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 8\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 5\n",
      "Current state: 8\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 8\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 6\n",
      "Current state: 8\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 16\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 7\n",
      "Current state: 16\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 16\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 8\n",
      "Current state: 16\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 17\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 9\n",
      "Current state: 17\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 9\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 10\n",
      "Current state: 9\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 8\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 11\n",
      "Current state: 8\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 12\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 13\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 8\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 14\n",
      "Current state: 8\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 16\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 15\n",
      "Current state: 16\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 24\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 16\n",
      "Current state: 24\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 24\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 17\n",
      "Current state: 24\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 16\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 18\n",
      "Current state: 16\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 16\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 19\n",
      "Current state: 16\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 24\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 20\n",
      "Current state: 24\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 32\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 21\n",
      "Current state: 32\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 33\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 22\n",
      "Current state: 33\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 32\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 23\n",
      "Current state: 32\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 24\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 24\n",
      "Current state: 24\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 24\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 25\n",
      "Current state: 24\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 32\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 26\n",
      "Current state: 32\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 33\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 27\n",
      "Current state: 33\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 34\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 28\n",
      "Current state: 34\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 26\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 29\n",
      "Current state: 26\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 25\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 30\n",
      "Current state: 25\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 26\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 31\n",
      "Current state: 26\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 27\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 32\n",
      "Current state: 27\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 26\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 33\n",
      "Current state: 26\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 34\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 34\n",
      "Current state: 34\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 35\n",
      "Reward: 0.0\n",
      "Terminated: True\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Episode 50\n",
      "Starting state: 0\n",
      "\n",
      "Step 0\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 8\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 1\n",
      "Current state: 8\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 8\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 2\n",
      "Current state: 8\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 9\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 3\n",
      "Current state: 9\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 17\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 4\n",
      "Current state: 17\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 18\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 5\n",
      "Current state: 18\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 19\n",
      "Reward: 0.0\n",
      "Terminated: True\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Successful episodes so far: []\n",
      "\n",
      "Episode 51\n",
      "Starting state: 0\n",
      "\n",
      "Step 0\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 1\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 8\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 2\n",
      "Current state: 8\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 9\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 3\n",
      "Current state: 9\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 1\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 4\n",
      "Current state: 1\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 2\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 5\n",
      "Current state: 2\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 10\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 6\n",
      "Current state: 10\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 9\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 7\n",
      "Current state: 9\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 10\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 8\n",
      "Current state: 10\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 11\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 9\n",
      "Current state: 11\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 10\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 10\n",
      "Current state: 10\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 18\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 11\n",
      "Current state: 18\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 10\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 12\n",
      "Current state: 10\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 2\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 13\n",
      "Current state: 2\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 2\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 14\n",
      "Current state: 2\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 10\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 15\n",
      "Current state: 10\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 9\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 16\n",
      "Current state: 9\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 10\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 17\n",
      "Current state: 10\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 9\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 18\n",
      "Current state: 9\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 17\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 19\n",
      "Current state: 17\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 9\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 20\n",
      "Current state: 9\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 8\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 21\n",
      "Current state: 8\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 16\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 22\n",
      "Current state: 16\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 8\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 23\n",
      "Current state: 8\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 9\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 24\n",
      "Current state: 9\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 17\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 25\n",
      "Current state: 17\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 18\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 26\n",
      "Current state: 18\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 17\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 27\n",
      "Current state: 17\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 18\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 28\n",
      "Current state: 18\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 17\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 29\n",
      "Current state: 17\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 25\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 30\n",
      "Current state: 25\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 33\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 31\n",
      "Current state: 33\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 41\n",
      "Reward: 0.0\n",
      "Terminated: True\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Episode 52\n",
      "Starting state: 0\n",
      "\n",
      "Step 0\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 8\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 1\n",
      "Current state: 8\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 9\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 2\n",
      "Current state: 9\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 17\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 3\n",
      "Current state: 17\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 18\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 4\n",
      "Current state: 18\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 17\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 5\n",
      "Current state: 17\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 25\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 6\n",
      "Current state: 25\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 26\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 7\n",
      "Current state: 26\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 25\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 8\n",
      "Current state: 25\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 33\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 9\n",
      "Current state: 33\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 34\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 10\n",
      "Current state: 34\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 42\n",
      "Reward: 0.0\n",
      "Terminated: True\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Episode 53\n",
      "Starting state: 0\n",
      "\n",
      "Step 0\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 1\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 1\n",
      "Current state: 1\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 2\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 2\n",
      "Current state: 2\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 10\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 3\n",
      "Current state: 10\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 9\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 4\n",
      "Current state: 9\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 10\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 5\n",
      "Current state: 10\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 11\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 6\n",
      "Current state: 11\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 12\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 7\n",
      "Current state: 12\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 11\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 8\n",
      "Current state: 11\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 19\n",
      "Reward: 0.0\n",
      "Terminated: True\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Episode 54\n",
      "Starting state: 0\n",
      "\n",
      "Step 0\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 8\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 1\n",
      "Current state: 8\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 16\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 2\n",
      "Current state: 16\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 8\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 3\n",
      "Current state: 8\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 9\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 4\n",
      "Current state: 9\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 8\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 5\n",
      "Current state: 8\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 9\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 6\n",
      "Current state: 9\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 8\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 7\n",
      "Current state: 8\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 9\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 8\n",
      "Current state: 9\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 10\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 9\n",
      "Current state: 10\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 2\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 10\n",
      "Current state: 2\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 10\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 11\n",
      "Current state: 10\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 11\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 12\n",
      "Current state: 11\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 19\n",
      "Reward: 0.0\n",
      "Terminated: True\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Episode 55\n",
      "Starting state: 0\n",
      "\n",
      "Step 0\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 1\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 2\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 8\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 3\n",
      "Current state: 8\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 8\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 4\n",
      "Current state: 8\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 16\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 5\n",
      "Current state: 16\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 17\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 6\n",
      "Current state: 17\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 25\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 7\n",
      "Current state: 25\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 26\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 8\n",
      "Current state: 26\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 27\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 9\n",
      "Current state: 27\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 28\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 10\n",
      "Current state: 28\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 27\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 11\n",
      "Current state: 27\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 26\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 12\n",
      "Current state: 26\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 18\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 13\n",
      "Current state: 18\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 17\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 14\n",
      "Current state: 17\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 18\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 15\n",
      "Current state: 18\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 26\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 16\n",
      "Current state: 26\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 18\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 17\n",
      "Current state: 18\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 19\n",
      "Reward: 0.0\n",
      "Terminated: True\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Episode 56\n",
      "Starting state: 0\n",
      "\n",
      "Step 0\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 1\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 8\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 2\n",
      "Current state: 8\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 8\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 3\n",
      "Current state: 8\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 16\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 4\n",
      "Current state: 16\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 24\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 5\n",
      "Current state: 24\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 24\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 6\n",
      "Current state: 24\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 24\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 7\n",
      "Current state: 24\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 24\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 8\n",
      "Current state: 24\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 25\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 9\n",
      "Current state: 25\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 24\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 10\n",
      "Current state: 24\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 32\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 11\n",
      "Current state: 32\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 40\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 12\n",
      "Current state: 40\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 32\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 13\n",
      "Current state: 32\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 40\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 14\n",
      "Current state: 40\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 40\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 15\n",
      "Current state: 40\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 32\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 16\n",
      "Current state: 32\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 24\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 17\n",
      "Current state: 24\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 25\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 18\n",
      "Current state: 25\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 24\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 19\n",
      "Current state: 24\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 32\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 20\n",
      "Current state: 32\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 24\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 21\n",
      "Current state: 24\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 32\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 22\n",
      "Current state: 32\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 40\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 23\n",
      "Current state: 40\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 48\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 24\n",
      "Current state: 48\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 49\n",
      "Reward: 0.0\n",
      "Terminated: True\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Episode 57\n",
      "Starting state: 0\n",
      "\n",
      "Step 0\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 1\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 8\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 2\n",
      "Current state: 8\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 16\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 3\n",
      "Current state: 16\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 17\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 4\n",
      "Current state: 17\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 18\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 5\n",
      "Current state: 18\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 26\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 6\n",
      "Current state: 26\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 34\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 7\n",
      "Current state: 34\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 33\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 8\n",
      "Current state: 33\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 34\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 9\n",
      "Current state: 34\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 33\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 10\n",
      "Current state: 33\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 34\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 11\n",
      "Current state: 34\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 42\n",
      "Reward: 0.0\n",
      "Terminated: True\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Episode 58\n",
      "Starting state: 0\n",
      "\n",
      "Step 0\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 1\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 2\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 3\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 4\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 8\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 5\n",
      "Current state: 8\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 6\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 8\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 7\n",
      "Current state: 8\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 8\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 8\n",
      "Current state: 8\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 9\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 9\n",
      "Current state: 9\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 8\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 10\n",
      "Current state: 8\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 9\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 11\n",
      "Current state: 9\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 8\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 12\n",
      "Current state: 8\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 8\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 13\n",
      "Current state: 8\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 9\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 14\n",
      "Current state: 9\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 8\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 15\n",
      "Current state: 8\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 9\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 16\n",
      "Current state: 9\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 10\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 17\n",
      "Current state: 10\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 11\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 18\n",
      "Current state: 11\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 10\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 19\n",
      "Current state: 10\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 9\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 20\n",
      "Current state: 9\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 10\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 21\n",
      "Current state: 10\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 9\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 22\n",
      "Current state: 9\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 10\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 23\n",
      "Current state: 10\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 18\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 24\n",
      "Current state: 18\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 19\n",
      "Reward: 0.0\n",
      "Terminated: True\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Episode 59\n",
      "Starting state: 0\n",
      "\n",
      "Step 0\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 1\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 1\n",
      "Current state: 1\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 2\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 2\n",
      "Current state: 2\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 3\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 3\n",
      "Current state: 3\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 3\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 4\n",
      "Current state: 3\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 4\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 5\n",
      "Current state: 4\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 3\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 6\n",
      "Current state: 3\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 2\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 7\n",
      "Current state: 2\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 10\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 8\n",
      "Current state: 10\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 9\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 9\n",
      "Current state: 9\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 17\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 10\n",
      "Current state: 17\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 18\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 11\n",
      "Current state: 18\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 19\n",
      "Reward: 0.0\n",
      "Terminated: True\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Episode 60\n",
      "Starting state: 0\n",
      "\n",
      "Step 0\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 1\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 1\n",
      "Current state: 1\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 2\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 2\n",
      "Current state: 2\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 1\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 3\n",
      "Current state: 1\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 4\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 5\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 6\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 1\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 7\n",
      "Current state: 1\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 1\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 8\n",
      "Current state: 1\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 1\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 9\n",
      "Current state: 1\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 1\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 10\n",
      "Current state: 1\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 9\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 11\n",
      "Current state: 9\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 8\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 12\n",
      "Current state: 8\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 16\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 13\n",
      "Current state: 16\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 16\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 14\n",
      "Current state: 16\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 8\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 15\n",
      "Current state: 8\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 9\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 16\n",
      "Current state: 9\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 17\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 17\n",
      "Current state: 17\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 25\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 18\n",
      "Current state: 25\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 17\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 19\n",
      "Current state: 17\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 18\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 20\n",
      "Current state: 18\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 17\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 21\n",
      "Current state: 17\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 9\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 22\n",
      "Current state: 9\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 17\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 23\n",
      "Current state: 17\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 16\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 24\n",
      "Current state: 16\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 8\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 25\n",
      "Current state: 8\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 9\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 26\n",
      "Current state: 9\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 10\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 27\n",
      "Current state: 10\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 11\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 28\n",
      "Current state: 11\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 10\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 29\n",
      "Current state: 10\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 11\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 30\n",
      "Current state: 11\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 10\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 31\n",
      "Current state: 10\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 18\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 32\n",
      "Current state: 18\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 17\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 33\n",
      "Current state: 17\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 18\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 34\n",
      "Current state: 18\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 10\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 35\n",
      "Current state: 10\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 2\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 36\n",
      "Current state: 2\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 3\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 37\n",
      "Current state: 3\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 3\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 38\n",
      "Current state: 3\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 11\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 39\n",
      "Current state: 11\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 12\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 40\n",
      "Current state: 12\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 13\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 41\n",
      "Current state: 13\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 5\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 42\n",
      "Current state: 5\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 13\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 43\n",
      "Current state: 13\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 12\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 44\n",
      "Current state: 12\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 11\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 45\n",
      "Current state: 11\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 3\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 46\n",
      "Current state: 3\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 2\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 47\n",
      "Current state: 2\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 10\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 48\n",
      "Current state: 10\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 2\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 49\n",
      "Current state: 2\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 10\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 50\n",
      "Current state: 10\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 11\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 51\n",
      "Current state: 11\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 3\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 52\n",
      "Current state: 3\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 2\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 53\n",
      "Current state: 2\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 2\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 54\n",
      "Current state: 2\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 10\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 55\n",
      "Current state: 10\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 11\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 56\n",
      "Current state: 11\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 10\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 57\n",
      "Current state: 10\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 2\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 58\n",
      "Current state: 2\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 10\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 59\n",
      "Current state: 10\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 2\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 60\n",
      "Current state: 2\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 10\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 61\n",
      "Current state: 10\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 9\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 62\n",
      "Current state: 9\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 10\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 63\n",
      "Current state: 10\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 9\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 64\n",
      "Current state: 9\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 8\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 65\n",
      "Current state: 8\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 16\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 66\n",
      "Current state: 16\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 16\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 67\n",
      "Current state: 16\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 24\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 68\n",
      "Current state: 24\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 24\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 69\n",
      "Current state: 24\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 24\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 70\n",
      "Current state: 24\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 24\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 71\n",
      "Current state: 24\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 32\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 72\n",
      "Current state: 32\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 40\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 73\n",
      "Current state: 40\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 48\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 74\n",
      "Current state: 48\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 40\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 75\n",
      "Current state: 40\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 32\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 76\n",
      "Current state: 32\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 40\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 77\n",
      "Current state: 40\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 41\n",
      "Reward: 0.0\n",
      "Terminated: True\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Successful episodes so far: []\n",
      "\n",
      "Episode 61\n",
      "Starting state: 0\n",
      "\n",
      "Step 0\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 1\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 2\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 8\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 3\n",
      "Current state: 8\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 16\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 4\n",
      "Current state: 16\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 16\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 5\n",
      "Current state: 16\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 17\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 6\n",
      "Current state: 17\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 16\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 7\n",
      "Current state: 16\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 24\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 8\n",
      "Current state: 24\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 16\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 9\n",
      "Current state: 16\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 24\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 10\n",
      "Current state: 24\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 16\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 11\n",
      "Current state: 16\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 17\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 12\n",
      "Current state: 17\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 18\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 13\n",
      "Current state: 18\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 10\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 14\n",
      "Current state: 10\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 9\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 15\n",
      "Current state: 9\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 10\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 16\n",
      "Current state: 10\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 2\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 17\n",
      "Current state: 2\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 3\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 18\n",
      "Current state: 3\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 2\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 19\n",
      "Current state: 2\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 3\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 20\n",
      "Current state: 3\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 2\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 21\n",
      "Current state: 2\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 1\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 22\n",
      "Current state: 1\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 1\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 23\n",
      "Current state: 1\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 2\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 24\n",
      "Current state: 2\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 3\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 25\n",
      "Current state: 3\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 3\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 26\n",
      "Current state: 3\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 3\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 27\n",
      "Current state: 3\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 4\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 28\n",
      "Current state: 4\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 3\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 29\n",
      "Current state: 3\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 3\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 30\n",
      "Current state: 3\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 4\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 31\n",
      "Current state: 4\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 5\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 32\n",
      "Current state: 5\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 6\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 33\n",
      "Current state: 6\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 7\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 34\n",
      "Current state: 7\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 15\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 35\n",
      "Current state: 15\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 14\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 36\n",
      "Current state: 14\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 6\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 37\n",
      "Current state: 6\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 6\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 38\n",
      "Current state: 6\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 5\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 39\n",
      "Current state: 5\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 5\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 40\n",
      "Current state: 5\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 5\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 41\n",
      "Current state: 5\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 13\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 42\n",
      "Current state: 13\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 21\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 43\n",
      "Current state: 21\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 13\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 44\n",
      "Current state: 13\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 12\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 45\n",
      "Current state: 12\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 11\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 46\n",
      "Current state: 11\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 10\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 47\n",
      "Current state: 10\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 18\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 48\n",
      "Current state: 18\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 17\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 49\n",
      "Current state: 17\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 9\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 50\n",
      "Current state: 9\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 17\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 51\n",
      "Current state: 17\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 16\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 52\n",
      "Current state: 16\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 16\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 53\n",
      "Current state: 16\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 17\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 54\n",
      "Current state: 17\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 18\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 55\n",
      "Current state: 18\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 17\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 56\n",
      "Current state: 17\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 16\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 57\n",
      "Current state: 16\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 16\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 58\n",
      "Current state: 16\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 24\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 59\n",
      "Current state: 24\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 16\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 60\n",
      "Current state: 16\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 24\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 61\n",
      "Current state: 24\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 32\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 62\n",
      "Current state: 32\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 24\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 63\n",
      "Current state: 24\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 24\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 64\n",
      "Current state: 24\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 32\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 65\n",
      "Current state: 32\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 24\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 66\n",
      "Current state: 24\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 24\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 67\n",
      "Current state: 24\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 25\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 68\n",
      "Current state: 25\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 26\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 69\n",
      "Current state: 26\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 27\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 70\n",
      "Current state: 27\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 28\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 71\n",
      "Current state: 28\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 29\n",
      "Reward: 0.0\n",
      "Terminated: True\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Episode 62\n",
      "Starting state: 0\n",
      "\n",
      "Step 0\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 1\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 1\n",
      "Current state: 1\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 9\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 2\n",
      "Current state: 9\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 17\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 3\n",
      "Current state: 17\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 18\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 4\n",
      "Current state: 18\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 26\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 5\n",
      "Current state: 26\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 27\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 6\n",
      "Current state: 27\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 19\n",
      "Reward: 0.0\n",
      "Terminated: True\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Episode 63\n",
      "Starting state: 0\n",
      "\n",
      "Step 0\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 1\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 1\n",
      "Current state: 1\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 2\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 1\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 3\n",
      "Current state: 1\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 4\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 1\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 5\n",
      "Current state: 1\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 6\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 1\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 7\n",
      "Current state: 1\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 2\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 8\n",
      "Current state: 2\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 10\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 9\n",
      "Current state: 10\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 9\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 10\n",
      "Current state: 9\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 17\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 11\n",
      "Current state: 17\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 25\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 12\n",
      "Current state: 25\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 33\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 13\n",
      "Current state: 33\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 32\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 14\n",
      "Current state: 32\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 33\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 15\n",
      "Current state: 33\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 34\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 16\n",
      "Current state: 34\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 35\n",
      "Reward: 0.0\n",
      "Terminated: True\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Episode 64\n",
      "Starting state: 0\n",
      "\n",
      "Step 0\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 1\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 8\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 2\n",
      "Current state: 8\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 8\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 3\n",
      "Current state: 8\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 8\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 4\n",
      "Current state: 8\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 16\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 5\n",
      "Current state: 16\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 8\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 6\n",
      "Current state: 8\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 8\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 7\n",
      "Current state: 8\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 16\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 8\n",
      "Current state: 16\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 16\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 9\n",
      "Current state: 16\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 17\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 10\n",
      "Current state: 17\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 9\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 11\n",
      "Current state: 9\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 17\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 12\n",
      "Current state: 17\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 16\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 13\n",
      "Current state: 16\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 8\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 14\n",
      "Current state: 8\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 16\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 15\n",
      "Current state: 16\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 8\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 16\n",
      "Current state: 8\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 17\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 18\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 8\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 19\n",
      "Current state: 8\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 16\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 20\n",
      "Current state: 16\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 24\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 21\n",
      "Current state: 24\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 32\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 22\n",
      "Current state: 32\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 33\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 23\n",
      "Current state: 33\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 25\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 24\n",
      "Current state: 25\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 33\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 25\n",
      "Current state: 33\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 32\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 26\n",
      "Current state: 32\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 32\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 27\n",
      "Current state: 32\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 32\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 28\n",
      "Current state: 32\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 33\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 29\n",
      "Current state: 33\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 32\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 30\n",
      "Current state: 32\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 40\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 31\n",
      "Current state: 40\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 48\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 32\n",
      "Current state: 48\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 56\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 33\n",
      "Current state: 56\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 48\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 34\n",
      "Current state: 48\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 40\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 35\n",
      "Current state: 40\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 48\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 36\n",
      "Current state: 48\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 48\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 37\n",
      "Current state: 48\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 56\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 38\n",
      "Current state: 56\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 56\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 39\n",
      "Current state: 56\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 48\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 40\n",
      "Current state: 48\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 48\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 41\n",
      "Current state: 48\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 49\n",
      "Reward: 0.0\n",
      "Terminated: True\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Episode 65\n",
      "Starting state: 0\n",
      "\n",
      "Step 0\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 8\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 1\n",
      "Current state: 8\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 16\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 2\n",
      "Current state: 16\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 16\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 3\n",
      "Current state: 16\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 17\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 4\n",
      "Current state: 17\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 9\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 5\n",
      "Current state: 9\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 8\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 6\n",
      "Current state: 8\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 16\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 7\n",
      "Current state: 16\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 17\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 8\n",
      "Current state: 17\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 18\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 9\n",
      "Current state: 18\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 19\n",
      "Reward: 0.0\n",
      "Terminated: True\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Episode 66\n",
      "Starting state: 0\n",
      "\n",
      "Step 0\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 1\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 2\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 1\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 3\n",
      "Current state: 1\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 4\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 1\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 5\n",
      "Current state: 1\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 9\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 6\n",
      "Current state: 9\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 1\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 7\n",
      "Current state: 1\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 2\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 8\n",
      "Current state: 2\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 10\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 9\n",
      "Current state: 10\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 11\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 10\n",
      "Current state: 11\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 10\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 11\n",
      "Current state: 10\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 9\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 12\n",
      "Current state: 9\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 10\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 13\n",
      "Current state: 10\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 18\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 14\n",
      "Current state: 18\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 10\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 15\n",
      "Current state: 10\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 2\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 16\n",
      "Current state: 2\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 3\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 17\n",
      "Current state: 3\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 4\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 18\n",
      "Current state: 4\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 12\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 19\n",
      "Current state: 12\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 11\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 20\n",
      "Current state: 11\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 3\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 21\n",
      "Current state: 3\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 3\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 22\n",
      "Current state: 3\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 11\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 23\n",
      "Current state: 11\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 3\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 24\n",
      "Current state: 3\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 11\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 25\n",
      "Current state: 11\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 19\n",
      "Reward: 0.0\n",
      "Terminated: True\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Episode 67\n",
      "Starting state: 0\n",
      "\n",
      "Step 0\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 8\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 1\n",
      "Current state: 8\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 8\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 2\n",
      "Current state: 8\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 3\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 4\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 8\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 5\n",
      "Current state: 8\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 8\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 6\n",
      "Current state: 8\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 7\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 8\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 9\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 8\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 10\n",
      "Current state: 8\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 16\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 11\n",
      "Current state: 16\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 8\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 12\n",
      "Current state: 8\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 13\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 14\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 15\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 16\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 17\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 18\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 1\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 19\n",
      "Current state: 1\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 1\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 20\n",
      "Current state: 1\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 21\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 22\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 23\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 24\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 1\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 25\n",
      "Current state: 1\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 1\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 26\n",
      "Current state: 1\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 27\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 28\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 1\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 29\n",
      "Current state: 1\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 30\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 31\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 32\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 1\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 33\n",
      "Current state: 1\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 34\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 8\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 35\n",
      "Current state: 8\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 8\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 36\n",
      "Current state: 8\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 16\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 37\n",
      "Current state: 16\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 24\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 38\n",
      "Current state: 24\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 16\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 39\n",
      "Current state: 16\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 17\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 40\n",
      "Current state: 17\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 9\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 41\n",
      "Current state: 9\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 10\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 42\n",
      "Current state: 10\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 18\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 43\n",
      "Current state: 18\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 17\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 44\n",
      "Current state: 17\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 16\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 45\n",
      "Current state: 16\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 8\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 46\n",
      "Current state: 8\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 47\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 1\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 48\n",
      "Current state: 1\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 49\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 50\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 1\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 51\n",
      "Current state: 1\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 52\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 1\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 53\n",
      "Current state: 1\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 2\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 54\n",
      "Current state: 2\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 10\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 55\n",
      "Current state: 10\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 18\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 56\n",
      "Current state: 18\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 17\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 57\n",
      "Current state: 17\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 18\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 58\n",
      "Current state: 18\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 10\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 59\n",
      "Current state: 10\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 18\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 60\n",
      "Current state: 18\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 17\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 61\n",
      "Current state: 17\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 16\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 62\n",
      "Current state: 16\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 8\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 63\n",
      "Current state: 8\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 8\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 64\n",
      "Current state: 8\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 16\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 65\n",
      "Current state: 16\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 17\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 66\n",
      "Current state: 17\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 18\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 67\n",
      "Current state: 18\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 26\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 68\n",
      "Current state: 26\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 27\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 69\n",
      "Current state: 27\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 35\n",
      "Reward: 0.0\n",
      "Terminated: True\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Episode 68\n",
      "Starting state: 0\n",
      "\n",
      "Step 0\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 1\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 2\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 8\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 3\n",
      "Current state: 8\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 16\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 4\n",
      "Current state: 16\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 8\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 5\n",
      "Current state: 8\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 6\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 8\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 7\n",
      "Current state: 8\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 16\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 8\n",
      "Current state: 16\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 24\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 9\n",
      "Current state: 24\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 32\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 10\n",
      "Current state: 32\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 40\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 11\n",
      "Current state: 40\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 48\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 12\n",
      "Current state: 48\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 49\n",
      "Reward: 0.0\n",
      "Terminated: True\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Episode 69\n",
      "Starting state: 0\n",
      "\n",
      "Step 0\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 1\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 1\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 2\n",
      "Current state: 1\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 3\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 4\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 5\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 6\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 8\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 7\n",
      "Current state: 8\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 9\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 8\n",
      "Current state: 9\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 10\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 9\n",
      "Current state: 10\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 18\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 10\n",
      "Current state: 18\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 26\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 11\n",
      "Current state: 26\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 34\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 12\n",
      "Current state: 34\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 33\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 13\n",
      "Current state: 33\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 41\n",
      "Reward: 0.0\n",
      "Terminated: True\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Episode 70\n",
      "Starting state: 0\n",
      "\n",
      "Step 0\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 8\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 1\n",
      "Current state: 8\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 9\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 2\n",
      "Current state: 9\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 1\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 3\n",
      "Current state: 1\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 1\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 4\n",
      "Current state: 1\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 9\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 5\n",
      "Current state: 9\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 10\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 6\n",
      "Current state: 10\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 18\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 7\n",
      "Current state: 18\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 26\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 8\n",
      "Current state: 26\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 27\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 9\n",
      "Current state: 27\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 19\n",
      "Reward: 0.0\n",
      "Terminated: True\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Successful episodes so far: []\n",
      "\n",
      "Episode 71\n",
      "Starting state: 0\n",
      "\n",
      "Step 0\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 1\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 2\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 3\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 8\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 4\n",
      "Current state: 8\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 9\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 5\n",
      "Current state: 9\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 10\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 6\n",
      "Current state: 10\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 18\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 7\n",
      "Current state: 18\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 19\n",
      "Reward: 0.0\n",
      "Terminated: True\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Episode 72\n",
      "Starting state: 0\n",
      "\n",
      "Step 0\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 8\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 1\n",
      "Current state: 8\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 8\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 2\n",
      "Current state: 8\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 16\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 3\n",
      "Current state: 16\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 8\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 4\n",
      "Current state: 8\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 9\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 5\n",
      "Current state: 9\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 8\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 6\n",
      "Current state: 8\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 8\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 7\n",
      "Current state: 8\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 8\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 8\n",
      "Current state: 8\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 9\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 9\n",
      "Current state: 9\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 8\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 10\n",
      "Current state: 8\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 16\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 11\n",
      "Current state: 16\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 8\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 12\n",
      "Current state: 8\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 9\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 13\n",
      "Current state: 9\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 1\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 14\n",
      "Current state: 1\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 15\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 1\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 16\n",
      "Current state: 1\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 2\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 17\n",
      "Current state: 2\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 3\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 18\n",
      "Current state: 3\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 4\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 19\n",
      "Current state: 4\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 5\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 20\n",
      "Current state: 5\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 6\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 21\n",
      "Current state: 6\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 14\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 22\n",
      "Current state: 14\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 13\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 23\n",
      "Current state: 13\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 12\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 24\n",
      "Current state: 12\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 13\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 25\n",
      "Current state: 13\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 12\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 26\n",
      "Current state: 12\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 20\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 27\n",
      "Current state: 20\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 12\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 28\n",
      "Current state: 12\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 4\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 29\n",
      "Current state: 4\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 3\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 30\n",
      "Current state: 3\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 4\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 31\n",
      "Current state: 4\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 4\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 32\n",
      "Current state: 4\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 5\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 33\n",
      "Current state: 5\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 6\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 34\n",
      "Current state: 6\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 5\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 35\n",
      "Current state: 5\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 4\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 36\n",
      "Current state: 4\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 4\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 37\n",
      "Current state: 4\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 12\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 38\n",
      "Current state: 12\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 13\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 39\n",
      "Current state: 13\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 21\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 40\n",
      "Current state: 21\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 20\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 41\n",
      "Current state: 20\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 19\n",
      "Reward: 0.0\n",
      "Terminated: True\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Episode 73\n",
      "Starting state: 0\n",
      "\n",
      "Step 0\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 1\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 8\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 2\n",
      "Current state: 8\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 3\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 4\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 1\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 5\n",
      "Current state: 1\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 9\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 6\n",
      "Current state: 9\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 1\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 7\n",
      "Current state: 1\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 2\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 8\n",
      "Current state: 2\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 3\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 9\n",
      "Current state: 3\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 11\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 10\n",
      "Current state: 11\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 19\n",
      "Reward: 0.0\n",
      "Terminated: True\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Episode 74\n",
      "Starting state: 0\n",
      "\n",
      "Step 0\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 1\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 1\n",
      "Current state: 1\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 2\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 1\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 3\n",
      "Current state: 1\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 1\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 4\n",
      "Current state: 1\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 9\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 5\n",
      "Current state: 9\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 1\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 6\n",
      "Current state: 1\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 7\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 8\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 8\n",
      "Current state: 8\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 9\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 8\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 10\n",
      "Current state: 8\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 16\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 11\n",
      "Current state: 16\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 8\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 12\n",
      "Current state: 8\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 8\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 13\n",
      "Current state: 8\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 16\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 14\n",
      "Current state: 16\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 8\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 15\n",
      "Current state: 8\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 9\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 16\n",
      "Current state: 9\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 17\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 17\n",
      "Current state: 17\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 18\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 18\n",
      "Current state: 18\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 17\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 19\n",
      "Current state: 17\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 9\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 20\n",
      "Current state: 9\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 17\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 21\n",
      "Current state: 17\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 25\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 22\n",
      "Current state: 25\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 26\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 23\n",
      "Current state: 26\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 27\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 24\n",
      "Current state: 27\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 19\n",
      "Reward: 0.0\n",
      "Terminated: True\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Episode 75\n",
      "Starting state: 0\n",
      "\n",
      "Step 0\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 8\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 1\n",
      "Current state: 8\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 16\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 2\n",
      "Current state: 16\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 17\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 3\n",
      "Current state: 17\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 25\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 4\n",
      "Current state: 25\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 33\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 5\n",
      "Current state: 33\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 25\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 6\n",
      "Current state: 25\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 24\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 7\n",
      "Current state: 24\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 24\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 8\n",
      "Current state: 24\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 16\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 9\n",
      "Current state: 16\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 8\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 10\n",
      "Current state: 8\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 9\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 11\n",
      "Current state: 9\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 10\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 12\n",
      "Current state: 10\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 11\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 13\n",
      "Current state: 11\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 10\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 14\n",
      "Current state: 10\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 11\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 15\n",
      "Current state: 11\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 3\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 16\n",
      "Current state: 3\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 4\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 17\n",
      "Current state: 4\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 12\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 18\n",
      "Current state: 12\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 4\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 19\n",
      "Current state: 4\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 3\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 20\n",
      "Current state: 3\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 3\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 21\n",
      "Current state: 3\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 11\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 22\n",
      "Current state: 11\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 12\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 23\n",
      "Current state: 12\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 11\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 24\n",
      "Current state: 11\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 19\n",
      "Reward: 0.0\n",
      "Terminated: True\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Episode 76\n",
      "Starting state: 0\n",
      "\n",
      "Step 0\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 1\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 8\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 2\n",
      "Current state: 8\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 3\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 8\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 4\n",
      "Current state: 8\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 8\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 5\n",
      "Current state: 8\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 9\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 6\n",
      "Current state: 9\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 10\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 7\n",
      "Current state: 10\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 11\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 8\n",
      "Current state: 11\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 19\n",
      "Reward: 0.0\n",
      "Terminated: True\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Episode 77\n",
      "Starting state: 0\n",
      "\n",
      "Step 0\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 1\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 1\n",
      "Current state: 1\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 2\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 3\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 8\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 4\n",
      "Current state: 8\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 8\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 5\n",
      "Current state: 8\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 9\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 6\n",
      "Current state: 9\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 1\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 7\n",
      "Current state: 1\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 2\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 8\n",
      "Current state: 2\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 1\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 9\n",
      "Current state: 1\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 2\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 10\n",
      "Current state: 2\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 3\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 11\n",
      "Current state: 3\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 11\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 12\n",
      "Current state: 11\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 10\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 13\n",
      "Current state: 10\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 18\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 14\n",
      "Current state: 18\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 26\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 15\n",
      "Current state: 26\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 25\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 16\n",
      "Current state: 25\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 33\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 17\n",
      "Current state: 33\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 41\n",
      "Reward: 0.0\n",
      "Terminated: True\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Episode 78\n",
      "Starting state: 0\n",
      "\n",
      "Step 0\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 1\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 2\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 3\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 1\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 4\n",
      "Current state: 1\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 1\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 5\n",
      "Current state: 1\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 9\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 6\n",
      "Current state: 9\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 17\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 7\n",
      "Current state: 17\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 18\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 8\n",
      "Current state: 18\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 26\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 9\n",
      "Current state: 26\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 25\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 10\n",
      "Current state: 25\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 26\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 11\n",
      "Current state: 26\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 18\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 12\n",
      "Current state: 18\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 17\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 13\n",
      "Current state: 17\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 18\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 14\n",
      "Current state: 18\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 26\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 15\n",
      "Current state: 26\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 25\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 16\n",
      "Current state: 25\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 17\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 17\n",
      "Current state: 17\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 25\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 18\n",
      "Current state: 25\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 24\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 19\n",
      "Current state: 24\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 24\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 20\n",
      "Current state: 24\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 16\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 21\n",
      "Current state: 16\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 24\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 22\n",
      "Current state: 24\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 32\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 23\n",
      "Current state: 32\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 32\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 24\n",
      "Current state: 32\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 32\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 25\n",
      "Current state: 32\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 33\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 26\n",
      "Current state: 33\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 41\n",
      "Reward: 0.0\n",
      "Terminated: True\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Episode 79\n",
      "Starting state: 0\n",
      "\n",
      "Step 0\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 1\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 1\n",
      "Current state: 1\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 2\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 1\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 3\n",
      "Current state: 1\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 1\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 4\n",
      "Current state: 1\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 1\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 5\n",
      "Current state: 1\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 6\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 1\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 7\n",
      "Current state: 1\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 1\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 8\n",
      "Current state: 1\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 9\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 10\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 11\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 12\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 13\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 14\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 15\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 8\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 16\n",
      "Current state: 8\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 17\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 18\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 8\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 19\n",
      "Current state: 8\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 20\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 21\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 22\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 1\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 23\n",
      "Current state: 1\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 2\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 24\n",
      "Current state: 2\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 2\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 25\n",
      "Current state: 2\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 1\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 26\n",
      "Current state: 1\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 2\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 27\n",
      "Current state: 2\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 1\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 28\n",
      "Current state: 1\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 1\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 29\n",
      "Current state: 1\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 2\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 30\n",
      "Current state: 2\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 1\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 31\n",
      "Current state: 1\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 2\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 32\n",
      "Current state: 2\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 3\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 33\n",
      "Current state: 3\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 4\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 34\n",
      "Current state: 4\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 3\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 35\n",
      "Current state: 3\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 2\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 36\n",
      "Current state: 2\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 1\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 37\n",
      "Current state: 1\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 38\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 1\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 39\n",
      "Current state: 1\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 40\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 41\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 42\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 1\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 43\n",
      "Current state: 1\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 2\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 44\n",
      "Current state: 2\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 3\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 45\n",
      "Current state: 3\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 2\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 46\n",
      "Current state: 2\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 2\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 47\n",
      "Current state: 2\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 2\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 48\n",
      "Current state: 2\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 10\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 49\n",
      "Current state: 10\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 11\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 50\n",
      "Current state: 11\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 10\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 51\n",
      "Current state: 10\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 9\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 52\n",
      "Current state: 9\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 10\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 53\n",
      "Current state: 10\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 9\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 54\n",
      "Current state: 9\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 10\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 55\n",
      "Current state: 10\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 11\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 56\n",
      "Current state: 11\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 3\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 57\n",
      "Current state: 3\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 4\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 58\n",
      "Current state: 4\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 3\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 59\n",
      "Current state: 3\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 2\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 60\n",
      "Current state: 2\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 10\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 61\n",
      "Current state: 10\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 9\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 62\n",
      "Current state: 9\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 17\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 63\n",
      "Current state: 17\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 18\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 64\n",
      "Current state: 18\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 19\n",
      "Reward: 0.0\n",
      "Terminated: True\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Episode 80\n",
      "Starting state: 0\n",
      "\n",
      "Step 0\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 1\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 2\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 1\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 3\n",
      "Current state: 1\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 1\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 4\n",
      "Current state: 1\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 9\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 5\n",
      "Current state: 9\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 10\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 6\n",
      "Current state: 10\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 11\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 7\n",
      "Current state: 11\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 19\n",
      "Reward: 0.0\n",
      "Terminated: True\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Successful episodes so far: []\n",
      "\n",
      "Episode 81\n",
      "Starting state: 0\n",
      "\n",
      "Step 0\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 1\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 2\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 3\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 1\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 4\n",
      "Current state: 1\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 1\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 5\n",
      "Current state: 1\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 9\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 6\n",
      "Current state: 9\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 10\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 7\n",
      "Current state: 10\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 18\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 8\n",
      "Current state: 18\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 19\n",
      "Reward: 0.0\n",
      "Terminated: True\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Episode 82\n",
      "Starting state: 0\n",
      "\n",
      "Step 0\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 1\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 8\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 2\n",
      "Current state: 8\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 16\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 3\n",
      "Current state: 16\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 16\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 4\n",
      "Current state: 16\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 16\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 5\n",
      "Current state: 16\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 16\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 6\n",
      "Current state: 16\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 8\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 7\n",
      "Current state: 8\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 9\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 8\n",
      "Current state: 9\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 1\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 9\n",
      "Current state: 1\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 9\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 10\n",
      "Current state: 9\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 10\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 11\n",
      "Current state: 10\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 9\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 12\n",
      "Current state: 9\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 10\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 13\n",
      "Current state: 10\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 18\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 14\n",
      "Current state: 18\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 10\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 15\n",
      "Current state: 10\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 2\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 16\n",
      "Current state: 2\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 3\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 17\n",
      "Current state: 3\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 4\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 18\n",
      "Current state: 4\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 12\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 19\n",
      "Current state: 12\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 11\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 20\n",
      "Current state: 11\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 10\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 21\n",
      "Current state: 10\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 9\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 22\n",
      "Current state: 9\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 10\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 23\n",
      "Current state: 10\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 18\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 24\n",
      "Current state: 18\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 10\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 25\n",
      "Current state: 10\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 9\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 26\n",
      "Current state: 9\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 17\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 27\n",
      "Current state: 17\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 18\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 28\n",
      "Current state: 18\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 26\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 29\n",
      "Current state: 26\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 18\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 30\n",
      "Current state: 18\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 26\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 31\n",
      "Current state: 26\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 18\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 32\n",
      "Current state: 18\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 17\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 33\n",
      "Current state: 17\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 16\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 34\n",
      "Current state: 16\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 8\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 35\n",
      "Current state: 8\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 36\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 37\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 1\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 38\n",
      "Current state: 1\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 39\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 1\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 40\n",
      "Current state: 1\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 9\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 41\n",
      "Current state: 9\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 1\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 42\n",
      "Current state: 1\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 43\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 44\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 1\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 45\n",
      "Current state: 1\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 46\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 8\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 47\n",
      "Current state: 8\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 8\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 48\n",
      "Current state: 8\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 9\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 49\n",
      "Current state: 9\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 1\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 50\n",
      "Current state: 1\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 51\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 52\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 53\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 8\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 54\n",
      "Current state: 8\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 9\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 55\n",
      "Current state: 9\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 17\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 56\n",
      "Current state: 17\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 9\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 57\n",
      "Current state: 9\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 8\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 58\n",
      "Current state: 8\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 9\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 59\n",
      "Current state: 9\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 17\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 60\n",
      "Current state: 17\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 16\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 61\n",
      "Current state: 16\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 17\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 62\n",
      "Current state: 17\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 18\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 63\n",
      "Current state: 18\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 17\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 64\n",
      "Current state: 17\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 16\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 65\n",
      "Current state: 16\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 16\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 66\n",
      "Current state: 16\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 8\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 67\n",
      "Current state: 8\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 68\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 69\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 70\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 1\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 71\n",
      "Current state: 1\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 1\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 72\n",
      "Current state: 1\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 1\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 73\n",
      "Current state: 1\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 1\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 74\n",
      "Current state: 1\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 75\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 1\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 76\n",
      "Current state: 1\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 77\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 8\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 78\n",
      "Current state: 8\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 8\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 79\n",
      "Current state: 8\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 9\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 80\n",
      "Current state: 9\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 8\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 81\n",
      "Current state: 8\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 16\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 82\n",
      "Current state: 16\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 16\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 83\n",
      "Current state: 16\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 16\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 84\n",
      "Current state: 16\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 24\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 85\n",
      "Current state: 24\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 24\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 86\n",
      "Current state: 24\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 32\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 87\n",
      "Current state: 32\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 40\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 88\n",
      "Current state: 40\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 32\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 89\n",
      "Current state: 32\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 40\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 90\n",
      "Current state: 40\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 41\n",
      "Reward: 0.0\n",
      "Terminated: True\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Episode 83\n",
      "Starting state: 0\n",
      "\n",
      "Step 0\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 8\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 1\n",
      "Current state: 8\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 9\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 2\n",
      "Current state: 9\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 10\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 3\n",
      "Current state: 10\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 18\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 4\n",
      "Current state: 18\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 17\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 5\n",
      "Current state: 17\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 25\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 6\n",
      "Current state: 25\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 24\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 7\n",
      "Current state: 24\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 24\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 8\n",
      "Current state: 24\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 24\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 9\n",
      "Current state: 24\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 16\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 10\n",
      "Current state: 16\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 17\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 11\n",
      "Current state: 17\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 16\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 12\n",
      "Current state: 16\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 24\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 13\n",
      "Current state: 24\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 25\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 14\n",
      "Current state: 25\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 17\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 15\n",
      "Current state: 17\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 18\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 16\n",
      "Current state: 18\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 17\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 17\n",
      "Current state: 17\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 9\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 18\n",
      "Current state: 9\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 10\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 19\n",
      "Current state: 10\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 18\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 20\n",
      "Current state: 18\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 19\n",
      "Reward: 0.0\n",
      "Terminated: True\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Episode 84\n",
      "Starting state: 0\n",
      "\n",
      "Step 0\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 1\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 1\n",
      "Current state: 1\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 9\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 2\n",
      "Current state: 9\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 8\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 3\n",
      "Current state: 8\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 9\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 4\n",
      "Current state: 9\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 10\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 5\n",
      "Current state: 10\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 11\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 6\n",
      "Current state: 11\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 3\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 7\n",
      "Current state: 3\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 4\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 8\n",
      "Current state: 4\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 3\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 9\n",
      "Current state: 3\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 3\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 10\n",
      "Current state: 3\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 3\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 11\n",
      "Current state: 3\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 11\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 12\n",
      "Current state: 11\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 19\n",
      "Reward: 0.0\n",
      "Terminated: True\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Episode 85\n",
      "Starting state: 0\n",
      "\n",
      "Step 0\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 1\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 1\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 2\n",
      "Current state: 1\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 2\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 3\n",
      "Current state: 2\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 1\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 4\n",
      "Current state: 1\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 1\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 5\n",
      "Current state: 1\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 1\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 6\n",
      "Current state: 1\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 1\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 7\n",
      "Current state: 1\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 8\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 9\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 1\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 10\n",
      "Current state: 1\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 2\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 11\n",
      "Current state: 2\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 3\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 12\n",
      "Current state: 3\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 4\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 13\n",
      "Current state: 4\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 4\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 14\n",
      "Current state: 4\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 4\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 15\n",
      "Current state: 4\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 4\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 16\n",
      "Current state: 4\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 4\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 17\n",
      "Current state: 4\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 5\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 18\n",
      "Current state: 5\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 6\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 19\n",
      "Current state: 6\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 7\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 20\n",
      "Current state: 7\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 6\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 21\n",
      "Current state: 6\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 14\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 22\n",
      "Current state: 14\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 15\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 23\n",
      "Current state: 15\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 14\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 24\n",
      "Current state: 14\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 15\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 25\n",
      "Current state: 15\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 15\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 26\n",
      "Current state: 15\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 14\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 27\n",
      "Current state: 14\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 22\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 28\n",
      "Current state: 22\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 23\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 29\n",
      "Current state: 23\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 31\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 30\n",
      "Current state: 31\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 23\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 31\n",
      "Current state: 23\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 31\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 32\n",
      "Current state: 31\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 39\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 33\n",
      "Current state: 39\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 39\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 34\n",
      "Current state: 39\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 38\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 35\n",
      "Current state: 38\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 30\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 36\n",
      "Current state: 30\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 29\n",
      "Reward: 0.0\n",
      "Terminated: True\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Episode 86\n",
      "Starting state: 0\n",
      "\n",
      "Step 0\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 1\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 1\n",
      "Current state: 1\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 1\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 2\n",
      "Current state: 1\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 9\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 3\n",
      "Current state: 9\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 8\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 4\n",
      "Current state: 8\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 8\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 5\n",
      "Current state: 8\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 6\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 1\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 7\n",
      "Current state: 1\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 9\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 8\n",
      "Current state: 9\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 10\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 9\n",
      "Current state: 10\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 18\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 10\n",
      "Current state: 18\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 26\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 11\n",
      "Current state: 26\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 34\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 12\n",
      "Current state: 34\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 33\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 13\n",
      "Current state: 33\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 41\n",
      "Reward: 0.0\n",
      "Terminated: True\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Episode 87\n",
      "Starting state: 0\n",
      "\n",
      "Step 0\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 1\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 1\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 2\n",
      "Current state: 1\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 9\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 3\n",
      "Current state: 9\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 17\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 4\n",
      "Current state: 17\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 25\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 5\n",
      "Current state: 25\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 26\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 6\n",
      "Current state: 26\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 27\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 7\n",
      "Current state: 27\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 35\n",
      "Reward: 0.0\n",
      "Terminated: True\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Episode 88\n",
      "Starting state: 0\n",
      "\n",
      "Step 0\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 8\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 1\n",
      "Current state: 8\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 8\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 2\n",
      "Current state: 8\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 16\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 3\n",
      "Current state: 16\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 8\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 4\n",
      "Current state: 8\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 8\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 5\n",
      "Current state: 8\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 16\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 6\n",
      "Current state: 16\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 24\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 7\n",
      "Current state: 24\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 32\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 8\n",
      "Current state: 32\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 40\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 9\n",
      "Current state: 40\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 40\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 10\n",
      "Current state: 40\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 40\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 11\n",
      "Current state: 40\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 32\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 12\n",
      "Current state: 32\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 33\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 13\n",
      "Current state: 33\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 34\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 14\n",
      "Current state: 34\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 42\n",
      "Reward: 0.0\n",
      "Terminated: True\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Episode 89\n",
      "Starting state: 0\n",
      "\n",
      "Step 0\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 1\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 2\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 3\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 8\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 4\n",
      "Current state: 8\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 9\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 5\n",
      "Current state: 9\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 17\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 6\n",
      "Current state: 17\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 9\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 7\n",
      "Current state: 9\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 1\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 8\n",
      "Current state: 1\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 9\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 9\n",
      "Current state: 9\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 17\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 10\n",
      "Current state: 17\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 18\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 11\n",
      "Current state: 18\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 17\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 12\n",
      "Current state: 17\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 9\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 13\n",
      "Current state: 9\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 17\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 14\n",
      "Current state: 17\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 16\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 15\n",
      "Current state: 16\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 17\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 16\n",
      "Current state: 17\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 16\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 17\n",
      "Current state: 16\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 24\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 18\n",
      "Current state: 24\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 16\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 19\n",
      "Current state: 16\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 24\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 20\n",
      "Current state: 24\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 32\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 21\n",
      "Current state: 32\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 32\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 22\n",
      "Current state: 32\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 40\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 23\n",
      "Current state: 40\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 48\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 24\n",
      "Current state: 48\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 40\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 25\n",
      "Current state: 40\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 41\n",
      "Reward: 0.0\n",
      "Terminated: True\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Episode 90\n",
      "Starting state: 0\n",
      "\n",
      "Step 0\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 8\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 1\n",
      "Current state: 8\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 16\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 2\n",
      "Current state: 16\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 17\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 3\n",
      "Current state: 17\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 16\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 4\n",
      "Current state: 16\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 17\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 5\n",
      "Current state: 17\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 18\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 6\n",
      "Current state: 18\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 17\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 7\n",
      "Current state: 17\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 18\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 8\n",
      "Current state: 18\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 26\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 9\n",
      "Current state: 26\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 25\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 10\n",
      "Current state: 25\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 17\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 11\n",
      "Current state: 17\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 16\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 12\n",
      "Current state: 16\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 17\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 13\n",
      "Current state: 17\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 9\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 14\n",
      "Current state: 9\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 10\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 15\n",
      "Current state: 10\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 18\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 16\n",
      "Current state: 18\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 26\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 17\n",
      "Current state: 26\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 18\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 18\n",
      "Current state: 18\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 17\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 19\n",
      "Current state: 17\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 9\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 20\n",
      "Current state: 9\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 1\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 21\n",
      "Current state: 1\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 2\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 22\n",
      "Current state: 2\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 10\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 23\n",
      "Current state: 10\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 9\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 24\n",
      "Current state: 9\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 17\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 25\n",
      "Current state: 17\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 18\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 26\n",
      "Current state: 18\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 19\n",
      "Reward: 0.0\n",
      "Terminated: True\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Successful episodes so far: []\n",
      "\n",
      "Episode 91\n",
      "Starting state: 0\n",
      "\n",
      "Step 0\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 8\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 1\n",
      "Current state: 8\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 2\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 3\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 8\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 4\n",
      "Current state: 8\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 16\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 5\n",
      "Current state: 16\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 24\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 6\n",
      "Current state: 24\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 32\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 7\n",
      "Current state: 32\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 32\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 8\n",
      "Current state: 32\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 32\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 9\n",
      "Current state: 32\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 33\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 10\n",
      "Current state: 33\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 34\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 11\n",
      "Current state: 34\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 42\n",
      "Reward: 0.0\n",
      "Terminated: True\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Episode 92\n",
      "Starting state: 0\n",
      "\n",
      "Step 0\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 1\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 1\n",
      "Current state: 1\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 1\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 2\n",
      "Current state: 1\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 1\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 3\n",
      "Current state: 1\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 1\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 4\n",
      "Current state: 1\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 5\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 8\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 6\n",
      "Current state: 8\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 9\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 7\n",
      "Current state: 9\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 8\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 8\n",
      "Current state: 8\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 9\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 8\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 10\n",
      "Current state: 8\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 11\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 1\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 12\n",
      "Current state: 1\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 1\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 13\n",
      "Current state: 1\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 2\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 14\n",
      "Current state: 2\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 10\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 15\n",
      "Current state: 10\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 2\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 16\n",
      "Current state: 2\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 3\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 17\n",
      "Current state: 3\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 11\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 18\n",
      "Current state: 11\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 3\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 19\n",
      "Current state: 3\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 4\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 20\n",
      "Current state: 4\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 5\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 21\n",
      "Current state: 5\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 5\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 22\n",
      "Current state: 5\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 6\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 23\n",
      "Current state: 6\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 6\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 24\n",
      "Current state: 6\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 7\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 25\n",
      "Current state: 7\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 7\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 26\n",
      "Current state: 7\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 6\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 27\n",
      "Current state: 6\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 6\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 28\n",
      "Current state: 6\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 14\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 29\n",
      "Current state: 14\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 13\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 30\n",
      "Current state: 13\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 5\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 31\n",
      "Current state: 5\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 6\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 32\n",
      "Current state: 6\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 7\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 33\n",
      "Current state: 7\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 15\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 34\n",
      "Current state: 15\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 23\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 35\n",
      "Current state: 23\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 22\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 36\n",
      "Current state: 22\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 23\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 37\n",
      "Current state: 23\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 31\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 38\n",
      "Current state: 31\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 39\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 39\n",
      "Current state: 39\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 31\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 40\n",
      "Current state: 31\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 30\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 41\n",
      "Current state: 30\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 31\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 42\n",
      "Current state: 31\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 39\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 43\n",
      "Current state: 39\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 38\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 44\n",
      "Current state: 38\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 37\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 45\n",
      "Current state: 37\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 45\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 46\n",
      "Current state: 45\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 44\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 47\n",
      "Current state: 44\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 36\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 48\n",
      "Current state: 36\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 44\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 49\n",
      "Current state: 44\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 45\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 50\n",
      "Current state: 45\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 46\n",
      "Reward: 0.0\n",
      "Terminated: True\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Episode 93\n",
      "Starting state: 0\n",
      "\n",
      "Step 0\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 1\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 1\n",
      "Current state: 1\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 2\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 3\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 4\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 5\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 1\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 6\n",
      "Current state: 1\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 1\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 7\n",
      "Current state: 1\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 8\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 8\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 9\n",
      "Current state: 8\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 8\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 10\n",
      "Current state: 8\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 16\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 11\n",
      "Current state: 16\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 16\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 12\n",
      "Current state: 16\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 17\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 13\n",
      "Current state: 17\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 16\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 14\n",
      "Current state: 16\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 8\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 15\n",
      "Current state: 8\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 16\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 16\n",
      "Current state: 16\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 16\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 17\n",
      "Current state: 16\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 16\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 18\n",
      "Current state: 16\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 24\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 19\n",
      "Current state: 24\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 25\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 20\n",
      "Current state: 25\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 24\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 21\n",
      "Current state: 24\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 16\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 22\n",
      "Current state: 16\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 24\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 23\n",
      "Current state: 24\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 32\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 24\n",
      "Current state: 32\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 32\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 25\n",
      "Current state: 32\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 40\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 26\n",
      "Current state: 40\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 41\n",
      "Reward: 0.0\n",
      "Terminated: True\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Episode 94\n",
      "Starting state: 0\n",
      "\n",
      "Step 0\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 1\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 2\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 3\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 4\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 8\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 5\n",
      "Current state: 8\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 6\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 7\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 8\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 1\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 9\n",
      "Current state: 1\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 1\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 10\n",
      "Current state: 1\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 2\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 11\n",
      "Current state: 2\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 1\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 12\n",
      "Current state: 1\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 1\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 13\n",
      "Current state: 1\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 9\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 14\n",
      "Current state: 9\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 8\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 15\n",
      "Current state: 8\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 9\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 16\n",
      "Current state: 9\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 1\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 17\n",
      "Current state: 1\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 2\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 18\n",
      "Current state: 2\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 3\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 19\n",
      "Current state: 3\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 4\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 20\n",
      "Current state: 4\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 3\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 21\n",
      "Current state: 3\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 11\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 22\n",
      "Current state: 11\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 3\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 23\n",
      "Current state: 3\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 11\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 24\n",
      "Current state: 11\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 19\n",
      "Reward: 0.0\n",
      "Terminated: True\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Episode 95\n",
      "Starting state: 0\n",
      "\n",
      "Step 0\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 1\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 2\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 3\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 4\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 5\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 6\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 8\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 7\n",
      "Current state: 8\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 9\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 8\n",
      "Current state: 9\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 10\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 9\n",
      "Current state: 10\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 9\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 10\n",
      "Current state: 9\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 10\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 11\n",
      "Current state: 10\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 11\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 12\n",
      "Current state: 11\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 12\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 13\n",
      "Current state: 12\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 4\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 14\n",
      "Current state: 4\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 3\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 15\n",
      "Current state: 3\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 3\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 16\n",
      "Current state: 3\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 3\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 17\n",
      "Current state: 3\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 2\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 18\n",
      "Current state: 2\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 3\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 19\n",
      "Current state: 3\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 2\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 20\n",
      "Current state: 2\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 1\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 21\n",
      "Current state: 1\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 1\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 22\n",
      "Current state: 1\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 9\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 23\n",
      "Current state: 9\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 1\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 24\n",
      "Current state: 1\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 25\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 26\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 27\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 28\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 8\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 29\n",
      "Current state: 8\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 8\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 30\n",
      "Current state: 8\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 9\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 31\n",
      "Current state: 9\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 17\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 32\n",
      "Current state: 17\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 25\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 33\n",
      "Current state: 25\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 26\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 34\n",
      "Current state: 26\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 25\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 35\n",
      "Current state: 25\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 26\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 36\n",
      "Current state: 26\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 25\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 37\n",
      "Current state: 25\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 24\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 38\n",
      "Current state: 24\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 25\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 39\n",
      "Current state: 25\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 24\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 40\n",
      "Current state: 24\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 25\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 41\n",
      "Current state: 25\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 17\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 42\n",
      "Current state: 17\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 18\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 43\n",
      "Current state: 18\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 19\n",
      "Reward: 0.0\n",
      "Terminated: True\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Episode 96\n",
      "Starting state: 0\n",
      "\n",
      "Step 0\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 1\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 8\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 2\n",
      "Current state: 8\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 16\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 3\n",
      "Current state: 16\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 8\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 4\n",
      "Current state: 8\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 8\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 5\n",
      "Current state: 8\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 16\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 6\n",
      "Current state: 16\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 17\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 7\n",
      "Current state: 17\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 9\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 8\n",
      "Current state: 9\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 17\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 9\n",
      "Current state: 17\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 25\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 10\n",
      "Current state: 25\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 24\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 11\n",
      "Current state: 24\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 16\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 12\n",
      "Current state: 16\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 24\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 13\n",
      "Current state: 24\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 25\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 14\n",
      "Current state: 25\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 26\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 15\n",
      "Current state: 26\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 34\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 16\n",
      "Current state: 34\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 42\n",
      "Reward: 0.0\n",
      "Terminated: True\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Episode 97\n",
      "Starting state: 0\n",
      "\n",
      "Step 0\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 1\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 1\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 2\n",
      "Current state: 1\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 3\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 4\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 5\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 1\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 6\n",
      "Current state: 1\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 1\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 7\n",
      "Current state: 1\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 9\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 8\n",
      "Current state: 9\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 10\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 9\n",
      "Current state: 10\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 18\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 10\n",
      "Current state: 18\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 26\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 11\n",
      "Current state: 26\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 25\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 12\n",
      "Current state: 25\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 33\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 13\n",
      "Current state: 33\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 32\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 14\n",
      "Current state: 32\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 33\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 15\n",
      "Current state: 33\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 41\n",
      "Reward: 0.0\n",
      "Terminated: True\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Episode 98\n",
      "Starting state: 0\n",
      "\n",
      "Step 0\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 1\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 2\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 3\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 4\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 8\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 5\n",
      "Current state: 8\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 16\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 6\n",
      "Current state: 16\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 24\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 7\n",
      "Current state: 24\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 16\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 8\n",
      "Current state: 16\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 24\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 9\n",
      "Current state: 24\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 16\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 10\n",
      "Current state: 16\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 16\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 11\n",
      "Current state: 16\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 8\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 12\n",
      "Current state: 8\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 8\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 13\n",
      "Current state: 8\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 14\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 15\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 16\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 17\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 1\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 18\n",
      "Current state: 1\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 2\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 19\n",
      "Current state: 2\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 10\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 20\n",
      "Current state: 10\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 2\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 21\n",
      "Current state: 2\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 1\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 22\n",
      "Current state: 1\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 2\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 23\n",
      "Current state: 2\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 10\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 24\n",
      "Current state: 10\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 18\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 25\n",
      "Current state: 18\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 26\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 26\n",
      "Current state: 26\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 27\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 27\n",
      "Current state: 27\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 26\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 28\n",
      "Current state: 26\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 18\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 29\n",
      "Current state: 18\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 26\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 30\n",
      "Current state: 26\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 18\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 31\n",
      "Current state: 18\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 17\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 32\n",
      "Current state: 17\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 25\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 33\n",
      "Current state: 25\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 24\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 34\n",
      "Current state: 24\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 32\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 35\n",
      "Current state: 32\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 32\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 36\n",
      "Current state: 32\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 33\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 37\n",
      "Current state: 33\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 41\n",
      "Reward: 0.0\n",
      "Terminated: True\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Episode 99\n",
      "Starting state: 0\n",
      "\n",
      "Step 0\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 1\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 8\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 2\n",
      "Current state: 8\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 3\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 1\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 4\n",
      "Current state: 1\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 5\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 6\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 7\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 8\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 9\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 10\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 11\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 1\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 12\n",
      "Current state: 1\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 1\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 13\n",
      "Current state: 1\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 9\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 14\n",
      "Current state: 9\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 1\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 15\n",
      "Current state: 1\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 9\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 16\n",
      "Current state: 9\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 8\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 17\n",
      "Current state: 8\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 16\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 18\n",
      "Current state: 16\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 17\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 19\n",
      "Current state: 17\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 16\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 20\n",
      "Current state: 16\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 16\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 21\n",
      "Current state: 16\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 24\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 22\n",
      "Current state: 24\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 24\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 23\n",
      "Current state: 24\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 16\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 24\n",
      "Current state: 16\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 24\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 25\n",
      "Current state: 24\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 32\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 26\n",
      "Current state: 32\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 33\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 27\n",
      "Current state: 33\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 41\n",
      "Reward: 0.0\n",
      "Terminated: True\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Debug Summary:\n",
      "Total successful episodes: 0\n",
      "Successful episodes: []\n"
     ]
    }
   ],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-17T12:30:25.274380Z",
     "start_time": "2025-02-17T12:30:25.071625Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# First reset the parameters to make debugging easier\n",
    "n_training_episodes = 100  # Reduced for debugging\n",
    "learning_rate = 0.1\n",
    "max_steps = 200\n",
    "gamma = 0.95\n",
    "\n",
    "# Exploration parameters\n",
    "max_epsilon = 1.0\n",
    "min_epsilon = 0.01\n",
    "decay_rate = 0.001\n",
    "\n",
    "# Initialize a fresh Q-table\n",
    "Qtable_frozenlake = initialize_q_table(state_space, action_space)\n",
    "\n",
    "# Now run the debug training instead of the regular training\n",
    "print(\"\\nStarting debug training...\")\n",
    "Qtable_frozenlake, successful_episodes = debug_train(\n",
    "    n_training_episodes=n_training_episodes,\n",
    "    min_epsilon=max_epsilon,\n",
    "    max_epsilon=max_epsilon,\n",
    "    decay_rate=decay_rate,\n",
    "    env=env,\n",
    "    max_steps=max_steps,\n",
    "    Qtable=Qtable_frozenlake\n",
    ")\n",
    "\n",
    "print(\"\\nDebug Summary:\")\n",
    "print(f\"Total successful episodes: {len(successful_episodes)}\")\n",
    "print(f\"Successful episodes: {successful_episodes}\")"
   ],
   "id": "ec42d8deba4f3556",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting debug training...\n",
      "\n",
      "Episode 0\n",
      "Starting state: 0\n",
      "\n",
      "Step 0\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 8\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 1\n",
      "Current state: 8\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 9\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 2\n",
      "Current state: 9\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 17\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 3\n",
      "Current state: 17\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 18\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 4\n",
      "Current state: 18\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 17\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 5\n",
      "Current state: 17\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 25\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 6\n",
      "Current state: 25\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 17\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 7\n",
      "Current state: 17\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 18\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 8\n",
      "Current state: 18\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 17\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 9\n",
      "Current state: 17\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 9\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 10\n",
      "Current state: 9\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 10\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 11\n",
      "Current state: 10\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 11\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 12\n",
      "Current state: 11\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 3\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 13\n",
      "Current state: 3\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 11\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 14\n",
      "Current state: 11\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 12\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 15\n",
      "Current state: 12\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 20\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 16\n",
      "Current state: 20\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 19\n",
      "Reward: 0.0\n",
      "Terminated: True\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Successful episodes so far: []\n",
      "\n",
      "Episode 1\n",
      "Starting state: 0\n",
      "\n",
      "Step 0\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 8\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 1\n",
      "Current state: 8\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 8\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 2\n",
      "Current state: 8\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 9\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 3\n",
      "Current state: 9\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 10\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 4\n",
      "Current state: 10\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 11\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 5\n",
      "Current state: 11\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 12\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 6\n",
      "Current state: 12\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 11\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 7\n",
      "Current state: 11\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 12\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 8\n",
      "Current state: 12\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 4\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 9\n",
      "Current state: 4\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 5\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 10\n",
      "Current state: 5\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 6\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 11\n",
      "Current state: 6\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 14\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 12\n",
      "Current state: 14\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 15\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 13\n",
      "Current state: 15\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 23\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 14\n",
      "Current state: 23\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 23\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 15\n",
      "Current state: 23\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 31\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 16\n",
      "Current state: 31\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 39\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 17\n",
      "Current state: 39\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 47\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 18\n",
      "Current state: 47\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 39\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 19\n",
      "Current state: 39\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 38\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 20\n",
      "Current state: 38\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 30\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 21\n",
      "Current state: 30\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 31\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 22\n",
      "Current state: 31\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 31\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 23\n",
      "Current state: 31\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 39\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 24\n",
      "Current state: 39\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 47\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 25\n",
      "Current state: 47\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 39\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 26\n",
      "Current state: 39\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 31\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 27\n",
      "Current state: 31\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 30\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 28\n",
      "Current state: 30\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 38\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 29\n",
      "Current state: 38\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 30\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 30\n",
      "Current state: 30\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 22\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 31\n",
      "Current state: 22\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 14\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 32\n",
      "Current state: 14\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 15\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 33\n",
      "Current state: 15\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 14\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 34\n",
      "Current state: 14\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 15\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 35\n",
      "Current state: 15\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 7\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 36\n",
      "Current state: 7\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 15\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 37\n",
      "Current state: 15\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 15\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 38\n",
      "Current state: 15\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 14\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 39\n",
      "Current state: 14\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 6\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 40\n",
      "Current state: 6\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 14\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 41\n",
      "Current state: 14\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 13\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 42\n",
      "Current state: 13\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 5\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 43\n",
      "Current state: 5\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 4\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 44\n",
      "Current state: 4\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 3\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 45\n",
      "Current state: 3\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 11\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 46\n",
      "Current state: 11\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 10\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 47\n",
      "Current state: 10\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 9\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 48\n",
      "Current state: 9\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 17\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 49\n",
      "Current state: 17\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 16\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 50\n",
      "Current state: 16\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 24\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 51\n",
      "Current state: 24\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 32\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 52\n",
      "Current state: 32\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 40\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 53\n",
      "Current state: 40\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 40\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 54\n",
      "Current state: 40\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 48\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 55\n",
      "Current state: 48\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 49\n",
      "Reward: 0.0\n",
      "Terminated: True\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Episode 2\n",
      "Starting state: 0\n",
      "\n",
      "Step 0\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 1\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 2\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 3\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 4\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 5\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 6\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 7\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 1\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 8\n",
      "Current state: 1\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 9\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 1\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 10\n",
      "Current state: 1\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 1\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 11\n",
      "Current state: 1\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 12\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 1\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 13\n",
      "Current state: 1\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 14\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 8\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 15\n",
      "Current state: 8\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 9\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 16\n",
      "Current state: 9\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 8\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 17\n",
      "Current state: 8\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 18\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 1\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 19\n",
      "Current state: 1\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 2\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 20\n",
      "Current state: 2\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 3\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 21\n",
      "Current state: 3\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 11\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 22\n",
      "Current state: 11\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 12\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 23\n",
      "Current state: 12\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 11\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 24\n",
      "Current state: 11\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 19\n",
      "Reward: 0.0\n",
      "Terminated: True\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Episode 3\n",
      "Starting state: 0\n",
      "\n",
      "Step 0\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 1\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 1\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 2\n",
      "Current state: 1\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 9\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 3\n",
      "Current state: 9\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 10\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 4\n",
      "Current state: 10\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 18\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 5\n",
      "Current state: 18\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 26\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 6\n",
      "Current state: 26\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 34\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 7\n",
      "Current state: 34\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 35\n",
      "Reward: 0.0\n",
      "Terminated: True\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Episode 4\n",
      "Starting state: 0\n",
      "\n",
      "Step 0\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 8\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 1\n",
      "Current state: 8\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 9\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 2\n",
      "Current state: 9\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 10\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 3\n",
      "Current state: 10\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 2\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 4\n",
      "Current state: 2\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 2\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 5\n",
      "Current state: 2\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 2\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 6\n",
      "Current state: 2\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 1\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 7\n",
      "Current state: 1\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 8\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 9\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 10\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 8\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 11\n",
      "Current state: 8\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 16\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 12\n",
      "Current state: 16\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 16\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 13\n",
      "Current state: 16\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 16\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 14\n",
      "Current state: 16\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 24\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 15\n",
      "Current state: 24\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 25\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 16\n",
      "Current state: 25\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 33\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 17\n",
      "Current state: 33\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 34\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 18\n",
      "Current state: 34\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 42\n",
      "Reward: 0.0\n",
      "Terminated: True\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Episode 5\n",
      "Starting state: 0\n",
      "\n",
      "Step 0\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 8\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 1\n",
      "Current state: 8\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 16\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 2\n",
      "Current state: 16\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 17\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 3\n",
      "Current state: 17\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 25\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 4\n",
      "Current state: 25\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 33\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 5\n",
      "Current state: 33\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 32\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 6\n",
      "Current state: 32\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 33\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 7\n",
      "Current state: 33\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 25\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 8\n",
      "Current state: 25\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 33\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 9\n",
      "Current state: 33\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 41\n",
      "Reward: 0.0\n",
      "Terminated: True\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Episode 6\n",
      "Starting state: 0\n",
      "\n",
      "Step 0\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 1\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 8\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 2\n",
      "Current state: 8\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 16\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 3\n",
      "Current state: 16\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 8\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 4\n",
      "Current state: 8\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 5\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 6\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 1\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 7\n",
      "Current state: 1\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 9\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 8\n",
      "Current state: 9\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 8\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 9\n",
      "Current state: 8\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 10\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 8\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 11\n",
      "Current state: 8\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 16\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 12\n",
      "Current state: 16\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 8\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 13\n",
      "Current state: 8\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 16\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 14\n",
      "Current state: 16\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 8\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 15\n",
      "Current state: 8\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 16\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 16\n",
      "Current state: 16\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 8\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 17\n",
      "Current state: 8\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 18\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 1\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 19\n",
      "Current state: 1\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 1\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 20\n",
      "Current state: 1\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 2\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 21\n",
      "Current state: 2\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 10\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 22\n",
      "Current state: 10\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 11\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 23\n",
      "Current state: 11\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 10\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 24\n",
      "Current state: 10\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 2\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 25\n",
      "Current state: 2\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 3\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 26\n",
      "Current state: 3\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 11\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 27\n",
      "Current state: 11\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 3\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 28\n",
      "Current state: 3\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 11\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 29\n",
      "Current state: 11\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 19\n",
      "Reward: 0.0\n",
      "Terminated: True\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Episode 7\n",
      "Starting state: 0\n",
      "\n",
      "Step 0\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 8\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 1\n",
      "Current state: 8\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 16\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 2\n",
      "Current state: 16\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 17\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 3\n",
      "Current state: 17\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 18\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 4\n",
      "Current state: 18\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 26\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 5\n",
      "Current state: 26\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 27\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 6\n",
      "Current state: 27\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 35\n",
      "Reward: 0.0\n",
      "Terminated: True\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Episode 8\n",
      "Starting state: 0\n",
      "\n",
      "Step 0\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 1\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 8\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 2\n",
      "Current state: 8\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 8\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 3\n",
      "Current state: 8\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 16\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 4\n",
      "Current state: 16\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 8\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 5\n",
      "Current state: 8\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 8\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 6\n",
      "Current state: 8\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 9\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 7\n",
      "Current state: 9\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 10\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 8\n",
      "Current state: 10\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 11\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 9\n",
      "Current state: 11\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 10\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 10\n",
      "Current state: 10\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 9\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 11\n",
      "Current state: 9\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 8\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 12\n",
      "Current state: 8\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 9\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 13\n",
      "Current state: 9\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 17\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 14\n",
      "Current state: 17\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 18\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 15\n",
      "Current state: 18\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 26\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 16\n",
      "Current state: 26\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 27\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 17\n",
      "Current state: 27\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 26\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 18\n",
      "Current state: 26\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 34\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 19\n",
      "Current state: 34\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 33\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 20\n",
      "Current state: 33\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 32\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 21\n",
      "Current state: 32\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 40\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 22\n",
      "Current state: 40\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 40\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 23\n",
      "Current state: 40\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 32\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 24\n",
      "Current state: 32\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 32\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 25\n",
      "Current state: 32\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 33\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 26\n",
      "Current state: 33\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 34\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 27\n",
      "Current state: 34\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 33\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 28\n",
      "Current state: 33\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 32\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 29\n",
      "Current state: 32\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 33\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 30\n",
      "Current state: 33\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 32\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 31\n",
      "Current state: 32\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 24\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 32\n",
      "Current state: 24\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 24\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 33\n",
      "Current state: 24\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 25\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 34\n",
      "Current state: 25\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 17\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 35\n",
      "Current state: 17\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 25\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 36\n",
      "Current state: 25\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 24\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 37\n",
      "Current state: 24\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 24\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 38\n",
      "Current state: 24\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 32\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 39\n",
      "Current state: 32\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 33\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 40\n",
      "Current state: 33\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 41\n",
      "Reward: 0.0\n",
      "Terminated: True\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Episode 9\n",
      "Starting state: 0\n",
      "\n",
      "Step 0\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 1\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 1\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 2\n",
      "Current state: 1\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 3\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 4\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 5\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 1\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 6\n",
      "Current state: 1\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 7\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 1\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 8\n",
      "Current state: 1\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 2\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 9\n",
      "Current state: 2\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 2\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 10\n",
      "Current state: 2\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 10\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 11\n",
      "Current state: 10\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 11\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 12\n",
      "Current state: 11\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 19\n",
      "Reward: 0.0\n",
      "Terminated: True\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Episode 10\n",
      "Starting state: 0\n",
      "\n",
      "Step 0\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 8\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 1\n",
      "Current state: 8\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 16\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 2\n",
      "Current state: 16\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 16\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 3\n",
      "Current state: 16\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 16\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 4\n",
      "Current state: 16\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 8\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 5\n",
      "Current state: 8\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 9\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 6\n",
      "Current state: 9\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 10\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 7\n",
      "Current state: 10\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 11\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 8\n",
      "Current state: 11\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 12\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 9\n",
      "Current state: 12\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 4\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 10\n",
      "Current state: 4\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 3\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 11\n",
      "Current state: 3\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 3\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 12\n",
      "Current state: 3\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 2\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 13\n",
      "Current state: 2\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 3\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 14\n",
      "Current state: 3\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 4\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 15\n",
      "Current state: 4\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 12\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 16\n",
      "Current state: 12\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 11\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 17\n",
      "Current state: 11\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 10\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 18\n",
      "Current state: 10\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 2\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 19\n",
      "Current state: 2\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 2\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 20\n",
      "Current state: 2\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 3\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 21\n",
      "Current state: 3\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 4\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 22\n",
      "Current state: 4\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 12\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 23\n",
      "Current state: 12\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 11\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 24\n",
      "Current state: 11\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 3\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 25\n",
      "Current state: 3\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 11\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 26\n",
      "Current state: 11\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 3\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 27\n",
      "Current state: 3\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 3\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 28\n",
      "Current state: 3\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 2\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 29\n",
      "Current state: 2\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 2\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 30\n",
      "Current state: 2\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 2\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 31\n",
      "Current state: 2\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 3\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 32\n",
      "Current state: 3\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 3\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 33\n",
      "Current state: 3\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 11\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 34\n",
      "Current state: 11\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 12\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 35\n",
      "Current state: 12\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 13\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 36\n",
      "Current state: 13\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 21\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 37\n",
      "Current state: 21\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 22\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 38\n",
      "Current state: 22\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 23\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 39\n",
      "Current state: 23\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 31\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 40\n",
      "Current state: 31\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 31\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 41\n",
      "Current state: 31\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 23\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 42\n",
      "Current state: 23\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 31\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 43\n",
      "Current state: 31\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 30\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 44\n",
      "Current state: 30\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 38\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 45\n",
      "Current state: 38\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 46\n",
      "Reward: 0.0\n",
      "Terminated: True\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Successful episodes so far: []\n",
      "\n",
      "Episode 11\n",
      "Starting state: 0\n",
      "\n",
      "Step 0\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 8\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 1\n",
      "Current state: 8\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 2\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 3\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 4\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 1\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 5\n",
      "Current state: 1\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 2\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 6\n",
      "Current state: 2\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 1\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 7\n",
      "Current state: 1\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 9\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 8\n",
      "Current state: 9\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 8\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 9\n",
      "Current state: 8\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 9\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 10\n",
      "Current state: 9\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 8\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 11\n",
      "Current state: 8\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 9\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 12\n",
      "Current state: 9\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 10\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 13\n",
      "Current state: 10\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 18\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 14\n",
      "Current state: 18\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 19\n",
      "Reward: 0.0\n",
      "Terminated: True\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Episode 12\n",
      "Starting state: 0\n",
      "\n",
      "Step 0\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 8\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 1\n",
      "Current state: 8\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 9\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 2\n",
      "Current state: 9\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 17\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 3\n",
      "Current state: 17\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 9\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 4\n",
      "Current state: 9\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 8\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 5\n",
      "Current state: 8\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 8\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 6\n",
      "Current state: 8\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 9\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 7\n",
      "Current state: 9\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 17\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 8\n",
      "Current state: 17\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 16\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 9\n",
      "Current state: 16\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 17\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 10\n",
      "Current state: 17\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 18\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 11\n",
      "Current state: 18\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 19\n",
      "Reward: 0.0\n",
      "Terminated: True\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Episode 13\n",
      "Starting state: 0\n",
      "\n",
      "Step 0\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 1\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 2\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 1\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 3\n",
      "Current state: 1\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 2\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 4\n",
      "Current state: 2\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 2\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 5\n",
      "Current state: 2\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 1\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 6\n",
      "Current state: 1\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 2\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 7\n",
      "Current state: 2\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 2\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 8\n",
      "Current state: 2\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 3\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 9\n",
      "Current state: 3\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 2\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 10\n",
      "Current state: 2\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 3\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 11\n",
      "Current state: 3\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 11\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 12\n",
      "Current state: 11\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 12\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 13\n",
      "Current state: 12\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 13\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 14\n",
      "Current state: 13\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 5\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 15\n",
      "Current state: 5\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 4\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 16\n",
      "Current state: 4\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 4\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 17\n",
      "Current state: 4\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 3\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 18\n",
      "Current state: 3\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 4\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 19\n",
      "Current state: 4\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 4\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 20\n",
      "Current state: 4\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 3\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 21\n",
      "Current state: 3\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 11\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 22\n",
      "Current state: 11\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 12\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 23\n",
      "Current state: 12\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 11\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 24\n",
      "Current state: 11\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 10\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 25\n",
      "Current state: 10\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 9\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 26\n",
      "Current state: 9\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 8\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 27\n",
      "Current state: 8\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 16\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 28\n",
      "Current state: 16\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 8\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 29\n",
      "Current state: 8\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 16\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 30\n",
      "Current state: 16\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 16\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 31\n",
      "Current state: 16\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 8\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 32\n",
      "Current state: 8\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 9\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 33\n",
      "Current state: 9\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 1\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 34\n",
      "Current state: 1\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 2\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 35\n",
      "Current state: 2\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 10\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 36\n",
      "Current state: 10\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 18\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 37\n",
      "Current state: 18\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 19\n",
      "Reward: 0.0\n",
      "Terminated: True\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Episode 14\n",
      "Starting state: 0\n",
      "\n",
      "Step 0\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 1\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 1\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 2\n",
      "Current state: 1\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 3\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 1\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 4\n",
      "Current state: 1\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 5\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 6\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 1\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 7\n",
      "Current state: 1\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 8\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 9\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 1\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 10\n",
      "Current state: 1\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 11\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 1\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 12\n",
      "Current state: 1\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 13\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 14\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 1\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 15\n",
      "Current state: 1\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 16\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 8\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 17\n",
      "Current state: 8\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 9\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 18\n",
      "Current state: 9\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 17\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 19\n",
      "Current state: 17\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 18\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 20\n",
      "Current state: 18\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 17\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 21\n",
      "Current state: 17\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 9\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 22\n",
      "Current state: 9\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 8\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 23\n",
      "Current state: 8\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 24\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 25\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 26\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 27\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 8\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 28\n",
      "Current state: 8\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 9\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 29\n",
      "Current state: 9\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 1\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 30\n",
      "Current state: 1\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 1\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 31\n",
      "Current state: 1\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 1\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 32\n",
      "Current state: 1\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 1\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 33\n",
      "Current state: 1\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 1\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 34\n",
      "Current state: 1\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 35\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 36\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 37\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 38\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 8\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 39\n",
      "Current state: 8\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 40\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 41\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 42\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 8\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 43\n",
      "Current state: 8\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 8\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 44\n",
      "Current state: 8\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 8\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 45\n",
      "Current state: 8\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 16\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 46\n",
      "Current state: 16\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 24\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 47\n",
      "Current state: 24\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 25\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 48\n",
      "Current state: 25\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 24\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 49\n",
      "Current state: 24\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 24\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 50\n",
      "Current state: 24\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 24\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 51\n",
      "Current state: 24\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 24\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 52\n",
      "Current state: 24\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 24\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 53\n",
      "Current state: 24\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 32\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 54\n",
      "Current state: 32\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 33\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 55\n",
      "Current state: 33\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 34\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 56\n",
      "Current state: 34\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 42\n",
      "Reward: 0.0\n",
      "Terminated: True\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Episode 15\n",
      "Starting state: 0\n",
      "\n",
      "Step 0\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 1\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 1\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 2\n",
      "Current state: 1\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 2\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 3\n",
      "Current state: 2\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 10\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 4\n",
      "Current state: 10\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 11\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 5\n",
      "Current state: 11\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 10\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 6\n",
      "Current state: 10\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 2\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 7\n",
      "Current state: 2\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 3\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 8\n",
      "Current state: 3\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 2\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 9\n",
      "Current state: 2\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 3\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 10\n",
      "Current state: 3\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 3\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 11\n",
      "Current state: 3\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 11\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 12\n",
      "Current state: 11\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 3\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 13\n",
      "Current state: 3\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 3\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 14\n",
      "Current state: 3\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 3\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 15\n",
      "Current state: 3\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 11\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 16\n",
      "Current state: 11\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 19\n",
      "Reward: 0.0\n",
      "Terminated: True\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Episode 16\n",
      "Starting state: 0\n",
      "\n",
      "Step 0\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 8\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 1\n",
      "Current state: 8\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 9\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 2\n",
      "Current state: 9\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 10\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 3\n",
      "Current state: 10\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 18\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 4\n",
      "Current state: 18\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 17\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 5\n",
      "Current state: 17\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 9\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 6\n",
      "Current state: 9\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 1\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 7\n",
      "Current state: 1\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 9\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 8\n",
      "Current state: 9\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 8\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 9\n",
      "Current state: 8\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 9\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 10\n",
      "Current state: 9\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 10\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 11\n",
      "Current state: 10\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 18\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 12\n",
      "Current state: 18\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 10\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 13\n",
      "Current state: 10\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 18\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 14\n",
      "Current state: 18\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 10\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 15\n",
      "Current state: 10\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 11\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 16\n",
      "Current state: 11\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 3\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 17\n",
      "Current state: 3\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 3\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 18\n",
      "Current state: 3\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 2\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 19\n",
      "Current state: 2\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 2\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 20\n",
      "Current state: 2\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 2\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 21\n",
      "Current state: 2\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 3\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 22\n",
      "Current state: 3\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 4\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 23\n",
      "Current state: 4\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 5\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 24\n",
      "Current state: 5\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 13\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 25\n",
      "Current state: 13\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 5\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 26\n",
      "Current state: 5\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 5\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 27\n",
      "Current state: 5\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 13\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 28\n",
      "Current state: 13\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 14\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 29\n",
      "Current state: 14\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 6\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 30\n",
      "Current state: 6\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 5\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 31\n",
      "Current state: 5\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 4\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 32\n",
      "Current state: 4\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 12\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 33\n",
      "Current state: 12\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 13\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 34\n",
      "Current state: 13\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 12\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 35\n",
      "Current state: 12\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 20\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 36\n",
      "Current state: 20\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 19\n",
      "Reward: 0.0\n",
      "Terminated: True\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Episode 17\n",
      "Starting state: 0\n",
      "\n",
      "Step 0\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 8\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 1\n",
      "Current state: 8\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 2\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 8\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 3\n",
      "Current state: 8\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 4\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 8\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 5\n",
      "Current state: 8\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 6\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 8\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 7\n",
      "Current state: 8\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 8\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 8\n",
      "Current state: 8\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 9\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 10\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 1\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 11\n",
      "Current state: 1\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 2\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 12\n",
      "Current state: 2\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 1\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 13\n",
      "Current state: 1\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 9\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 14\n",
      "Current state: 9\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 8\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 15\n",
      "Current state: 8\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 16\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 16\n",
      "Current state: 16\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 8\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 17\n",
      "Current state: 8\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 18\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 1\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 19\n",
      "Current state: 1\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 1\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 20\n",
      "Current state: 1\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 9\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 21\n",
      "Current state: 9\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 17\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 22\n",
      "Current state: 17\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 16\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 23\n",
      "Current state: 16\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 24\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 24\n",
      "Current state: 24\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 25\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 25\n",
      "Current state: 25\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 17\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 26\n",
      "Current state: 17\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 18\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 27\n",
      "Current state: 18\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 17\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 28\n",
      "Current state: 17\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 9\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 29\n",
      "Current state: 9\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 17\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 30\n",
      "Current state: 17\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 16\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 31\n",
      "Current state: 16\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 17\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 32\n",
      "Current state: 17\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 16\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 33\n",
      "Current state: 16\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 16\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 34\n",
      "Current state: 16\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 8\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 35\n",
      "Current state: 8\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 9\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 36\n",
      "Current state: 9\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 10\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 37\n",
      "Current state: 10\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 18\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 38\n",
      "Current state: 18\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 26\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 39\n",
      "Current state: 26\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 27\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 40\n",
      "Current state: 27\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 19\n",
      "Reward: 0.0\n",
      "Terminated: True\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Episode 18\n",
      "Starting state: 0\n",
      "\n",
      "Step 0\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 1\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 1\n",
      "Current state: 1\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 1\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 2\n",
      "Current state: 1\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 2\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 3\n",
      "Current state: 2\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 10\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 4\n",
      "Current state: 10\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 18\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 5\n",
      "Current state: 18\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 17\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 6\n",
      "Current state: 17\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 25\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 7\n",
      "Current state: 25\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 24\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 8\n",
      "Current state: 24\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 25\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 9\n",
      "Current state: 25\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 26\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 10\n",
      "Current state: 26\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 18\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 11\n",
      "Current state: 18\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 17\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 12\n",
      "Current state: 17\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 16\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 13\n",
      "Current state: 16\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 16\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 14\n",
      "Current state: 16\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 8\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 15\n",
      "Current state: 8\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 9\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 16\n",
      "Current state: 9\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 17\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 17\n",
      "Current state: 17\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 18\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 18\n",
      "Current state: 18\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 19\n",
      "Reward: 0.0\n",
      "Terminated: True\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Episode 19\n",
      "Starting state: 0\n",
      "\n",
      "Step 0\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 8\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 1\n",
      "Current state: 8\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 2\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 3\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 4\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 5\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 1\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 6\n",
      "Current state: 1\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 9\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 7\n",
      "Current state: 9\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 17\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 8\n",
      "Current state: 17\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 16\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 9\n",
      "Current state: 16\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 24\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 10\n",
      "Current state: 24\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 32\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 11\n",
      "Current state: 32\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 32\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 12\n",
      "Current state: 32\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 32\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 13\n",
      "Current state: 32\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 40\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 14\n",
      "Current state: 40\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 32\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 15\n",
      "Current state: 32\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 24\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 16\n",
      "Current state: 24\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 25\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 17\n",
      "Current state: 25\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 17\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 18\n",
      "Current state: 17\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 18\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 19\n",
      "Current state: 18\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 19\n",
      "Reward: 0.0\n",
      "Terminated: True\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Episode 20\n",
      "Starting state: 0\n",
      "\n",
      "Step 0\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 8\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 1\n",
      "Current state: 8\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 2\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 3\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 1\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 4\n",
      "Current state: 1\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 9\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 5\n",
      "Current state: 9\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 10\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 6\n",
      "Current state: 10\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 2\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 7\n",
      "Current state: 2\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 1\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 8\n",
      "Current state: 1\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 9\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 8\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 10\n",
      "Current state: 8\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 8\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 11\n",
      "Current state: 8\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 9\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 12\n",
      "Current state: 9\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 8\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 13\n",
      "Current state: 8\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 8\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 14\n",
      "Current state: 8\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 15\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 16\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 8\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 17\n",
      "Current state: 8\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 16\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 18\n",
      "Current state: 16\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 17\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 19\n",
      "Current state: 17\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 25\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 20\n",
      "Current state: 25\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 26\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 21\n",
      "Current state: 26\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 27\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 22\n",
      "Current state: 27\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 28\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 23\n",
      "Current state: 28\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 29\n",
      "Reward: 0.0\n",
      "Terminated: True\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Successful episodes so far: []\n",
      "\n",
      "Episode 21\n",
      "Starting state: 0\n",
      "\n",
      "Step 0\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 1\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 1\n",
      "Current state: 1\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 9\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 2\n",
      "Current state: 9\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 10\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 3\n",
      "Current state: 10\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 2\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 4\n",
      "Current state: 2\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 2\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 5\n",
      "Current state: 2\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 10\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 6\n",
      "Current state: 10\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 11\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 7\n",
      "Current state: 11\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 12\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 8\n",
      "Current state: 12\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 4\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 9\n",
      "Current state: 4\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 4\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 10\n",
      "Current state: 4\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 12\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 11\n",
      "Current state: 12\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 20\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 12\n",
      "Current state: 20\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 21\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 13\n",
      "Current state: 21\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 13\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 14\n",
      "Current state: 13\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 5\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 15\n",
      "Current state: 5\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 4\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 16\n",
      "Current state: 4\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 4\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 17\n",
      "Current state: 4\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 5\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 18\n",
      "Current state: 5\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 6\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 19\n",
      "Current state: 6\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 14\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 20\n",
      "Current state: 14\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 22\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 21\n",
      "Current state: 22\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 14\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 22\n",
      "Current state: 14\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 15\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 23\n",
      "Current state: 15\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 7\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 24\n",
      "Current state: 7\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 7\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 25\n",
      "Current state: 7\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 7\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 26\n",
      "Current state: 7\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 15\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 27\n",
      "Current state: 15\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 23\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 28\n",
      "Current state: 23\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 23\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 29\n",
      "Current state: 23\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 15\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 30\n",
      "Current state: 15\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 14\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 31\n",
      "Current state: 14\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 13\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 32\n",
      "Current state: 13\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 5\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 33\n",
      "Current state: 5\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 5\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 34\n",
      "Current state: 5\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 4\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 35\n",
      "Current state: 4\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 5\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 36\n",
      "Current state: 5\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 4\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 37\n",
      "Current state: 4\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 4\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 38\n",
      "Current state: 4\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 4\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 39\n",
      "Current state: 4\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 12\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 40\n",
      "Current state: 12\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 4\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 41\n",
      "Current state: 4\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 4\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 42\n",
      "Current state: 4\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 12\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 43\n",
      "Current state: 12\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 11\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 44\n",
      "Current state: 11\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 19\n",
      "Reward: 0.0\n",
      "Terminated: True\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Episode 22\n",
      "Starting state: 0\n",
      "\n",
      "Step 0\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 1\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 1\n",
      "Current state: 1\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 1\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 2\n",
      "Current state: 1\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 2\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 3\n",
      "Current state: 2\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 1\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 4\n",
      "Current state: 1\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 1\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 5\n",
      "Current state: 1\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 9\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 6\n",
      "Current state: 9\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 1\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 7\n",
      "Current state: 1\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 8\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 8\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 9\n",
      "Current state: 8\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 16\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 10\n",
      "Current state: 16\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 17\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 11\n",
      "Current state: 17\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 25\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 12\n",
      "Current state: 25\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 17\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 13\n",
      "Current state: 17\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 9\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 14\n",
      "Current state: 9\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 8\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 15\n",
      "Current state: 8\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 16\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 8\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 17\n",
      "Current state: 8\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 16\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 18\n",
      "Current state: 16\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 17\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 19\n",
      "Current state: 17\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 9\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 20\n",
      "Current state: 9\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 17\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 21\n",
      "Current state: 17\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 18\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 22\n",
      "Current state: 18\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 17\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 23\n",
      "Current state: 17\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 25\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 24\n",
      "Current state: 25\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 26\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 25\n",
      "Current state: 26\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 34\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 26\n",
      "Current state: 34\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 33\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 27\n",
      "Current state: 33\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 41\n",
      "Reward: 0.0\n",
      "Terminated: True\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Episode 23\n",
      "Starting state: 0\n",
      "\n",
      "Step 0\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 8\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 1\n",
      "Current state: 8\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 8\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 2\n",
      "Current state: 8\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 9\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 3\n",
      "Current state: 9\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 10\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 4\n",
      "Current state: 10\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 18\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 5\n",
      "Current state: 18\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 10\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 6\n",
      "Current state: 10\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 9\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 7\n",
      "Current state: 9\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 17\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 8\n",
      "Current state: 17\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 25\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 9\n",
      "Current state: 25\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 24\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 10\n",
      "Current state: 24\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 16\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 11\n",
      "Current state: 16\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 24\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 12\n",
      "Current state: 24\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 24\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 13\n",
      "Current state: 24\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 16\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 14\n",
      "Current state: 16\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 8\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 15\n",
      "Current state: 8\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 16\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 17\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 18\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 19\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 20\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 1\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 21\n",
      "Current state: 1\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 22\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 23\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 24\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 25\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 26\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 1\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 27\n",
      "Current state: 1\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 28\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 1\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 29\n",
      "Current state: 1\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 30\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 31\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 8\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 32\n",
      "Current state: 8\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 33\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 34\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 1\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 35\n",
      "Current state: 1\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 2\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 36\n",
      "Current state: 2\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 2\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 37\n",
      "Current state: 2\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 10\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 38\n",
      "Current state: 10\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 18\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 39\n",
      "Current state: 18\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 19\n",
      "Reward: 0.0\n",
      "Terminated: True\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Episode 24\n",
      "Starting state: 0\n",
      "\n",
      "Step 0\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 1\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 2\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 1\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 3\n",
      "Current state: 1\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 4\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 8\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 5\n",
      "Current state: 8\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 6\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 8\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 7\n",
      "Current state: 8\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 8\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 8\n",
      "Current state: 8\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 9\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 8\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 10\n",
      "Current state: 8\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 11\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 1\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 12\n",
      "Current state: 1\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 13\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 14\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 15\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 1\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 16\n",
      "Current state: 1\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 2\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 17\n",
      "Current state: 2\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 2\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 18\n",
      "Current state: 2\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 1\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 19\n",
      "Current state: 1\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 1\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 20\n",
      "Current state: 1\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 21\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 1\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 22\n",
      "Current state: 1\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 2\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 23\n",
      "Current state: 2\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 1\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 24\n",
      "Current state: 1\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 1\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 25\n",
      "Current state: 1\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 26\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 1\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 27\n",
      "Current state: 1\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 2\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 28\n",
      "Current state: 2\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 2\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 29\n",
      "Current state: 2\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 1\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 30\n",
      "Current state: 1\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 1\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 31\n",
      "Current state: 1\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 1\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 32\n",
      "Current state: 1\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 2\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 33\n",
      "Current state: 2\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 1\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 34\n",
      "Current state: 1\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 9\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 35\n",
      "Current state: 9\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 8\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 36\n",
      "Current state: 8\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 16\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 37\n",
      "Current state: 16\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 8\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 38\n",
      "Current state: 8\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 9\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 39\n",
      "Current state: 9\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 10\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 40\n",
      "Current state: 10\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 11\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 41\n",
      "Current state: 11\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 12\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 42\n",
      "Current state: 12\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 13\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 43\n",
      "Current state: 13\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 5\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 44\n",
      "Current state: 5\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 13\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 45\n",
      "Current state: 13\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 14\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 46\n",
      "Current state: 14\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 15\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 47\n",
      "Current state: 15\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 15\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 48\n",
      "Current state: 15\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 15\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 49\n",
      "Current state: 15\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 23\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 50\n",
      "Current state: 23\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 22\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 51\n",
      "Current state: 22\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 30\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 52\n",
      "Current state: 30\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 31\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 53\n",
      "Current state: 31\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 23\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 54\n",
      "Current state: 23\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 15\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 55\n",
      "Current state: 15\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 14\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 56\n",
      "Current state: 14\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 13\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 57\n",
      "Current state: 13\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 12\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 58\n",
      "Current state: 12\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 13\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 59\n",
      "Current state: 13\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 14\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 60\n",
      "Current state: 14\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 13\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 61\n",
      "Current state: 13\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 14\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 62\n",
      "Current state: 14\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 22\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 63\n",
      "Current state: 22\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 30\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 64\n",
      "Current state: 30\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 31\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 65\n",
      "Current state: 31\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 23\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 66\n",
      "Current state: 23\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 23\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 67\n",
      "Current state: 23\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 23\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 68\n",
      "Current state: 23\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 15\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 69\n",
      "Current state: 15\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 7\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 70\n",
      "Current state: 7\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 7\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 71\n",
      "Current state: 7\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 6\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 72\n",
      "Current state: 6\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 7\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 73\n",
      "Current state: 7\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 7\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 74\n",
      "Current state: 7\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 15\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 75\n",
      "Current state: 15\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 14\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 76\n",
      "Current state: 14\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 6\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 77\n",
      "Current state: 6\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 6\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 78\n",
      "Current state: 6\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 7\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 79\n",
      "Current state: 7\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 7\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 80\n",
      "Current state: 7\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 15\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 81\n",
      "Current state: 15\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 7\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 82\n",
      "Current state: 7\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 6\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 83\n",
      "Current state: 6\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 7\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 84\n",
      "Current state: 7\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 15\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 85\n",
      "Current state: 15\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 7\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 86\n",
      "Current state: 7\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 6\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 87\n",
      "Current state: 6\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 14\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 88\n",
      "Current state: 14\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 15\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 89\n",
      "Current state: 15\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 7\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 90\n",
      "Current state: 7\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 7\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 91\n",
      "Current state: 7\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 7\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 92\n",
      "Current state: 7\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 7\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 93\n",
      "Current state: 7\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 7\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 94\n",
      "Current state: 7\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 6\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 95\n",
      "Current state: 6\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 7\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 96\n",
      "Current state: 7\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 7\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 97\n",
      "Current state: 7\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 7\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 98\n",
      "Current state: 7\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 15\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 99\n",
      "Current state: 15\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 15\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Episode 25\n",
      "Starting state: 0\n",
      "\n",
      "Step 0\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 1\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 1\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 2\n",
      "Current state: 1\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 2\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 3\n",
      "Current state: 2\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 10\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 4\n",
      "Current state: 10\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 18\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 5\n",
      "Current state: 18\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 26\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 6\n",
      "Current state: 26\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 18\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 7\n",
      "Current state: 18\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 19\n",
      "Reward: 0.0\n",
      "Terminated: True\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Episode 26\n",
      "Starting state: 0\n",
      "\n",
      "Step 0\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 1\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 1\n",
      "Current state: 1\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 2\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 2\n",
      "Current state: 2\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 1\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 3\n",
      "Current state: 1\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 9\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 4\n",
      "Current state: 9\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 17\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 5\n",
      "Current state: 17\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 9\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 6\n",
      "Current state: 9\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 17\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 7\n",
      "Current state: 17\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 25\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 8\n",
      "Current state: 25\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 17\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 9\n",
      "Current state: 17\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 16\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 10\n",
      "Current state: 16\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 8\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 11\n",
      "Current state: 8\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 9\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 12\n",
      "Current state: 9\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 1\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 13\n",
      "Current state: 1\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 14\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 15\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 16\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 8\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 17\n",
      "Current state: 8\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 9\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 18\n",
      "Current state: 9\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 1\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 19\n",
      "Current state: 1\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 9\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 20\n",
      "Current state: 9\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 1\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 21\n",
      "Current state: 1\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 9\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 22\n",
      "Current state: 9\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 17\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 23\n",
      "Current state: 17\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 9\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 24\n",
      "Current state: 9\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 10\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 25\n",
      "Current state: 10\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 11\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 26\n",
      "Current state: 11\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 3\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 27\n",
      "Current state: 3\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 3\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 28\n",
      "Current state: 3\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 3\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 29\n",
      "Current state: 3\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 3\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 30\n",
      "Current state: 3\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 3\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 31\n",
      "Current state: 3\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 11\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 32\n",
      "Current state: 11\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 19\n",
      "Reward: 0.0\n",
      "Terminated: True\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Episode 27\n",
      "Starting state: 0\n",
      "\n",
      "Step 0\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 8\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 1\n",
      "Current state: 8\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 8\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 2\n",
      "Current state: 8\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 3\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 4\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 8\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 5\n",
      "Current state: 8\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 16\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 6\n",
      "Current state: 16\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 8\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 7\n",
      "Current state: 8\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 9\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 8\n",
      "Current state: 9\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 8\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 9\n",
      "Current state: 8\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 8\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 10\n",
      "Current state: 8\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 16\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 11\n",
      "Current state: 16\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 24\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 12\n",
      "Current state: 24\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 24\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 13\n",
      "Current state: 24\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 24\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 14\n",
      "Current state: 24\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 24\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 15\n",
      "Current state: 24\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 24\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 16\n",
      "Current state: 24\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 32\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 17\n",
      "Current state: 32\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 24\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 18\n",
      "Current state: 24\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 32\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 19\n",
      "Current state: 32\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 33\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 20\n",
      "Current state: 33\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 41\n",
      "Reward: 0.0\n",
      "Terminated: True\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Episode 28\n",
      "Starting state: 0\n",
      "\n",
      "Step 0\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 8\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 1\n",
      "Current state: 8\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 9\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 2\n",
      "Current state: 9\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 17\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 3\n",
      "Current state: 17\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 25\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 4\n",
      "Current state: 25\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 17\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 5\n",
      "Current state: 17\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 18\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 6\n",
      "Current state: 18\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 10\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 7\n",
      "Current state: 10\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 11\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 8\n",
      "Current state: 11\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 10\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 9\n",
      "Current state: 10\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 11\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 10\n",
      "Current state: 11\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 3\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 11\n",
      "Current state: 3\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 11\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 12\n",
      "Current state: 11\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 12\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 13\n",
      "Current state: 12\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 4\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 14\n",
      "Current state: 4\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 5\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 15\n",
      "Current state: 5\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 4\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 16\n",
      "Current state: 4\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 5\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 17\n",
      "Current state: 5\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 6\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 18\n",
      "Current state: 6\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 14\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 19\n",
      "Current state: 14\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 6\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 20\n",
      "Current state: 6\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 5\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 21\n",
      "Current state: 5\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 6\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 22\n",
      "Current state: 6\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 5\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 23\n",
      "Current state: 5\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 5\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 24\n",
      "Current state: 5\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 4\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 25\n",
      "Current state: 4\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 3\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 26\n",
      "Current state: 3\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 2\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 27\n",
      "Current state: 2\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 1\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 28\n",
      "Current state: 1\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 9\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 29\n",
      "Current state: 9\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 17\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 30\n",
      "Current state: 17\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 18\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 31\n",
      "Current state: 18\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 26\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 32\n",
      "Current state: 26\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 25\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 33\n",
      "Current state: 25\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 24\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 34\n",
      "Current state: 24\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 32\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 35\n",
      "Current state: 32\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 24\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 36\n",
      "Current state: 24\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 16\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 37\n",
      "Current state: 16\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 17\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 38\n",
      "Current state: 17\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 18\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 39\n",
      "Current state: 18\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 26\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 40\n",
      "Current state: 26\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 27\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 41\n",
      "Current state: 27\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 28\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 42\n",
      "Current state: 28\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 29\n",
      "Reward: 0.0\n",
      "Terminated: True\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Episode 29\n",
      "Starting state: 0\n",
      "\n",
      "Step 0\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 8\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 1\n",
      "Current state: 8\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 9\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 2\n",
      "Current state: 9\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 10\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 3\n",
      "Current state: 10\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 9\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 4\n",
      "Current state: 9\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 17\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 5\n",
      "Current state: 17\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 16\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 6\n",
      "Current state: 16\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 24\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 7\n",
      "Current state: 24\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 24\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 8\n",
      "Current state: 24\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 25\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 9\n",
      "Current state: 25\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 33\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 10\n",
      "Current state: 33\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 25\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 11\n",
      "Current state: 25\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 24\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 12\n",
      "Current state: 24\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 25\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 13\n",
      "Current state: 25\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 33\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 14\n",
      "Current state: 33\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 34\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 15\n",
      "Current state: 34\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 42\n",
      "Reward: 0.0\n",
      "Terminated: True\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Episode 30\n",
      "Starting state: 0\n",
      "\n",
      "Step 0\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 1\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 2\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 3\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 1\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 4\n",
      "Current state: 1\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 2\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 5\n",
      "Current state: 2\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 1\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 6\n",
      "Current state: 1\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 2\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 7\n",
      "Current state: 2\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 10\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 8\n",
      "Current state: 10\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 18\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 9\n",
      "Current state: 18\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 17\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 10\n",
      "Current state: 17\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 18\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 11\n",
      "Current state: 18\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 19\n",
      "Reward: 0.0\n",
      "Terminated: True\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Successful episodes so far: []\n",
      "\n",
      "Episode 31\n",
      "Starting state: 0\n",
      "\n",
      "Step 0\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 8\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 1\n",
      "Current state: 8\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 9\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 2\n",
      "Current state: 9\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 10\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 3\n",
      "Current state: 10\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 2\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 4\n",
      "Current state: 2\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 2\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 5\n",
      "Current state: 2\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 2\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 6\n",
      "Current state: 2\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 2\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 7\n",
      "Current state: 2\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 1\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 8\n",
      "Current state: 1\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 2\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 9\n",
      "Current state: 2\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 2\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 10\n",
      "Current state: 2\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 1\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 11\n",
      "Current state: 1\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 2\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 12\n",
      "Current state: 2\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 2\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 13\n",
      "Current state: 2\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 3\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 14\n",
      "Current state: 3\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 2\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 15\n",
      "Current state: 2\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 3\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 16\n",
      "Current state: 3\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 4\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 17\n",
      "Current state: 4\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 12\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 18\n",
      "Current state: 12\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 13\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 19\n",
      "Current state: 13\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 14\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 20\n",
      "Current state: 14\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 15\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 21\n",
      "Current state: 15\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 7\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 22\n",
      "Current state: 7\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 6\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 23\n",
      "Current state: 6\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 6\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 24\n",
      "Current state: 6\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 7\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 25\n",
      "Current state: 7\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 7\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 26\n",
      "Current state: 7\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 6\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 27\n",
      "Current state: 6\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 14\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 28\n",
      "Current state: 14\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 13\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 29\n",
      "Current state: 13\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 12\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 30\n",
      "Current state: 12\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 11\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 31\n",
      "Current state: 11\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 10\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 32\n",
      "Current state: 10\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 2\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 33\n",
      "Current state: 2\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 2\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 34\n",
      "Current state: 2\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 2\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 35\n",
      "Current state: 2\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 2\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 36\n",
      "Current state: 2\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 10\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 37\n",
      "Current state: 10\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 18\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 38\n",
      "Current state: 18\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 10\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 39\n",
      "Current state: 10\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 9\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 40\n",
      "Current state: 9\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 17\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 41\n",
      "Current state: 17\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 25\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 42\n",
      "Current state: 25\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 33\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 43\n",
      "Current state: 33\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 34\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 44\n",
      "Current state: 34\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 35\n",
      "Reward: 0.0\n",
      "Terminated: True\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Episode 32\n",
      "Starting state: 0\n",
      "\n",
      "Step 0\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 1\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 1\n",
      "Current state: 1\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 2\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 3\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 4\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 5\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 6\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 7\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 8\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 9\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 8\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 10\n",
      "Current state: 8\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 8\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 11\n",
      "Current state: 8\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 8\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 12\n",
      "Current state: 8\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 13\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 14\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 15\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 16\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 17\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 8\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 18\n",
      "Current state: 8\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 8\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 19\n",
      "Current state: 8\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 8\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 20\n",
      "Current state: 8\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 8\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 21\n",
      "Current state: 8\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 16\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 22\n",
      "Current state: 16\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 8\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 23\n",
      "Current state: 8\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 8\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 24\n",
      "Current state: 8\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 9\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 25\n",
      "Current state: 9\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 10\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 26\n",
      "Current state: 10\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 11\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 27\n",
      "Current state: 11\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 10\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 28\n",
      "Current state: 10\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 2\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 29\n",
      "Current state: 2\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 3\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 30\n",
      "Current state: 3\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 11\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 31\n",
      "Current state: 11\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 3\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 32\n",
      "Current state: 3\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 3\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 33\n",
      "Current state: 3\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 2\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 34\n",
      "Current state: 2\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 10\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 35\n",
      "Current state: 10\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 11\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 36\n",
      "Current state: 11\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 19\n",
      "Reward: 0.0\n",
      "Terminated: True\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Episode 33\n",
      "Starting state: 0\n",
      "\n",
      "Step 0\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 1\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 8\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 2\n",
      "Current state: 8\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 16\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 3\n",
      "Current state: 16\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 17\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 4\n",
      "Current state: 17\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 25\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 5\n",
      "Current state: 25\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 17\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 6\n",
      "Current state: 17\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 16\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 7\n",
      "Current state: 16\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 24\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 8\n",
      "Current state: 24\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 32\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 9\n",
      "Current state: 32\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 24\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 10\n",
      "Current state: 24\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 25\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 11\n",
      "Current state: 25\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 26\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 12\n",
      "Current state: 26\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 18\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 13\n",
      "Current state: 18\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 17\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 14\n",
      "Current state: 17\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 25\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 15\n",
      "Current state: 25\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 17\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 16\n",
      "Current state: 17\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 25\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 17\n",
      "Current state: 25\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 24\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 18\n",
      "Current state: 24\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 24\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 19\n",
      "Current state: 24\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 16\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 20\n",
      "Current state: 16\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 17\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 21\n",
      "Current state: 17\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 16\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 22\n",
      "Current state: 16\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 24\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 23\n",
      "Current state: 24\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 16\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 24\n",
      "Current state: 16\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 17\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 25\n",
      "Current state: 17\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 16\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 26\n",
      "Current state: 16\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 17\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 27\n",
      "Current state: 17\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 18\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 28\n",
      "Current state: 18\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 17\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 29\n",
      "Current state: 17\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 18\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 30\n",
      "Current state: 18\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 17\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 31\n",
      "Current state: 17\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 18\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 32\n",
      "Current state: 18\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 10\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 33\n",
      "Current state: 10\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 18\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 34\n",
      "Current state: 18\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 19\n",
      "Reward: 0.0\n",
      "Terminated: True\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Episode 34\n",
      "Starting state: 0\n",
      "\n",
      "Step 0\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 1\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 1\n",
      "Current state: 1\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 2\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 2\n",
      "Current state: 2\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 1\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 3\n",
      "Current state: 1\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 4\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 1\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 5\n",
      "Current state: 1\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 6\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 8\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 7\n",
      "Current state: 8\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 8\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 9\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 10\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 11\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 12\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 1\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 13\n",
      "Current state: 1\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 9\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 14\n",
      "Current state: 9\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 10\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 15\n",
      "Current state: 10\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 18\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 16\n",
      "Current state: 18\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 19\n",
      "Reward: 0.0\n",
      "Terminated: True\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Episode 35\n",
      "Starting state: 0\n",
      "\n",
      "Step 0\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 1\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 1\n",
      "Current state: 1\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 2\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 2\n",
      "Current state: 2\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 2\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 3\n",
      "Current state: 2\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 10\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 4\n",
      "Current state: 10\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 9\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 5\n",
      "Current state: 9\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 1\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 6\n",
      "Current state: 1\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 2\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 7\n",
      "Current state: 2\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 3\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 8\n",
      "Current state: 3\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 11\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 9\n",
      "Current state: 11\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 3\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 10\n",
      "Current state: 3\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 2\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 11\n",
      "Current state: 2\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 3\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 12\n",
      "Current state: 3\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 11\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 13\n",
      "Current state: 11\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 12\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 14\n",
      "Current state: 12\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 4\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 15\n",
      "Current state: 4\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 4\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 16\n",
      "Current state: 4\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 5\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 17\n",
      "Current state: 5\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 6\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 18\n",
      "Current state: 6\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 7\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 19\n",
      "Current state: 7\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 7\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 20\n",
      "Current state: 7\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 15\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 21\n",
      "Current state: 15\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 23\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 22\n",
      "Current state: 23\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 15\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 23\n",
      "Current state: 15\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 23\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 24\n",
      "Current state: 23\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 15\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 25\n",
      "Current state: 15\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 14\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 26\n",
      "Current state: 14\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 13\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 27\n",
      "Current state: 13\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 12\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 28\n",
      "Current state: 12\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 4\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 29\n",
      "Current state: 4\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 12\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 30\n",
      "Current state: 12\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 13\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 31\n",
      "Current state: 13\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 21\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 32\n",
      "Current state: 21\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 20\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 33\n",
      "Current state: 20\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 28\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 34\n",
      "Current state: 28\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 36\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 35\n",
      "Current state: 36\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 28\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 36\n",
      "Current state: 28\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 20\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 37\n",
      "Current state: 20\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 21\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 38\n",
      "Current state: 21\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 13\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 39\n",
      "Current state: 13\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 5\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 40\n",
      "Current state: 5\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 5\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 41\n",
      "Current state: 5\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 5\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 42\n",
      "Current state: 5\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 4\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 43\n",
      "Current state: 4\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 5\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 44\n",
      "Current state: 5\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 4\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 45\n",
      "Current state: 4\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 5\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 46\n",
      "Current state: 5\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 5\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 47\n",
      "Current state: 5\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 6\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 48\n",
      "Current state: 6\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 14\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 49\n",
      "Current state: 14\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 15\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 50\n",
      "Current state: 15\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 15\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 51\n",
      "Current state: 15\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 23\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 52\n",
      "Current state: 23\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 15\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 53\n",
      "Current state: 15\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 14\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 54\n",
      "Current state: 14\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 6\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 55\n",
      "Current state: 6\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 6\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 56\n",
      "Current state: 6\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 5\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 57\n",
      "Current state: 5\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 5\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 58\n",
      "Current state: 5\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 6\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 59\n",
      "Current state: 6\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 7\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 60\n",
      "Current state: 7\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 6\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 61\n",
      "Current state: 6\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 6\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 62\n",
      "Current state: 6\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 7\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 63\n",
      "Current state: 7\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 7\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 64\n",
      "Current state: 7\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 7\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 65\n",
      "Current state: 7\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 7\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 66\n",
      "Current state: 7\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 6\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 67\n",
      "Current state: 6\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 14\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 68\n",
      "Current state: 14\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 6\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 69\n",
      "Current state: 6\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 14\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 70\n",
      "Current state: 14\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 13\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 71\n",
      "Current state: 13\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 12\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 72\n",
      "Current state: 12\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 11\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 73\n",
      "Current state: 11\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 19\n",
      "Reward: 0.0\n",
      "Terminated: True\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Episode 36\n",
      "Starting state: 0\n",
      "\n",
      "Step 0\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 1\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 1\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 2\n",
      "Current state: 1\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 2\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 3\n",
      "Current state: 2\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 3\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 4\n",
      "Current state: 3\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 3\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 5\n",
      "Current state: 3\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 4\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 6\n",
      "Current state: 4\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 3\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 7\n",
      "Current state: 3\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 2\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 8\n",
      "Current state: 2\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 1\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 9\n",
      "Current state: 1\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 1\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 10\n",
      "Current state: 1\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 2\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 11\n",
      "Current state: 2\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 3\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 12\n",
      "Current state: 3\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 3\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 13\n",
      "Current state: 3\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 2\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 14\n",
      "Current state: 2\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 3\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 15\n",
      "Current state: 3\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 3\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 16\n",
      "Current state: 3\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 4\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 17\n",
      "Current state: 4\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 12\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 18\n",
      "Current state: 12\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 13\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 19\n",
      "Current state: 13\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 14\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 20\n",
      "Current state: 14\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 15\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 21\n",
      "Current state: 15\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 7\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 22\n",
      "Current state: 7\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 6\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 23\n",
      "Current state: 6\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 7\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 24\n",
      "Current state: 7\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 7\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 25\n",
      "Current state: 7\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 6\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 26\n",
      "Current state: 6\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 7\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 27\n",
      "Current state: 7\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 15\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 28\n",
      "Current state: 15\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 7\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 29\n",
      "Current state: 7\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 7\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 30\n",
      "Current state: 7\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 15\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 31\n",
      "Current state: 15\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 7\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 32\n",
      "Current state: 7\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 7\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 33\n",
      "Current state: 7\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 6\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 34\n",
      "Current state: 6\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 5\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 35\n",
      "Current state: 5\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 13\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 36\n",
      "Current state: 13\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 21\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 37\n",
      "Current state: 21\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 22\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 38\n",
      "Current state: 22\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 21\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 39\n",
      "Current state: 21\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 13\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 40\n",
      "Current state: 13\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 21\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 41\n",
      "Current state: 21\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 13\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 42\n",
      "Current state: 13\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 5\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 43\n",
      "Current state: 5\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 4\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 44\n",
      "Current state: 4\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 12\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 45\n",
      "Current state: 12\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 20\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 46\n",
      "Current state: 20\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 28\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 47\n",
      "Current state: 28\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 20\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 48\n",
      "Current state: 20\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 12\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 49\n",
      "Current state: 12\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 11\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 50\n",
      "Current state: 11\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 12\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 51\n",
      "Current state: 12\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 4\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 52\n",
      "Current state: 4\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 5\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 53\n",
      "Current state: 5\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 13\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 54\n",
      "Current state: 13\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 5\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 55\n",
      "Current state: 5\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 4\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 56\n",
      "Current state: 4\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 3\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 57\n",
      "Current state: 3\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 4\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 58\n",
      "Current state: 4\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 5\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 59\n",
      "Current state: 5\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 4\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 60\n",
      "Current state: 4\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 3\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 61\n",
      "Current state: 3\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 11\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 62\n",
      "Current state: 11\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 19\n",
      "Reward: 0.0\n",
      "Terminated: True\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Episode 37\n",
      "Starting state: 0\n",
      "\n",
      "Step 0\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 8\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 1\n",
      "Current state: 8\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 2\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 3\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 1\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 4\n",
      "Current state: 1\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 5\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 8\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 6\n",
      "Current state: 8\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 16\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 7\n",
      "Current state: 16\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 16\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 8\n",
      "Current state: 16\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 16\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 9\n",
      "Current state: 16\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 24\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 10\n",
      "Current state: 24\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 25\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 11\n",
      "Current state: 25\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 26\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 12\n",
      "Current state: 26\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 27\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 13\n",
      "Current state: 27\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 19\n",
      "Reward: 0.0\n",
      "Terminated: True\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Episode 38\n",
      "Starting state: 0\n",
      "\n",
      "Step 0\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 1\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 8\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 2\n",
      "Current state: 8\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 3\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 4\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 1\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 5\n",
      "Current state: 1\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 6\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 1\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 7\n",
      "Current state: 1\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 9\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 8\n",
      "Current state: 9\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 8\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 9\n",
      "Current state: 8\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 10\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 11\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 8\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 12\n",
      "Current state: 8\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 13\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 8\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 14\n",
      "Current state: 8\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 15\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 1\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 16\n",
      "Current state: 1\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 9\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 17\n",
      "Current state: 9\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 17\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 18\n",
      "Current state: 17\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 18\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 19\n",
      "Current state: 18\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 10\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 20\n",
      "Current state: 10\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 9\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 21\n",
      "Current state: 9\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 1\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 22\n",
      "Current state: 1\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 2\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 23\n",
      "Current state: 2\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 10\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 24\n",
      "Current state: 10\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 9\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 25\n",
      "Current state: 9\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 1\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 26\n",
      "Current state: 1\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 9\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 27\n",
      "Current state: 9\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 17\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 28\n",
      "Current state: 17\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 16\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 29\n",
      "Current state: 16\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 8\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 30\n",
      "Current state: 8\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 9\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 31\n",
      "Current state: 9\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 17\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 32\n",
      "Current state: 17\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 18\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 33\n",
      "Current state: 18\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 10\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 34\n",
      "Current state: 10\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 2\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 35\n",
      "Current state: 2\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 2\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 36\n",
      "Current state: 2\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 1\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 37\n",
      "Current state: 1\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 38\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 39\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 40\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 8\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 41\n",
      "Current state: 8\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 42\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 8\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 43\n",
      "Current state: 8\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 9\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 44\n",
      "Current state: 9\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 10\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 45\n",
      "Current state: 10\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 18\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 46\n",
      "Current state: 18\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 10\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 47\n",
      "Current state: 10\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 2\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 48\n",
      "Current state: 2\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 10\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 49\n",
      "Current state: 10\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 9\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 50\n",
      "Current state: 9\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 10\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 51\n",
      "Current state: 10\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 18\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 52\n",
      "Current state: 18\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 10\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 53\n",
      "Current state: 10\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 11\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 54\n",
      "Current state: 11\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 10\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 55\n",
      "Current state: 10\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 18\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 56\n",
      "Current state: 18\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 10\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 57\n",
      "Current state: 10\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 9\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 58\n",
      "Current state: 9\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 17\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 59\n",
      "Current state: 17\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 9\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 60\n",
      "Current state: 9\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 8\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 61\n",
      "Current state: 8\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 16\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 62\n",
      "Current state: 16\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 16\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 63\n",
      "Current state: 16\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 24\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 64\n",
      "Current state: 24\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 24\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 65\n",
      "Current state: 24\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 25\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 66\n",
      "Current state: 25\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 33\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 67\n",
      "Current state: 33\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 41\n",
      "Reward: 0.0\n",
      "Terminated: True\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Episode 39\n",
      "Starting state: 0\n",
      "\n",
      "Step 0\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 8\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 1\n",
      "Current state: 8\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 2\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 3\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 4\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 8\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 5\n",
      "Current state: 8\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 6\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 7\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 8\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 9\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 10\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 11\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 12\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 13\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 14\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 8\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 15\n",
      "Current state: 8\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 9\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 16\n",
      "Current state: 9\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 10\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 17\n",
      "Current state: 10\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 9\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 18\n",
      "Current state: 9\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 8\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 19\n",
      "Current state: 8\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 20\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 1\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 21\n",
      "Current state: 1\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 1\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 22\n",
      "Current state: 1\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 1\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 23\n",
      "Current state: 1\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 24\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 1\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 25\n",
      "Current state: 1\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 2\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 26\n",
      "Current state: 2\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 3\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 27\n",
      "Current state: 3\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 4\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 28\n",
      "Current state: 4\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 12\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 29\n",
      "Current state: 12\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 13\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 30\n",
      "Current state: 13\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 12\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 31\n",
      "Current state: 12\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 11\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 32\n",
      "Current state: 11\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 12\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 33\n",
      "Current state: 12\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 11\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 34\n",
      "Current state: 11\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 12\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 35\n",
      "Current state: 12\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 4\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 36\n",
      "Current state: 4\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 3\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 37\n",
      "Current state: 3\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 2\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 38\n",
      "Current state: 2\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 10\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 39\n",
      "Current state: 10\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 2\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 40\n",
      "Current state: 2\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 1\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 41\n",
      "Current state: 1\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 2\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 42\n",
      "Current state: 2\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 3\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 43\n",
      "Current state: 3\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 3\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 44\n",
      "Current state: 3\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 2\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 45\n",
      "Current state: 2\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 2\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 46\n",
      "Current state: 2\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 1\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 47\n",
      "Current state: 1\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 9\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 48\n",
      "Current state: 9\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 17\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 49\n",
      "Current state: 17\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 25\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 50\n",
      "Current state: 25\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 17\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 51\n",
      "Current state: 17\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 9\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 52\n",
      "Current state: 9\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 1\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 53\n",
      "Current state: 1\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 2\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 54\n",
      "Current state: 2\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 2\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 55\n",
      "Current state: 2\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 1\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 56\n",
      "Current state: 1\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 1\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 57\n",
      "Current state: 1\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 1\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 58\n",
      "Current state: 1\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 2\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 59\n",
      "Current state: 2\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 2\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 60\n",
      "Current state: 2\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 2\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 61\n",
      "Current state: 2\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 10\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 62\n",
      "Current state: 10\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 9\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 63\n",
      "Current state: 9\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 10\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 64\n",
      "Current state: 10\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 18\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 65\n",
      "Current state: 18\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 26\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 66\n",
      "Current state: 26\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 27\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 67\n",
      "Current state: 27\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 19\n",
      "Reward: 0.0\n",
      "Terminated: True\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Episode 40\n",
      "Starting state: 0\n",
      "\n",
      "Step 0\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 8\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 1\n",
      "Current state: 8\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 2\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 1\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 3\n",
      "Current state: 1\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 1\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 4\n",
      "Current state: 1\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 5\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 6\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 7\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 8\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 8\n",
      "Current state: 8\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 16\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 9\n",
      "Current state: 16\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 16\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 10\n",
      "Current state: 16\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 17\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 11\n",
      "Current state: 17\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 9\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 12\n",
      "Current state: 9\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 10\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 13\n",
      "Current state: 10\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 9\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 14\n",
      "Current state: 9\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 17\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 15\n",
      "Current state: 17\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 9\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 16\n",
      "Current state: 9\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 1\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 17\n",
      "Current state: 1\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 1\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 18\n",
      "Current state: 1\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 19\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 20\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 8\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 21\n",
      "Current state: 8\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 16\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 22\n",
      "Current state: 16\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 24\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 23\n",
      "Current state: 24\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 24\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 24\n",
      "Current state: 24\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 16\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 25\n",
      "Current state: 16\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 17\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 26\n",
      "Current state: 17\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 9\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 27\n",
      "Current state: 9\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 10\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 28\n",
      "Current state: 10\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 9\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 29\n",
      "Current state: 9\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 10\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 30\n",
      "Current state: 10\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 18\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 31\n",
      "Current state: 18\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 19\n",
      "Reward: 0.0\n",
      "Terminated: True\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Successful episodes so far: []\n",
      "\n",
      "Episode 41\n",
      "Starting state: 0\n",
      "\n",
      "Step 0\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 1\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 1\n",
      "Current state: 1\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 2\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 3\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 8\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 4\n",
      "Current state: 8\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 5\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 6\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 8\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 7\n",
      "Current state: 8\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 8\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 8\n",
      "Current state: 8\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 9\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 1\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 10\n",
      "Current state: 1\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 1\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 11\n",
      "Current state: 1\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 2\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 12\n",
      "Current state: 2\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 2\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 13\n",
      "Current state: 2\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 1\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 14\n",
      "Current state: 1\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 9\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 15\n",
      "Current state: 9\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 10\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 16\n",
      "Current state: 10\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 11\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 17\n",
      "Current state: 11\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 10\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 18\n",
      "Current state: 10\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 18\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 19\n",
      "Current state: 18\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 17\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 20\n",
      "Current state: 17\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 16\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 21\n",
      "Current state: 16\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 16\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 22\n",
      "Current state: 16\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 16\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 23\n",
      "Current state: 16\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 16\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 24\n",
      "Current state: 16\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 17\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 25\n",
      "Current state: 17\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 25\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 26\n",
      "Current state: 25\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 33\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 27\n",
      "Current state: 33\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 25\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 28\n",
      "Current state: 25\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 33\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 29\n",
      "Current state: 33\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 32\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 30\n",
      "Current state: 32\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 33\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 31\n",
      "Current state: 33\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 34\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 32\n",
      "Current state: 34\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 26\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 33\n",
      "Current state: 26\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 27\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 34\n",
      "Current state: 27\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 26\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 35\n",
      "Current state: 26\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 27\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 36\n",
      "Current state: 27\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 19\n",
      "Reward: 0.0\n",
      "Terminated: True\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Episode 42\n",
      "Starting state: 0\n",
      "\n",
      "Step 0\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 1\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 8\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 2\n",
      "Current state: 8\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 8\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 3\n",
      "Current state: 8\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 4\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 8\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 5\n",
      "Current state: 8\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 9\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 6\n",
      "Current state: 9\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 8\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 7\n",
      "Current state: 8\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 8\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 8\n",
      "Current state: 8\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 8\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 9\n",
      "Current state: 8\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 16\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 10\n",
      "Current state: 16\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 8\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 11\n",
      "Current state: 8\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 9\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 12\n",
      "Current state: 9\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 17\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 13\n",
      "Current state: 17\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 16\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 14\n",
      "Current state: 16\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 24\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 15\n",
      "Current state: 24\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 24\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 16\n",
      "Current state: 24\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 25\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 17\n",
      "Current state: 25\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 33\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 18\n",
      "Current state: 33\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 34\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 19\n",
      "Current state: 34\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 35\n",
      "Reward: 0.0\n",
      "Terminated: True\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Episode 43\n",
      "Starting state: 0\n",
      "\n",
      "Step 0\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 1\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 8\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 2\n",
      "Current state: 8\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 8\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 3\n",
      "Current state: 8\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 9\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 4\n",
      "Current state: 9\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 17\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 5\n",
      "Current state: 17\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 18\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 6\n",
      "Current state: 18\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 17\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 7\n",
      "Current state: 17\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 18\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 8\n",
      "Current state: 18\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 10\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 9\n",
      "Current state: 10\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 2\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 10\n",
      "Current state: 2\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 2\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 11\n",
      "Current state: 2\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 3\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 12\n",
      "Current state: 3\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 4\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 13\n",
      "Current state: 4\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 3\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 14\n",
      "Current state: 3\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 3\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 15\n",
      "Current state: 3\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 4\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 16\n",
      "Current state: 4\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 5\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 17\n",
      "Current state: 5\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 13\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 18\n",
      "Current state: 13\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 5\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 19\n",
      "Current state: 5\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 5\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 20\n",
      "Current state: 5\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 5\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 21\n",
      "Current state: 5\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 5\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 22\n",
      "Current state: 5\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 4\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 23\n",
      "Current state: 4\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 4\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 24\n",
      "Current state: 4\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 3\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 25\n",
      "Current state: 3\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 3\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 26\n",
      "Current state: 3\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 4\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 27\n",
      "Current state: 4\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 12\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 28\n",
      "Current state: 12\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 20\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 29\n",
      "Current state: 20\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 12\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 30\n",
      "Current state: 12\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 13\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 31\n",
      "Current state: 13\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 12\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 32\n",
      "Current state: 12\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 4\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 33\n",
      "Current state: 4\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 3\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 34\n",
      "Current state: 3\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 3\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 35\n",
      "Current state: 3\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 4\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 36\n",
      "Current state: 4\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 5\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 37\n",
      "Current state: 5\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 6\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 38\n",
      "Current state: 6\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 6\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 39\n",
      "Current state: 6\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 14\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 40\n",
      "Current state: 14\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 13\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 41\n",
      "Current state: 13\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 21\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 42\n",
      "Current state: 21\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 20\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 43\n",
      "Current state: 20\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 21\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 44\n",
      "Current state: 21\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 22\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 45\n",
      "Current state: 22\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 30\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 46\n",
      "Current state: 30\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 38\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 47\n",
      "Current state: 38\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 37\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 48\n",
      "Current state: 37\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 36\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 49\n",
      "Current state: 36\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 37\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 50\n",
      "Current state: 37\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 45\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 51\n",
      "Current state: 45\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 37\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 52\n",
      "Current state: 37\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 29\n",
      "Reward: 0.0\n",
      "Terminated: True\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Episode 44\n",
      "Starting state: 0\n",
      "\n",
      "Step 0\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 1\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 2\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 3\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 4\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 8\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 5\n",
      "Current state: 8\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 8\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 6\n",
      "Current state: 8\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 8\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 7\n",
      "Current state: 8\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 8\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 8\n",
      "Current state: 8\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 16\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 9\n",
      "Current state: 16\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 24\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 10\n",
      "Current state: 24\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 25\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 11\n",
      "Current state: 25\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 26\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 12\n",
      "Current state: 26\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 27\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 13\n",
      "Current state: 27\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 35\n",
      "Reward: 0.0\n",
      "Terminated: True\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Episode 45\n",
      "Starting state: 0\n",
      "\n",
      "Step 0\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 1\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 1\n",
      "Current state: 1\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 1\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 2\n",
      "Current state: 1\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 9\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 3\n",
      "Current state: 9\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 8\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 4\n",
      "Current state: 8\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 16\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 5\n",
      "Current state: 16\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 8\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 6\n",
      "Current state: 8\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 9\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 7\n",
      "Current state: 9\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 1\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 8\n",
      "Current state: 1\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 1\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 9\n",
      "Current state: 1\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 9\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 10\n",
      "Current state: 9\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 8\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 11\n",
      "Current state: 8\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 16\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 12\n",
      "Current state: 16\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 8\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 13\n",
      "Current state: 8\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 9\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 14\n",
      "Current state: 9\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 1\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 15\n",
      "Current state: 1\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 1\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 16\n",
      "Current state: 1\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 2\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 17\n",
      "Current state: 2\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 10\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 18\n",
      "Current state: 10\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 9\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 19\n",
      "Current state: 9\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 1\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 20\n",
      "Current state: 1\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 1\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 21\n",
      "Current state: 1\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 22\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 1\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 23\n",
      "Current state: 1\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 2\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 24\n",
      "Current state: 2\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 2\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 25\n",
      "Current state: 2\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 3\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 26\n",
      "Current state: 3\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 3\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 27\n",
      "Current state: 3\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 3\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 28\n",
      "Current state: 3\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 3\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 29\n",
      "Current state: 3\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 3\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 30\n",
      "Current state: 3\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 4\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 31\n",
      "Current state: 4\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 3\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 32\n",
      "Current state: 3\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 11\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 33\n",
      "Current state: 11\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 12\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 34\n",
      "Current state: 12\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 11\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 35\n",
      "Current state: 11\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 3\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 36\n",
      "Current state: 3\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 4\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 37\n",
      "Current state: 4\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 3\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 38\n",
      "Current state: 3\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 11\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 39\n",
      "Current state: 11\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 3\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 40\n",
      "Current state: 3\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 3\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 41\n",
      "Current state: 3\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 4\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 42\n",
      "Current state: 4\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 12\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 43\n",
      "Current state: 12\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 20\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 44\n",
      "Current state: 20\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 19\n",
      "Reward: 0.0\n",
      "Terminated: True\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Episode 46\n",
      "Starting state: 0\n",
      "\n",
      "Step 0\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 1\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 8\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 2\n",
      "Current state: 8\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 9\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 3\n",
      "Current state: 9\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 10\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 4\n",
      "Current state: 10\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 18\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 5\n",
      "Current state: 18\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 19\n",
      "Reward: 0.0\n",
      "Terminated: True\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Episode 47\n",
      "Starting state: 0\n",
      "\n",
      "Step 0\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 1\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 8\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 2\n",
      "Current state: 8\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 9\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 3\n",
      "Current state: 9\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 17\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 4\n",
      "Current state: 17\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 16\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 5\n",
      "Current state: 16\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 8\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 6\n",
      "Current state: 8\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 16\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 7\n",
      "Current state: 16\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 8\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 8\n",
      "Current state: 8\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 16\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 9\n",
      "Current state: 16\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 16\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 10\n",
      "Current state: 16\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 24\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 11\n",
      "Current state: 24\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 24\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 12\n",
      "Current state: 24\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 32\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 13\n",
      "Current state: 32\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 33\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 14\n",
      "Current state: 33\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 25\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 15\n",
      "Current state: 25\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 17\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 16\n",
      "Current state: 17\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 18\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 17\n",
      "Current state: 18\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 17\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 18\n",
      "Current state: 17\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 16\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 19\n",
      "Current state: 16\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 17\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 20\n",
      "Current state: 17\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 18\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 21\n",
      "Current state: 18\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 17\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 22\n",
      "Current state: 17\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 16\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 23\n",
      "Current state: 16\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 16\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 24\n",
      "Current state: 16\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 24\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 25\n",
      "Current state: 24\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 24\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 26\n",
      "Current state: 24\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 16\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 27\n",
      "Current state: 16\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 8\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 28\n",
      "Current state: 8\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 29\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 8\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 30\n",
      "Current state: 8\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 8\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 31\n",
      "Current state: 8\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 16\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 32\n",
      "Current state: 16\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 17\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 33\n",
      "Current state: 17\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 9\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 34\n",
      "Current state: 9\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 17\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 35\n",
      "Current state: 17\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 18\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 36\n",
      "Current state: 18\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 10\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 37\n",
      "Current state: 10\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 9\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 38\n",
      "Current state: 9\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 10\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 39\n",
      "Current state: 10\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 2\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 40\n",
      "Current state: 2\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 3\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 41\n",
      "Current state: 3\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 11\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 42\n",
      "Current state: 11\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 3\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 43\n",
      "Current state: 3\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 3\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 44\n",
      "Current state: 3\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 2\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 45\n",
      "Current state: 2\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 1\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 46\n",
      "Current state: 1\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 2\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 47\n",
      "Current state: 2\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 1\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 48\n",
      "Current state: 1\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 9\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 49\n",
      "Current state: 9\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 17\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 50\n",
      "Current state: 17\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 18\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 51\n",
      "Current state: 18\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 26\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 52\n",
      "Current state: 26\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 25\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 53\n",
      "Current state: 25\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 33\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 54\n",
      "Current state: 33\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 41\n",
      "Reward: 0.0\n",
      "Terminated: True\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Episode 48\n",
      "Starting state: 0\n",
      "\n",
      "Step 0\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 8\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 1\n",
      "Current state: 8\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 2\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 3\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 4\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 1\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 5\n",
      "Current state: 1\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 2\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 6\n",
      "Current state: 2\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 2\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 7\n",
      "Current state: 2\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 3\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 8\n",
      "Current state: 3\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 3\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 9\n",
      "Current state: 3\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 4\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 10\n",
      "Current state: 4\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 4\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 11\n",
      "Current state: 4\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 4\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 12\n",
      "Current state: 4\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 3\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 13\n",
      "Current state: 3\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 2\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 14\n",
      "Current state: 2\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 10\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 15\n",
      "Current state: 10\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 2\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 16\n",
      "Current state: 2\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 2\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 17\n",
      "Current state: 2\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 3\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 18\n",
      "Current state: 3\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 3\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 19\n",
      "Current state: 3\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 3\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 20\n",
      "Current state: 3\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 4\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 21\n",
      "Current state: 4\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 3\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 22\n",
      "Current state: 3\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 3\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 23\n",
      "Current state: 3\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 11\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 24\n",
      "Current state: 11\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 19\n",
      "Reward: 0.0\n",
      "Terminated: True\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Episode 49\n",
      "Starting state: 0\n",
      "\n",
      "Step 0\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 1\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 1\n",
      "Current state: 1\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 2\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 8\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 3\n",
      "Current state: 8\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 4\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 5\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 6\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 7\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 8\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 8\n",
      "Current state: 8\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 9\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 9\n",
      "Current state: 9\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 1\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 10\n",
      "Current state: 1\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 1\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 11\n",
      "Current state: 1\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 1\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 12\n",
      "Current state: 1\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 9\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 13\n",
      "Current state: 9\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 1\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 14\n",
      "Current state: 1\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 15\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 16\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 1\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 17\n",
      "Current state: 1\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 9\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 18\n",
      "Current state: 9\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 10\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 19\n",
      "Current state: 10\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 18\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 20\n",
      "Current state: 18\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 26\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 21\n",
      "Current state: 26\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 27\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 22\n",
      "Current state: 27\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 19\n",
      "Reward: 0.0\n",
      "Terminated: True\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Episode 50\n",
      "Starting state: 0\n",
      "\n",
      "Step 0\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 1\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 2\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 8\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 3\n",
      "Current state: 8\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 16\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 4\n",
      "Current state: 16\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 16\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 5\n",
      "Current state: 16\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 16\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 6\n",
      "Current state: 16\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 17\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 7\n",
      "Current state: 17\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 16\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 8\n",
      "Current state: 16\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 16\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 9\n",
      "Current state: 16\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 17\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 10\n",
      "Current state: 17\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 18\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 11\n",
      "Current state: 18\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 26\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 12\n",
      "Current state: 26\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 18\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 13\n",
      "Current state: 18\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 17\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 14\n",
      "Current state: 17\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 9\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 15\n",
      "Current state: 9\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 8\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 16\n",
      "Current state: 8\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 17\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 18\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 19\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 8\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 20\n",
      "Current state: 8\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 8\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 21\n",
      "Current state: 8\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 9\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 22\n",
      "Current state: 9\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 8\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 23\n",
      "Current state: 8\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 16\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 24\n",
      "Current state: 16\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 8\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 25\n",
      "Current state: 8\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 9\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 26\n",
      "Current state: 9\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 1\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 27\n",
      "Current state: 1\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 9\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 28\n",
      "Current state: 9\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 8\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 29\n",
      "Current state: 8\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 16\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 30\n",
      "Current state: 16\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 17\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 31\n",
      "Current state: 17\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 25\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 32\n",
      "Current state: 25\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 33\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 33\n",
      "Current state: 33\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 25\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 34\n",
      "Current state: 25\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 17\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 35\n",
      "Current state: 17\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 25\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 36\n",
      "Current state: 25\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 26\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 37\n",
      "Current state: 26\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 27\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 38\n",
      "Current state: 27\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 28\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 39\n",
      "Current state: 28\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 36\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 40\n",
      "Current state: 36\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 37\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 41\n",
      "Current state: 37\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 45\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 42\n",
      "Current state: 45\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 46\n",
      "Reward: 0.0\n",
      "Terminated: True\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Successful episodes so far: []\n",
      "\n",
      "Episode 51\n",
      "Starting state: 0\n",
      "\n",
      "Step 0\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 1\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 2\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 1\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 3\n",
      "Current state: 1\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 2\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 4\n",
      "Current state: 2\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 3\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 5\n",
      "Current state: 3\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 2\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 6\n",
      "Current state: 2\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 10\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 7\n",
      "Current state: 10\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 2\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 8\n",
      "Current state: 2\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 10\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 9\n",
      "Current state: 10\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 2\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 10\n",
      "Current state: 2\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 10\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 11\n",
      "Current state: 10\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 9\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 12\n",
      "Current state: 9\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 8\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 13\n",
      "Current state: 8\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 9\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 14\n",
      "Current state: 9\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 10\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 15\n",
      "Current state: 10\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 18\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 16\n",
      "Current state: 18\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 19\n",
      "Reward: 0.0\n",
      "Terminated: True\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Episode 52\n",
      "Starting state: 0\n",
      "\n",
      "Step 0\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 1\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 8\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 2\n",
      "Current state: 8\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 3\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 4\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 5\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 1\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 6\n",
      "Current state: 1\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 1\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 7\n",
      "Current state: 1\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 8\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 9\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 1\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 10\n",
      "Current state: 1\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 11\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 12\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 1\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 13\n",
      "Current state: 1\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 1\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 14\n",
      "Current state: 1\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 2\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 15\n",
      "Current state: 2\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 10\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 16\n",
      "Current state: 10\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 11\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 17\n",
      "Current state: 11\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 12\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 18\n",
      "Current state: 12\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 4\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 19\n",
      "Current state: 4\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 4\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 20\n",
      "Current state: 4\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 12\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 21\n",
      "Current state: 12\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 11\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 22\n",
      "Current state: 11\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 10\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 23\n",
      "Current state: 10\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 9\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 24\n",
      "Current state: 9\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 8\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 25\n",
      "Current state: 8\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 16\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 26\n",
      "Current state: 16\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 16\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 27\n",
      "Current state: 16\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 8\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 28\n",
      "Current state: 8\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 8\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 29\n",
      "Current state: 8\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 9\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 30\n",
      "Current state: 9\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 8\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 31\n",
      "Current state: 8\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 8\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 32\n",
      "Current state: 8\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 16\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 33\n",
      "Current state: 16\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 17\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 34\n",
      "Current state: 17\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 9\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 35\n",
      "Current state: 9\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 17\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 36\n",
      "Current state: 17\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 25\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 37\n",
      "Current state: 25\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 33\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 38\n",
      "Current state: 33\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 32\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 39\n",
      "Current state: 32\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 40\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 40\n",
      "Current state: 40\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 40\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 41\n",
      "Current state: 40\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 32\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 42\n",
      "Current state: 32\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 24\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 43\n",
      "Current state: 24\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 32\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 44\n",
      "Current state: 32\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 33\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 45\n",
      "Current state: 33\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 25\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 46\n",
      "Current state: 25\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 24\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 47\n",
      "Current state: 24\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 32\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 48\n",
      "Current state: 32\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 40\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 49\n",
      "Current state: 40\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 41\n",
      "Reward: 0.0\n",
      "Terminated: True\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Episode 53\n",
      "Starting state: 0\n",
      "\n",
      "Step 0\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 1\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 1\n",
      "Current state: 1\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 2\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 1\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 3\n",
      "Current state: 1\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 4\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 5\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 8\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 6\n",
      "Current state: 8\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 16\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 7\n",
      "Current state: 16\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 17\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 8\n",
      "Current state: 17\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 16\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 9\n",
      "Current state: 16\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 17\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 10\n",
      "Current state: 17\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 18\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 11\n",
      "Current state: 18\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 26\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 12\n",
      "Current state: 26\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 27\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 13\n",
      "Current state: 27\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 19\n",
      "Reward: 0.0\n",
      "Terminated: True\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Episode 54\n",
      "Starting state: 0\n",
      "\n",
      "Step 0\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 8\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 1\n",
      "Current state: 8\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 8\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 2\n",
      "Current state: 8\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 9\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 3\n",
      "Current state: 9\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 8\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 4\n",
      "Current state: 8\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 5\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 8\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 6\n",
      "Current state: 8\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 8\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 7\n",
      "Current state: 8\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 8\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 9\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 10\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 1\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 11\n",
      "Current state: 1\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 12\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 8\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 13\n",
      "Current state: 8\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 9\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 14\n",
      "Current state: 9\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 8\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 15\n",
      "Current state: 8\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 16\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 8\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 17\n",
      "Current state: 8\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 8\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 18\n",
      "Current state: 8\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 8\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 19\n",
      "Current state: 8\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 8\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 20\n",
      "Current state: 8\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 9\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 21\n",
      "Current state: 9\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 10\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 22\n",
      "Current state: 10\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 11\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 23\n",
      "Current state: 11\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 3\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 24\n",
      "Current state: 3\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 11\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 25\n",
      "Current state: 11\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 3\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 26\n",
      "Current state: 3\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 2\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 27\n",
      "Current state: 2\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 3\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 28\n",
      "Current state: 3\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 11\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 29\n",
      "Current state: 11\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 12\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 30\n",
      "Current state: 12\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 20\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 31\n",
      "Current state: 20\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 19\n",
      "Reward: 0.0\n",
      "Terminated: True\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Episode 55\n",
      "Starting state: 0\n",
      "\n",
      "Step 0\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 1\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 1\n",
      "Current state: 1\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 2\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 3\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 4\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 5\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 6\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 7\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 1\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 8\n",
      "Current state: 1\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 2\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 9\n",
      "Current state: 2\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 3\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 10\n",
      "Current state: 3\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 11\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 11\n",
      "Current state: 11\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 12\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 12\n",
      "Current state: 12\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 13\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 13\n",
      "Current state: 13\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 21\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 14\n",
      "Current state: 21\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 20\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 15\n",
      "Current state: 20\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 21\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 16\n",
      "Current state: 21\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 29\n",
      "Reward: 0.0\n",
      "Terminated: True\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Episode 56\n",
      "Starting state: 0\n",
      "\n",
      "Step 0\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 1\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 8\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 2\n",
      "Current state: 8\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 8\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 3\n",
      "Current state: 8\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 4\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 1\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 5\n",
      "Current state: 1\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 9\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 6\n",
      "Current state: 9\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 1\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 7\n",
      "Current state: 1\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 1\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 8\n",
      "Current state: 1\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 1\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 9\n",
      "Current state: 1\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 10\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 1\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 11\n",
      "Current state: 1\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 1\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 12\n",
      "Current state: 1\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 13\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 8\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 14\n",
      "Current state: 8\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 9\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 15\n",
      "Current state: 9\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 17\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 16\n",
      "Current state: 17\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 18\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 17\n",
      "Current state: 18\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 19\n",
      "Reward: 0.0\n",
      "Terminated: True\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Episode 57\n",
      "Starting state: 0\n",
      "\n",
      "Step 0\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 1\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 8\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 2\n",
      "Current state: 8\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 16\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 3\n",
      "Current state: 16\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 8\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 4\n",
      "Current state: 8\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 5\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 6\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 7\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 8\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 8\n",
      "Current state: 8\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 16\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 9\n",
      "Current state: 16\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 24\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 10\n",
      "Current state: 24\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 24\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 11\n",
      "Current state: 24\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 25\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 12\n",
      "Current state: 25\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 33\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 13\n",
      "Current state: 33\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 34\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 14\n",
      "Current state: 34\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 35\n",
      "Reward: 0.0\n",
      "Terminated: True\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Episode 58\n",
      "Starting state: 0\n",
      "\n",
      "Step 0\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 1\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 2\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 8\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 3\n",
      "Current state: 8\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 8\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 4\n",
      "Current state: 8\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 9\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 5\n",
      "Current state: 9\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 8\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 6\n",
      "Current state: 8\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 7\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 8\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 8\n",
      "Current state: 8\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 8\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 9\n",
      "Current state: 8\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 8\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 10\n",
      "Current state: 8\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 8\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 11\n",
      "Current state: 8\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 12\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 13\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 1\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 14\n",
      "Current state: 1\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 9\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 15\n",
      "Current state: 9\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 17\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 16\n",
      "Current state: 17\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 16\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 17\n",
      "Current state: 16\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 24\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 18\n",
      "Current state: 24\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 25\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 19\n",
      "Current state: 25\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 33\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 20\n",
      "Current state: 33\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 41\n",
      "Reward: 0.0\n",
      "Terminated: True\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Episode 59\n",
      "Starting state: 0\n",
      "\n",
      "Step 0\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 1\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 1\n",
      "Current state: 1\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 9\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 2\n",
      "Current state: 9\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 1\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 3\n",
      "Current state: 1\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 4\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 8\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 5\n",
      "Current state: 8\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 8\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 6\n",
      "Current state: 8\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 16\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 7\n",
      "Current state: 16\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 8\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 8\n",
      "Current state: 8\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 9\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 9\n",
      "Current state: 9\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 8\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 10\n",
      "Current state: 8\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 16\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 11\n",
      "Current state: 16\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 17\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 12\n",
      "Current state: 17\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 9\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 13\n",
      "Current state: 9\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 17\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 14\n",
      "Current state: 17\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 9\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 15\n",
      "Current state: 9\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 8\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 16\n",
      "Current state: 8\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 9\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 17\n",
      "Current state: 9\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 8\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 18\n",
      "Current state: 8\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 9\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 19\n",
      "Current state: 9\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 8\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 20\n",
      "Current state: 8\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 9\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 21\n",
      "Current state: 9\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 8\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 22\n",
      "Current state: 8\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 16\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 23\n",
      "Current state: 16\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 16\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 24\n",
      "Current state: 16\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 17\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 25\n",
      "Current state: 17\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 25\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 26\n",
      "Current state: 25\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 26\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 27\n",
      "Current state: 26\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 27\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 28\n",
      "Current state: 27\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 28\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 29\n",
      "Current state: 28\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 20\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 30\n",
      "Current state: 20\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 21\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 31\n",
      "Current state: 21\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 29\n",
      "Reward: 0.0\n",
      "Terminated: True\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Episode 60\n",
      "Starting state: 0\n",
      "\n",
      "Step 0\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 1\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 1\n",
      "Current state: 1\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 2\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 3\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 8\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 4\n",
      "Current state: 8\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 5\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 6\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 1\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 7\n",
      "Current state: 1\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 9\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 8\n",
      "Current state: 9\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 10\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 9\n",
      "Current state: 10\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 11\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 10\n",
      "Current state: 11\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 3\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 11\n",
      "Current state: 3\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 2\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 12\n",
      "Current state: 2\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 3\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 13\n",
      "Current state: 3\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 3\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 14\n",
      "Current state: 3\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 2\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 15\n",
      "Current state: 2\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 3\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 16\n",
      "Current state: 3\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 4\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 17\n",
      "Current state: 4\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 3\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 18\n",
      "Current state: 3\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 4\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 19\n",
      "Current state: 4\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 4\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 20\n",
      "Current state: 4\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 5\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 21\n",
      "Current state: 5\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 5\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 22\n",
      "Current state: 5\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 6\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 23\n",
      "Current state: 6\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 6\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 24\n",
      "Current state: 6\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 5\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 25\n",
      "Current state: 5\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 13\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 26\n",
      "Current state: 13\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 12\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 27\n",
      "Current state: 12\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 13\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 28\n",
      "Current state: 13\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 5\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 29\n",
      "Current state: 5\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 6\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 30\n",
      "Current state: 6\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 14\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 31\n",
      "Current state: 14\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 13\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 32\n",
      "Current state: 13\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 14\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 33\n",
      "Current state: 14\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 6\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 34\n",
      "Current state: 6\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 5\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 35\n",
      "Current state: 5\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 4\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 36\n",
      "Current state: 4\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 4\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 37\n",
      "Current state: 4\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 3\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 38\n",
      "Current state: 3\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 11\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 39\n",
      "Current state: 11\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 3\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 40\n",
      "Current state: 3\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 3\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 41\n",
      "Current state: 3\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 2\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 42\n",
      "Current state: 2\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 1\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 43\n",
      "Current state: 1\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 44\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 8\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 45\n",
      "Current state: 8\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 8\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 46\n",
      "Current state: 8\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 8\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 47\n",
      "Current state: 8\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 9\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 48\n",
      "Current state: 9\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 1\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 49\n",
      "Current state: 1\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 50\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 51\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 1\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 52\n",
      "Current state: 1\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 9\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 53\n",
      "Current state: 9\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 17\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 54\n",
      "Current state: 17\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 16\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 55\n",
      "Current state: 16\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 8\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 56\n",
      "Current state: 8\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 8\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 57\n",
      "Current state: 8\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 16\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 58\n",
      "Current state: 16\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 8\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 59\n",
      "Current state: 8\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 9\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 60\n",
      "Current state: 9\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 8\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 61\n",
      "Current state: 8\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 8\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 62\n",
      "Current state: 8\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 9\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 63\n",
      "Current state: 9\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 1\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 64\n",
      "Current state: 1\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 9\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 65\n",
      "Current state: 9\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 17\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 66\n",
      "Current state: 17\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 25\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 67\n",
      "Current state: 25\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 17\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 68\n",
      "Current state: 17\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 16\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 69\n",
      "Current state: 16\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 16\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 70\n",
      "Current state: 16\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 24\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 71\n",
      "Current state: 24\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 24\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 72\n",
      "Current state: 24\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 32\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 73\n",
      "Current state: 32\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 32\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 74\n",
      "Current state: 32\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 24\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 75\n",
      "Current state: 24\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 32\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 76\n",
      "Current state: 32\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 24\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 77\n",
      "Current state: 24\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 32\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 78\n",
      "Current state: 32\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 40\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 79\n",
      "Current state: 40\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 32\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 80\n",
      "Current state: 32\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 32\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 81\n",
      "Current state: 32\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 33\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 82\n",
      "Current state: 33\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 41\n",
      "Reward: 0.0\n",
      "Terminated: True\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Successful episodes so far: []\n",
      "\n",
      "Episode 61\n",
      "Starting state: 0\n",
      "\n",
      "Step 0\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 1\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 2\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 8\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 3\n",
      "Current state: 8\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 8\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 4\n",
      "Current state: 8\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 5\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 1\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 6\n",
      "Current state: 1\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 1\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 7\n",
      "Current state: 1\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 9\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 8\n",
      "Current state: 9\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 17\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 9\n",
      "Current state: 17\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 9\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 10\n",
      "Current state: 9\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 8\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 11\n",
      "Current state: 8\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 8\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 12\n",
      "Current state: 8\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 13\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 14\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 1\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 15\n",
      "Current state: 1\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 16\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 8\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 17\n",
      "Current state: 8\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 16\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 18\n",
      "Current state: 16\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 17\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 19\n",
      "Current state: 17\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 16\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 20\n",
      "Current state: 16\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 24\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 21\n",
      "Current state: 24\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 25\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 22\n",
      "Current state: 25\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 24\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 23\n",
      "Current state: 24\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 25\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 24\n",
      "Current state: 25\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 26\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 25\n",
      "Current state: 26\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 25\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 26\n",
      "Current state: 25\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 24\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 27\n",
      "Current state: 24\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 16\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 28\n",
      "Current state: 16\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 17\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 29\n",
      "Current state: 17\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 16\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 30\n",
      "Current state: 16\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 8\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 31\n",
      "Current state: 8\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 8\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 32\n",
      "Current state: 8\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 16\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 33\n",
      "Current state: 16\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 16\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 34\n",
      "Current state: 16\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 16\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 35\n",
      "Current state: 16\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 8\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 36\n",
      "Current state: 8\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 9\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 37\n",
      "Current state: 9\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 10\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 38\n",
      "Current state: 10\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 18\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 39\n",
      "Current state: 18\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 26\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 40\n",
      "Current state: 26\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 27\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 41\n",
      "Current state: 27\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 35\n",
      "Reward: 0.0\n",
      "Terminated: True\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Episode 62\n",
      "Starting state: 0\n",
      "\n",
      "Step 0\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 1\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 2\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 1\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 3\n",
      "Current state: 1\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 9\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 4\n",
      "Current state: 9\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 1\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 5\n",
      "Current state: 1\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 6\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 7\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 8\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 8\n",
      "Current state: 8\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 9\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 9\n",
      "Current state: 9\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 17\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 10\n",
      "Current state: 17\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 9\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 11\n",
      "Current state: 9\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 17\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 12\n",
      "Current state: 17\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 18\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 13\n",
      "Current state: 18\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 10\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 14\n",
      "Current state: 10\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 18\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 15\n",
      "Current state: 18\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 26\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 16\n",
      "Current state: 26\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 25\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 17\n",
      "Current state: 25\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 26\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 18\n",
      "Current state: 26\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 34\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 19\n",
      "Current state: 34\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 26\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 20\n",
      "Current state: 26\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 25\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 21\n",
      "Current state: 25\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 24\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 22\n",
      "Current state: 24\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 25\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 23\n",
      "Current state: 25\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 26\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 24\n",
      "Current state: 26\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 34\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 25\n",
      "Current state: 34\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 35\n",
      "Reward: 0.0\n",
      "Terminated: True\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Episode 63\n",
      "Starting state: 0\n",
      "\n",
      "Step 0\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 1\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 1\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 2\n",
      "Current state: 1\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 1\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 3\n",
      "Current state: 1\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 1\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 4\n",
      "Current state: 1\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 1\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 5\n",
      "Current state: 1\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 9\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 6\n",
      "Current state: 9\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 10\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 7\n",
      "Current state: 10\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 2\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 8\n",
      "Current state: 2\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 3\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 9\n",
      "Current state: 3\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 3\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 10\n",
      "Current state: 3\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 4\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 11\n",
      "Current state: 4\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 12\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 12\n",
      "Current state: 12\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 20\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 13\n",
      "Current state: 20\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 21\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 14\n",
      "Current state: 21\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 20\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 15\n",
      "Current state: 20\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 19\n",
      "Reward: 0.0\n",
      "Terminated: True\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Episode 64\n",
      "Starting state: 0\n",
      "\n",
      "Step 0\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 8\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 1\n",
      "Current state: 8\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 8\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 2\n",
      "Current state: 8\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 8\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 3\n",
      "Current state: 8\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 9\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 4\n",
      "Current state: 9\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 8\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 5\n",
      "Current state: 8\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 9\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 6\n",
      "Current state: 9\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 8\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 7\n",
      "Current state: 8\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 8\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 8\n",
      "Current state: 8\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 9\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 10\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 11\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 8\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 12\n",
      "Current state: 8\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 9\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 13\n",
      "Current state: 9\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 8\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 14\n",
      "Current state: 8\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 8\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 15\n",
      "Current state: 8\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 16\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 1\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 17\n",
      "Current state: 1\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 2\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 18\n",
      "Current state: 2\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 2\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 19\n",
      "Current state: 2\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 1\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 20\n",
      "Current state: 1\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 1\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 21\n",
      "Current state: 1\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 9\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 22\n",
      "Current state: 9\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 8\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 23\n",
      "Current state: 8\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 16\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 24\n",
      "Current state: 16\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 8\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 25\n",
      "Current state: 8\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 26\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 1\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 27\n",
      "Current state: 1\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 28\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 29\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 30\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 8\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 31\n",
      "Current state: 8\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 9\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 32\n",
      "Current state: 9\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 17\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 33\n",
      "Current state: 17\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 9\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 34\n",
      "Current state: 9\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 10\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 35\n",
      "Current state: 10\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 18\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 36\n",
      "Current state: 18\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 19\n",
      "Reward: 0.0\n",
      "Terminated: True\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Episode 65\n",
      "Starting state: 0\n",
      "\n",
      "Step 0\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 1\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 1\n",
      "Current state: 1\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 1\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 2\n",
      "Current state: 1\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 1\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 3\n",
      "Current state: 1\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 4\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 5\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 8\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 6\n",
      "Current state: 8\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 8\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 7\n",
      "Current state: 8\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 9\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 8\n",
      "Current state: 9\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 8\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 9\n",
      "Current state: 8\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 10\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 11\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 1\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 12\n",
      "Current state: 1\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 1\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 13\n",
      "Current state: 1\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 14\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 1\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 15\n",
      "Current state: 1\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 9\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 16\n",
      "Current state: 9\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 8\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 17\n",
      "Current state: 8\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 18\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 19\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 20\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 8\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 21\n",
      "Current state: 8\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 22\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 23\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 24\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 25\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 1\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 26\n",
      "Current state: 1\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 27\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 8\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 28\n",
      "Current state: 8\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 29\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 30\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 8\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 31\n",
      "Current state: 8\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 8\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 32\n",
      "Current state: 8\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 33\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 1\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 34\n",
      "Current state: 1\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 1\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 35\n",
      "Current state: 1\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 9\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 36\n",
      "Current state: 9\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 10\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 37\n",
      "Current state: 10\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 2\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 38\n",
      "Current state: 2\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 2\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 39\n",
      "Current state: 2\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 1\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 40\n",
      "Current state: 1\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 2\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 41\n",
      "Current state: 2\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 3\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 42\n",
      "Current state: 3\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 4\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 43\n",
      "Current state: 4\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 12\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 44\n",
      "Current state: 12\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 13\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 45\n",
      "Current state: 13\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 21\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 46\n",
      "Current state: 21\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 22\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 47\n",
      "Current state: 22\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 14\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 48\n",
      "Current state: 14\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 22\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 49\n",
      "Current state: 22\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 30\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 50\n",
      "Current state: 30\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 29\n",
      "Reward: 0.0\n",
      "Terminated: True\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Episode 66\n",
      "Starting state: 0\n",
      "\n",
      "Step 0\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 1\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 1\n",
      "Current state: 1\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 2\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 2\n",
      "Current state: 2\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 10\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 3\n",
      "Current state: 10\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 9\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 4\n",
      "Current state: 9\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 10\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 5\n",
      "Current state: 10\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 9\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 6\n",
      "Current state: 9\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 1\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 7\n",
      "Current state: 1\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 1\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 8\n",
      "Current state: 1\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 1\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 9\n",
      "Current state: 1\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 1\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 10\n",
      "Current state: 1\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 1\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 11\n",
      "Current state: 1\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 9\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 12\n",
      "Current state: 9\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 17\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 13\n",
      "Current state: 17\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 18\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 14\n",
      "Current state: 18\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 19\n",
      "Reward: 0.0\n",
      "Terminated: True\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Episode 67\n",
      "Starting state: 0\n",
      "\n",
      "Step 0\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 8\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 1\n",
      "Current state: 8\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 2\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 8\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 3\n",
      "Current state: 8\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 8\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 4\n",
      "Current state: 8\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 8\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 5\n",
      "Current state: 8\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 8\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 6\n",
      "Current state: 8\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 7\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 8\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 8\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 9\n",
      "Current state: 8\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 8\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 10\n",
      "Current state: 8\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 9\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 11\n",
      "Current state: 9\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 1\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 12\n",
      "Current state: 1\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 9\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 13\n",
      "Current state: 9\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 8\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 14\n",
      "Current state: 8\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 9\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 15\n",
      "Current state: 9\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 10\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 16\n",
      "Current state: 10\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 9\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 17\n",
      "Current state: 9\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 17\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 18\n",
      "Current state: 17\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 16\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 19\n",
      "Current state: 16\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 17\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 20\n",
      "Current state: 17\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 18\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 21\n",
      "Current state: 18\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 10\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 22\n",
      "Current state: 10\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 2\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 23\n",
      "Current state: 2\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 10\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 24\n",
      "Current state: 10\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 11\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 25\n",
      "Current state: 11\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 3\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 26\n",
      "Current state: 3\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 4\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 27\n",
      "Current state: 4\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 3\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 28\n",
      "Current state: 3\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 4\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 29\n",
      "Current state: 4\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 5\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 30\n",
      "Current state: 5\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 13\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 31\n",
      "Current state: 13\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 12\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 32\n",
      "Current state: 12\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 4\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 33\n",
      "Current state: 4\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 5\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 34\n",
      "Current state: 5\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 13\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 35\n",
      "Current state: 13\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 14\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 36\n",
      "Current state: 14\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 22\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 37\n",
      "Current state: 22\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 14\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 38\n",
      "Current state: 14\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 13\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 39\n",
      "Current state: 13\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 21\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 40\n",
      "Current state: 21\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 13\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 41\n",
      "Current state: 13\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 12\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 42\n",
      "Current state: 12\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 4\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 43\n",
      "Current state: 4\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 5\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 44\n",
      "Current state: 5\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 5\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 45\n",
      "Current state: 5\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 5\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 46\n",
      "Current state: 5\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 5\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 47\n",
      "Current state: 5\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 4\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 48\n",
      "Current state: 4\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 5\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 49\n",
      "Current state: 5\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 6\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 50\n",
      "Current state: 6\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 14\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 51\n",
      "Current state: 14\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 13\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 52\n",
      "Current state: 13\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 12\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 53\n",
      "Current state: 12\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 11\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 54\n",
      "Current state: 11\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 12\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 55\n",
      "Current state: 12\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 4\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 56\n",
      "Current state: 4\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 3\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 57\n",
      "Current state: 3\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 11\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 58\n",
      "Current state: 11\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 10\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 59\n",
      "Current state: 10\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 11\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 60\n",
      "Current state: 11\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 3\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 61\n",
      "Current state: 3\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 2\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 62\n",
      "Current state: 2\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 10\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 63\n",
      "Current state: 10\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 11\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 64\n",
      "Current state: 11\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 3\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 65\n",
      "Current state: 3\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 2\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 66\n",
      "Current state: 2\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 10\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 67\n",
      "Current state: 10\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 2\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 68\n",
      "Current state: 2\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 1\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 69\n",
      "Current state: 1\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 1\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 70\n",
      "Current state: 1\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 2\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 71\n",
      "Current state: 2\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 3\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 72\n",
      "Current state: 3\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 3\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 73\n",
      "Current state: 3\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 3\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 74\n",
      "Current state: 3\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 11\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 75\n",
      "Current state: 11\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 10\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 76\n",
      "Current state: 10\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 9\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 77\n",
      "Current state: 9\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 17\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 78\n",
      "Current state: 17\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 25\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 79\n",
      "Current state: 25\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 17\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 80\n",
      "Current state: 17\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 25\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 81\n",
      "Current state: 25\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 24\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 82\n",
      "Current state: 24\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 25\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 83\n",
      "Current state: 25\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 24\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 84\n",
      "Current state: 24\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 25\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 85\n",
      "Current state: 25\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 24\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 86\n",
      "Current state: 24\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 25\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 87\n",
      "Current state: 25\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 17\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 88\n",
      "Current state: 17\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 16\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 89\n",
      "Current state: 16\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 24\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 90\n",
      "Current state: 24\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 16\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 91\n",
      "Current state: 16\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 16\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 92\n",
      "Current state: 16\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 24\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 93\n",
      "Current state: 24\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 16\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 94\n",
      "Current state: 16\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 16\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 95\n",
      "Current state: 16\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 24\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 96\n",
      "Current state: 24\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 25\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 97\n",
      "Current state: 25\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 24\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 98\n",
      "Current state: 24\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 32\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 99\n",
      "Current state: 32\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 40\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Episode 68\n",
      "Starting state: 0\n",
      "\n",
      "Step 0\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 1\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 8\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 2\n",
      "Current state: 8\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 3\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 8\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 4\n",
      "Current state: 8\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 5\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 6\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 8\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 7\n",
      "Current state: 8\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 16\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 8\n",
      "Current state: 16\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 16\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 9\n",
      "Current state: 16\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 16\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 10\n",
      "Current state: 16\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 16\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 11\n",
      "Current state: 16\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 16\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 12\n",
      "Current state: 16\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 16\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 13\n",
      "Current state: 16\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 16\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 14\n",
      "Current state: 16\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 8\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 15\n",
      "Current state: 8\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 9\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 16\n",
      "Current state: 9\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 17\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 17\n",
      "Current state: 17\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 9\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 18\n",
      "Current state: 9\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 1\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 19\n",
      "Current state: 1\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 1\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 20\n",
      "Current state: 1\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 1\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 21\n",
      "Current state: 1\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 2\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 22\n",
      "Current state: 2\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 1\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 23\n",
      "Current state: 1\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 1\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 24\n",
      "Current state: 1\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 2\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 25\n",
      "Current state: 2\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 2\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 26\n",
      "Current state: 2\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 2\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 27\n",
      "Current state: 2\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 10\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 28\n",
      "Current state: 10\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 9\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 29\n",
      "Current state: 9\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 10\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 30\n",
      "Current state: 10\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 18\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 31\n",
      "Current state: 18\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 17\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 32\n",
      "Current state: 17\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 16\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 33\n",
      "Current state: 16\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 16\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 34\n",
      "Current state: 16\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 17\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 35\n",
      "Current state: 17\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 18\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 36\n",
      "Current state: 18\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 19\n",
      "Reward: 0.0\n",
      "Terminated: True\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Episode 69\n",
      "Starting state: 0\n",
      "\n",
      "Step 0\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 1\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 1\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 2\n",
      "Current state: 1\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 2\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 3\n",
      "Current state: 2\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 2\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 4\n",
      "Current state: 2\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 1\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 5\n",
      "Current state: 1\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 2\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 6\n",
      "Current state: 2\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 2\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 7\n",
      "Current state: 2\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 3\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 8\n",
      "Current state: 3\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 4\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 9\n",
      "Current state: 4\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 12\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 10\n",
      "Current state: 12\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 4\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 11\n",
      "Current state: 4\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 4\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 12\n",
      "Current state: 4\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 12\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 13\n",
      "Current state: 12\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 13\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 14\n",
      "Current state: 13\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 21\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 15\n",
      "Current state: 21\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 20\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 16\n",
      "Current state: 20\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 12\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 17\n",
      "Current state: 12\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 11\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 18\n",
      "Current state: 11\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 10\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 19\n",
      "Current state: 10\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 2\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 20\n",
      "Current state: 2\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 1\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 21\n",
      "Current state: 1\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 2\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 22\n",
      "Current state: 2\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 1\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 23\n",
      "Current state: 1\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 9\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 24\n",
      "Current state: 9\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 8\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 25\n",
      "Current state: 8\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 26\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 8\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 27\n",
      "Current state: 8\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 16\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 28\n",
      "Current state: 16\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 17\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 29\n",
      "Current state: 17\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 9\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 30\n",
      "Current state: 9\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 17\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 31\n",
      "Current state: 17\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 25\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 32\n",
      "Current state: 25\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 33\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 33\n",
      "Current state: 33\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 34\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 34\n",
      "Current state: 34\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 26\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 35\n",
      "Current state: 26\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 27\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 36\n",
      "Current state: 27\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 19\n",
      "Reward: 0.0\n",
      "Terminated: True\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Episode 70\n",
      "Starting state: 0\n",
      "\n",
      "Step 0\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 1\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 1\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 2\n",
      "Current state: 1\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 3\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 1\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 4\n",
      "Current state: 1\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 9\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 5\n",
      "Current state: 9\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 17\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 6\n",
      "Current state: 17\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 16\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 7\n",
      "Current state: 16\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 8\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 8\n",
      "Current state: 8\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 9\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 8\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 10\n",
      "Current state: 8\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 11\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 8\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 12\n",
      "Current state: 8\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 8\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 13\n",
      "Current state: 8\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 16\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 14\n",
      "Current state: 16\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 8\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 15\n",
      "Current state: 8\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 9\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 16\n",
      "Current state: 9\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 8\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 17\n",
      "Current state: 8\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 9\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 18\n",
      "Current state: 9\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 8\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 19\n",
      "Current state: 8\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 8\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 20\n",
      "Current state: 8\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 8\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 21\n",
      "Current state: 8\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 22\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 23\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 24\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 25\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 26\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 8\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 27\n",
      "Current state: 8\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 9\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 28\n",
      "Current state: 9\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 17\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 29\n",
      "Current state: 17\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 9\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 30\n",
      "Current state: 9\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 10\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 31\n",
      "Current state: 10\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 9\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 32\n",
      "Current state: 9\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 8\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 33\n",
      "Current state: 8\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 8\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 34\n",
      "Current state: 8\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 35\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 36\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 1\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 37\n",
      "Current state: 1\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 9\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 38\n",
      "Current state: 9\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 1\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 39\n",
      "Current state: 1\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 2\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 40\n",
      "Current state: 2\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 2\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 41\n",
      "Current state: 2\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 3\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 42\n",
      "Current state: 3\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 4\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 43\n",
      "Current state: 4\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 4\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 44\n",
      "Current state: 4\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 3\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 45\n",
      "Current state: 3\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 3\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 46\n",
      "Current state: 3\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 3\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 47\n",
      "Current state: 3\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 2\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 48\n",
      "Current state: 2\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 10\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 49\n",
      "Current state: 10\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 18\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 50\n",
      "Current state: 18\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 19\n",
      "Reward: 0.0\n",
      "Terminated: True\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Successful episodes so far: []\n",
      "\n",
      "Episode 71\n",
      "Starting state: 0\n",
      "\n",
      "Step 0\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 1\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 8\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 2\n",
      "Current state: 8\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 16\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 3\n",
      "Current state: 16\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 16\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 4\n",
      "Current state: 16\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 16\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 5\n",
      "Current state: 16\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 17\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 6\n",
      "Current state: 17\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 18\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 7\n",
      "Current state: 18\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 17\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 8\n",
      "Current state: 17\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 9\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 9\n",
      "Current state: 9\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 10\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 10\n",
      "Current state: 10\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 18\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 11\n",
      "Current state: 18\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 26\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 12\n",
      "Current state: 26\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 34\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 13\n",
      "Current state: 34\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 26\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 14\n",
      "Current state: 26\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 18\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 15\n",
      "Current state: 18\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 26\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 16\n",
      "Current state: 26\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 25\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 17\n",
      "Current state: 25\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 26\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 18\n",
      "Current state: 26\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 34\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 19\n",
      "Current state: 34\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 35\n",
      "Reward: 0.0\n",
      "Terminated: True\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Episode 72\n",
      "Starting state: 0\n",
      "\n",
      "Step 0\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 8\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 1\n",
      "Current state: 8\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 9\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 2\n",
      "Current state: 9\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 1\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 3\n",
      "Current state: 1\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 2\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 4\n",
      "Current state: 2\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 1\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 5\n",
      "Current state: 1\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 9\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 6\n",
      "Current state: 9\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 1\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 7\n",
      "Current state: 1\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 2\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 8\n",
      "Current state: 2\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 10\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 9\n",
      "Current state: 10\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 11\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 10\n",
      "Current state: 11\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 12\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 11\n",
      "Current state: 12\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 4\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 12\n",
      "Current state: 4\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 4\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 13\n",
      "Current state: 4\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 4\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 14\n",
      "Current state: 4\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 12\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 15\n",
      "Current state: 12\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 11\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 16\n",
      "Current state: 11\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 10\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 17\n",
      "Current state: 10\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 11\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 18\n",
      "Current state: 11\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 10\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 19\n",
      "Current state: 10\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 2\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 20\n",
      "Current state: 2\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 10\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 21\n",
      "Current state: 10\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 2\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 22\n",
      "Current state: 2\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 1\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 23\n",
      "Current state: 1\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 1\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 24\n",
      "Current state: 1\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 2\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 25\n",
      "Current state: 2\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 10\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 26\n",
      "Current state: 10\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 11\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 27\n",
      "Current state: 11\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 10\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 28\n",
      "Current state: 10\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 2\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 29\n",
      "Current state: 2\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 1\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 30\n",
      "Current state: 1\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 1\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 31\n",
      "Current state: 1\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 2\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 32\n",
      "Current state: 2\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 2\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 33\n",
      "Current state: 2\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 1\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 34\n",
      "Current state: 1\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 35\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 36\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 37\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 1\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 38\n",
      "Current state: 1\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 1\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 39\n",
      "Current state: 1\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 1\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 40\n",
      "Current state: 1\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 2\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 41\n",
      "Current state: 2\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 1\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 42\n",
      "Current state: 1\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 43\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 1\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 44\n",
      "Current state: 1\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 1\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 45\n",
      "Current state: 1\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 9\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 46\n",
      "Current state: 9\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 8\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 47\n",
      "Current state: 8\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 48\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 49\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 50\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 1\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 51\n",
      "Current state: 1\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 52\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 8\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 53\n",
      "Current state: 8\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 16\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 54\n",
      "Current state: 16\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 8\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 55\n",
      "Current state: 8\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 8\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 56\n",
      "Current state: 8\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 9\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 57\n",
      "Current state: 9\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 17\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 58\n",
      "Current state: 17\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 25\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 59\n",
      "Current state: 25\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 33\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 60\n",
      "Current state: 33\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 34\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 61\n",
      "Current state: 34\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 33\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 62\n",
      "Current state: 33\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 34\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 63\n",
      "Current state: 34\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 33\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 64\n",
      "Current state: 33\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 25\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 65\n",
      "Current state: 25\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 33\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 66\n",
      "Current state: 33\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 34\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 67\n",
      "Current state: 34\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 26\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 68\n",
      "Current state: 26\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 18\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 69\n",
      "Current state: 18\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 26\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 70\n",
      "Current state: 26\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 34\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 71\n",
      "Current state: 34\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 35\n",
      "Reward: 0.0\n",
      "Terminated: True\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Episode 73\n",
      "Starting state: 0\n",
      "\n",
      "Step 0\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 1\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 8\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 2\n",
      "Current state: 8\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 16\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 3\n",
      "Current state: 16\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 8\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 4\n",
      "Current state: 8\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 5\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 1\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 6\n",
      "Current state: 1\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 7\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 8\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 8\n",
      "Current state: 8\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 8\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 9\n",
      "Current state: 8\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 16\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 10\n",
      "Current state: 16\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 16\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 11\n",
      "Current state: 16\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 17\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 12\n",
      "Current state: 17\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 25\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 13\n",
      "Current state: 25\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 24\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 14\n",
      "Current state: 24\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 25\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 15\n",
      "Current state: 25\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 26\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 16\n",
      "Current state: 26\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 27\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 17\n",
      "Current state: 27\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 35\n",
      "Reward: 0.0\n",
      "Terminated: True\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Episode 74\n",
      "Starting state: 0\n",
      "\n",
      "Step 0\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 1\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 1\n",
      "Current state: 1\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 2\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 2\n",
      "Current state: 2\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 2\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 3\n",
      "Current state: 2\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 1\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 4\n",
      "Current state: 1\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 5\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 1\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 6\n",
      "Current state: 1\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 2\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 7\n",
      "Current state: 2\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 3\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 8\n",
      "Current state: 3\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 3\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 9\n",
      "Current state: 3\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 2\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 10\n",
      "Current state: 2\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 3\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 11\n",
      "Current state: 3\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 11\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 12\n",
      "Current state: 11\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 12\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 13\n",
      "Current state: 12\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 11\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 14\n",
      "Current state: 11\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 10\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 15\n",
      "Current state: 10\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 2\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 16\n",
      "Current state: 2\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 1\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 17\n",
      "Current state: 1\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 18\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 19\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 1\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 20\n",
      "Current state: 1\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 1\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 21\n",
      "Current state: 1\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 2\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 22\n",
      "Current state: 2\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 3\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 23\n",
      "Current state: 3\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 2\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 24\n",
      "Current state: 2\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 10\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 25\n",
      "Current state: 10\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 9\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 26\n",
      "Current state: 9\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 1\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 27\n",
      "Current state: 1\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 1\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 28\n",
      "Current state: 1\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 2\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 29\n",
      "Current state: 2\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 10\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 30\n",
      "Current state: 10\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 9\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 31\n",
      "Current state: 9\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 10\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 32\n",
      "Current state: 10\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 11\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 33\n",
      "Current state: 11\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 10\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 34\n",
      "Current state: 10\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 9\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 35\n",
      "Current state: 9\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 1\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 36\n",
      "Current state: 1\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 1\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 37\n",
      "Current state: 1\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 2\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 38\n",
      "Current state: 2\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 2\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 39\n",
      "Current state: 2\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 10\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 40\n",
      "Current state: 10\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 18\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 41\n",
      "Current state: 18\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 26\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 42\n",
      "Current state: 26\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 34\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 43\n",
      "Current state: 34\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 26\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 44\n",
      "Current state: 26\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 18\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 45\n",
      "Current state: 18\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 10\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 46\n",
      "Current state: 10\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 11\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 47\n",
      "Current state: 11\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 3\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 48\n",
      "Current state: 3\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 3\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 49\n",
      "Current state: 3\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 3\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 50\n",
      "Current state: 3\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 2\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 51\n",
      "Current state: 2\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 1\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 52\n",
      "Current state: 1\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 53\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 1\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 54\n",
      "Current state: 1\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 1\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 55\n",
      "Current state: 1\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 56\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 8\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 57\n",
      "Current state: 8\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 58\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 59\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 60\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 61\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 1\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 62\n",
      "Current state: 1\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 63\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 8\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 64\n",
      "Current state: 8\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 16\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 65\n",
      "Current state: 16\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 24\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 66\n",
      "Current state: 24\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 25\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 67\n",
      "Current state: 25\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 17\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 68\n",
      "Current state: 17\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 9\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 69\n",
      "Current state: 9\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 10\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 70\n",
      "Current state: 10\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 2\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 71\n",
      "Current state: 2\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 3\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 72\n",
      "Current state: 3\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 3\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 73\n",
      "Current state: 3\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 3\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 74\n",
      "Current state: 3\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 2\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 75\n",
      "Current state: 2\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 2\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 76\n",
      "Current state: 2\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 2\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 77\n",
      "Current state: 2\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 10\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 78\n",
      "Current state: 10\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 18\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 79\n",
      "Current state: 18\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 17\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 80\n",
      "Current state: 17\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 18\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 81\n",
      "Current state: 18\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 17\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 82\n",
      "Current state: 17\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 25\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 83\n",
      "Current state: 25\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 33\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 84\n",
      "Current state: 33\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 25\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 85\n",
      "Current state: 25\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 24\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 86\n",
      "Current state: 24\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 16\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 87\n",
      "Current state: 16\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 24\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 88\n",
      "Current state: 24\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 16\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 89\n",
      "Current state: 16\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 24\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 90\n",
      "Current state: 24\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 32\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 91\n",
      "Current state: 32\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 40\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 92\n",
      "Current state: 40\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 32\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 93\n",
      "Current state: 32\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 32\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 94\n",
      "Current state: 32\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 32\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 95\n",
      "Current state: 32\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 40\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 96\n",
      "Current state: 40\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 48\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 97\n",
      "Current state: 48\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 48\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 98\n",
      "Current state: 48\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 56\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 99\n",
      "Current state: 56\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 56\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Episode 75\n",
      "Starting state: 0\n",
      "\n",
      "Step 0\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 8\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 1\n",
      "Current state: 8\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 16\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 2\n",
      "Current state: 16\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 8\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 3\n",
      "Current state: 8\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 9\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 4\n",
      "Current state: 9\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 17\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 5\n",
      "Current state: 17\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 18\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 6\n",
      "Current state: 18\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 19\n",
      "Reward: 0.0\n",
      "Terminated: True\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Episode 76\n",
      "Starting state: 0\n",
      "\n",
      "Step 0\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 8\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 1\n",
      "Current state: 8\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 2\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 1\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 3\n",
      "Current state: 1\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 9\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 4\n",
      "Current state: 9\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 10\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 5\n",
      "Current state: 10\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 18\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 6\n",
      "Current state: 18\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 10\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 7\n",
      "Current state: 10\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 9\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 8\n",
      "Current state: 9\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 8\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 9\n",
      "Current state: 8\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 16\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 10\n",
      "Current state: 16\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 16\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 11\n",
      "Current state: 16\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 8\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 12\n",
      "Current state: 8\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 13\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 1\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 14\n",
      "Current state: 1\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 9\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 15\n",
      "Current state: 9\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 17\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 16\n",
      "Current state: 17\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 16\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 17\n",
      "Current state: 16\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 24\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 18\n",
      "Current state: 24\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 24\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 19\n",
      "Current state: 24\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 32\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 20\n",
      "Current state: 32\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 32\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 21\n",
      "Current state: 32\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 40\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 22\n",
      "Current state: 40\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 40\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 23\n",
      "Current state: 40\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 41\n",
      "Reward: 0.0\n",
      "Terminated: True\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Episode 77\n",
      "Starting state: 0\n",
      "\n",
      "Step 0\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 1\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 2\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 3\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 8\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 4\n",
      "Current state: 8\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 16\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 5\n",
      "Current state: 16\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 17\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 6\n",
      "Current state: 17\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 9\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 7\n",
      "Current state: 9\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 10\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 8\n",
      "Current state: 10\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 9\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 9\n",
      "Current state: 9\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 1\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 10\n",
      "Current state: 1\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 11\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 8\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 12\n",
      "Current state: 8\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 8\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 13\n",
      "Current state: 8\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 8\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 14\n",
      "Current state: 8\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 16\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 15\n",
      "Current state: 16\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 24\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 16\n",
      "Current state: 24\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 24\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 17\n",
      "Current state: 24\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 16\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 18\n",
      "Current state: 16\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 17\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 19\n",
      "Current state: 17\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 16\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 20\n",
      "Current state: 16\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 17\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 21\n",
      "Current state: 17\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 25\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 22\n",
      "Current state: 25\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 17\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 23\n",
      "Current state: 17\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 25\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 24\n",
      "Current state: 25\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 26\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 25\n",
      "Current state: 26\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 18\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 26\n",
      "Current state: 18\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 10\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 27\n",
      "Current state: 10\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 2\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 28\n",
      "Current state: 2\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 1\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 29\n",
      "Current state: 1\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 2\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 30\n",
      "Current state: 2\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 1\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 31\n",
      "Current state: 1\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 1\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 32\n",
      "Current state: 1\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 1\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 33\n",
      "Current state: 1\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 34\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 1\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 35\n",
      "Current state: 1\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 2\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 36\n",
      "Current state: 2\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 2\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 37\n",
      "Current state: 2\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 2\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 38\n",
      "Current state: 2\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 2\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 39\n",
      "Current state: 2\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 2\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 40\n",
      "Current state: 2\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 1\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 41\n",
      "Current state: 1\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 42\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 8\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 43\n",
      "Current state: 8\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 9\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 44\n",
      "Current state: 9\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 10\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 45\n",
      "Current state: 10\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 2\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 46\n",
      "Current state: 2\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 3\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 47\n",
      "Current state: 3\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 4\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 48\n",
      "Current state: 4\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 12\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 49\n",
      "Current state: 12\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 4\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 50\n",
      "Current state: 4\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 5\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 51\n",
      "Current state: 5\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 13\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 52\n",
      "Current state: 13\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 5\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 53\n",
      "Current state: 5\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 5\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 54\n",
      "Current state: 5\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 4\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 55\n",
      "Current state: 4\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 3\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 56\n",
      "Current state: 3\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 3\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 57\n",
      "Current state: 3\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 2\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 58\n",
      "Current state: 2\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 3\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 59\n",
      "Current state: 3\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 3\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 60\n",
      "Current state: 3\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 4\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 61\n",
      "Current state: 4\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 4\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 62\n",
      "Current state: 4\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 5\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 63\n",
      "Current state: 5\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 4\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 64\n",
      "Current state: 4\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 12\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 65\n",
      "Current state: 12\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 11\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 66\n",
      "Current state: 11\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 3\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 67\n",
      "Current state: 3\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 3\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 68\n",
      "Current state: 3\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 3\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 69\n",
      "Current state: 3\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 2\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 70\n",
      "Current state: 2\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 1\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 71\n",
      "Current state: 1\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 72\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 1\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 73\n",
      "Current state: 1\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 1\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 74\n",
      "Current state: 1\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 1\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 75\n",
      "Current state: 1\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 1\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 76\n",
      "Current state: 1\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 2\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 77\n",
      "Current state: 2\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 10\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 78\n",
      "Current state: 10\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 2\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 79\n",
      "Current state: 2\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 3\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 80\n",
      "Current state: 3\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 3\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 81\n",
      "Current state: 3\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 4\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 82\n",
      "Current state: 4\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 12\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 83\n",
      "Current state: 12\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 4\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 84\n",
      "Current state: 4\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 3\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 85\n",
      "Current state: 3\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 4\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 86\n",
      "Current state: 4\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 3\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 87\n",
      "Current state: 3\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 2\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 88\n",
      "Current state: 2\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 3\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 89\n",
      "Current state: 3\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 2\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 90\n",
      "Current state: 2\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 10\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 91\n",
      "Current state: 10\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 2\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 92\n",
      "Current state: 2\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 10\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 93\n",
      "Current state: 10\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 2\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 94\n",
      "Current state: 2\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 3\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 95\n",
      "Current state: 3\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 2\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 96\n",
      "Current state: 2\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 1\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 97\n",
      "Current state: 1\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 1\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 98\n",
      "Current state: 1\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 1\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 99\n",
      "Current state: 1\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 9\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Episode 78\n",
      "Starting state: 0\n",
      "\n",
      "Step 0\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 1\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 1\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 2\n",
      "Current state: 1\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 2\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 3\n",
      "Current state: 2\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 3\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 4\n",
      "Current state: 3\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 11\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 5\n",
      "Current state: 11\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 12\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 6\n",
      "Current state: 12\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 13\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 7\n",
      "Current state: 13\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 14\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 8\n",
      "Current state: 14\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 15\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 9\n",
      "Current state: 15\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 7\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 10\n",
      "Current state: 7\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 7\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 11\n",
      "Current state: 7\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 7\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 12\n",
      "Current state: 7\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 7\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 13\n",
      "Current state: 7\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 6\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 14\n",
      "Current state: 6\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 7\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 15\n",
      "Current state: 7\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 15\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 16\n",
      "Current state: 15\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 23\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 17\n",
      "Current state: 23\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 15\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 18\n",
      "Current state: 15\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 7\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 19\n",
      "Current state: 7\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 7\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 20\n",
      "Current state: 7\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 7\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 21\n",
      "Current state: 7\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 7\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 22\n",
      "Current state: 7\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 7\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 23\n",
      "Current state: 7\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 15\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 24\n",
      "Current state: 15\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 15\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 25\n",
      "Current state: 15\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 23\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 26\n",
      "Current state: 23\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 31\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 27\n",
      "Current state: 31\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 23\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 28\n",
      "Current state: 23\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 22\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 29\n",
      "Current state: 22\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 30\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 30\n",
      "Current state: 30\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 31\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 31\n",
      "Current state: 31\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 39\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 32\n",
      "Current state: 39\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 47\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 33\n",
      "Current state: 47\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 47\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 34\n",
      "Current state: 47\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 55\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 35\n",
      "Current state: 55\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 55\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 36\n",
      "Current state: 55\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 54\n",
      "Reward: 0.0\n",
      "Terminated: True\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Episode 79\n",
      "Starting state: 0\n",
      "\n",
      "Step 0\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 8\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 1\n",
      "Current state: 8\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 16\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 2\n",
      "Current state: 16\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 24\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 3\n",
      "Current state: 24\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 32\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 4\n",
      "Current state: 32\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 32\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 5\n",
      "Current state: 32\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 32\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 6\n",
      "Current state: 32\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 33\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 7\n",
      "Current state: 33\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 34\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 8\n",
      "Current state: 34\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 35\n",
      "Reward: 0.0\n",
      "Terminated: True\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Episode 80\n",
      "Starting state: 0\n",
      "\n",
      "Step 0\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 8\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 1\n",
      "Current state: 8\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 9\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 2\n",
      "Current state: 9\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 8\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 3\n",
      "Current state: 8\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 8\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 4\n",
      "Current state: 8\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 5\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 6\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 7\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 8\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 1\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 9\n",
      "Current state: 1\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 9\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 10\n",
      "Current state: 9\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 17\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 11\n",
      "Current state: 17\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 18\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 12\n",
      "Current state: 18\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 19\n",
      "Reward: 0.0\n",
      "Terminated: True\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Successful episodes so far: []\n",
      "\n",
      "Episode 81\n",
      "Starting state: 0\n",
      "\n",
      "Step 0\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 1\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 1\n",
      "Current state: 1\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 2\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 2\n",
      "Current state: 2\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 3\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 3\n",
      "Current state: 3\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 11\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 4\n",
      "Current state: 11\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 10\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 5\n",
      "Current state: 10\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 18\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 6\n",
      "Current state: 18\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 19\n",
      "Reward: 0.0\n",
      "Terminated: True\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Episode 82\n",
      "Starting state: 0\n",
      "\n",
      "Step 0\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 1\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 1\n",
      "Current state: 1\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 1\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 2\n",
      "Current state: 1\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 2\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 3\n",
      "Current state: 2\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 3\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 4\n",
      "Current state: 3\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 3\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 5\n",
      "Current state: 3\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 2\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 6\n",
      "Current state: 2\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 1\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 7\n",
      "Current state: 1\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 2\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 8\n",
      "Current state: 2\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 2\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 9\n",
      "Current state: 2\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 3\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 10\n",
      "Current state: 3\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 4\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 11\n",
      "Current state: 4\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 5\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 12\n",
      "Current state: 5\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 4\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 13\n",
      "Current state: 4\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 5\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 14\n",
      "Current state: 5\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 4\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 15\n",
      "Current state: 4\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 3\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 16\n",
      "Current state: 3\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 11\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 17\n",
      "Current state: 11\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 10\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 18\n",
      "Current state: 10\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 9\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 19\n",
      "Current state: 9\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 10\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 20\n",
      "Current state: 10\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 2\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 21\n",
      "Current state: 2\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 2\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 22\n",
      "Current state: 2\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 10\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 23\n",
      "Current state: 10\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 2\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 24\n",
      "Current state: 2\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 10\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 25\n",
      "Current state: 10\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 18\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 26\n",
      "Current state: 18\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 10\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 27\n",
      "Current state: 10\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 18\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 28\n",
      "Current state: 18\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 17\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 29\n",
      "Current state: 17\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 9\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 30\n",
      "Current state: 9\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 10\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 31\n",
      "Current state: 10\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 9\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 32\n",
      "Current state: 9\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 8\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 33\n",
      "Current state: 8\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 9\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 34\n",
      "Current state: 9\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 17\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 35\n",
      "Current state: 17\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 16\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 36\n",
      "Current state: 16\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 16\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 37\n",
      "Current state: 16\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 24\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 38\n",
      "Current state: 24\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 16\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 39\n",
      "Current state: 16\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 24\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 40\n",
      "Current state: 24\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 24\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 41\n",
      "Current state: 24\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 32\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 42\n",
      "Current state: 32\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 33\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 43\n",
      "Current state: 33\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 41\n",
      "Reward: 0.0\n",
      "Terminated: True\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Episode 83\n",
      "Starting state: 0\n",
      "\n",
      "Step 0\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 1\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 8\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 2\n",
      "Current state: 8\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 3\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 1\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 4\n",
      "Current state: 1\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 1\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 5\n",
      "Current state: 1\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 9\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 6\n",
      "Current state: 9\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 17\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 7\n",
      "Current state: 17\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 18\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 8\n",
      "Current state: 18\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 26\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 9\n",
      "Current state: 26\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 27\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 10\n",
      "Current state: 27\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 26\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 11\n",
      "Current state: 26\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 25\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 12\n",
      "Current state: 25\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 26\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 13\n",
      "Current state: 26\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 34\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 14\n",
      "Current state: 34\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 42\n",
      "Reward: 0.0\n",
      "Terminated: True\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Episode 84\n",
      "Starting state: 0\n",
      "\n",
      "Step 0\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 1\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 2\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 8\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 3\n",
      "Current state: 8\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 16\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 4\n",
      "Current state: 16\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 16\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 5\n",
      "Current state: 16\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 16\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 6\n",
      "Current state: 16\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 24\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 7\n",
      "Current state: 24\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 24\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 8\n",
      "Current state: 24\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 16\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 9\n",
      "Current state: 16\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 17\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 10\n",
      "Current state: 17\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 18\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 11\n",
      "Current state: 18\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 26\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 12\n",
      "Current state: 26\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 25\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 13\n",
      "Current state: 25\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 33\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 14\n",
      "Current state: 33\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 41\n",
      "Reward: 0.0\n",
      "Terminated: True\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Episode 85\n",
      "Starting state: 0\n",
      "\n",
      "Step 0\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 1\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 1\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 2\n",
      "Current state: 1\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 1\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 3\n",
      "Current state: 1\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 2\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 4\n",
      "Current state: 2\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 3\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 5\n",
      "Current state: 3\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 4\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 6\n",
      "Current state: 4\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 3\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 7\n",
      "Current state: 3\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 4\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 8\n",
      "Current state: 4\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 5\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 9\n",
      "Current state: 5\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 4\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 10\n",
      "Current state: 4\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 4\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 11\n",
      "Current state: 4\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 12\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 12\n",
      "Current state: 12\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 13\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 13\n",
      "Current state: 13\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 21\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 14\n",
      "Current state: 21\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 20\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 15\n",
      "Current state: 20\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 19\n",
      "Reward: 0.0\n",
      "Terminated: True\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Episode 86\n",
      "Starting state: 0\n",
      "\n",
      "Step 0\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 8\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 1\n",
      "Current state: 8\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 8\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 2\n",
      "Current state: 8\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 3\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 4\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 8\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 5\n",
      "Current state: 8\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 8\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 6\n",
      "Current state: 8\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 9\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 7\n",
      "Current state: 9\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 8\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 8\n",
      "Current state: 8\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 9\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 1\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 10\n",
      "Current state: 1\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 9\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 11\n",
      "Current state: 9\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 17\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 12\n",
      "Current state: 17\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 16\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 13\n",
      "Current state: 16\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 8\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 14\n",
      "Current state: 8\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 15\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 1\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 16\n",
      "Current state: 1\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 9\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 17\n",
      "Current state: 9\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 10\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 18\n",
      "Current state: 10\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 9\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 19\n",
      "Current state: 9\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 10\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 20\n",
      "Current state: 10\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 2\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 21\n",
      "Current state: 2\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 1\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 22\n",
      "Current state: 1\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 1\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 23\n",
      "Current state: 1\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 2\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 24\n",
      "Current state: 2\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 3\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 25\n",
      "Current state: 3\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 3\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 26\n",
      "Current state: 3\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 4\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 27\n",
      "Current state: 4\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 3\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 28\n",
      "Current state: 3\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 11\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 29\n",
      "Current state: 11\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 19\n",
      "Reward: 0.0\n",
      "Terminated: True\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Episode 87\n",
      "Starting state: 0\n",
      "\n",
      "Step 0\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 1\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 1\n",
      "Current state: 1\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 2\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 3\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 8\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 4\n",
      "Current state: 8\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 16\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 5\n",
      "Current state: 16\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 16\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 6\n",
      "Current state: 16\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 8\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 7\n",
      "Current state: 8\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 8\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 1\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 9\n",
      "Current state: 1\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 10\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 8\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 11\n",
      "Current state: 8\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 9\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 12\n",
      "Current state: 9\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 17\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 13\n",
      "Current state: 17\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 16\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 14\n",
      "Current state: 16\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 8\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 15\n",
      "Current state: 8\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 16\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 17\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 1\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 18\n",
      "Current state: 1\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 9\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 19\n",
      "Current state: 9\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 1\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 20\n",
      "Current state: 1\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 1\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 21\n",
      "Current state: 1\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 9\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 22\n",
      "Current state: 9\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 17\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 23\n",
      "Current state: 17\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 25\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 24\n",
      "Current state: 25\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 26\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 25\n",
      "Current state: 26\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 34\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 26\n",
      "Current state: 34\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 35\n",
      "Reward: 0.0\n",
      "Terminated: True\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Episode 88\n",
      "Starting state: 0\n",
      "\n",
      "Step 0\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 8\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 1\n",
      "Current state: 8\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 9\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 2\n",
      "Current state: 9\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 10\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 3\n",
      "Current state: 10\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 18\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 4\n",
      "Current state: 18\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 17\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 5\n",
      "Current state: 17\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 18\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 6\n",
      "Current state: 18\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 10\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 7\n",
      "Current state: 10\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 9\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 8\n",
      "Current state: 9\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 8\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 9\n",
      "Current state: 8\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 9\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 10\n",
      "Current state: 9\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 8\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 11\n",
      "Current state: 8\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 16\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 12\n",
      "Current state: 16\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 24\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 13\n",
      "Current state: 24\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 16\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 14\n",
      "Current state: 16\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 16\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 15\n",
      "Current state: 16\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 16\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 16\n",
      "Current state: 16\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 17\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 17\n",
      "Current state: 17\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 9\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 18\n",
      "Current state: 9\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 1\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 19\n",
      "Current state: 1\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 20\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 1\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 21\n",
      "Current state: 1\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 1\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 22\n",
      "Current state: 1\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 23\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 1\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 24\n",
      "Current state: 1\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 9\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 25\n",
      "Current state: 9\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 1\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 26\n",
      "Current state: 1\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 27\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 1\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 28\n",
      "Current state: 1\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 2\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 29\n",
      "Current state: 2\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 3\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 30\n",
      "Current state: 3\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 3\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 31\n",
      "Current state: 3\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 2\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 32\n",
      "Current state: 2\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 3\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 33\n",
      "Current state: 3\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 3\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 34\n",
      "Current state: 3\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 4\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 35\n",
      "Current state: 4\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 3\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 36\n",
      "Current state: 3\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 2\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 37\n",
      "Current state: 2\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 3\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 38\n",
      "Current state: 3\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 2\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 39\n",
      "Current state: 2\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 10\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 40\n",
      "Current state: 10\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 11\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 41\n",
      "Current state: 11\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 3\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 42\n",
      "Current state: 3\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 2\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 43\n",
      "Current state: 2\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 2\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 44\n",
      "Current state: 2\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 1\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 45\n",
      "Current state: 1\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 2\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 46\n",
      "Current state: 2\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 3\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 47\n",
      "Current state: 3\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 3\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 48\n",
      "Current state: 3\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 2\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 49\n",
      "Current state: 2\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 10\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 50\n",
      "Current state: 10\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 18\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 51\n",
      "Current state: 18\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 17\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 52\n",
      "Current state: 17\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 25\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 53\n",
      "Current state: 25\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 17\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 54\n",
      "Current state: 17\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 16\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 55\n",
      "Current state: 16\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 16\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 56\n",
      "Current state: 16\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 16\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 57\n",
      "Current state: 16\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 8\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 58\n",
      "Current state: 8\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 16\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 59\n",
      "Current state: 16\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 16\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 60\n",
      "Current state: 16\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 24\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 61\n",
      "Current state: 24\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 25\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 62\n",
      "Current state: 25\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 24\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 63\n",
      "Current state: 24\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 16\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 64\n",
      "Current state: 16\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 16\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 65\n",
      "Current state: 16\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 24\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 66\n",
      "Current state: 24\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 25\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 67\n",
      "Current state: 25\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 17\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 68\n",
      "Current state: 17\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 18\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 69\n",
      "Current state: 18\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 19\n",
      "Reward: 0.0\n",
      "Terminated: True\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Episode 89\n",
      "Starting state: 0\n",
      "\n",
      "Step 0\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 8\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 1\n",
      "Current state: 8\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 9\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 2\n",
      "Current state: 9\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 17\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 3\n",
      "Current state: 17\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 25\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 4\n",
      "Current state: 25\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 17\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 5\n",
      "Current state: 17\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 16\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 6\n",
      "Current state: 16\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 24\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 7\n",
      "Current state: 24\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 25\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 8\n",
      "Current state: 25\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 24\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 9\n",
      "Current state: 24\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 32\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 10\n",
      "Current state: 32\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 32\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 11\n",
      "Current state: 32\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 32\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 12\n",
      "Current state: 32\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 33\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 13\n",
      "Current state: 33\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 41\n",
      "Reward: 0.0\n",
      "Terminated: True\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Episode 90\n",
      "Starting state: 0\n",
      "\n",
      "Step 0\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 1\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 1\n",
      "Current state: 1\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 9\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 2\n",
      "Current state: 9\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 17\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 3\n",
      "Current state: 17\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 16\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 4\n",
      "Current state: 16\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 24\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 5\n",
      "Current state: 24\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 24\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 6\n",
      "Current state: 24\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 24\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 7\n",
      "Current state: 24\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 24\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 8\n",
      "Current state: 24\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 24\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 9\n",
      "Current state: 24\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 32\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 10\n",
      "Current state: 32\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 24\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 11\n",
      "Current state: 24\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 25\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 12\n",
      "Current state: 25\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 33\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 13\n",
      "Current state: 33\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 34\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 14\n",
      "Current state: 34\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 26\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 15\n",
      "Current state: 26\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 34\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 16\n",
      "Current state: 34\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 42\n",
      "Reward: 0.0\n",
      "Terminated: True\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Successful episodes so far: []\n",
      "\n",
      "Episode 91\n",
      "Starting state: 0\n",
      "\n",
      "Step 0\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 1\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 2\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 3\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 4\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 5\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 6\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 8\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 7\n",
      "Current state: 8\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 16\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 8\n",
      "Current state: 16\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 17\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 9\n",
      "Current state: 17\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 18\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 10\n",
      "Current state: 18\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 10\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 11\n",
      "Current state: 10\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 9\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 12\n",
      "Current state: 9\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 10\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 13\n",
      "Current state: 10\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 2\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 14\n",
      "Current state: 2\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 2\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 15\n",
      "Current state: 2\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 1\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 16\n",
      "Current state: 1\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 1\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 17\n",
      "Current state: 1\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 2\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 18\n",
      "Current state: 2\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 1\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 19\n",
      "Current state: 1\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 9\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 20\n",
      "Current state: 9\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 17\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 21\n",
      "Current state: 17\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 9\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 22\n",
      "Current state: 9\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 17\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 23\n",
      "Current state: 17\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 9\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 24\n",
      "Current state: 9\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 17\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 25\n",
      "Current state: 17\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 25\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 26\n",
      "Current state: 25\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 33\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 27\n",
      "Current state: 33\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 34\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 28\n",
      "Current state: 34\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 35\n",
      "Reward: 0.0\n",
      "Terminated: True\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Episode 92\n",
      "Starting state: 0\n",
      "\n",
      "Step 0\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 1\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 2\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 3\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 8\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 4\n",
      "Current state: 8\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 16\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 5\n",
      "Current state: 16\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 24\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 6\n",
      "Current state: 24\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 25\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 7\n",
      "Current state: 25\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 33\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 8\n",
      "Current state: 33\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 41\n",
      "Reward: 0.0\n",
      "Terminated: True\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Episode 93\n",
      "Starting state: 0\n",
      "\n",
      "Step 0\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 1\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 2\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 3\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 1\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 4\n",
      "Current state: 1\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 9\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 5\n",
      "Current state: 9\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 1\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 6\n",
      "Current state: 1\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 9\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 7\n",
      "Current state: 9\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 8\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 8\n",
      "Current state: 8\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 9\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 10\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 11\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 12\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 8\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 13\n",
      "Current state: 8\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 8\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 14\n",
      "Current state: 8\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 16\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 15\n",
      "Current state: 16\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 8\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 16\n",
      "Current state: 8\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 9\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 17\n",
      "Current state: 9\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 10\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 18\n",
      "Current state: 10\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 9\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 19\n",
      "Current state: 9\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 8\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 20\n",
      "Current state: 8\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 16\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 21\n",
      "Current state: 16\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 24\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 22\n",
      "Current state: 24\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 25\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 23\n",
      "Current state: 25\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 26\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 24\n",
      "Current state: 26\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 27\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 25\n",
      "Current state: 27\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 26\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 26\n",
      "Current state: 26\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 25\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 27\n",
      "Current state: 25\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 33\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 28\n",
      "Current state: 33\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 25\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 29\n",
      "Current state: 25\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 24\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 30\n",
      "Current state: 24\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 24\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 31\n",
      "Current state: 24\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 24\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 32\n",
      "Current state: 24\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 25\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 33\n",
      "Current state: 25\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 17\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 34\n",
      "Current state: 17\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 9\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 35\n",
      "Current state: 9\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 1\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 36\n",
      "Current state: 1\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 9\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 37\n",
      "Current state: 9\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 8\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 38\n",
      "Current state: 8\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 9\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 39\n",
      "Current state: 9\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 1\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 40\n",
      "Current state: 1\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 2\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 41\n",
      "Current state: 2\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 1\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 42\n",
      "Current state: 1\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 9\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 43\n",
      "Current state: 9\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 10\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 44\n",
      "Current state: 10\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 11\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 45\n",
      "Current state: 11\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 19\n",
      "Reward: 0.0\n",
      "Terminated: True\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Episode 94\n",
      "Starting state: 0\n",
      "\n",
      "Step 0\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 1\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 1\n",
      "Current state: 1\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 2\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 2\n",
      "Current state: 2\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 10\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 3\n",
      "Current state: 10\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 11\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 4\n",
      "Current state: 11\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 10\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 5\n",
      "Current state: 10\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 18\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 6\n",
      "Current state: 18\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 19\n",
      "Reward: 0.0\n",
      "Terminated: True\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Episode 95\n",
      "Starting state: 0\n",
      "\n",
      "Step 0\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 8\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 1\n",
      "Current state: 8\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 16\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 2\n",
      "Current state: 16\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 17\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 3\n",
      "Current state: 17\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 9\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 4\n",
      "Current state: 9\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 17\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 5\n",
      "Current state: 17\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 16\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 6\n",
      "Current state: 16\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 16\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 7\n",
      "Current state: 16\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 24\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 8\n",
      "Current state: 24\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 24\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 9\n",
      "Current state: 24\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 24\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 10\n",
      "Current state: 24\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 24\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 11\n",
      "Current state: 24\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 24\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 12\n",
      "Current state: 24\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 16\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 13\n",
      "Current state: 16\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 16\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 14\n",
      "Current state: 16\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 8\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 15\n",
      "Current state: 8\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 9\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 16\n",
      "Current state: 9\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 8\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 17\n",
      "Current state: 8\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 18\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 19\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 1\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 20\n",
      "Current state: 1\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 21\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 1\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 22\n",
      "Current state: 1\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 9\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 23\n",
      "Current state: 9\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 8\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 24\n",
      "Current state: 8\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 8\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 25\n",
      "Current state: 8\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 8\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 26\n",
      "Current state: 8\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 9\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 27\n",
      "Current state: 9\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 17\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 28\n",
      "Current state: 17\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 25\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 29\n",
      "Current state: 25\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 33\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 30\n",
      "Current state: 33\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 41\n",
      "Reward: 0.0\n",
      "Terminated: True\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Episode 96\n",
      "Starting state: 0\n",
      "\n",
      "Step 0\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 8\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 1\n",
      "Current state: 8\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 2\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 3\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 4\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 1\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 5\n",
      "Current state: 1\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 9\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 6\n",
      "Current state: 9\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 10\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 7\n",
      "Current state: 10\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 11\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 8\n",
      "Current state: 11\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 12\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 9\n",
      "Current state: 12\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 13\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 10\n",
      "Current state: 13\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 5\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 11\n",
      "Current state: 5\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 4\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 12\n",
      "Current state: 4\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 4\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 13\n",
      "Current state: 4\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 5\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 14\n",
      "Current state: 5\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 6\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 15\n",
      "Current state: 6\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 6\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 16\n",
      "Current state: 6\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 5\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 17\n",
      "Current state: 5\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 6\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 18\n",
      "Current state: 6\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 14\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 19\n",
      "Current state: 14\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 6\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 20\n",
      "Current state: 6\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 7\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 21\n",
      "Current state: 7\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 7\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 22\n",
      "Current state: 7\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 15\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 23\n",
      "Current state: 15\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 15\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 24\n",
      "Current state: 15\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 7\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 25\n",
      "Current state: 7\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 7\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 26\n",
      "Current state: 7\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 15\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 27\n",
      "Current state: 15\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 7\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 28\n",
      "Current state: 7\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 6\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 29\n",
      "Current state: 6\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 7\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 30\n",
      "Current state: 7\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 15\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 31\n",
      "Current state: 15\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 7\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 32\n",
      "Current state: 7\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 7\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 33\n",
      "Current state: 7\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 15\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 34\n",
      "Current state: 15\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 23\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 35\n",
      "Current state: 23\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 15\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 36\n",
      "Current state: 15\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 7\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 37\n",
      "Current state: 7\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 7\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 38\n",
      "Current state: 7\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 7\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 39\n",
      "Current state: 7\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 15\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 40\n",
      "Current state: 15\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 15\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 41\n",
      "Current state: 15\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 23\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 42\n",
      "Current state: 23\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 22\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 43\n",
      "Current state: 22\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 30\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 44\n",
      "Current state: 30\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 22\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 45\n",
      "Current state: 22\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 14\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 46\n",
      "Current state: 14\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 22\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 47\n",
      "Current state: 22\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 21\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 48\n",
      "Current state: 21\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 20\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 49\n",
      "Current state: 20\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 28\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 50\n",
      "Current state: 28\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 29\n",
      "Reward: 0.0\n",
      "Terminated: True\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Episode 97\n",
      "Starting state: 0\n",
      "\n",
      "Step 0\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 8\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 1\n",
      "Current state: 8\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 8\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 2\n",
      "Current state: 8\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 8\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 3\n",
      "Current state: 8\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 8\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 4\n",
      "Current state: 8\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 5\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 6\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 1\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 7\n",
      "Current state: 1\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 9\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 8\n",
      "Current state: 9\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 1\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 9\n",
      "Current state: 1\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 2\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 10\n",
      "Current state: 2\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 10\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 11\n",
      "Current state: 10\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 18\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 12\n",
      "Current state: 18\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 17\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 13\n",
      "Current state: 17\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 9\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 14\n",
      "Current state: 9\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 8\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 15\n",
      "Current state: 8\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 9\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 16\n",
      "Current state: 9\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 8\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 17\n",
      "Current state: 8\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 9\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 18\n",
      "Current state: 9\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 10\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 19\n",
      "Current state: 10\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 18\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 20\n",
      "Current state: 18\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 26\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 21\n",
      "Current state: 26\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 34\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 22\n",
      "Current state: 34\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 26\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 23\n",
      "Current state: 26\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 27\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 24\n",
      "Current state: 27\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 19\n",
      "Reward: 0.0\n",
      "Terminated: True\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Episode 98\n",
      "Starting state: 0\n",
      "\n",
      "Step 0\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 1\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 8\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 2\n",
      "Current state: 8\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 3\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 4\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 5\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 6\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 1\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 7\n",
      "Current state: 1\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 2\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 8\n",
      "Current state: 2\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 1\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 9\n",
      "Current state: 1\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 9\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 10\n",
      "Current state: 9\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 17\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 11\n",
      "Current state: 17\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 25\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 12\n",
      "Current state: 25\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 24\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 13\n",
      "Current state: 24\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 25\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 14\n",
      "Current state: 25\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 33\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 15\n",
      "Current state: 33\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 32\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 16\n",
      "Current state: 32\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 24\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 17\n",
      "Current state: 24\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 16\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 18\n",
      "Current state: 16\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 8\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 19\n",
      "Current state: 8\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 20\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 21\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 22\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 23\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 8\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 24\n",
      "Current state: 8\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 9\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 25\n",
      "Current state: 9\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 8\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 26\n",
      "Current state: 8\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 16\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 27\n",
      "Current state: 16\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 24\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 28\n",
      "Current state: 24\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 25\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 29\n",
      "Current state: 25\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 17\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 30\n",
      "Current state: 17\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 16\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 31\n",
      "Current state: 16\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 0\n",
      "New state: 16\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 32\n",
      "Current state: 16\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 17\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 33\n",
      "Current state: 17\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 25\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 34\n",
      "Current state: 25\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 33\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 35\n",
      "Current state: 33\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 41\n",
      "Reward: 0.0\n",
      "Terminated: True\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Episode 99\n",
      "Starting state: 0\n",
      "\n",
      "Step 0\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 1\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 8\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 2\n",
      "Current state: 8\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 3\n",
      "New state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 3\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 8\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 4\n",
      "Current state: 8\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 16\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 5\n",
      "Current state: 16\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 24\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 6\n",
      "Current state: 24\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 32\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 7\n",
      "Current state: 32\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 40\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 8\n",
      "Current state: 40\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 1\n",
      "New state: 48\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Step 9\n",
      "Current state: 48\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Chosen action: 2\n",
      "New state: 49\n",
      "Reward: 0.0\n",
      "Terminated: True\n",
      "Old Q-value: 0.0\n",
      "New Q-value: 0.0\n",
      "\n",
      "Debug Summary:\n",
      "Total successful episodes: 0\n",
      "Successful episodes: []\n"
     ]
    }
   ],
   "execution_count": 15
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-17T12:32:45.583745Z",
     "start_time": "2025-02-17T12:32:45.574074Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Reset and render the environment to see the layout\n",
    "env.reset()\n",
    "print(\"\\nEnvironment Layout (8x8 grid):\")\n",
    "print(env.render())\n",
    "\n",
    "# Print what each character means\n",
    "print(\"\\nSymbol meanings:\")\n",
    "print(\"S: Starting position (state 0)\")\n",
    "print(\"F: Frozen surface (safe)\")\n",
    "print(\"H: Hole (game over)\")\n",
    "print(\"G: Goal (state 63)\")"
   ],
   "id": "5bb393e3dcd47376",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Environment Layout (8x8 grid):\n",
      "[[[180 200 230]\n",
      "  [180 200 230]\n",
      "  [180 200 230]\n",
      "  ...\n",
      "  [180 200 230]\n",
      "  [180 200 230]\n",
      "  [180 200 230]]\n",
      "\n",
      " [[180 200 230]\n",
      "  [204 230 255]\n",
      "  [204 230 255]\n",
      "  ...\n",
      "  [204 230 255]\n",
      "  [204 230 255]\n",
      "  [180 200 230]]\n",
      "\n",
      " [[180 200 230]\n",
      "  [235 245 249]\n",
      "  [204 230 255]\n",
      "  ...\n",
      "  [204 230 255]\n",
      "  [204 230 255]\n",
      "  [180 200 230]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[180 200 230]\n",
      "  [235 245 249]\n",
      "  [235 245 249]\n",
      "  ...\n",
      "  [204 230 255]\n",
      "  [235 245 249]\n",
      "  [180 200 230]]\n",
      "\n",
      " [[180 200 230]\n",
      "  [235 245 249]\n",
      "  [235 245 249]\n",
      "  ...\n",
      "  [204 230 255]\n",
      "  [204 230 255]\n",
      "  [180 200 230]]\n",
      "\n",
      " [[180 200 230]\n",
      "  [180 200 230]\n",
      "  [180 200 230]\n",
      "  ...\n",
      "  [180 200 230]\n",
      "  [180 200 230]\n",
      "  [180 200 230]]]\n",
      "\n",
      "Symbol meanings:\n",
      "S: Starting position (state 0)\n",
      "F: Frozen surface (safe)\n",
      "H: Hole (game over)\n",
      "G: Goal (state 63)\n"
     ]
    }
   ],
   "execution_count": 16
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-17T12:33:10.399890Z",
     "start_time": "2025-02-17T12:33:10.143745Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def improved_debug_train(n_training_episodes, min_epsilon, max_epsilon, decay_rate, env, max_steps, Qtable):\n",
    "    # Lists to keep track of rewards\n",
    "    episode_rewards = []\n",
    "    successful_episodes = []\n",
    "\n",
    "    for episode in range(min(100, n_training_episodes)):\n",
    "        epsilon = min_epsilon + (max_epsilon - min_epsilon)*np.exp(-decay_rate*episode)\n",
    "        state, info = env.reset()\n",
    "        total_reward = 0\n",
    "\n",
    "        print(f\"\\nEpisode {episode}\")\n",
    "        print(f\"Starting state: {state}\")\n",
    "\n",
    "        for step in range(max_steps):\n",
    "            # Current state info\n",
    "            print(f\"\\nStep {step}\")\n",
    "            print(f\"Current state: {state}\")\n",
    "            print(f\"Q-values for current state: {Qtable[state]}\")\n",
    "\n",
    "            # Choose action with epsilon-greedy\n",
    "            random_num = random.uniform(0, 1)\n",
    "            if random_num > epsilon:\n",
    "                action = np.argmax(Qtable[state])\n",
    "                print(f\"Exploiting - Chosen action: {action}\")\n",
    "            else:\n",
    "                action = env.action_space.sample()\n",
    "                print(f\"Exploring - Chosen action: {action}\")\n",
    "\n",
    "            # Take action\n",
    "            old_q = Qtable[state][action]\n",
    "            new_state, reward, terminated, truncated, info = env.step(action)\n",
    "\n",
    "            # Add small negative reward for each step to encourage shorter paths\n",
    "            reward = reward - 0.01\n",
    "\n",
    "            # Calculate new Q-value\n",
    "            if terminated and reward <= 0:  # If terminated in a hole\n",
    "                new_q = Qtable[state][action] + learning_rate * (reward - Qtable[state][action])\n",
    "            else:  # Normal case or reached goal\n",
    "                new_q = Qtable[state][action] + learning_rate * (\n",
    "                        reward + gamma * np.max(Qtable[new_state]) - Qtable[state][action]\n",
    "                )\n",
    "\n",
    "            # Update Q-value\n",
    "            Qtable[state][action] = new_q\n",
    "\n",
    "            print(f\"New state: {new_state}\")\n",
    "            print(f\"Reward: {reward}\")\n",
    "            print(f\"Terminated: {terminated}\")\n",
    "            print(f\"Old Q-value: {old_q}\")\n",
    "            print(f\"New Q-value: {new_q}\")\n",
    "\n",
    "            total_reward += reward\n",
    "\n",
    "            if reward > 0:\n",
    "                print(\"GOAL REACHED!\")\n",
    "                successful_episodes.append(episode)\n",
    "\n",
    "            if terminated or truncated:\n",
    "                break\n",
    "\n",
    "            state = new_state\n",
    "\n",
    "        episode_rewards.append(total_reward)\n",
    "        if episode % 10 == 0:\n",
    "            print(f\"\\nEpisode {episode} summary:\")\n",
    "            print(f\"Total reward: {total_reward}\")\n",
    "            print(f\"Successful episodes so far: {successful_episodes}\")\n",
    "            print(f\"Average reward over last {min(episode+1, 10)} episodes: {np.mean(episode_rewards[-10:]):.3f}\")\n",
    "\n",
    "    return Qtable, successful_episodes, episode_rewards\n",
    "\n",
    "# Updated parameters\n",
    "learning_rate = 0.1\n",
    "gamma = 0.95\n",
    "max_steps = 200\n",
    "max_epsilon = 1.0\n",
    "min_epsilon = 0.01\n",
    "decay_rate = 0.001\n",
    "\n",
    "# Initialize fresh Q-table\n",
    "Qtable_frozenlake = initialize_q_table(state_space, action_space)\n",
    "\n",
    "# Run improved debug training\n",
    "print(\"\\nStarting improved debug training...\")\n",
    "Qtable_frozenlake, successful_episodes, episode_rewards = improved_debug_train(\n",
    "    n_training_episodes=100,\n",
    "    min_epsilon=max_epsilon,\n",
    "    max_epsilon=max_epsilon,\n",
    "    decay_rate=decay_rate,\n",
    "    env=env,\n",
    "    max_steps=max_steps,\n",
    "    Qtable=Qtable_frozenlake\n",
    ")"
   ],
   "id": "116e2d194fd5ffeb",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting improved debug training...\n",
      "\n",
      "Episode 0\n",
      "Starting state: 0\n",
      "\n",
      "Step 0\n",
      "Current state: 0\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 8\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: -0.001\n",
      "\n",
      "Step 1\n",
      "Current state: 8\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 8\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: -0.001\n",
      "\n",
      "Step 2\n",
      "Current state: 8\n",
      "Q-values for current state: [-0.001  0.     0.     0.   ]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 0\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: -0.001\n",
      "\n",
      "Step 3\n",
      "Current state: 0\n",
      "Q-values for current state: [ 0.    -0.001  0.     0.   ]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 1\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: -0.001\n",
      "\n",
      "Step 4\n",
      "Current state: 1\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 1\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: -0.001\n",
      "\n",
      "Step 5\n",
      "Current state: 1\n",
      "Q-values for current state: [ 0.     0.     0.    -0.001]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 2\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: -0.001\n",
      "\n",
      "Step 6\n",
      "Current state: 2\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 3\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: -0.001\n",
      "\n",
      "Step 7\n",
      "Current state: 3\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 11\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: -0.001\n",
      "\n",
      "Step 8\n",
      "Current state: 11\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 12\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: -0.001\n",
      "\n",
      "Step 9\n",
      "Current state: 12\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 13\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: -0.001\n",
      "\n",
      "Step 10\n",
      "Current state: 13\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 12\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: -0.001\n",
      "\n",
      "Step 11\n",
      "Current state: 12\n",
      "Q-values for current state: [ 0.     0.    -0.001  0.   ]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 13\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.001\n",
      "New Q-value: -0.0019000000000000002\n",
      "\n",
      "Step 12\n",
      "Current state: 13\n",
      "Q-values for current state: [-0.001  0.     0.     0.   ]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 5\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: -0.001\n",
      "\n",
      "Step 13\n",
      "Current state: 5\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 6\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: -0.001\n",
      "\n",
      "Step 14\n",
      "Current state: 6\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 7\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: -0.001\n",
      "\n",
      "Step 15\n",
      "Current state: 7\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 15\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: -0.001\n",
      "\n",
      "Step 16\n",
      "Current state: 15\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 7\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: -0.001\n",
      "\n",
      "Step 17\n",
      "Current state: 7\n",
      "Q-values for current state: [ 0.    -0.001  0.     0.   ]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 6\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: -0.001\n",
      "\n",
      "Step 18\n",
      "Current state: 6\n",
      "Q-values for current state: [ 0.     0.    -0.001  0.   ]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 5\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: -0.001\n",
      "\n",
      "Step 19\n",
      "Current state: 5\n",
      "Q-values for current state: [ 0.     0.    -0.001  0.   ]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 6\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.001\n",
      "New Q-value: -0.0019000000000000002\n",
      "\n",
      "Step 20\n",
      "Current state: 6\n",
      "Q-values for current state: [-0.001  0.    -0.001  0.   ]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 14\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: -0.001\n",
      "\n",
      "Step 21\n",
      "Current state: 14\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 22\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: -0.001\n",
      "\n",
      "Step 22\n",
      "Current state: 22\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 30\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: -0.001\n",
      "\n",
      "Step 23\n",
      "Current state: 30\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 22\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: -0.001\n",
      "\n",
      "Step 24\n",
      "Current state: 22\n",
      "Q-values for current state: [ 0.    -0.001  0.     0.   ]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 23\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: -0.001\n",
      "\n",
      "Step 25\n",
      "Current state: 23\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 15\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: -0.001\n",
      "\n",
      "Step 26\n",
      "Current state: 15\n",
      "Q-values for current state: [ 0.     0.     0.    -0.001]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 7\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.001\n",
      "New Q-value: -0.0019000000000000002\n",
      "\n",
      "Step 27\n",
      "Current state: 7\n",
      "Q-values for current state: [-0.001 -0.001  0.     0.   ]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 7\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: -0.001\n",
      "\n",
      "Step 28\n",
      "Current state: 7\n",
      "Q-values for current state: [-0.001 -0.001  0.    -0.001]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 15\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.001\n",
      "New Q-value: -0.0019000000000000002\n",
      "\n",
      "Step 29\n",
      "Current state: 15\n",
      "Q-values for current state: [ 0.      0.      0.     -0.0019]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 14\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: -0.001\n",
      "\n",
      "Step 30\n",
      "Current state: 14\n",
      "Q-values for current state: [ 0.    -0.001  0.     0.   ]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 13\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: -0.001\n",
      "\n",
      "Step 31\n",
      "Current state: 13\n",
      "Q-values for current state: [-0.001  0.     0.    -0.001]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 21\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: -0.001\n",
      "\n",
      "Step 32\n",
      "Current state: 21\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 13\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: -0.001\n",
      "\n",
      "Step 33\n",
      "Current state: 13\n",
      "Q-values for current state: [-0.001 -0.001  0.    -0.001]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 21\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.001\n",
      "New Q-value: -0.0019000000000000002\n",
      "\n",
      "Step 34\n",
      "Current state: 21\n",
      "Q-values for current state: [ 0.     0.     0.    -0.001]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 22\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: -0.001\n",
      "\n",
      "Step 35\n",
      "Current state: 22\n",
      "Q-values for current state: [ 0.    -0.001 -0.001  0.   ]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 14\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: -0.001\n",
      "\n",
      "Step 36\n",
      "Current state: 14\n",
      "Q-values for current state: [-0.001 -0.001  0.     0.   ]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 13\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.001\n",
      "New Q-value: -0.0019000000000000002\n",
      "\n",
      "Step 37\n",
      "Current state: 13\n",
      "Q-values for current state: [-0.001  -0.0019  0.     -0.001 ]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 5\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.001\n",
      "New Q-value: -0.0019000000000000002\n",
      "\n",
      "Step 38\n",
      "Current state: 5\n",
      "Q-values for current state: [ 0.      0.     -0.0019  0.    ]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 4\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: -0.001\n",
      "\n",
      "Step 39\n",
      "Current state: 4\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 3\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: -0.001\n",
      "\n",
      "Step 40\n",
      "Current state: 3\n",
      "Q-values for current state: [ 0.    -0.001  0.     0.   ]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 4\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: -0.001\n",
      "\n",
      "Step 41\n",
      "Current state: 4\n",
      "Q-values for current state: [-0.001  0.     0.     0.   ]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 4\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: -0.001\n",
      "\n",
      "Step 42\n",
      "Current state: 4\n",
      "Q-values for current state: [-0.001  0.     0.    -0.001]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 12\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: -0.001\n",
      "\n",
      "Step 43\n",
      "Current state: 12\n",
      "Q-values for current state: [ 0.      0.     -0.0019  0.    ]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 11\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: -0.001\n",
      "\n",
      "Step 44\n",
      "Current state: 11\n",
      "Q-values for current state: [ 0.     0.    -0.001  0.   ]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 19\n",
      "Reward: -0.01\n",
      "Terminated: True\n",
      "Old Q-value: 0.0\n",
      "New Q-value: -0.001\n",
      "\n",
      "Episode 0 summary:\n",
      "Total reward: -0.45000000000000023\n",
      "Successful episodes so far: []\n",
      "Average reward over last 1 episodes: -0.450\n",
      "\n",
      "Episode 1\n",
      "Starting state: 0\n",
      "\n",
      "Step 0\n",
      "Current state: 0\n",
      "Q-values for current state: [ 0.    -0.001 -0.001  0.   ]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 8\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.001\n",
      "New Q-value: -0.0019000000000000002\n",
      "\n",
      "Step 1\n",
      "Current state: 8\n",
      "Q-values for current state: [-0.001  0.     0.    -0.001]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 8\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.001\n",
      "New Q-value: -0.0019000000000000002\n",
      "\n",
      "Step 2\n",
      "Current state: 8\n",
      "Q-values for current state: [-0.0019  0.      0.     -0.001 ]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 16\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: -0.001\n",
      "\n",
      "Step 3\n",
      "Current state: 16\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 16\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: -0.001\n",
      "\n",
      "Step 4\n",
      "Current state: 16\n",
      "Q-values for current state: [-0.001  0.     0.     0.   ]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 17\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: -0.001\n",
      "\n",
      "Step 5\n",
      "Current state: 17\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 25\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: -0.001\n",
      "\n",
      "Step 6\n",
      "Current state: 25\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 24\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: -0.001\n",
      "\n",
      "Step 7\n",
      "Current state: 24\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 32\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: -0.001\n",
      "\n",
      "Step 8\n",
      "Current state: 32\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 24\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: -0.001\n",
      "\n",
      "Step 9\n",
      "Current state: 24\n",
      "Q-values for current state: [ 0.    -0.001  0.     0.   ]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 24\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: -0.001\n",
      "\n",
      "Step 10\n",
      "Current state: 24\n",
      "Q-values for current state: [-0.001 -0.001  0.     0.   ]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 24\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.001\n",
      "New Q-value: -0.0019000000000000002\n",
      "\n",
      "Step 11\n",
      "Current state: 24\n",
      "Q-values for current state: [-0.0019 -0.001   0.      0.    ]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 24\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.0019000000000000002\n",
      "New Q-value: -0.00271\n",
      "\n",
      "Step 12\n",
      "Current state: 24\n",
      "Q-values for current state: [-0.00271 -0.001    0.       0.     ]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 16\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: -0.001\n",
      "\n",
      "Step 13\n",
      "Current state: 16\n",
      "Q-values for current state: [-0.001  0.    -0.001  0.   ]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 24\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: -0.001\n",
      "\n",
      "Step 14\n",
      "Current state: 24\n",
      "Q-values for current state: [-0.00271 -0.001    0.      -0.001  ]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 32\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.001\n",
      "New Q-value: -0.0019000000000000002\n",
      "\n",
      "Step 15\n",
      "Current state: 32\n",
      "Q-values for current state: [ 0.     0.     0.    -0.001]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 33\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: -0.001\n",
      "\n",
      "Step 16\n",
      "Current state: 33\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 41\n",
      "Reward: -0.01\n",
      "Terminated: True\n",
      "Old Q-value: 0.0\n",
      "New Q-value: -0.001\n",
      "\n",
      "Episode 2\n",
      "Starting state: 0\n",
      "\n",
      "Step 0\n",
      "Current state: 0\n",
      "Q-values for current state: [ 0.     -0.0019 -0.001   0.    ]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 1\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.001\n",
      "New Q-value: -0.0019000000000000002\n",
      "\n",
      "Step 1\n",
      "Current state: 1\n",
      "Q-values for current state: [ 0.     0.    -0.001 -0.001]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 9\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: -0.001\n",
      "\n",
      "Step 2\n",
      "Current state: 9\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 1\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: -0.001\n",
      "\n",
      "Step 3\n",
      "Current state: 1\n",
      "Q-values for current state: [ 0.    -0.001 -0.001 -0.001]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 2\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.001\n",
      "New Q-value: -0.0019000000000000002\n",
      "\n",
      "Step 4\n",
      "Current state: 2\n",
      "Q-values for current state: [ 0.     0.    -0.001  0.   ]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 1\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: -0.001\n",
      "\n",
      "Step 5\n",
      "Current state: 1\n",
      "Q-values for current state: [ 0.     -0.001  -0.0019 -0.001 ]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 2\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.0019000000000000002\n",
      "New Q-value: -0.00271\n",
      "\n",
      "Step 6\n",
      "Current state: 2\n",
      "Q-values for current state: [-0.001  0.    -0.001  0.   ]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 10\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: -0.001\n",
      "\n",
      "Step 7\n",
      "Current state: 10\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 18\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: -0.001\n",
      "\n",
      "Step 8\n",
      "Current state: 18\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 17\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: -0.001\n",
      "\n",
      "Step 9\n",
      "Current state: 17\n",
      "Q-values for current state: [ 0.    -0.001  0.     0.   ]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 9\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: -0.001\n",
      "\n",
      "Step 10\n",
      "Current state: 9\n",
      "Q-values for current state: [ 0.     0.     0.    -0.001]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 10\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: -0.001\n",
      "\n",
      "Step 11\n",
      "Current state: 10\n",
      "Q-values for current state: [ 0.    -0.001  0.     0.   ]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 18\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.001\n",
      "New Q-value: -0.0019000000000000002\n",
      "\n",
      "Step 12\n",
      "Current state: 18\n",
      "Q-values for current state: [-0.001  0.     0.     0.   ]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 19\n",
      "Reward: -0.01\n",
      "Terminated: True\n",
      "Old Q-value: 0.0\n",
      "New Q-value: -0.001\n",
      "\n",
      "Episode 3\n",
      "Starting state: 0\n",
      "\n",
      "Step 0\n",
      "Current state: 0\n",
      "Q-values for current state: [ 0.     -0.0019 -0.0019  0.    ]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 0\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: -0.001\n",
      "\n",
      "Step 1\n",
      "Current state: 0\n",
      "Q-values for current state: [ 0.     -0.0019 -0.0019 -0.001 ]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 1\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.0019000000000000002\n",
      "New Q-value: -0.00271\n",
      "\n",
      "Step 2\n",
      "Current state: 1\n",
      "Q-values for current state: [ 0.      -0.001   -0.00271 -0.001  ]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 0\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: -0.001\n",
      "\n",
      "Step 3\n",
      "Current state: 0\n",
      "Q-values for current state: [ 0.      -0.0019  -0.00271 -0.001  ]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 0\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.001\n",
      "New Q-value: -0.0019000000000000002\n",
      "\n",
      "Step 4\n",
      "Current state: 0\n",
      "Q-values for current state: [ 0.      -0.0019  -0.00271 -0.0019 ]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 1\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.00271\n",
      "New Q-value: -0.0035340000000000002\n",
      "\n",
      "Step 5\n",
      "Current state: 1\n",
      "Q-values for current state: [-0.001   -0.001   -0.00271 -0.001  ]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 2\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.00271\n",
      "New Q-value: -0.003439\n",
      "\n",
      "Step 6\n",
      "Current state: 2\n",
      "Q-values for current state: [-0.001 -0.001 -0.001  0.   ]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 10\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.001\n",
      "New Q-value: -0.0019000000000000002\n",
      "\n",
      "Step 7\n",
      "Current state: 10\n",
      "Q-values for current state: [ 0.     -0.0019  0.      0.    ]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 9\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: -0.001\n",
      "\n",
      "Step 8\n",
      "Current state: 9\n",
      "Q-values for current state: [ 0.     0.    -0.001 -0.001]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 17\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: -0.001\n",
      "\n",
      "Step 9\n",
      "Current state: 17\n",
      "Q-values for current state: [ 0.    -0.001  0.    -0.001]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 16\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: -0.001\n",
      "\n",
      "Step 10\n",
      "Current state: 16\n",
      "Q-values for current state: [-0.001 -0.001 -0.001  0.   ]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 17\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.001\n",
      "New Q-value: -0.0019000000000000002\n",
      "\n",
      "Step 11\n",
      "Current state: 17\n",
      "Q-values for current state: [-0.001 -0.001  0.    -0.001]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 9\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.001\n",
      "New Q-value: -0.0019000000000000002\n",
      "\n",
      "Step 12\n",
      "Current state: 9\n",
      "Q-values for current state: [ 0.    -0.001 -0.001 -0.001]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 17\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.001\n",
      "New Q-value: -0.0019000000000000002\n",
      "\n",
      "Step 13\n",
      "Current state: 17\n",
      "Q-values for current state: [-0.001  -0.001   0.     -0.0019]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 9\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.0019000000000000002\n",
      "New Q-value: -0.00271\n",
      "\n",
      "Step 14\n",
      "Current state: 9\n",
      "Q-values for current state: [ 0.     -0.0019 -0.001  -0.001 ]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 1\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.001\n",
      "New Q-value: -0.0019950000000000002\n",
      "\n",
      "Step 15\n",
      "Current state: 1\n",
      "Q-values for current state: [-0.001    -0.001    -0.003439 -0.001   ]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 1\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.001\n",
      "New Q-value: -0.0019950000000000002\n",
      "\n",
      "Step 16\n",
      "Current state: 1\n",
      "Q-values for current state: [-0.001    -0.001    -0.003439 -0.001995]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 9\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.001\n",
      "New Q-value: -0.0019000000000000002\n",
      "\n",
      "Step 17\n",
      "Current state: 9\n",
      "Q-values for current state: [ 0.       -0.0019   -0.001    -0.001995]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 10\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.001\n",
      "New Q-value: -0.0019000000000000002\n",
      "\n",
      "Step 18\n",
      "Current state: 10\n",
      "Q-values for current state: [-0.001  -0.0019  0.      0.    ]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 9\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.001\n",
      "New Q-value: -0.0019000000000000002\n",
      "\n",
      "Step 19\n",
      "Current state: 9\n",
      "Q-values for current state: [ 0.       -0.0019   -0.0019   -0.001995]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 8\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: -0.001\n",
      "\n",
      "Step 20\n",
      "Current state: 8\n",
      "Q-values for current state: [-0.0019 -0.001   0.     -0.001 ]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 16\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.001\n",
      "New Q-value: -0.0019000000000000002\n",
      "\n",
      "Step 21\n",
      "Current state: 16\n",
      "Q-values for current state: [-0.001  -0.001  -0.0019  0.    ]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 8\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: -0.001\n",
      "\n",
      "Step 22\n",
      "Current state: 8\n",
      "Q-values for current state: [-0.0019 -0.0019  0.     -0.001 ]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 9\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: -0.001095\n",
      "\n",
      "Step 23\n",
      "Current state: 9\n",
      "Q-values for current state: [-0.001    -0.0019   -0.0019   -0.001995]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 10\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.0019000000000000002\n",
      "New Q-value: -0.00271\n",
      "\n",
      "Step 24\n",
      "Current state: 10\n",
      "Q-values for current state: [-0.0019 -0.0019  0.      0.    ]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 9\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.0019000000000000002\n",
      "New Q-value: -0.002805\n",
      "\n",
      "Step 25\n",
      "Current state: 9\n",
      "Q-values for current state: [-0.001    -0.0019   -0.00271  -0.001995]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 8\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.001\n",
      "New Q-value: -0.0019950000000000002\n",
      "\n",
      "Step 26\n",
      "Current state: 8\n",
      "Q-values for current state: [-0.0019   -0.0019   -0.001095 -0.001   ]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 16\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.0019000000000000002\n",
      "New Q-value: -0.002805\n",
      "\n",
      "Step 27\n",
      "Current state: 16\n",
      "Q-values for current state: [-0.001  -0.001  -0.0019 -0.001 ]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 16\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.001\n",
      "New Q-value: -0.0019950000000000002\n",
      "\n",
      "Step 28\n",
      "Current state: 16\n",
      "Q-values for current state: [-0.001995 -0.001    -0.0019   -0.001   ]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 8\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.001\n",
      "New Q-value: -0.0019950000000000002\n",
      "\n",
      "Step 29\n",
      "Current state: 8\n",
      "Q-values for current state: [-0.0019   -0.002805 -0.001095 -0.001   ]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 8\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.0019000000000000002\n",
      "New Q-value: -0.002805\n",
      "\n",
      "Step 30\n",
      "Current state: 8\n",
      "Q-values for current state: [-0.002805 -0.002805 -0.001095 -0.001   ]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 8\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.002805\n",
      "New Q-value: -0.0036195000000000003\n",
      "\n",
      "Step 31\n",
      "Current state: 8\n",
      "Q-values for current state: [-0.0036195 -0.002805  -0.001095  -0.001    ]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 16\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.002805\n",
      "New Q-value: -0.0036195000000000003\n",
      "\n",
      "Step 32\n",
      "Current state: 16\n",
      "Q-values for current state: [-0.001995 -0.001    -0.0019   -0.001995]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 16\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.0019950000000000002\n",
      "New Q-value: -0.0028905000000000003\n",
      "\n",
      "Step 33\n",
      "Current state: 16\n",
      "Q-values for current state: [-0.0028905 -0.001     -0.0019    -0.001995 ]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 24\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.001\n",
      "New Q-value: -0.0019000000000000002\n",
      "\n",
      "Step 34\n",
      "Current state: 24\n",
      "Q-values for current state: [-0.00271 -0.0019   0.      -0.001  ]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 32\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.0019000000000000002\n",
      "New Q-value: -0.00271\n",
      "\n",
      "Step 35\n",
      "Current state: 32\n",
      "Q-values for current state: [ 0.     0.    -0.001 -0.001]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 32\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: -0.001\n",
      "\n",
      "Step 36\n",
      "Current state: 32\n",
      "Q-values for current state: [-0.001  0.    -0.001 -0.001]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 40\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: -0.001\n",
      "\n",
      "Step 37\n",
      "Current state: 40\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 40\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: -0.001\n",
      "\n",
      "Step 38\n",
      "Current state: 40\n",
      "Q-values for current state: [-0.001  0.     0.     0.   ]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 48\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: -0.001\n",
      "\n",
      "Step 39\n",
      "Current state: 48\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 48\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: -0.001\n",
      "\n",
      "Step 40\n",
      "Current state: 48\n",
      "Q-values for current state: [-0.001  0.     0.     0.   ]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 56\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: -0.001\n",
      "\n",
      "Step 41\n",
      "Current state: 56\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 57\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: -0.001\n",
      "\n",
      "Step 42\n",
      "Current state: 57\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 57\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: -0.001\n",
      "\n",
      "Step 43\n",
      "Current state: 57\n",
      "Q-values for current state: [ 0.    -0.001  0.     0.   ]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 57\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.001\n",
      "New Q-value: -0.0019000000000000002\n",
      "\n",
      "Step 44\n",
      "Current state: 57\n",
      "Q-values for current state: [ 0.     -0.0019  0.      0.    ]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 58\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: -0.001\n",
      "\n",
      "Step 45\n",
      "Current state: 58\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 59\n",
      "Reward: -0.01\n",
      "Terminated: True\n",
      "Old Q-value: 0.0\n",
      "New Q-value: -0.001\n",
      "\n",
      "Episode 4\n",
      "Starting state: 0\n",
      "\n",
      "Step 0\n",
      "Current state: 0\n",
      "Q-values for current state: [ 0.       -0.0019   -0.003534 -0.0019  ]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 1\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.0035340000000000002\n",
      "New Q-value: -0.0042756\n",
      "\n",
      "Step 1\n",
      "Current state: 1\n",
      "Q-values for current state: [-0.001    -0.0019   -0.003439 -0.001995]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 9\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.0019000000000000002\n",
      "New Q-value: -0.0028905\n",
      "\n",
      "Step 2\n",
      "Current state: 9\n",
      "Q-values for current state: [-0.001995 -0.0019   -0.00271  -0.001995]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 10\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.00271\n",
      "New Q-value: -0.003439\n",
      "\n",
      "Step 3\n",
      "Current state: 10\n",
      "Q-values for current state: [-0.002805 -0.0019    0.        0.      ]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 9\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.002805\n",
      "New Q-value: -0.003705\n",
      "\n",
      "Step 4\n",
      "Current state: 9\n",
      "Q-values for current state: [-0.001995 -0.0019   -0.003439 -0.001995]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 10\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.003439\n",
      "New Q-value: -0.0040951\n",
      "\n",
      "Step 5\n",
      "Current state: 10\n",
      "Q-values for current state: [-0.003705 -0.0019    0.        0.      ]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 2\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: -0.001\n",
      "\n",
      "Step 6\n",
      "Current state: 2\n",
      "Q-values for current state: [-0.001  -0.0019 -0.001   0.    ]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 3\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.001\n",
      "New Q-value: -0.0019000000000000002\n",
      "\n",
      "Step 7\n",
      "Current state: 3\n",
      "Q-values for current state: [ 0.    -0.001 -0.001  0.   ]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 11\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.001\n",
      "New Q-value: -0.0019000000000000002\n",
      "\n",
      "Step 8\n",
      "Current state: 11\n",
      "Q-values for current state: [ 0.    -0.001 -0.001  0.   ]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 3\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: -0.001\n",
      "\n",
      "Step 9\n",
      "Current state: 3\n",
      "Q-values for current state: [ 0.     -0.0019 -0.001   0.    ]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 2\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: -0.001\n",
      "\n",
      "Step 10\n",
      "Current state: 2\n",
      "Q-values for current state: [-0.001  -0.0019 -0.0019  0.    ]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 3\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.0019000000000000002\n",
      "New Q-value: -0.00271\n",
      "\n",
      "Step 11\n",
      "Current state: 3\n",
      "Q-values for current state: [-0.001  -0.0019 -0.001   0.    ]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 4\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.001\n",
      "New Q-value: -0.0019000000000000002\n",
      "\n",
      "Step 12\n",
      "Current state: 4\n",
      "Q-values for current state: [-0.001 -0.001  0.    -0.001]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 12\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.001\n",
      "New Q-value: -0.0019000000000000002\n",
      "\n",
      "Step 13\n",
      "Current state: 12\n",
      "Q-values for current state: [-0.001   0.     -0.0019  0.    ]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 4\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: -0.001\n",
      "\n",
      "Step 14\n",
      "Current state: 4\n",
      "Q-values for current state: [-0.001  -0.0019  0.     -0.001 ]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 5\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: -0.001\n",
      "\n",
      "Step 15\n",
      "Current state: 5\n",
      "Q-values for current state: [-0.001   0.     -0.0019  0.    ]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 5\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: -0.001\n",
      "\n",
      "Step 16\n",
      "Current state: 5\n",
      "Q-values for current state: [-0.001   0.     -0.0019 -0.001 ]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 5\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.001\n",
      "New Q-value: -0.0019000000000000002\n",
      "\n",
      "Step 17\n",
      "Current state: 5\n",
      "Q-values for current state: [-0.001   0.     -0.0019 -0.0019]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 6\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.0019000000000000002\n",
      "New Q-value: -0.00271\n",
      "\n",
      "Step 18\n",
      "Current state: 6\n",
      "Q-values for current state: [-0.001 -0.001 -0.001  0.   ]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 6\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: -0.001\n",
      "\n",
      "Step 19\n",
      "Current state: 6\n",
      "Q-values for current state: [-0.001 -0.001 -0.001 -0.001]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 6\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.001\n",
      "New Q-value: -0.0019950000000000002\n",
      "\n",
      "Step 20\n",
      "Current state: 6\n",
      "Q-values for current state: [-0.001    -0.001    -0.001    -0.001995]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 6\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.0019950000000000002\n",
      "New Q-value: -0.0028905000000000003\n",
      "\n",
      "Step 21\n",
      "Current state: 6\n",
      "Q-values for current state: [-0.001     -0.001     -0.001     -0.0028905]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 5\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.001\n",
      "New Q-value: -0.0019000000000000002\n",
      "\n",
      "Step 22\n",
      "Current state: 5\n",
      "Q-values for current state: [-0.001    0.      -0.00271 -0.0019 ]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 5\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.0019000000000000002\n",
      "New Q-value: -0.00271\n",
      "\n",
      "Step 23\n",
      "Current state: 5\n",
      "Q-values for current state: [-0.001    0.      -0.00271 -0.00271]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 13\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: -0.001\n",
      "\n",
      "Step 24\n",
      "Current state: 13\n",
      "Q-values for current state: [-0.001  -0.0019  0.     -0.0019]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 5\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.0019000000000000002\n",
      "New Q-value: -0.002805\n",
      "\n",
      "Step 25\n",
      "Current state: 5\n",
      "Q-values for current state: [-0.001   -0.001   -0.00271 -0.00271]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 13\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.001\n",
      "New Q-value: -0.0019000000000000002\n",
      "\n",
      "Step 26\n",
      "Current state: 13\n",
      "Q-values for current state: [-0.001    -0.0019    0.       -0.002805]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 14\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: -0.001\n",
      "\n",
      "Step 27\n",
      "Current state: 14\n",
      "Q-values for current state: [-0.0019 -0.001   0.      0.    ]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 13\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.0019000000000000002\n",
      "New Q-value: -0.002805\n",
      "\n",
      "Step 28\n",
      "Current state: 13\n",
      "Q-values for current state: [-0.001    -0.0019   -0.001    -0.002805]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 5\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.002805\n",
      "New Q-value: -0.0036195000000000003\n",
      "\n",
      "Step 29\n",
      "Current state: 5\n",
      "Q-values for current state: [-0.001   -0.0019  -0.00271 -0.00271]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 13\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.0019000000000000002\n",
      "New Q-value: -0.002805\n",
      "\n",
      "Step 30\n",
      "Current state: 13\n",
      "Q-values for current state: [-0.001     -0.0019    -0.001     -0.0036195]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 5\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.0036195000000000003\n",
      "New Q-value: -0.00435255\n",
      "\n",
      "Step 31\n",
      "Current state: 5\n",
      "Q-values for current state: [-0.001    -0.002805 -0.00271  -0.00271 ]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 4\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.001\n",
      "New Q-value: -0.0019950000000000002\n",
      "\n",
      "Step 32\n",
      "Current state: 4\n",
      "Q-values for current state: [-0.001  -0.0019 -0.001  -0.001 ]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 3\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.001\n",
      "New Q-value: -0.0019000000000000002\n",
      "\n",
      "Step 33\n",
      "Current state: 3\n",
      "Q-values for current state: [-0.001  -0.0019 -0.0019  0.    ]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 2\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.001\n",
      "New Q-value: -0.0019000000000000002\n",
      "\n",
      "Step 34\n",
      "Current state: 2\n",
      "Q-values for current state: [-0.001   -0.0019  -0.00271  0.     ]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 3\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.00271\n",
      "New Q-value: -0.003439\n",
      "\n",
      "Step 35\n",
      "Current state: 3\n",
      "Q-values for current state: [-0.0019 -0.0019 -0.0019  0.    ]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 4\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.0019000000000000002\n",
      "New Q-value: -0.002805\n",
      "\n",
      "Step 36\n",
      "Current state: 4\n",
      "Q-values for current state: [-0.0019 -0.0019 -0.001  -0.001 ]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 12\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.0019000000000000002\n",
      "New Q-value: -0.00271\n",
      "\n",
      "Step 37\n",
      "Current state: 12\n",
      "Q-values for current state: [-0.001   0.     -0.0019 -0.001 ]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 13\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.0019000000000000002\n",
      "New Q-value: -0.002805\n",
      "\n",
      "Step 38\n",
      "Current state: 13\n",
      "Q-values for current state: [-0.001      -0.0019     -0.001      -0.00435255]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 12\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.001\n",
      "New Q-value: -0.0019000000000000002\n",
      "\n",
      "Step 39\n",
      "Current state: 12\n",
      "Q-values for current state: [-0.001     0.       -0.002805 -0.001   ]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 13\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.002805\n",
      "New Q-value: -0.0036195000000000003\n",
      "\n",
      "Step 40\n",
      "Current state: 13\n",
      "Q-values for current state: [-0.0019     -0.0019     -0.001      -0.00435255]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 5\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.00435255\n",
      "New Q-value: -0.00510682\n",
      "\n",
      "Step 41\n",
      "Current state: 5\n",
      "Q-values for current state: [-0.001995 -0.002805 -0.00271  -0.00271 ]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 5\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.00271\n",
      "New Q-value: -0.003628525\n",
      "\n",
      "Step 42\n",
      "Current state: 5\n",
      "Q-values for current state: [-0.001995   -0.002805   -0.00271    -0.00362853]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 13\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.002805\n",
      "New Q-value: -0.0036195000000000003\n",
      "\n",
      "Step 43\n",
      "Current state: 13\n",
      "Q-values for current state: [-0.0019     -0.0019     -0.001      -0.00510682]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 14\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.001\n",
      "New Q-value: -0.0019000000000000002\n",
      "\n",
      "Step 44\n",
      "Current state: 14\n",
      "Q-values for current state: [-0.002805 -0.001     0.        0.      ]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 15\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: -0.001\n",
      "\n",
      "Step 45\n",
      "Current state: 15\n",
      "Q-values for current state: [-0.001   0.      0.     -0.0019]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 7\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.0019000000000000002\n",
      "New Q-value: -0.00271\n",
      "\n",
      "Step 46\n",
      "Current state: 7\n",
      "Q-values for current state: [-0.001  -0.0019  0.     -0.001 ]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 7\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.001\n",
      "New Q-value: -0.0019000000000000002\n",
      "\n",
      "Step 47\n",
      "Current state: 7\n",
      "Q-values for current state: [-0.001  -0.0019  0.     -0.0019]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 15\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.0019000000000000002\n",
      "New Q-value: -0.00271\n",
      "\n",
      "Step 48\n",
      "Current state: 15\n",
      "Q-values for current state: [-0.001    0.       0.      -0.00271]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 15\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: -0.001\n",
      "\n",
      "Step 49\n",
      "Current state: 15\n",
      "Q-values for current state: [-0.001    0.      -0.001   -0.00271]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 7\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.00271\n",
      "New Q-value: -0.003439\n",
      "\n",
      "Step 50\n",
      "Current state: 7\n",
      "Q-values for current state: [-0.001   -0.00271  0.      -0.0019 ]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 7\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: -0.001\n",
      "\n",
      "Step 51\n",
      "Current state: 7\n",
      "Q-values for current state: [-0.001   -0.00271 -0.001   -0.0019 ]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 6\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.001\n",
      "New Q-value: -0.0019950000000000002\n",
      "\n",
      "Step 52\n",
      "Current state: 6\n",
      "Q-values for current state: [-0.0019    -0.001     -0.001     -0.0028905]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 14\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.001\n",
      "New Q-value: -0.0019000000000000002\n",
      "\n",
      "Step 53\n",
      "Current state: 14\n",
      "Q-values for current state: [-0.002805 -0.001    -0.001     0.      ]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 22\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.001\n",
      "New Q-value: -0.0019000000000000002\n",
      "\n",
      "Step 54\n",
      "Current state: 22\n",
      "Q-values for current state: [ 0.    -0.001 -0.001 -0.001]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 30\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.001\n",
      "New Q-value: -0.0019000000000000002\n",
      "\n",
      "Step 55\n",
      "Current state: 30\n",
      "Q-values for current state: [ 0.     0.     0.    -0.001]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 22\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.001\n",
      "New Q-value: -0.0019000000000000002\n",
      "\n",
      "Step 56\n",
      "Current state: 22\n",
      "Q-values for current state: [ 0.     -0.0019 -0.001  -0.001 ]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 21\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: -0.001\n",
      "\n",
      "Step 57\n",
      "Current state: 21\n",
      "Q-values for current state: [ 0.     0.    -0.001 -0.001]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 20\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: -0.001\n",
      "\n",
      "Step 58\n",
      "Current state: 20\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 12\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: -0.001\n",
      "\n",
      "Step 59\n",
      "Current state: 12\n",
      "Q-values for current state: [-0.001      0.        -0.0036195 -0.001    ]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 4\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.001\n",
      "New Q-value: -0.0019950000000000002\n",
      "\n",
      "Step 60\n",
      "Current state: 4\n",
      "Q-values for current state: [-0.0019  -0.00271 -0.001   -0.001  ]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 3\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.0019000000000000002\n",
      "New Q-value: -0.00271\n",
      "\n",
      "Step 61\n",
      "Current state: 3\n",
      "Q-values for current state: [-0.0019   -0.0019   -0.002805  0.      ]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 4\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.002805\n",
      "New Q-value: -0.0036195000000000003\n",
      "\n",
      "Step 62\n",
      "Current state: 4\n",
      "Q-values for current state: [-0.00271 -0.00271 -0.001   -0.001  ]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 4\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.001\n",
      "New Q-value: -0.0019950000000000002\n",
      "\n",
      "Step 63\n",
      "Current state: 4\n",
      "Q-values for current state: [-0.00271  -0.00271  -0.001    -0.001995]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 4\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.0019950000000000002\n",
      "New Q-value: -0.0028905000000000003\n",
      "\n",
      "Step 64\n",
      "Current state: 4\n",
      "Q-values for current state: [-0.00271   -0.00271   -0.001     -0.0028905]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 5\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.001\n",
      "New Q-value: -0.002089525\n",
      "\n",
      "Step 65\n",
      "Current state: 5\n",
      "Q-values for current state: [-0.001995   -0.0036195  -0.00271    -0.00362853]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 5\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.003628525\n",
      "New Q-value: -0.0044551975\n",
      "\n",
      "Step 66\n",
      "Current state: 5\n",
      "Q-values for current state: [-0.001995  -0.0036195 -0.00271   -0.0044552]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 4\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.0019950000000000002\n",
      "New Q-value: -0.002994004875\n",
      "\n",
      "Step 67\n",
      "Current state: 4\n",
      "Q-values for current state: [-0.00271    -0.00271    -0.00208953 -0.0028905 ]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 12\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.00271\n",
      "New Q-value: -0.003439\n",
      "\n",
      "Step 68\n",
      "Current state: 12\n",
      "Q-values for current state: [-0.001      0.        -0.0036195 -0.001995 ]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 11\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.001\n",
      "New Q-value: -0.0019000000000000002\n",
      "\n",
      "Step 69\n",
      "Current state: 11\n",
      "Q-values for current state: [ 0.    -0.001 -0.001 -0.001]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 3\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.001\n",
      "New Q-value: -0.0019000000000000002\n",
      "\n",
      "Step 70\n",
      "Current state: 3\n",
      "Q-values for current state: [-0.0019    -0.0019    -0.0036195  0.       ]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 3\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: -0.001\n",
      "\n",
      "Step 71\n",
      "Current state: 3\n",
      "Q-values for current state: [-0.0019    -0.0019    -0.0036195 -0.001    ]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 11\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.0019000000000000002\n",
      "New Q-value: -0.00271\n",
      "\n",
      "Step 72\n",
      "Current state: 11\n",
      "Q-values for current state: [ 0.     -0.001  -0.001  -0.0019]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 19\n",
      "Reward: -0.01\n",
      "Terminated: True\n",
      "Old Q-value: -0.001\n",
      "New Q-value: -0.0019000000000000002\n",
      "\n",
      "Episode 5\n",
      "Starting state: 0\n",
      "\n",
      "Step 0\n",
      "Current state: 0\n",
      "Q-values for current state: [ 0.        -0.0019    -0.0042756 -0.0019   ]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 0\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.0019000000000000002\n",
      "New Q-value: -0.00271\n",
      "\n",
      "Step 1\n",
      "Current state: 0\n",
      "Q-values for current state: [ 0.        -0.0019    -0.0042756 -0.00271  ]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 1\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.0042756\n",
      "New Q-value: -0.00494304\n",
      "\n",
      "Step 2\n",
      "Current state: 1\n",
      "Q-values for current state: [-0.001     -0.0028905 -0.003439  -0.001995 ]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 0\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.001\n",
      "New Q-value: -0.0019000000000000002\n",
      "\n",
      "Step 3\n",
      "Current state: 0\n",
      "Q-values for current state: [ 0.         -0.0019     -0.00494304 -0.00271   ]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 0\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.00271\n",
      "New Q-value: -0.003439\n",
      "\n",
      "Step 4\n",
      "Current state: 0\n",
      "Q-values for current state: [ 0.         -0.0019     -0.00494304 -0.003439  ]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 0\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.003439\n",
      "New Q-value: -0.0040951\n",
      "\n",
      "Step 5\n",
      "Current state: 0\n",
      "Q-values for current state: [ 0.         -0.0019     -0.00494304 -0.0040951 ]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 8\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.0019000000000000002\n",
      "New Q-value: -0.002805\n",
      "\n",
      "Step 6\n",
      "Current state: 8\n",
      "Q-values for current state: [-0.0036195 -0.0036195 -0.001095  -0.001    ]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 0\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.001\n",
      "New Q-value: -0.0019000000000000002\n",
      "\n",
      "Step 7\n",
      "Current state: 0\n",
      "Q-values for current state: [ 0.         -0.002805   -0.00494304 -0.0040951 ]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 0\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.0040951\n",
      "New Q-value: -0.00468559\n",
      "\n",
      "Step 8\n",
      "Current state: 0\n",
      "Q-values for current state: [ 0.         -0.002805   -0.00494304 -0.00468559]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 1\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.00494304\n",
      "New Q-value: -0.005629236\n",
      "\n",
      "Step 9\n",
      "Current state: 1\n",
      "Q-values for current state: [-0.0019    -0.0028905 -0.003439  -0.001995 ]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 1\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.0019950000000000002\n",
      "New Q-value: -0.0029760000000000003\n",
      "\n",
      "Step 10\n",
      "Current state: 1\n",
      "Q-values for current state: [-0.0019    -0.0028905 -0.003439  -0.002976 ]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 2\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.003439\n",
      "New Q-value: -0.0040951\n",
      "\n",
      "Step 11\n",
      "Current state: 2\n",
      "Q-values for current state: [-0.001    -0.0019   -0.003439  0.      ]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 10\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.0019000000000000002\n",
      "New Q-value: -0.00271\n",
      "\n",
      "Step 12\n",
      "Current state: 10\n",
      "Q-values for current state: [-0.003705 -0.0019    0.       -0.001   ]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 11\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: -0.001\n",
      "\n",
      "Step 13\n",
      "Current state: 11\n",
      "Q-values for current state: [ 0.     -0.0019 -0.001  -0.0019]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 19\n",
      "Reward: -0.01\n",
      "Terminated: True\n",
      "Old Q-value: -0.0019000000000000002\n",
      "New Q-value: -0.00271\n",
      "\n",
      "Episode 6\n",
      "Starting state: 0\n",
      "\n",
      "Step 0\n",
      "Current state: 0\n",
      "Q-values for current state: [ 0.         -0.002805   -0.00562924 -0.00468559]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 0\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.00468559\n",
      "New Q-value: -0.005217031\n",
      "\n",
      "Step 1\n",
      "Current state: 0\n",
      "Q-values for current state: [ 0.         -0.002805   -0.00562924 -0.00521703]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 0\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.005217031\n",
      "New Q-value: -0.0056953279\n",
      "\n",
      "Step 2\n",
      "Current state: 0\n",
      "Q-values for current state: [ 0.         -0.002805   -0.00562924 -0.00569533]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 0\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.0056953279\n",
      "New Q-value: -0.00612579511\n",
      "\n",
      "Step 3\n",
      "Current state: 0\n",
      "Q-values for current state: [ 0.         -0.002805   -0.00562924 -0.0061258 ]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 0\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: -0.001\n",
      "\n",
      "Step 4\n",
      "Current state: 0\n",
      "Q-values for current state: [-0.001      -0.002805   -0.00562924 -0.0061258 ]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 1\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.005629236\n",
      "New Q-value: -0.0062468124\n",
      "\n",
      "Step 5\n",
      "Current state: 1\n",
      "Q-values for current state: [-0.0019    -0.0028905 -0.0040951 -0.002976 ]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 0\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.0019000000000000002\n",
      "New Q-value: -0.002805\n",
      "\n",
      "Step 6\n",
      "Current state: 0\n",
      "Q-values for current state: [-0.001      -0.002805   -0.00624681 -0.0061258 ]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 1\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.0062468124\n",
      "New Q-value: -0.0068886061600000005\n",
      "\n",
      "Step 7\n",
      "Current state: 1\n",
      "Q-values for current state: [-0.002805  -0.0028905 -0.0040951 -0.002976 ]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 9\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.0028905\n",
      "New Q-value: -0.0037819499999999996\n",
      "\n",
      "Step 8\n",
      "Current state: 9\n",
      "Q-values for current state: [-0.001995  -0.0019    -0.0040951 -0.001995 ]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 10\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.0040951\n",
      "New Q-value: -0.0047805899999999995\n",
      "\n",
      "Step 9\n",
      "Current state: 10\n",
      "Q-values for current state: [-0.003705 -0.0019   -0.001    -0.001   ]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 2\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.001\n",
      "New Q-value: -0.0019000000000000002\n",
      "\n",
      "Step 10\n",
      "Current state: 2\n",
      "Q-values for current state: [-0.001    -0.00271  -0.003439  0.      ]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 3\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.003439\n",
      "New Q-value: -0.0041901\n",
      "\n",
      "Step 11\n",
      "Current state: 3\n",
      "Q-values for current state: [-0.0019    -0.00271   -0.0036195 -0.001    ]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 3\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.001\n",
      "New Q-value: -0.0019950000000000002\n",
      "\n",
      "Step 12\n",
      "Current state: 3\n",
      "Q-values for current state: [-0.0019    -0.00271   -0.0036195 -0.001995 ]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 2\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.0019000000000000002\n",
      "New Q-value: -0.00271\n",
      "\n",
      "Step 13\n",
      "Current state: 2\n",
      "Q-values for current state: [-0.001     -0.00271   -0.0041901  0.       ]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 2\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: -0.001\n",
      "\n",
      "Step 14\n",
      "Current state: 2\n",
      "Q-values for current state: [-0.001     -0.00271   -0.0041901 -0.001    ]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 3\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.0041901\n",
      "New Q-value: -0.004960615000000001\n",
      "\n",
      "Step 15\n",
      "Current state: 3\n",
      "Q-values for current state: [-0.00271   -0.00271   -0.0036195 -0.001995 ]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 2\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.00271\n",
      "New Q-value: -0.0035340000000000002\n",
      "\n",
      "Step 16\n",
      "Current state: 2\n",
      "Q-values for current state: [-0.001      -0.00271    -0.00496062 -0.001     ]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 1\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.001\n",
      "New Q-value: -0.002166475\n",
      "\n",
      "Step 17\n",
      "Current state: 1\n",
      "Q-values for current state: [-0.002805   -0.00378195 -0.0040951  -0.002976  ]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 1\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.0029760000000000003\n",
      "New Q-value: -0.0039448750000000005\n",
      "\n",
      "Step 18\n",
      "Current state: 1\n",
      "Q-values for current state: [-0.002805   -0.00378195 -0.0040951  -0.00394488]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 1\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.0039448750000000005\n",
      "New Q-value: -0.0048168625000000005\n",
      "\n",
      "Step 19\n",
      "Current state: 1\n",
      "Q-values for current state: [-0.002805   -0.00378195 -0.0040951  -0.00481686]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 1\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.0048168625000000005\n",
      "New Q-value: -0.005601651250000001\n",
      "\n",
      "Step 20\n",
      "Current state: 1\n",
      "Q-values for current state: [-0.002805   -0.00378195 -0.0040951  -0.00560165]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 2\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.0040951\n",
      "New Q-value: -0.0047805899999999995\n",
      "\n",
      "Step 21\n",
      "Current state: 2\n",
      "Q-values for current state: [-0.00216648 -0.00271    -0.00496062 -0.001     ]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 3\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.004960615000000001\n",
      "New Q-value: -0.0056540785000000005\n",
      "\n",
      "Step 22\n",
      "Current state: 3\n",
      "Q-values for current state: [-0.003534  -0.00271   -0.0036195 -0.001995 ]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 11\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.00271\n",
      "New Q-value: -0.003439\n",
      "\n",
      "Step 23\n",
      "Current state: 11\n",
      "Q-values for current state: [ 0.      -0.00271 -0.001   -0.0019 ]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 12\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.001\n",
      "New Q-value: -0.0019000000000000002\n",
      "\n",
      "Step 24\n",
      "Current state: 12\n",
      "Q-values for current state: [-0.0019     0.        -0.0036195 -0.001995 ]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 13\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.0036195000000000003\n",
      "New Q-value: -0.004438050000000001\n",
      "\n",
      "Step 25\n",
      "Current state: 13\n",
      "Q-values for current state: [-0.0019     -0.0019     -0.0019     -0.00510682]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 21\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.0019000000000000002\n",
      "New Q-value: -0.00271\n",
      "\n",
      "Step 26\n",
      "Current state: 21\n",
      "Q-values for current state: [-0.001  0.    -0.001 -0.001]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 22\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.001\n",
      "New Q-value: -0.0019950000000000002\n",
      "\n",
      "Step 27\n",
      "Current state: 22\n",
      "Q-values for current state: [-0.001  -0.0019 -0.001  -0.001 ]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 21\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.001\n",
      "New Q-value: -0.0019000000000000002\n",
      "\n",
      "Step 28\n",
      "Current state: 21\n",
      "Q-values for current state: [-0.001     0.       -0.001995 -0.001   ]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 22\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.0019950000000000002\n",
      "New Q-value: -0.0028905000000000003\n",
      "\n",
      "Step 29\n",
      "Current state: 22\n",
      "Q-values for current state: [-0.0019 -0.0019 -0.001  -0.001 ]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 30\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.0019000000000000002\n",
      "New Q-value: -0.00271\n",
      "\n",
      "Step 30\n",
      "Current state: 30\n",
      "Q-values for current state: [ 0.      0.      0.     -0.0019]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 31\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: -0.001\n",
      "\n",
      "Step 31\n",
      "Current state: 31\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 30\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: -0.001\n",
      "\n",
      "Step 32\n",
      "Current state: 30\n",
      "Q-values for current state: [ 0.      0.     -0.001  -0.0019]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 22\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.0019000000000000002\n",
      "New Q-value: -0.002805\n",
      "\n",
      "Step 33\n",
      "Current state: 22\n",
      "Q-values for current state: [-0.0019  -0.00271 -0.001   -0.001  ]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 30\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.00271\n",
      "New Q-value: -0.003439\n",
      "\n",
      "Step 34\n",
      "Current state: 30\n",
      "Q-values for current state: [ 0.        0.       -0.001    -0.002805]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 22\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.002805\n",
      "New Q-value: -0.0036195000000000003\n",
      "\n",
      "Step 35\n",
      "Current state: 22\n",
      "Q-values for current state: [-0.0019   -0.003439 -0.001    -0.001   ]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 14\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.001\n",
      "New Q-value: -0.0019000000000000002\n",
      "\n",
      "Step 36\n",
      "Current state: 14\n",
      "Q-values for current state: [-0.002805 -0.0019   -0.001     0.      ]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 13\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.002805\n",
      "New Q-value: -0.003705\n",
      "\n",
      "Step 37\n",
      "Current state: 13\n",
      "Q-values for current state: [-0.0019     -0.00271    -0.0019     -0.00510682]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 14\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.0019000000000000002\n",
      "New Q-value: -0.00271\n",
      "\n",
      "Step 38\n",
      "Current state: 14\n",
      "Q-values for current state: [-0.003705 -0.0019   -0.001     0.      ]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 13\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.003705\n",
      "New Q-value: -0.004515\n",
      "\n",
      "Step 39\n",
      "Current state: 13\n",
      "Q-values for current state: [-0.0019     -0.00271    -0.00271    -0.00510682]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 21\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.00271\n",
      "New Q-value: -0.003439\n",
      "\n",
      "Step 40\n",
      "Current state: 21\n",
      "Q-values for current state: [-0.001      0.        -0.0028905 -0.001    ]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 29\n",
      "Reward: -0.01\n",
      "Terminated: True\n",
      "Old Q-value: 0.0\n",
      "New Q-value: -0.001\n",
      "\n",
      "Episode 7\n",
      "Starting state: 0\n",
      "\n",
      "Step 0\n",
      "Current state: 0\n",
      "Q-values for current state: [-0.001      -0.002805   -0.00688861 -0.0061258 ]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 8\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.002805\n",
      "New Q-value: -0.003628525\n",
      "\n",
      "Step 1\n",
      "Current state: 8\n",
      "Q-values for current state: [-0.0036195 -0.0036195 -0.001095  -0.0019   ]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 9\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.001095\n",
      "New Q-value: -0.002166\n",
      "\n",
      "Step 2\n",
      "Current state: 9\n",
      "Q-values for current state: [-0.001995   -0.0019     -0.00478059 -0.001995  ]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 1\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.0019950000000000002\n",
      "New Q-value: -0.0030619750000000006\n",
      "\n",
      "Step 3\n",
      "Current state: 1\n",
      "Q-values for current state: [-0.002805   -0.00378195 -0.00478059 -0.00560165]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 2\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.0047805899999999995\n",
      "New Q-value: -0.005397531\n",
      "\n",
      "Step 4\n",
      "Current state: 2\n",
      "Q-values for current state: [-0.00216648 -0.00271    -0.00565408 -0.001     ]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 1\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.002166475\n",
      "New Q-value: -0.0032163025000000005\n",
      "\n",
      "Step 5\n",
      "Current state: 1\n",
      "Q-values for current state: [-0.002805   -0.00378195 -0.00539753 -0.00560165]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 9\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.0037819499999999996\n",
      "New Q-value: -0.004584255\n",
      "\n",
      "Step 6\n",
      "Current state: 9\n",
      "Q-values for current state: [-0.001995   -0.0019     -0.00478059 -0.00306198]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 8\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.0019950000000000002\n",
      "New Q-value: -0.0029760000000000003\n",
      "\n",
      "Step 7\n",
      "Current state: 8\n",
      "Q-values for current state: [-0.0036195 -0.0036195 -0.002166  -0.0019   ]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 0\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.0019000000000000002\n",
      "New Q-value: -0.002805\n",
      "\n",
      "Step 8\n",
      "Current state: 0\n",
      "Q-values for current state: [-0.001      -0.00362853 -0.00688861 -0.0061258 ]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 8\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.003628525\n",
      "New Q-value: -0.0044714425000000006\n",
      "\n",
      "Step 9\n",
      "Current state: 8\n",
      "Q-values for current state: [-0.0036195 -0.0036195 -0.002166  -0.002805 ]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 9\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.002166\n",
      "New Q-value: -0.0031299\n",
      "\n",
      "Step 10\n",
      "Current state: 9\n",
      "Q-values for current state: [-0.002976   -0.0019     -0.00478059 -0.00306198]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 10\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.0047805899999999995\n",
      "New Q-value: -0.005397531\n",
      "\n",
      "Step 11\n",
      "Current state: 10\n",
      "Q-values for current state: [-0.003705 -0.0019   -0.001    -0.0019  ]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 9\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.003705\n",
      "New Q-value: -0.004515\n",
      "\n",
      "Step 12\n",
      "Current state: 9\n",
      "Q-values for current state: [-0.002976   -0.0019     -0.00539753 -0.00306198]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 1\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.0030619750000000006\n",
      "New Q-value: -0.004022252500000001\n",
      "\n",
      "Step 13\n",
      "Current state: 1\n",
      "Q-values for current state: [-0.002805   -0.00458425 -0.00539753 -0.00560165]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 0\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.002805\n",
      "New Q-value: -0.0036195000000000003\n",
      "\n",
      "Step 14\n",
      "Current state: 0\n",
      "Q-values for current state: [-0.001      -0.00447144 -0.00688861 -0.0061258 ]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 0\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.001\n",
      "New Q-value: -0.0019950000000000002\n",
      "\n",
      "Step 15\n",
      "Current state: 0\n",
      "Q-values for current state: [-0.001995   -0.00447144 -0.00688861 -0.0061258 ]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 8\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.0044714425000000006\n",
      "New Q-value: -0.00529077325\n",
      "\n",
      "Step 16\n",
      "Current state: 8\n",
      "Q-values for current state: [-0.0036195 -0.0036195 -0.0031299 -0.002805 ]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 0\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.002805\n",
      "New Q-value: -0.003714025\n",
      "\n",
      "Step 17\n",
      "Current state: 0\n",
      "Q-values for current state: [-0.001995   -0.00529077 -0.00688861 -0.0061258 ]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 0\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.00612579511\n",
      "New Q-value: -0.006702740599\n",
      "\n",
      "Step 18\n",
      "Current state: 0\n",
      "Q-values for current state: [-0.001995   -0.00529077 -0.00688861 -0.00670274]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 0\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.006702740599\n",
      "New Q-value: -0.0072219915391\n",
      "\n",
      "Step 19\n",
      "Current state: 0\n",
      "Q-values for current state: [-0.001995   -0.00529077 -0.00688861 -0.00722199]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 0\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.0019950000000000002\n",
      "New Q-value: -0.0029850250000000005\n",
      "\n",
      "Step 20\n",
      "Current state: 0\n",
      "Q-values for current state: [-0.00298503 -0.00529077 -0.00688861 -0.00722199]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 8\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.00529077325\n",
      "New Q-value: -0.006059036425000001\n",
      "\n",
      "Step 21\n",
      "Current state: 8\n",
      "Q-values for current state: [-0.0036195  -0.0036195  -0.0031299  -0.00371403]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 0\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.003714025\n",
      "New Q-value: -0.0046261998750000005\n",
      "\n",
      "Step 22\n",
      "Current state: 0\n",
      "Q-values for current state: [-0.00298503 -0.00605904 -0.00688861 -0.00722199]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 8\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.006059036425000001\n",
      "New Q-value: -0.006750473282500001\n",
      "\n",
      "Step 23\n",
      "Current state: 8\n",
      "Q-values for current state: [-0.0036195 -0.0036195 -0.0031299 -0.0046262]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 0\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.0046261998750000005\n",
      "New Q-value: -0.0054471572625\n",
      "\n",
      "Step 24\n",
      "Current state: 0\n",
      "Q-values for current state: [-0.00298503 -0.00675047 -0.00688861 -0.00722199]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 0\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.0072219915391\n",
      "New Q-value: -0.00778336976019\n",
      "\n",
      "Step 25\n",
      "Current state: 0\n",
      "Q-values for current state: [-0.00298503 -0.00675047 -0.00688861 -0.00778337]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 0\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.00778336976019\n",
      "New Q-value: -0.008288610159171001\n",
      "\n",
      "Step 26\n",
      "Current state: 0\n",
      "Q-values for current state: [-0.00298503 -0.00675047 -0.00688861 -0.00828861]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 0\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.008288610159171001\n",
      "New Q-value: -0.0087433265182539\n",
      "\n",
      "Step 27\n",
      "Current state: 0\n",
      "Q-values for current state: [-0.00298503 -0.00675047 -0.00688861 -0.00874333]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 0\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.0087433265182539\n",
      "New Q-value: -0.00915257124142851\n",
      "\n",
      "Step 28\n",
      "Current state: 0\n",
      "Q-values for current state: [-0.00298503 -0.00675047 -0.00688861 -0.00915257]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 0\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.00915257124142851\n",
      "New Q-value: -0.00952089149228566\n",
      "\n",
      "Step 29\n",
      "Current state: 0\n",
      "Q-values for current state: [-0.00298503 -0.00675047 -0.00688861 -0.00952089]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 0\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.0029850250000000005\n",
      "New Q-value: -0.003970099875000001\n",
      "\n",
      "Step 30\n",
      "Current state: 0\n",
      "Q-values for current state: [-0.0039701  -0.00675047 -0.00688861 -0.00952089]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 0\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.003970099875000001\n",
      "New Q-value: -0.004950249375625001\n",
      "\n",
      "Step 31\n",
      "Current state: 0\n",
      "Q-values for current state: [-0.00495025 -0.00675047 -0.00688861 -0.00952089]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 1\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.0068886061600000005\n",
      "New Q-value: -0.007543598044\n",
      "\n",
      "Step 32\n",
      "Current state: 1\n",
      "Q-values for current state: [-0.0036195  -0.00458425 -0.00539753 -0.00560165]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 0\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.0036195000000000003\n",
      "New Q-value: -0.004727823690684375\n",
      "\n",
      "Step 33\n",
      "Current state: 0\n",
      "Q-values for current state: [-0.00495025 -0.00675047 -0.0075436  -0.00952089]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 8\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.006750473282500001\n",
      "New Q-value: -0.007372766454250001\n",
      "\n",
      "Step 34\n",
      "Current state: 8\n",
      "Q-values for current state: [-0.0036195  -0.0036195  -0.0031299  -0.00544716]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 9\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.0031299\n",
      "New Q-value: -0.00399741\n",
      "\n",
      "Step 35\n",
      "Current state: 9\n",
      "Q-values for current state: [-0.002976   -0.0019     -0.00539753 -0.00402225]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 17\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.0019000000000000002\n",
      "New Q-value: -0.00271\n",
      "\n",
      "Step 36\n",
      "Current state: 17\n",
      "Q-values for current state: [-0.001   -0.001    0.      -0.00271]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 16\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.001\n",
      "New Q-value: -0.0020805\n",
      "\n",
      "Step 37\n",
      "Current state: 16\n",
      "Q-values for current state: [-0.0028905 -0.0019    -0.0019    -0.001995 ]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 8\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.0019950000000000002\n",
      "New Q-value: -0.0031393525000000004\n",
      "\n",
      "Step 38\n",
      "Current state: 8\n",
      "Q-values for current state: [-0.0036195  -0.0036195  -0.00399741 -0.00544716]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 16\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.0036195000000000003\n",
      "New Q-value: -0.004438050000000001\n",
      "\n",
      "Step 39\n",
      "Current state: 16\n",
      "Q-values for current state: [-0.0028905  -0.0019     -0.0019     -0.00313935]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 17\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.0019000000000000002\n",
      "New Q-value: -0.00271\n",
      "\n",
      "Step 40\n",
      "Current state: 17\n",
      "Q-values for current state: [-0.0020805 -0.001      0.        -0.00271  ]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 16\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.0020805\n",
      "New Q-value: -0.00305295\n",
      "\n",
      "Step 41\n",
      "Current state: 16\n",
      "Q-values for current state: [-0.0028905  -0.0019     -0.00271    -0.00313935]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 24\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.0019000000000000002\n",
      "New Q-value: -0.00271\n",
      "\n",
      "Step 42\n",
      "Current state: 24\n",
      "Q-values for current state: [-0.00271 -0.00271  0.      -0.001  ]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 16\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.001\n",
      "New Q-value: -0.00215745\n",
      "\n",
      "Step 43\n",
      "Current state: 16\n",
      "Q-values for current state: [-0.0028905  -0.00271    -0.00271    -0.00313935]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 17\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.00271\n",
      "New Q-value: -0.003439\n",
      "\n",
      "Step 44\n",
      "Current state: 17\n",
      "Q-values for current state: [-0.00305295 -0.001       0.         -0.00271   ]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 9\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.00271\n",
      "New Q-value: -0.0036964500000000004\n",
      "\n",
      "Step 45\n",
      "Current state: 9\n",
      "Q-values for current state: [-0.002976   -0.00271    -0.00539753 -0.00402225]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 8\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.0029760000000000003\n",
      "New Q-value: -0.0040222525\n",
      "\n",
      "Step 46\n",
      "Current state: 8\n",
      "Q-values for current state: [-0.0036195  -0.00443805 -0.00399741 -0.00544716]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 8\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.0036195000000000003\n",
      "New Q-value: -0.004601402500000001\n",
      "\n",
      "Step 47\n",
      "Current state: 8\n",
      "Q-values for current state: [-0.0046014  -0.00443805 -0.00399741 -0.00544716]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 16\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.004438050000000001\n",
      "New Q-value: -0.005251695000000001\n",
      "\n",
      "Step 48\n",
      "Current state: 16\n",
      "Q-values for current state: [-0.0028905  -0.00271    -0.003439   -0.00313935]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 24\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.00271\n",
      "New Q-value: -0.003439\n",
      "\n",
      "Step 49\n",
      "Current state: 24\n",
      "Q-values for current state: [-0.00271    -0.00271     0.         -0.00215745]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 24\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.00271\n",
      "New Q-value: -0.003439\n",
      "\n",
      "Step 50\n",
      "Current state: 24\n",
      "Q-values for current state: [-0.003439   -0.00271     0.         -0.00215745]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 24\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.003439\n",
      "New Q-value: -0.0040951\n",
      "\n",
      "Step 51\n",
      "Current state: 24\n",
      "Q-values for current state: [-0.0040951  -0.00271     0.         -0.00215745]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 25\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: -0.001\n",
      "\n",
      "Step 52\n",
      "Current state: 25\n",
      "Q-values for current state: [-0.001  0.     0.     0.   ]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 24\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.001\n",
      "New Q-value: -0.0019950000000000002\n",
      "\n",
      "Step 53\n",
      "Current state: 24\n",
      "Q-values for current state: [-0.0040951  -0.00271    -0.001      -0.00215745]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 24\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.0040951\n",
      "New Q-value: -0.0047805899999999995\n",
      "\n",
      "Step 54\n",
      "Current state: 24\n",
      "Q-values for current state: [-0.00478059 -0.00271    -0.001      -0.00215745]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 24\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.0047805899999999995\n",
      "New Q-value: -0.005397531\n",
      "\n",
      "Step 55\n",
      "Current state: 24\n",
      "Q-values for current state: [-0.00539753 -0.00271    -0.001      -0.00215745]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 16\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.00215745\n",
      "New Q-value: -0.0032163025\n",
      "\n",
      "Step 56\n",
      "Current state: 16\n",
      "Q-values for current state: [-0.0028905  -0.003439   -0.003439   -0.00313935]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 24\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.003439\n",
      "New Q-value: -0.0041901\n",
      "\n",
      "Step 57\n",
      "Current state: 24\n",
      "Q-values for current state: [-0.00539753 -0.00271    -0.001      -0.0032163 ]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 24\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.005397531\n",
      "New Q-value: -0.005952777899999999\n",
      "\n",
      "Step 58\n",
      "Current state: 24\n",
      "Q-values for current state: [-0.00595278 -0.00271    -0.001      -0.0032163 ]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 32\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.00271\n",
      "New Q-value: -0.0035340000000000002\n",
      "\n",
      "Step 59\n",
      "Current state: 32\n",
      "Q-values for current state: [-0.001 -0.001 -0.001 -0.001]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 40\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.001\n",
      "New Q-value: -0.0019000000000000002\n",
      "\n",
      "Step 60\n",
      "Current state: 40\n",
      "Q-values for current state: [-0.001 -0.001  0.     0.   ]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 41\n",
      "Reward: -0.01\n",
      "Terminated: True\n",
      "Old Q-value: 0.0\n",
      "New Q-value: -0.001\n",
      "\n",
      "Episode 8\n",
      "Starting state: 0\n",
      "\n",
      "Step 0\n",
      "Current state: 0\n",
      "Q-values for current state: [-0.00495025 -0.00737277 -0.0075436  -0.00952089]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 0\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.00952089149228566\n",
      "New Q-value: -0.010039076033741469\n",
      "\n",
      "Step 1\n",
      "Current state: 0\n",
      "Q-values for current state: [-0.00495025 -0.00737277 -0.0075436  -0.01003908]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 0\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.010039076033741469\n",
      "New Q-value: -0.010505442121051697\n",
      "\n",
      "Step 2\n",
      "Current state: 0\n",
      "Q-values for current state: [-0.00495025 -0.00737277 -0.0075436  -0.01050544]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 0\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.004950249375625001\n",
      "New Q-value: -0.005925498128746876\n",
      "\n",
      "Step 3\n",
      "Current state: 0\n",
      "Q-values for current state: [-0.0059255  -0.00737277 -0.0075436  -0.01050544]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 1\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.007543598044\n",
      "New Q-value: -0.008224742464600001\n",
      "\n",
      "Step 4\n",
      "Current state: 1\n",
      "Q-values for current state: [-0.00472782 -0.00458425 -0.00539753 -0.00560165]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 9\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.004584255\n",
      "New Q-value: -0.0053832795\n",
      "\n",
      "Step 5\n",
      "Current state: 9\n",
      "Q-values for current state: [-0.00402225 -0.00271    -0.00539753 -0.00402225]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 10\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.005397531\n",
      "New Q-value: -0.005952777899999999\n",
      "\n",
      "Step 6\n",
      "Current state: 10\n",
      "Q-values for current state: [-0.004515 -0.0019   -0.001    -0.0019  ]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 18\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.0019000000000000002\n",
      "New Q-value: -0.00271\n",
      "\n",
      "Step 7\n",
      "Current state: 18\n",
      "Q-values for current state: [-0.001  0.    -0.001  0.   ]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 17\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.001\n",
      "New Q-value: -0.0019000000000000002\n",
      "\n",
      "Step 8\n",
      "Current state: 17\n",
      "Q-values for current state: [-0.00305295 -0.001       0.         -0.00369645]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 16\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.00305295\n",
      "New Q-value: -0.0040222525\n",
      "\n",
      "Step 9\n",
      "Current state: 16\n",
      "Q-values for current state: [-0.0028905  -0.0041901  -0.003439   -0.00313935]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 8\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.0031393525000000004\n",
      "New Q-value: -0.004205171200000001\n",
      "\n",
      "Step 10\n",
      "Current state: 8\n",
      "Q-values for current state: [-0.0046014  -0.0052517  -0.00399741 -0.00544716]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 16\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.005251695000000001\n",
      "New Q-value: -0.006001123000000001\n",
      "\n",
      "Step 11\n",
      "Current state: 16\n",
      "Q-values for current state: [-0.0028905  -0.0041901  -0.003439   -0.00420517]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 8\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.004205171200000001\n",
      "New Q-value: -0.005164408030000001\n",
      "\n",
      "Step 12\n",
      "Current state: 8\n",
      "Q-values for current state: [-0.0046014  -0.00600112 -0.00399741 -0.00544716]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 8\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.004601402500000001\n",
      "New Q-value: -0.005521016200000001\n",
      "\n",
      "Step 13\n",
      "Current state: 8\n",
      "Q-values for current state: [-0.00552102 -0.00600112 -0.00399741 -0.00544716]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 16\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.006001123000000001\n",
      "New Q-value: -0.0066756082000000005\n",
      "\n",
      "Step 14\n",
      "Current state: 16\n",
      "Q-values for current state: [-0.0028905  -0.0041901  -0.003439   -0.00516441]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 17\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.003439\n",
      "New Q-value: -0.0040951\n",
      "\n",
      "Step 15\n",
      "Current state: 17\n",
      "Q-values for current state: [-0.00402225 -0.001       0.         -0.00369645]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 9\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.0036964500000000004\n",
      "New Q-value: -0.004584255000000001\n",
      "\n",
      "Step 16\n",
      "Current state: 9\n",
      "Q-values for current state: [-0.00402225 -0.00271    -0.00595278 -0.00402225]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 1\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.004022252500000001\n",
      "New Q-value: -0.005069170500615017\n",
      "\n",
      "Step 17\n",
      "Current state: 1\n",
      "Q-values for current state: [-0.00472782 -0.00538328 -0.00539753 -0.00560165]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 2\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.005397531\n",
      "New Q-value: -0.005952777899999999\n",
      "\n",
      "Step 18\n",
      "Current state: 2\n",
      "Q-values for current state: [-0.0032163  -0.00271    -0.00565408 -0.001     ]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 1\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.0032163025000000005\n",
      "New Q-value: -0.004343815500615016\n",
      "\n",
      "Step 19\n",
      "Current state: 1\n",
      "Q-values for current state: [-0.00472782 -0.00538328 -0.00595278 -0.00560165]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 9\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.0053832795\n",
      "New Q-value: -0.00610240155\n",
      "\n",
      "Step 20\n",
      "Current state: 9\n",
      "Q-values for current state: [-0.00402225 -0.00271    -0.00595278 -0.00506917]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 1\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.005069170500615017\n",
      "New Q-value: -0.006011396701168531\n",
      "\n",
      "Step 21\n",
      "Current state: 1\n",
      "Q-values for current state: [-0.00472782 -0.0061024  -0.00595278 -0.00560165]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 0\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.004727823690684375\n",
      "New Q-value: -0.005817963643846891\n",
      "\n",
      "Step 22\n",
      "Current state: 0\n",
      "Q-values for current state: [-0.0059255  -0.00737277 -0.00822474 -0.01050544]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 1\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.008224742464600001\n",
      "New Q-value: -0.008934425086890001\n",
      "\n",
      "Step 23\n",
      "Current state: 1\n",
      "Q-values for current state: [-0.00581796 -0.0061024  -0.00595278 -0.00560165]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 0\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.005817963643846891\n",
      "New Q-value: -0.006799089601693156\n",
      "\n",
      "Step 24\n",
      "Current state: 0\n",
      "Q-values for current state: [-0.0059255  -0.00737277 -0.00893443 -0.01050544]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 0\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.005925498128746876\n",
      "New Q-value: -0.0068958706381031425\n",
      "\n",
      "Step 25\n",
      "Current state: 0\n",
      "Q-values for current state: [-0.00689587 -0.00737277 -0.00893443 -0.01050544]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 0\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.010505442121051697\n",
      "New Q-value: -0.011110005619566326\n",
      "\n",
      "Step 26\n",
      "Current state: 0\n",
      "Q-values for current state: [-0.00689587 -0.00737277 -0.00893443 -0.01111001]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 8\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.007372766454250001\n",
      "New Q-value: -0.008015243758825001\n",
      "\n",
      "Step 27\n",
      "Current state: 8\n",
      "Q-values for current state: [-0.00552102 -0.00667561 -0.00399741 -0.00544716]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 16\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.0066756082000000005\n",
      "New Q-value: -0.00728264488\n",
      "\n",
      "Step 28\n",
      "Current state: 16\n",
      "Q-values for current state: [-0.0028905  -0.0041901  -0.0040951  -0.00516441]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 8\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.005164408030000001\n",
      "New Q-value: -0.006027721177000001\n",
      "\n",
      "Step 29\n",
      "Current state: 8\n",
      "Q-values for current state: [-0.00552102 -0.00728264 -0.00399741 -0.00544716]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 16\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.00728264488\n",
      "New Q-value: -0.007828977892\n",
      "\n",
      "Step 30\n",
      "Current state: 16\n",
      "Q-values for current state: [-0.0028905  -0.0041901  -0.0040951  -0.00602772]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 17\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.0040951\n",
      "New Q-value: -0.00468559\n",
      "\n",
      "Step 31\n",
      "Current state: 17\n",
      "Q-values for current state: [-0.00402225 -0.001       0.         -0.00458426]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 16\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.0040222525\n",
      "New Q-value: -0.00489462475\n",
      "\n",
      "Step 32\n",
      "Current state: 16\n",
      "Q-values for current state: [-0.0028905  -0.0041901  -0.00468559 -0.00602772]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 24\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.0041901\n",
      "New Q-value: -0.00486609\n",
      "\n",
      "Step 33\n",
      "Current state: 24\n",
      "Q-values for current state: [-0.00595278 -0.003534   -0.001      -0.0032163 ]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 32\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.0035340000000000002\n",
      "New Q-value: -0.0042756\n",
      "\n",
      "Step 34\n",
      "Current state: 32\n",
      "Q-values for current state: [-0.001  -0.0019 -0.001  -0.001 ]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 33\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.001\n",
      "New Q-value: -0.0019000000000000002\n",
      "\n",
      "Step 35\n",
      "Current state: 33\n",
      "Q-values for current state: [ 0.    -0.001  0.     0.   ]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 32\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: -0.001095\n",
      "\n",
      "Step 36\n",
      "Current state: 32\n",
      "Q-values for current state: [-0.001  -0.0019 -0.0019 -0.001 ]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 40\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.0019000000000000002\n",
      "New Q-value: -0.00271\n",
      "\n",
      "Step 37\n",
      "Current state: 40\n",
      "Q-values for current state: [-0.001 -0.001 -0.001  0.   ]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 32\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: -0.001095\n",
      "\n",
      "Step 38\n",
      "Current state: 32\n",
      "Q-values for current state: [-0.001   -0.00271 -0.0019  -0.001  ]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 32\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.001\n",
      "New Q-value: -0.0019950000000000002\n",
      "\n",
      "Step 39\n",
      "Current state: 32\n",
      "Q-values for current state: [-0.001995 -0.00271  -0.0019   -0.001   ]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 32\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.0019950000000000002\n",
      "New Q-value: -0.0028905000000000003\n",
      "\n",
      "Step 40\n",
      "Current state: 32\n",
      "Q-values for current state: [-0.0028905 -0.00271   -0.0019    -0.001    ]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 32\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.0028905000000000003\n",
      "New Q-value: -0.0036964500000000004\n",
      "\n",
      "Step 41\n",
      "Current state: 32\n",
      "Q-values for current state: [-0.00369645 -0.00271    -0.0019     -0.001     ]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 32\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.0036964500000000004\n",
      "New Q-value: -0.004421805000000001\n",
      "\n",
      "Step 42\n",
      "Current state: 32\n",
      "Q-values for current state: [-0.00442181 -0.00271    -0.0019     -0.001     ]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 40\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.00271\n",
      "New Q-value: -0.0035340000000000002\n",
      "\n",
      "Step 43\n",
      "Current state: 40\n",
      "Q-values for current state: [-0.001    -0.001    -0.001    -0.001095]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 40\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.001\n",
      "New Q-value: -0.0019950000000000002\n",
      "\n",
      "Step 44\n",
      "Current state: 40\n",
      "Q-values for current state: [-0.001995 -0.001    -0.001    -0.001095]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 48\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.001\n",
      "New Q-value: -0.0019000000000000002\n",
      "\n",
      "Step 45\n",
      "Current state: 48\n",
      "Q-values for current state: [-0.001 -0.001  0.     0.   ]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 48\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.001\n",
      "New Q-value: -0.0019000000000000002\n",
      "\n",
      "Step 46\n",
      "Current state: 48\n",
      "Q-values for current state: [-0.0019 -0.001   0.      0.    ]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 40\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: -0.001095\n",
      "\n",
      "Step 47\n",
      "Current state: 40\n",
      "Q-values for current state: [-0.001995 -0.0019   -0.001    -0.001095]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 40\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.0019950000000000002\n",
      "New Q-value: -0.0028905000000000003\n",
      "\n",
      "Step 48\n",
      "Current state: 40\n",
      "Q-values for current state: [-0.0028905 -0.0019    -0.001     -0.001095 ]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 48\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.0019000000000000002\n",
      "New Q-value: -0.00271\n",
      "\n",
      "Step 49\n",
      "Current state: 48\n",
      "Q-values for current state: [-0.0019   -0.001     0.       -0.001095]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 49\n",
      "Reward: -0.01\n",
      "Terminated: True\n",
      "Old Q-value: 0.0\n",
      "New Q-value: -0.001\n",
      "\n",
      "Episode 9\n",
      "Starting state: 0\n",
      "\n",
      "Step 0\n",
      "Current state: 0\n",
      "Q-values for current state: [-0.00689587 -0.00801524 -0.00893443 -0.01111001]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 1\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.008934425086890001\n",
      "New Q-value: -0.009573139446951002\n",
      "\n",
      "Step 1\n",
      "Current state: 1\n",
      "Q-values for current state: [-0.00679909 -0.0061024  -0.00595278 -0.00560165]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 2\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.005952777899999999\n",
      "New Q-value: -0.0064525001099999995\n",
      "\n",
      "Step 2\n",
      "Current state: 2\n",
      "Q-values for current state: [-0.00434382 -0.00271    -0.00565408 -0.001     ]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 2\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.001\n",
      "New Q-value: -0.0019950000000000002\n",
      "\n",
      "Step 3\n",
      "Current state: 2\n",
      "Q-values for current state: [-0.00434382 -0.00271    -0.00565408 -0.001995  ]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 1\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.004343815500615016\n",
      "New Q-value: -0.005441590819303514\n",
      "\n",
      "Step 4\n",
      "Current state: 1\n",
      "Q-values for current state: [-0.00679909 -0.0061024  -0.0064525  -0.00560165]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 2\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.0064525001099999995\n",
      "New Q-value: -0.006996775098999999\n",
      "\n",
      "Step 5\n",
      "Current state: 2\n",
      "Q-values for current state: [-0.00544159 -0.00271    -0.00565408 -0.001995  ]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 10\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.00271\n",
      "New Q-value: -0.0035340000000000002\n",
      "\n",
      "Step 6\n",
      "Current state: 10\n",
      "Q-values for current state: [-0.004515 -0.00271  -0.001    -0.0019  ]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 18\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.00271\n",
      "New Q-value: -0.003439\n",
      "\n",
      "Step 7\n",
      "Current state: 18\n",
      "Q-values for current state: [-0.0019  0.     -0.001   0.    ]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 26\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: -0.001\n",
      "\n",
      "Step 8\n",
      "Current state: 26\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 27\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: -0.001\n",
      "\n",
      "Step 9\n",
      "Current state: 27\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 35\n",
      "Reward: -0.01\n",
      "Terminated: True\n",
      "Old Q-value: 0.0\n",
      "New Q-value: -0.001\n",
      "\n",
      "Episode 10\n",
      "Starting state: 0\n",
      "\n",
      "Step 0\n",
      "Current state: 0\n",
      "Q-values for current state: [-0.00689587 -0.00801524 -0.00957314 -0.01111001]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 0\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.011110005619566326\n",
      "New Q-value: -0.011654112768229492\n",
      "\n",
      "Step 1\n",
      "Current state: 0\n",
      "Q-values for current state: [-0.00689587 -0.00801524 -0.00957314 -0.01165411]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 0\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.011654112768229492\n",
      "New Q-value: -0.01214380920202634\n",
      "\n",
      "Step 2\n",
      "Current state: 0\n",
      "Q-values for current state: [-0.00689587 -0.00801524 -0.00957314 -0.01214381]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 1\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.009573139446951002\n",
      "New Q-value: -0.010147982371005901\n",
      "\n",
      "Step 3\n",
      "Current state: 1\n",
      "Q-values for current state: [-0.00679909 -0.0061024  -0.00699678 -0.00560165]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 2\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.006996775098999999\n",
      "New Q-value: -0.0074866225891\n",
      "\n",
      "Step 4\n",
      "Current state: 2\n",
      "Q-values for current state: [-0.00544159 -0.003534   -0.00565408 -0.001995  ]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 10\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.0035340000000000002\n",
      "New Q-value: -0.0042756\n",
      "\n",
      "Step 5\n",
      "Current state: 10\n",
      "Q-values for current state: [-0.004515 -0.003439 -0.001    -0.0019  ]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 11\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.001\n",
      "New Q-value: -0.0019000000000000002\n",
      "\n",
      "Step 6\n",
      "Current state: 11\n",
      "Q-values for current state: [ 0.      -0.00271 -0.0019  -0.0019 ]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 10\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: -0.0011805000000000001\n",
      "\n",
      "Step 7\n",
      "Current state: 10\n",
      "Q-values for current state: [-0.004515 -0.003439 -0.0019   -0.0019  ]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 11\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.0019000000000000002\n",
      "New Q-value: -0.0028221475000000003\n",
      "\n",
      "Step 8\n",
      "Current state: 11\n",
      "Q-values for current state: [-0.0011805 -0.00271   -0.0019    -0.0019   ]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 3\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.0019000000000000002\n",
      "New Q-value: -0.002899525\n",
      "\n",
      "Step 9\n",
      "Current state: 3\n",
      "Q-values for current state: [-0.003534  -0.003439  -0.0036195 -0.001995 ]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 4\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.0036195000000000003\n",
      "New Q-value: -0.0044560548750000005\n",
      "\n",
      "Step 10\n",
      "Current state: 4\n",
      "Q-values for current state: [-0.00271    -0.003439   -0.00208953 -0.0028905 ]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 12\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.003439\n",
      "New Q-value: -0.0040951\n",
      "\n",
      "Step 11\n",
      "Current state: 12\n",
      "Q-values for current state: [-0.0019      0.         -0.00443805 -0.001995  ]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 20\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: -0.001\n",
      "\n",
      "Step 12\n",
      "Current state: 20\n",
      "Q-values for current state: [ 0.     0.     0.    -0.001]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 19\n",
      "Reward: -0.01\n",
      "Terminated: True\n",
      "Old Q-value: 0.0\n",
      "New Q-value: -0.001\n",
      "\n",
      "Episode 10 summary:\n",
      "Total reward: -0.12999999999999998\n",
      "Successful episodes so far: []\n",
      "Average reward over last 10 episodes: -0.338\n",
      "\n",
      "Episode 11\n",
      "Starting state: 0\n",
      "\n",
      "Step 0\n",
      "Current state: 0\n",
      "Q-values for current state: [-0.00689587 -0.00801524 -0.01014798 -0.01214381]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 0\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.01214380920202634\n",
      "New Q-value: -0.012584535992443505\n",
      "\n",
      "Step 1\n",
      "Current state: 0\n",
      "Q-values for current state: [-0.00689587 -0.00801524 -0.01014798 -0.01258454]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 0\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.0068958706381031425\n",
      "New Q-value: -0.007861391284912627\n",
      "\n",
      "Step 2\n",
      "Current state: 0\n",
      "Q-values for current state: [-0.00786139 -0.00801524 -0.01014798 -0.01258454]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 1\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.010147982371005901\n",
      "New Q-value: -0.010665341002655312\n",
      "\n",
      "Step 3\n",
      "Current state: 1\n",
      "Q-values for current state: [-0.00679909 -0.0061024  -0.00748662 -0.00560165]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 0\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.006799089601693156\n",
      "New Q-value: -0.00786601281359054\n",
      "\n",
      "Step 4\n",
      "Current state: 0\n",
      "Q-values for current state: [-0.00786139 -0.00801524 -0.01066534 -0.01258454]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 8\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.008015243758825001\n",
      "New Q-value: -0.0085934733329425\n",
      "\n",
      "Step 5\n",
      "Current state: 8\n",
      "Q-values for current state: [-0.00552102 -0.00782898 -0.00399741 -0.00544716]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 8\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.005521016200000001\n",
      "New Q-value: -0.006348668530000001\n",
      "\n",
      "Step 6\n",
      "Current state: 8\n",
      "Q-values for current state: [-0.00634867 -0.00782898 -0.00399741 -0.00544716]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 9\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.00399741\n",
      "New Q-value: -0.004855119\n",
      "\n",
      "Step 7\n",
      "Current state: 9\n",
      "Q-values for current state: [-0.00402225 -0.00271    -0.00595278 -0.0060114 ]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 17\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.00271\n",
      "New Q-value: -0.003439\n",
      "\n",
      "Step 8\n",
      "Current state: 17\n",
      "Q-values for current state: [-0.00489462 -0.001       0.         -0.00458426]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 25\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.001\n",
      "New Q-value: -0.0019000000000000002\n",
      "\n",
      "Step 9\n",
      "Current state: 25\n",
      "Q-values for current state: [-0.001995  0.        0.        0.      ]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 24\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.0019950000000000002\n",
      "New Q-value: -0.0028905000000000003\n",
      "\n",
      "Step 10\n",
      "Current state: 24\n",
      "Q-values for current state: [-0.00595278 -0.0042756  -0.001      -0.0032163 ]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 24\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.005952777899999999\n",
      "New Q-value: -0.0064525001099999995\n",
      "\n",
      "Step 11\n",
      "Current state: 24\n",
      "Q-values for current state: [-0.0064525 -0.0042756 -0.001     -0.0032163]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 32\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.0042756\n",
      "New Q-value: -0.00494304\n",
      "\n",
      "Step 12\n",
      "Current state: 32\n",
      "Q-values for current state: [-0.00442181 -0.003534   -0.0019     -0.001     ]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 24\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.001\n",
      "New Q-value: -0.0019950000000000002\n",
      "\n",
      "Step 13\n",
      "Current state: 24\n",
      "Q-values for current state: [-0.0064525  -0.00494304 -0.001      -0.0032163 ]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 32\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.00494304\n",
      "New Q-value: -0.005629236\n",
      "\n",
      "Step 14\n",
      "Current state: 32\n",
      "Q-values for current state: [-0.00442181 -0.003534   -0.0019     -0.001995  ]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 32\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.004421805000000001\n",
      "New Q-value: -0.005160124500000001\n",
      "\n",
      "Step 15\n",
      "Current state: 32\n",
      "Q-values for current state: [-0.00516012 -0.003534   -0.0019     -0.001995  ]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 33\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.0019000000000000002\n",
      "New Q-value: -0.00271\n",
      "\n",
      "Step 16\n",
      "Current state: 33\n",
      "Q-values for current state: [-0.001095 -0.001     0.        0.      ]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 25\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: -0.001\n",
      "\n",
      "Step 17\n",
      "Current state: 25\n",
      "Q-values for current state: [-0.0028905  0.         0.         0.       ]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 24\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.0028905000000000003\n",
      "New Q-value: -0.0036964500000000004\n",
      "\n",
      "Step 18\n",
      "Current state: 24\n",
      "Q-values for current state: [-0.0064525  -0.00562924 -0.001      -0.0032163 ]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 24\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.0064525001099999995\n",
      "New Q-value: -0.006902250099\n",
      "\n",
      "Step 19\n",
      "Current state: 24\n",
      "Q-values for current state: [-0.00690225 -0.00562924 -0.001      -0.0032163 ]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 32\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.005629236\n",
      "New Q-value: -0.0062558374\n",
      "\n",
      "Step 20\n",
      "Current state: 32\n",
      "Q-values for current state: [-0.00516012 -0.003534   -0.00271    -0.001995  ]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 32\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.005160124500000001\n",
      "New Q-value: -0.005833637050000001\n",
      "\n",
      "Step 21\n",
      "Current state: 32\n",
      "Q-values for current state: [-0.00583364 -0.003534   -0.00271    -0.001995  ]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 40\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.0035340000000000002\n",
      "New Q-value: -0.0042756\n",
      "\n",
      "Step 22\n",
      "Current state: 40\n",
      "Q-values for current state: [-0.0028905 -0.00271   -0.001     -0.001095 ]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 48\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.00271\n",
      "New Q-value: -0.0035340000000000002\n",
      "\n",
      "Step 23\n",
      "Current state: 48\n",
      "Q-values for current state: [-0.0019   -0.001    -0.001    -0.001095]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 49\n",
      "Reward: -0.01\n",
      "Terminated: True\n",
      "Old Q-value: -0.001\n",
      "New Q-value: -0.0019000000000000002\n",
      "\n",
      "Episode 12\n",
      "Starting state: 0\n",
      "\n",
      "Step 0\n",
      "Current state: 0\n",
      "Q-values for current state: [-0.00786139 -0.00859347 -0.01066534 -0.01258454]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 8\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.0085934733329425\n",
      "New Q-value: -0.009195362304648251\n",
      "\n",
      "Step 1\n",
      "Current state: 8\n",
      "Q-values for current state: [-0.00634867 -0.00782898 -0.00485512 -0.00544716]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 8\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.006348668530000001\n",
      "New Q-value: -0.007175037982000002\n",
      "\n",
      "Step 2\n",
      "Current state: 8\n",
      "Q-values for current state: [-0.00717504 -0.00782898 -0.00485512 -0.00544716]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 0\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.0054471572625\n",
      "New Q-value: -0.0066492737083167\n",
      "\n",
      "Step 3\n",
      "Current state: 0\n",
      "Q-values for current state: [-0.00786139 -0.00919536 -0.01066534 -0.01258454]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 0\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.012584535992443505\n",
      "New Q-value: -0.013072914565265855\n",
      "\n",
      "Step 4\n",
      "Current state: 0\n",
      "Q-values for current state: [-0.00786139 -0.00919536 -0.01066534 -0.01307291]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 8\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.009195362304648251\n",
      "New Q-value: -0.009737062379183425\n",
      "\n",
      "Step 5\n",
      "Current state: 8\n",
      "Q-values for current state: [-0.00717504 -0.00782898 -0.00485512 -0.00664927]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 0\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.0066492737083167\n",
      "New Q-value: -0.00773117850955173\n",
      "\n",
      "Step 6\n",
      "Current state: 0\n",
      "Q-values for current state: [-0.00786139 -0.00973706 -0.01066534 -0.01307291]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 0\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.013072914565265855\n",
      "New Q-value: -0.013512455280805968\n",
      "\n",
      "Step 7\n",
      "Current state: 0\n",
      "Q-values for current state: [-0.00786139 -0.00973706 -0.01066534 -0.01351246]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 0\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.007861391284912627\n",
      "New Q-value: -0.008822084328488064\n",
      "\n",
      "Step 8\n",
      "Current state: 0\n",
      "Q-values for current state: [-0.00882208 -0.00973706 -0.01066534 -0.01351246]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 1\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.010665341002655312\n",
      "New Q-value: -0.011130963771139781\n",
      "\n",
      "Step 9\n",
      "Current state: 1\n",
      "Q-values for current state: [-0.00786601 -0.0061024  -0.00748662 -0.00560165]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 1\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.005601651250000001\n",
      "New Q-value: -0.006573642993750001\n",
      "\n",
      "Step 10\n",
      "Current state: 1\n",
      "Q-values for current state: [-0.00786601 -0.0061024  -0.00748662 -0.00657364]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 2\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.0074866225891\n",
      "New Q-value: -0.00792748533019\n",
      "\n",
      "Step 11\n",
      "Current state: 2\n",
      "Q-values for current state: [-0.00544159 -0.0042756  -0.00565408 -0.001995  ]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 10\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.0042756\n",
      "New Q-value: -0.00502854\n",
      "\n",
      "Step 12\n",
      "Current state: 10\n",
      "Q-values for current state: [-0.004515   -0.003439   -0.00282215 -0.0019    ]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 9\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.004515\n",
      "New Q-value: -0.005390205\n",
      "\n",
      "Step 13\n",
      "Current state: 9\n",
      "Q-values for current state: [-0.00402225 -0.003439   -0.00595278 -0.0060114 ]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 10\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.005952777899999999\n",
      "New Q-value: -0.006538000109999999\n",
      "\n",
      "Step 14\n",
      "Current state: 10\n",
      "Q-values for current state: [-0.00539021 -0.003439   -0.00282215 -0.0019    ]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 11\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.0028221475000000003\n",
      "New Q-value: -0.00365208025\n",
      "\n",
      "Step 15\n",
      "Current state: 11\n",
      "Q-values for current state: [-0.0011805  -0.00271    -0.0019     -0.00289953]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 19\n",
      "Reward: -0.01\n",
      "Terminated: True\n",
      "Old Q-value: -0.00271\n",
      "New Q-value: -0.003439\n",
      "\n",
      "Episode 13\n",
      "Starting state: 0\n",
      "\n",
      "Step 0\n",
      "Current state: 0\n",
      "Q-values for current state: [-0.00882208 -0.00973706 -0.01113096 -0.01351246]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 1\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.011130963771139781\n",
      "New Q-value: -0.011597595541275802\n",
      "\n",
      "Step 1\n",
      "Current state: 1\n",
      "Q-values for current state: [-0.00786601 -0.0061024  -0.00792749 -0.00657364]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 2\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.00792748533019\n",
      "New Q-value: -0.008324261797171\n",
      "\n",
      "Step 2\n",
      "Current state: 2\n",
      "Q-values for current state: [-0.00544159 -0.00502854 -0.00565408 -0.001995  ]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 1\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.005441590819303514\n",
      "New Q-value: -0.0064771598846231635\n",
      "\n",
      "Step 3\n",
      "Current state: 1\n",
      "Q-values for current state: [-0.00786601 -0.0061024  -0.00832426 -0.00657364]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 0\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.00786601281359054\n",
      "New Q-value: -0.008917509543437853\n",
      "\n",
      "Step 4\n",
      "Current state: 0\n",
      "Q-values for current state: [-0.00882208 -0.00973706 -0.0115976  -0.01351246]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 0\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.013512455280805968\n",
      "New Q-value: -0.013999307763931738\n",
      "\n",
      "Step 5\n",
      "Current state: 0\n",
      "Q-values for current state: [-0.00882208 -0.00973706 -0.0115976  -0.01399931]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 0\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.013999307763931738\n",
      "New Q-value: -0.014437474998744931\n",
      "\n",
      "Step 6\n",
      "Current state: 0\n",
      "Q-values for current state: [-0.00882208 -0.00973706 -0.0115976  -0.01443747]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 0\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.014437474998744931\n",
      "New Q-value: -0.014831825510076804\n",
      "\n",
      "Step 7\n",
      "Current state: 0\n",
      "Q-values for current state: [-0.00882208 -0.00973706 -0.0115976  -0.01483183]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 8\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.009737062379183425\n",
      "New Q-value: -0.010224592446265083\n",
      "\n",
      "Step 8\n",
      "Current state: 8\n",
      "Q-values for current state: [-0.00717504 -0.00782898 -0.00485512 -0.00773118]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 9\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.004855119\n",
      "New Q-value: -0.0056963121\n",
      "\n",
      "Step 9\n",
      "Current state: 9\n",
      "Q-values for current state: [-0.00402225 -0.003439   -0.006538   -0.0060114 ]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 1\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.006011396701168531\n",
      "New Q-value: -0.006989985178301678\n",
      "\n",
      "Step 10\n",
      "Current state: 1\n",
      "Q-values for current state: [-0.00891751 -0.0061024  -0.00832426 -0.00657364]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 2\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.008324261797171\n",
      "New Q-value: -0.0086813606174539\n",
      "\n",
      "Step 11\n",
      "Current state: 2\n",
      "Q-values for current state: [-0.00647716 -0.00502854 -0.00565408 -0.001995  ]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 3\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.0056540785000000005\n",
      "New Q-value: -0.00627819565\n",
      "\n",
      "Step 12\n",
      "Current state: 3\n",
      "Q-values for current state: [-0.003534   -0.003439   -0.00445605 -0.001995  ]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 3\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.0019950000000000002\n",
      "New Q-value: -0.0029850250000000005\n",
      "\n",
      "Step 13\n",
      "Current state: 3\n",
      "Q-values for current state: [-0.003534   -0.003439   -0.00445605 -0.00298503]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 3\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.0029850250000000005\n",
      "New Q-value: -0.003970099875000001\n",
      "\n",
      "Step 14\n",
      "Current state: 3\n",
      "Q-values for current state: [-0.003534   -0.003439   -0.00445605 -0.0039701 ]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 11\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.003439\n",
      "New Q-value: -0.0042072475000000005\n",
      "\n",
      "Step 15\n",
      "Current state: 11\n",
      "Q-values for current state: [-0.0011805  -0.003439   -0.0019     -0.00289953]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 12\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.0019000000000000002\n",
      "New Q-value: -0.002805\n",
      "\n",
      "Step 16\n",
      "Current state: 12\n",
      "Q-values for current state: [-0.0019     -0.001      -0.00443805 -0.001995  ]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 13\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.004438050000000001\n",
      "New Q-value: -0.0051747450000000006\n",
      "\n",
      "Step 17\n",
      "Current state: 13\n",
      "Q-values for current state: [-0.0019     -0.003439   -0.00271    -0.00510682]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 12\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.0019000000000000002\n",
      "New Q-value: -0.002805\n",
      "\n",
      "Step 18\n",
      "Current state: 12\n",
      "Q-values for current state: [-0.0019     -0.001      -0.00517475 -0.001995  ]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 20\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.001\n",
      "New Q-value: -0.0019000000000000002\n",
      "\n",
      "Step 19\n",
      "Current state: 20\n",
      "Q-values for current state: [-0.001  0.     0.    -0.001]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 12\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.001\n",
      "New Q-value: -0.0020805\n",
      "\n",
      "Step 20\n",
      "Current state: 12\n",
      "Q-values for current state: [-0.0019     -0.0019     -0.00517475 -0.001995  ]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 20\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.0019000000000000002\n",
      "New Q-value: -0.00271\n",
      "\n",
      "Step 21\n",
      "Current state: 20\n",
      "Q-values for current state: [-0.001      0.         0.        -0.0020805]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 12\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.0020805\n",
      "New Q-value: -0.00305295\n",
      "\n",
      "Step 22\n",
      "Current state: 12\n",
      "Q-values for current state: [-0.0019     -0.00271    -0.00517475 -0.001995  ]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 11\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.0019000000000000002\n",
      "New Q-value: -0.0028221475000000003\n",
      "\n",
      "Step 23\n",
      "Current state: 11\n",
      "Q-values for current state: [-0.0011805  -0.003439   -0.002805   -0.00289953]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 12\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.002805\n",
      "New Q-value: -0.003714025\n",
      "\n",
      "Step 24\n",
      "Current state: 12\n",
      "Q-values for current state: [-0.00282215 -0.00271    -0.00517475 -0.001995  ]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 4\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.0019950000000000002\n",
      "New Q-value: -0.002994004875\n",
      "\n",
      "Step 25\n",
      "Current state: 4\n",
      "Q-values for current state: [-0.00271    -0.0040951  -0.00208953 -0.0028905 ]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 4\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.0028905000000000003\n",
      "New Q-value: -0.0037999548750000003\n",
      "\n",
      "Step 26\n",
      "Current state: 4\n",
      "Q-values for current state: [-0.00271    -0.0040951  -0.00208953 -0.00379995]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 12\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.0040951\n",
      "New Q-value: -0.00494304\n",
      "\n",
      "Step 27\n",
      "Current state: 12\n",
      "Q-values for current state: [-0.00282215 -0.00271    -0.00517475 -0.002994  ]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 11\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.0028221475000000003\n",
      "New Q-value: -0.00365208025\n",
      "\n",
      "Step 28\n",
      "Current state: 11\n",
      "Q-values for current state: [-0.0011805  -0.003439   -0.00371403 -0.00289953]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 19\n",
      "Reward: -0.01\n",
      "Terminated: True\n",
      "Old Q-value: -0.003439\n",
      "New Q-value: -0.0040951\n",
      "\n",
      "Episode 14\n",
      "Starting state: 0\n",
      "\n",
      "Step 0\n",
      "Current state: 0\n",
      "Q-values for current state: [-0.00882208 -0.01022459 -0.0115976  -0.01483183]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 1\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.011597595541275802\n",
      "New Q-value: -0.012017564134398223\n",
      "\n",
      "Step 1\n",
      "Current state: 1\n",
      "Q-values for current state: [-0.00891751 -0.0061024  -0.00868136 -0.00657364]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 2\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.0086813606174539\n",
      "New Q-value: -0.00900274955570851\n",
      "\n",
      "Step 2\n",
      "Current state: 2\n",
      "Q-values for current state: [-0.00647716 -0.00502854 -0.0062782  -0.001995  ]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 1\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.0064771598846231635\n",
      "New Q-value: -0.007409172043410847\n",
      "\n",
      "Step 3\n",
      "Current state: 1\n",
      "Q-values for current state: [-0.00891751 -0.0061024  -0.00900275 -0.00657364]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 1\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.006573642993750001\n",
      "New Q-value: -0.007496006841625001\n",
      "\n",
      "Step 4\n",
      "Current state: 1\n",
      "Q-values for current state: [-0.00891751 -0.0061024  -0.00900275 -0.00749601]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 2\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.00900274955570851\n",
      "New Q-value: -0.009291999600137659\n",
      "\n",
      "Step 5\n",
      "Current state: 2\n",
      "Q-values for current state: [-0.00740917 -0.00502854 -0.0062782  -0.001995  ]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 10\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.00502854\n",
      "New Q-value: -0.005706186\n",
      "\n",
      "Step 6\n",
      "Current state: 10\n",
      "Q-values for current state: [-0.00539021 -0.003439   -0.00365208 -0.0019    ]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 11\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.00365208025\n",
      "New Q-value: -0.004399019725\n",
      "\n",
      "Step 7\n",
      "Current state: 11\n",
      "Q-values for current state: [-0.0011805  -0.0040951  -0.00371403 -0.00289953]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 19\n",
      "Reward: -0.01\n",
      "Terminated: True\n",
      "Old Q-value: -0.0040951\n",
      "New Q-value: -0.00468559\n",
      "\n",
      "Episode 15\n",
      "Starting state: 0\n",
      "\n",
      "Step 0\n",
      "Current state: 0\n",
      "Q-values for current state: [-0.00882208 -0.01022459 -0.01201756 -0.01483183]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 8\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.010224592446265083\n",
      "New Q-value: -0.010743282851138574\n",
      "\n",
      "Step 1\n",
      "Current state: 8\n",
      "Q-values for current state: [-0.00717504 -0.00782898 -0.00569631 -0.00773118]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 16\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.007828977892\n",
      "New Q-value: -0.008320677602800001\n",
      "\n",
      "Step 2\n",
      "Current state: 16\n",
      "Q-values for current state: [-0.0028905  -0.00486609 -0.00468559 -0.00602772]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 24\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.00486609\n",
      "New Q-value: -0.005474481\n",
      "\n",
      "Step 3\n",
      "Current state: 24\n",
      "Q-values for current state: [-0.00690225 -0.00625584 -0.001      -0.0032163 ]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 32\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.0062558374\n",
      "New Q-value: -0.00681977866\n",
      "\n",
      "Step 4\n",
      "Current state: 32\n",
      "Q-values for current state: [-0.00583364 -0.0042756  -0.00271    -0.001995  ]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 40\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.0042756\n",
      "New Q-value: -0.00494304\n",
      "\n",
      "Step 5\n",
      "Current state: 40\n",
      "Q-values for current state: [-0.0028905 -0.003534  -0.001     -0.001095 ]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 32\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.001095\n",
      "New Q-value: -0.0021750249999999997\n",
      "\n",
      "Step 6\n",
      "Current state: 32\n",
      "Q-values for current state: [-0.00583364 -0.00494304 -0.00271    -0.001995  ]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 24\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.0019950000000000002\n",
      "New Q-value: -0.0028905000000000003\n",
      "\n",
      "Step 7\n",
      "Current state: 24\n",
      "Q-values for current state: [-0.00690225 -0.00681978 -0.001      -0.0032163 ]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 25\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.001\n",
      "New Q-value: -0.0019000000000000002\n",
      "\n",
      "Step 8\n",
      "Current state: 25\n",
      "Q-values for current state: [-0.00369645  0.          0.          0.        ]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 26\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: -0.001\n",
      "\n",
      "Step 9\n",
      "Current state: 26\n",
      "Q-values for current state: [ 0.     0.    -0.001  0.   ]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 18\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: -0.001\n",
      "\n",
      "Step 10\n",
      "Current state: 18\n",
      "Q-values for current state: [-0.0019 -0.001  -0.001   0.    ]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 26\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.001\n",
      "New Q-value: -0.0019000000000000002\n",
      "\n",
      "Step 11\n",
      "Current state: 26\n",
      "Q-values for current state: [ 0.     0.    -0.001 -0.001]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 25\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: -0.001\n",
      "\n",
      "Step 12\n",
      "Current state: 25\n",
      "Q-values for current state: [-0.00369645  0.         -0.001       0.        ]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 17\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: -0.001\n",
      "\n",
      "Step 13\n",
      "Current state: 17\n",
      "Q-values for current state: [-0.00489462 -0.0019      0.         -0.00458426]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 18\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: -0.001\n",
      "\n",
      "Step 14\n",
      "Current state: 18\n",
      "Q-values for current state: [-0.0019 -0.0019 -0.001   0.    ]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 26\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.0019000000000000002\n",
      "New Q-value: -0.00271\n",
      "\n",
      "Step 15\n",
      "Current state: 26\n",
      "Q-values for current state: [-0.001  0.    -0.001 -0.001]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 25\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.001\n",
      "New Q-value: -0.0019000000000000002\n",
      "\n",
      "Step 16\n",
      "Current state: 25\n",
      "Q-values for current state: [-0.00369645  0.         -0.001      -0.001     ]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 24\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.0036964500000000004\n",
      "New Q-value: -0.0045073050000000005\n",
      "\n",
      "Step 17\n",
      "Current state: 24\n",
      "Q-values for current state: [-0.00690225 -0.00681978 -0.0019     -0.0032163 ]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 25\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.0019000000000000002\n",
      "New Q-value: -0.00271\n",
      "\n",
      "Step 18\n",
      "Current state: 25\n",
      "Q-values for current state: [-0.00450731  0.         -0.001      -0.001     ]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 24\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.0045073050000000005\n",
      "New Q-value: -0.0053140245\n",
      "\n",
      "Step 19\n",
      "Current state: 24\n",
      "Q-values for current state: [-0.00690225 -0.00681978 -0.00271    -0.0032163 ]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 16\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.0032163025\n",
      "New Q-value: -0.00416926975\n",
      "\n",
      "Step 20\n",
      "Current state: 16\n",
      "Q-values for current state: [-0.0028905  -0.00547448 -0.00468559 -0.00602772]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 16\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.0028905000000000003\n",
      "New Q-value: -0.0038760475000000003\n",
      "\n",
      "Step 21\n",
      "Current state: 16\n",
      "Q-values for current state: [-0.00387605 -0.00547448 -0.00468559 -0.00602772]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 8\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.006027721177000001\n",
      "New Q-value: -0.006966098708800001\n",
      "\n",
      "Step 22\n",
      "Current state: 8\n",
      "Q-values for current state: [-0.00717504 -0.00832068 -0.00569631 -0.00773118]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 0\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.00773117850955173\n",
      "New Q-value: -0.008796158669802923\n",
      "\n",
      "Step 23\n",
      "Current state: 0\n",
      "Q-values for current state: [-0.00882208 -0.01074328 -0.01201756 -0.01483183]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 8\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.010743282851138574\n",
      "New Q-value: -0.011210104215524716\n",
      "\n",
      "Step 24\n",
      "Current state: 8\n",
      "Q-values for current state: [-0.00717504 -0.00832068 -0.00569631 -0.00879616]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 16\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.008320677602800001\n",
      "New Q-value: -0.00885683435502\n",
      "\n",
      "Step 25\n",
      "Current state: 16\n",
      "Q-values for current state: [-0.00387605 -0.00547448 -0.00468559 -0.0069661 ]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 16\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.0038760475000000003\n",
      "New Q-value: -0.0048566672625\n",
      "\n",
      "Step 26\n",
      "Current state: 16\n",
      "Q-values for current state: [-0.00485667 -0.00547448 -0.00468559 -0.0069661 ]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 16\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.0048566672625\n",
      "New Q-value: -0.00581613158625\n",
      "\n",
      "Step 27\n",
      "Current state: 16\n",
      "Q-values for current state: [-0.00581613 -0.00547448 -0.00468559 -0.0069661 ]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 16\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.00581613158625\n",
      "New Q-value: -0.0066796494776250005\n",
      "\n",
      "Step 28\n",
      "Current state: 16\n",
      "Q-values for current state: [-0.00667965 -0.00547448 -0.00468559 -0.0069661 ]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 8\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.006966098708800001\n",
      "New Q-value: -0.007810638487420001\n",
      "\n",
      "Step 29\n",
      "Current state: 8\n",
      "Q-values for current state: [-0.00717504 -0.00885683 -0.00569631 -0.00879616]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 8\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.007175037982000002\n",
      "New Q-value: -0.007998683833300001\n",
      "\n",
      "Step 30\n",
      "Current state: 8\n",
      "Q-values for current state: [-0.00799868 -0.00885683 -0.00569631 -0.00879616]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 0\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.008796158669802923\n",
      "New Q-value: -0.009754640814028997\n",
      "\n",
      "Step 31\n",
      "Current state: 0\n",
      "Q-values for current state: [-0.00882208 -0.0112101  -0.01201756 -0.01483183]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 0\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.014831825510076804\n",
      "New Q-value: -0.015186740970275489\n",
      "\n",
      "Step 32\n",
      "Current state: 0\n",
      "Q-values for current state: [-0.00882208 -0.0112101  -0.01201756 -0.01518674]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 8\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.011210104215524716\n",
      "New Q-value: -0.011630243443472245\n",
      "\n",
      "Step 33\n",
      "Current state: 8\n",
      "Q-values for current state: [-0.00799868 -0.00885683 -0.00569631 -0.00975464]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 8\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.007998683833300001\n",
      "New Q-value: -0.00873996509947\n",
      "\n",
      "Step 34\n",
      "Current state: 8\n",
      "Q-values for current state: [-0.00873997 -0.00885683 -0.00569631 -0.00975464]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 9\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.0056963121\n",
      "New Q-value: -0.00645338589\n",
      "\n",
      "Step 35\n",
      "Current state: 9\n",
      "Q-values for current state: [-0.00402225 -0.003439   -0.006538   -0.00698999]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 8\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.0040222525\n",
      "New Q-value: -0.00523309890955\n",
      "\n",
      "Step 36\n",
      "Current state: 8\n",
      "Q-values for current state: [-0.00873997 -0.00885683 -0.00645339 -0.00975464]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 16\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.00885683435502\n",
      "New Q-value: -0.009416281969518001\n",
      "\n",
      "Step 37\n",
      "Current state: 16\n",
      "Q-values for current state: [-0.00667965 -0.00547448 -0.00468559 -0.00781064]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 24\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.005474481\n",
      "New Q-value: -0.0061844829\n",
      "\n",
      "Step 38\n",
      "Current state: 24\n",
      "Q-values for current state: [-0.00690225 -0.00681978 -0.00271    -0.00416927]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 16\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.00416926975\n",
      "New Q-value: -0.005197473825\n",
      "\n",
      "Step 39\n",
      "Current state: 16\n",
      "Q-values for current state: [-0.00667965 -0.00618448 -0.00468559 -0.00781064]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 16\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.0066796494776250005\n",
      "New Q-value: -0.0074568155798625\n",
      "\n",
      "Step 40\n",
      "Current state: 16\n",
      "Q-values for current state: [-0.00745682 -0.00618448 -0.00468559 -0.00781064]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 8\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.007810638487420001\n",
      "New Q-value: -0.008642646298228002\n",
      "\n",
      "Step 41\n",
      "Current state: 8\n",
      "Q-values for current state: [-0.00873997 -0.00941628 -0.00645339 -0.00975464]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 8\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.00873996509947\n",
      "New Q-value: -0.009479040249073001\n",
      "\n",
      "Step 42\n",
      "Current state: 8\n",
      "Q-values for current state: [-0.00947904 -0.00941628 -0.00645339 -0.00975464]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 8\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.009479040249073001\n",
      "New Q-value: -0.0101442078837157\n",
      "\n",
      "Step 43\n",
      "Current state: 8\n",
      "Q-values for current state: [-0.01014421 -0.00941628 -0.00645339 -0.00975464]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 0\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.009754640814028997\n",
      "New Q-value: -0.010617274743832464\n",
      "\n",
      "Step 44\n",
      "Current state: 0\n",
      "Q-values for current state: [-0.00882208 -0.01163024 -0.01201756 -0.01518674]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 8\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.011630243443472245\n",
      "New Q-value: -0.01208029075867502\n",
      "\n",
      "Step 45\n",
      "Current state: 8\n",
      "Q-values for current state: [-0.01014421 -0.00941628 -0.00645339 -0.01061727]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 8\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.0101442078837157\n",
      "New Q-value: -0.01074285875489413\n",
      "\n",
      "Step 46\n",
      "Current state: 8\n",
      "Q-values for current state: [-0.01074286 -0.00941628 -0.00645339 -0.01061727]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 8\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.01074285875489413\n",
      "New Q-value: -0.011281644538954718\n",
      "\n",
      "Step 47\n",
      "Current state: 8\n",
      "Q-values for current state: [-0.01128164 -0.00941628 -0.00645339 -0.01061727]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 8\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.011281644538954718\n",
      "New Q-value: -0.011766551744609246\n",
      "\n",
      "Step 48\n",
      "Current state: 8\n",
      "Q-values for current state: [-0.01176655 -0.00941628 -0.00645339 -0.01061727]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 8\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.011766551744609246\n",
      "New Q-value: -0.012202968229698321\n",
      "\n",
      "Step 49\n",
      "Current state: 8\n",
      "Q-values for current state: [-0.01220297 -0.00941628 -0.00645339 -0.01061727]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 0\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.010617274743832464\n",
      "New Q-value: -0.011393645280655583\n",
      "\n",
      "Step 50\n",
      "Current state: 0\n",
      "Q-values for current state: [-0.00882208 -0.01208029 -0.01201756 -0.01518674]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 0\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.015186740970275489\n",
      "New Q-value: -0.015506164884454306\n",
      "\n",
      "Step 51\n",
      "Current state: 0\n",
      "Q-values for current state: [-0.00882208 -0.01208029 -0.01201756 -0.01550616]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 0\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.015506164884454306\n",
      "New Q-value: -0.015793646407215242\n",
      "\n",
      "Step 52\n",
      "Current state: 0\n",
      "Q-values for current state: [-0.00882208 -0.01208029 -0.01201756 -0.01579365]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 0\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.015793646407215242\n",
      "New Q-value: -0.016052379777700083\n",
      "\n",
      "Step 53\n",
      "Current state: 0\n",
      "Q-values for current state: [-0.00882208 -0.01208029 -0.01201756 -0.01605238]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 1\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.012017564134398223\n",
      "New Q-value: -0.0123955358682084\n",
      "\n",
      "Step 54\n",
      "Current state: 1\n",
      "Q-values for current state: [-0.00891751 -0.0061024  -0.009292   -0.00749601]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 1\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.007496006841625001\n",
      "New Q-value: -0.0083261343047125\n",
      "\n",
      "Step 55\n",
      "Current state: 1\n",
      "Q-values for current state: [-0.00891751 -0.0061024  -0.009292   -0.00832613]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 0\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.008917509543437853\n",
      "New Q-value: -0.009863856600300433\n",
      "\n",
      "Step 56\n",
      "Current state: 0\n",
      "Q-values for current state: [-0.00882208 -0.01208029 -0.01239554 -0.01605238]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 8\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.01208029075867502\n",
      "New Q-value: -0.012485333342357519\n",
      "\n",
      "Step 57\n",
      "Current state: 8\n",
      "Q-values for current state: [-0.01220297 -0.00941628 -0.00645339 -0.01139365]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 8\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.012202968229698321\n",
      "New Q-value: -0.01259574306627849\n",
      "\n",
      "Step 58\n",
      "Current state: 8\n",
      "Q-values for current state: [-0.01259574 -0.00941628 -0.00645339 -0.01139365]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 16\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.009416281969518001\n",
      "New Q-value: -0.0099197848225662\n",
      "\n",
      "Step 59\n",
      "Current state: 16\n",
      "Q-values for current state: [-0.00745682 -0.00618448 -0.00468559 -0.00864265]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 17\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.00468559\n",
      "New Q-value: -0.005312031\n",
      "\n",
      "Step 60\n",
      "Current state: 17\n",
      "Q-values for current state: [-0.00489462 -0.0019     -0.001      -0.00458426]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 9\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.004584255000000001\n",
      "New Q-value: -0.005452534500000001\n",
      "\n",
      "Step 61\n",
      "Current state: 9\n",
      "Q-values for current state: [-0.0052331  -0.003439   -0.006538   -0.00698999]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 8\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.00523309890955\n",
      "New Q-value: -0.006322860678145\n",
      "\n",
      "Step 62\n",
      "Current state: 8\n",
      "Q-values for current state: [-0.01259574 -0.00991978 -0.00645339 -0.01139365]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 8\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.01259574306627849\n",
      "New Q-value: -0.01294924041920064\n",
      "\n",
      "Step 63\n",
      "Current state: 8\n",
      "Q-values for current state: [-0.01294924 -0.00991978 -0.00645339 -0.01139365]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 0\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.011393645280655583\n",
      "New Q-value: -0.012092378763796392\n",
      "\n",
      "Step 64\n",
      "Current state: 0\n",
      "Q-values for current state: [-0.00882208 -0.01248533 -0.01239554 -0.01605238]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 0\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.008822084328488064\n",
      "New Q-value: -0.009777973906845623\n",
      "\n",
      "Step 65\n",
      "Current state: 0\n",
      "Q-values for current state: [-0.00977797 -0.01248533 -0.01239554 -0.01605238]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 8\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.012485333342357519\n",
      "New Q-value: -0.012849871667671767\n",
      "\n",
      "Step 66\n",
      "Current state: 8\n",
      "Q-values for current state: [-0.01294924 -0.00991978 -0.00645339 -0.01209238]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 9\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.00645338589\n",
      "New Q-value: -0.007134752301\n",
      "\n",
      "Step 67\n",
      "Current state: 9\n",
      "Q-values for current state: [-0.00632286 -0.003439   -0.006538   -0.00698999]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 8\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.006322860678145\n",
      "New Q-value: -0.0073683760789255\n",
      "\n",
      "Step 68\n",
      "Current state: 8\n",
      "Q-values for current state: [-0.01294924 -0.00991978 -0.00713475 -0.01209238]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 0\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.012092378763796392\n",
      "New Q-value: -0.012812048408567086\n",
      "\n",
      "Step 69\n",
      "Current state: 0\n",
      "Q-values for current state: [-0.00977797 -0.01284987 -0.01239554 -0.01605238]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 0\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.016052379777700083\n",
      "New Q-value: -0.01637604932108041\n",
      "\n",
      "Step 70\n",
      "Current state: 0\n",
      "Q-values for current state: [-0.00977797 -0.01284987 -0.01239554 -0.01637605]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 1\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.0123955358682084\n",
      "New Q-value: -0.01273571042863756\n",
      "\n",
      "Step 71\n",
      "Current state: 1\n",
      "Q-values for current state: [-0.00986386 -0.0061024  -0.009292   -0.00832613]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 1\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.0083261343047125\n",
      "New Q-value: -0.00907324902149125\n",
      "\n",
      "Step 72\n",
      "Current state: 1\n",
      "Q-values for current state: [-0.00986386 -0.0061024  -0.009292   -0.00907325]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 9\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.00610240155\n",
      "New Q-value: -0.0068188663949999995\n",
      "\n",
      "Step 73\n",
      "Current state: 9\n",
      "Q-values for current state: [-0.00736838 -0.003439   -0.006538   -0.00698999]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 8\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.0073683760789255\n",
      "New Q-value: -0.00830933993962795\n",
      "\n",
      "Step 74\n",
      "Current state: 8\n",
      "Q-values for current state: [-0.01294924 -0.00991978 -0.00713475 -0.01281205]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 0\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.012812048408567086\n",
      "New Q-value: -0.013459751088860711\n",
      "\n",
      "Step 75\n",
      "Current state: 0\n",
      "Q-values for current state: [-0.00977797 -0.01284987 -0.01273571 -0.01637605]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 1\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.01273571042863756\n",
      "New Q-value: -0.013109931693298804\n",
      "\n",
      "Step 76\n",
      "Current state: 1\n",
      "Q-values for current state: [-0.00986386 -0.00681887 -0.009292   -0.00907325]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 0\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.009863856600300433\n",
      "New Q-value: -0.010806378461420725\n",
      "\n",
      "Step 77\n",
      "Current state: 0\n",
      "Q-values for current state: [-0.00977797 -0.01284987 -0.01310993 -0.01637605]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 1\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.013109931693298804\n",
      "New Q-value: -0.013446730831493923\n",
      "\n",
      "Step 78\n",
      "Current state: 1\n",
      "Q-values for current state: [-0.01080638 -0.00681887 -0.009292   -0.00907325]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 9\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.0068188663949999995\n",
      "New Q-value: -0.007463684755499999\n",
      "\n",
      "Step 79\n",
      "Current state: 9\n",
      "Q-values for current state: [-0.00830934 -0.003439   -0.006538   -0.00698999]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 8\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.00830933993962795\n",
      "New Q-value: -0.009156207414260155\n",
      "\n",
      "Step 80\n",
      "Current state: 8\n",
      "Q-values for current state: [-0.01294924 -0.00991978 -0.00713475 -0.01345975]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 8\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.01294924041920064\n",
      "New Q-value: -0.013332117845875575\n",
      "\n",
      "Step 81\n",
      "Current state: 8\n",
      "Q-values for current state: [-0.01333212 -0.00991978 -0.00713475 -0.01345975]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 8\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.013332117845875575\n",
      "New Q-value: -0.013676707529883017\n",
      "\n",
      "Step 82\n",
      "Current state: 8\n",
      "Q-values for current state: [-0.01367671 -0.00991978 -0.00713475 -0.01345975]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 16\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.0099197848225662\n",
      "New Q-value: -0.01043244928530958\n",
      "\n",
      "Step 83\n",
      "Current state: 16\n",
      "Q-values for current state: [-0.00745682 -0.00618448 -0.00531203 -0.00864265]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 24\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.0061844829\n",
      "New Q-value: -0.00682348461\n",
      "\n",
      "Step 84\n",
      "Current state: 24\n",
      "Q-values for current state: [-0.00690225 -0.00681978 -0.00271    -0.00519747]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 16\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.005197473825\n",
      "New Q-value: -0.0061823693875\n",
      "\n",
      "Step 85\n",
      "Current state: 16\n",
      "Q-values for current state: [-0.00745682 -0.00682348 -0.00531203 -0.00864265]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 17\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.005312031\n",
      "New Q-value: -0.0058758279\n",
      "\n",
      "Step 86\n",
      "Current state: 17\n",
      "Q-values for current state: [-0.00489462 -0.0019     -0.001      -0.00545253]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 16\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.00489462475\n",
      "New Q-value: -0.0059633659254999995\n",
      "\n",
      "Step 87\n",
      "Current state: 16\n",
      "Q-values for current state: [-0.00745682 -0.00682348 -0.00587583 -0.00864265]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 8\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.008642646298228002\n",
      "New Q-value: -0.009456183137000201\n",
      "\n",
      "Step 88\n",
      "Current state: 8\n",
      "Q-values for current state: [-0.01367671 -0.01043245 -0.00713475 -0.01345975]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 0\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.013459751088860711\n",
      "New Q-value: -0.014042683501124975\n",
      "\n",
      "Step 89\n",
      "Current state: 0\n",
      "Q-values for current state: [-0.00977797 -0.01284987 -0.01344673 -0.01637605]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 1\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.013446730831493923\n",
      "New Q-value: -0.013811107800117031\n",
      "\n",
      "Step 90\n",
      "Current state: 1\n",
      "Q-values for current state: [-0.01080638 -0.00746368 -0.009292   -0.00907325]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 9\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.007463684755499999\n",
      "New Q-value: -0.008044021279949999\n",
      "\n",
      "Step 91\n",
      "Current state: 9\n",
      "Q-values for current state: [-0.00915621 -0.003439   -0.006538   -0.00698999]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 17\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.003439\n",
      "New Q-value: -0.0041901\n",
      "\n",
      "Step 92\n",
      "Current state: 17\n",
      "Q-values for current state: [-0.00596337 -0.0019     -0.001      -0.00545253]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 25\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.0019000000000000002\n",
      "New Q-value: -0.00271\n",
      "\n",
      "Step 93\n",
      "Current state: 25\n",
      "Q-values for current state: [-0.00531402  0.         -0.001      -0.001     ]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 26\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.001\n",
      "New Q-value: -0.0019000000000000002\n",
      "\n",
      "Step 94\n",
      "Current state: 26\n",
      "Q-values for current state: [-0.0019  0.     -0.001  -0.001 ]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 18\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.001\n",
      "New Q-value: -0.0019000000000000002\n",
      "\n",
      "Step 95\n",
      "Current state: 18\n",
      "Q-values for current state: [-0.0019  -0.00271 -0.001    0.     ]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 10\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: -0.0011805000000000001\n",
      "\n",
      "Step 96\n",
      "Current state: 10\n",
      "Q-values for current state: [-0.00539021 -0.003439   -0.00439902 -0.0019    ]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 2\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.0019000000000000002\n",
      "New Q-value: -0.002899525\n",
      "\n",
      "Step 97\n",
      "Current state: 2\n",
      "Q-values for current state: [-0.00740917 -0.00570619 -0.0062782  -0.001995  ]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 1\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.007409172043410847\n",
      "New Q-value: -0.008432436860665013\n",
      "\n",
      "Step 98\n",
      "Current state: 1\n",
      "Q-values for current state: [-0.01080638 -0.00804402 -0.009292   -0.00907325]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 1\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.00907324902149125\n",
      "New Q-value: -0.009930106140937374\n",
      "\n",
      "Step 99\n",
      "Current state: 1\n",
      "Q-values for current state: [-0.01080638 -0.00804402 -0.009292   -0.00993011]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 1\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.009930106140937374\n",
      "New Q-value: -0.010701277548438887\n",
      "\n",
      "Episode 16\n",
      "Starting state: 0\n",
      "\n",
      "Step 0\n",
      "Current state: 0\n",
      "Q-values for current state: [-0.00977797 -0.01284987 -0.01381111 -0.01637605]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 1\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.013811107800117031\n",
      "New Q-value: -0.014194179041700578\n",
      "\n",
      "Step 1\n",
      "Current state: 1\n",
      "Q-values for current state: [-0.01080638 -0.00804402 -0.009292   -0.01070128]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 2\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.009291999600137659\n",
      "New Q-value: -0.009552324640123892\n",
      "\n",
      "Step 2\n",
      "Current state: 2\n",
      "Q-values for current state: [-0.00843244 -0.00570619 -0.0062782  -0.001995  ]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 3\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.00627819565\n",
      "New Q-value: -0.006986106085000001\n",
      "\n",
      "Step 3\n",
      "Current state: 3\n",
      "Q-values for current state: [-0.003534   -0.00420725 -0.00445605 -0.0039701 ]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 2\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.0035340000000000002\n",
      "New Q-value: -0.004370125\n",
      "\n",
      "Step 4\n",
      "Current state: 2\n",
      "Q-values for current state: [-0.00843244 -0.00570619 -0.00698611 -0.001995  ]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 3\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.006986106085000001\n",
      "New Q-value: -0.007664654964625001\n",
      "\n",
      "Step 5\n",
      "Current state: 3\n",
      "Q-values for current state: [-0.00437012 -0.00420725 -0.00445605 -0.0039701 ]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 3\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.003970099875000001\n",
      "New Q-value: -0.004950249375625001\n",
      "\n",
      "Step 6\n",
      "Current state: 3\n",
      "Q-values for current state: [-0.00437012 -0.00420725 -0.00445605 -0.00495025]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 2\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.004370125\n",
      "New Q-value: -0.0051226375\n",
      "\n",
      "Step 7\n",
      "Current state: 2\n",
      "Q-values for current state: [-0.00843244 -0.00570619 -0.00766465 -0.001995  ]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 1\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.008432436860665013\n",
      "New Q-value: -0.009353375196193761\n",
      "\n",
      "Step 8\n",
      "Current state: 1\n",
      "Q-values for current state: [-0.01080638 -0.00804402 -0.00955232 -0.01070128]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 0\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.010806378461420725\n",
      "New Q-value: -0.011654648136428986\n",
      "\n",
      "Step 9\n",
      "Current state: 0\n",
      "Q-values for current state: [-0.00977797 -0.01284987 -0.01419418 -0.01637605]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 0\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.009777973906845623\n",
      "New Q-value: -0.010729084037311395\n",
      "\n",
      "Step 10\n",
      "Current state: 0\n",
      "Q-values for current state: [-0.01072908 -0.01284987 -0.01419418 -0.01637605]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 8\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.012849871667671767\n",
      "New Q-value: -0.01324268596949959\n",
      "\n",
      "Step 11\n",
      "Current state: 8\n",
      "Q-values for current state: [-0.01367671 -0.01043245 -0.00713475 -0.01404268]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 8\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.013676707529883017\n",
      "New Q-value: -0.013986838245489715\n",
      "\n",
      "Step 12\n",
      "Current state: 8\n",
      "Q-values for current state: [-0.01398684 -0.01043245 -0.00713475 -0.01404268]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 8\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.013986838245489715\n",
      "New Q-value: -0.014265955889535744\n",
      "\n",
      "Step 13\n",
      "Current state: 8\n",
      "Q-values for current state: [-0.01426596 -0.01043245 -0.00713475 -0.01404268]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 16\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.01043244928530958\n",
      "New Q-value: -0.010947408007278621\n",
      "\n",
      "Step 14\n",
      "Current state: 16\n",
      "Q-values for current state: [-0.00745682 -0.00682348 -0.00587583 -0.00945618]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 17\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.0058758279\n",
      "New Q-value: -0.00638324511\n",
      "\n",
      "Step 15\n",
      "Current state: 17\n",
      "Q-values for current state: [-0.00596337 -0.00271    -0.001      -0.00545253]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 9\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.005452534500000001\n",
      "New Q-value: -0.006305340550000001\n",
      "\n",
      "Step 16\n",
      "Current state: 9\n",
      "Q-values for current state: [-0.00915621 -0.0041901  -0.006538   -0.00698999]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 10\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.006538000109999999\n",
      "New Q-value: -0.007159654973999999\n",
      "\n",
      "Step 17\n",
      "Current state: 10\n",
      "Q-values for current state: [-0.00539021 -0.003439   -0.00439902 -0.00289953]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 18\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.003439\n",
      "New Q-value: -0.0041901\n",
      "\n",
      "Step 18\n",
      "Current state: 18\n",
      "Q-values for current state: [-0.0019    -0.00271   -0.001     -0.0011805]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 17\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.0019000000000000002\n",
      "New Q-value: -0.002805\n",
      "\n",
      "Step 19\n",
      "Current state: 17\n",
      "Q-values for current state: [-0.00596337 -0.00271    -0.001      -0.00630534]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 18\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.001\n",
      "New Q-value: -0.0019950000000000002\n",
      "\n",
      "Step 20\n",
      "Current state: 18\n",
      "Q-values for current state: [-0.002805  -0.00271   -0.001     -0.0011805]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 19\n",
      "Reward: -0.01\n",
      "Terminated: True\n",
      "Old Q-value: -0.001\n",
      "New Q-value: -0.0019000000000000002\n",
      "\n",
      "Episode 17\n",
      "Starting state: 0\n",
      "\n",
      "Step 0\n",
      "Current state: 0\n",
      "Q-values for current state: [-0.01072908 -0.01324269 -0.01419418 -0.01637605]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 1\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.014194179041700578\n",
      "New Q-value: -0.01453894315912577\n",
      "\n",
      "Step 1\n",
      "Current state: 1\n",
      "Q-values for current state: [-0.01165465 -0.00804402 -0.00955232 -0.01070128]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 0\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.011654648136428986\n",
      "New Q-value: -0.01250844630633067\n",
      "\n",
      "Step 2\n",
      "Current state: 0\n",
      "Q-values for current state: [-0.01072908 -0.01324269 -0.01453894 -0.01637605]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 1\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.01453894315912577\n",
      "New Q-value: -0.014849230864808443\n",
      "\n",
      "Step 3\n",
      "Current state: 1\n",
      "Q-values for current state: [-0.01250845 -0.00804402 -0.00955232 -0.01070128]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 2\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.009552324640123892\n",
      "New Q-value: -0.009786617176111503\n",
      "\n",
      "Step 4\n",
      "Current state: 2\n",
      "Q-values for current state: [-0.00935338 -0.00570619 -0.00766465 -0.001995  ]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 2\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.0019950000000000002\n",
      "New Q-value: -0.0029850250000000005\n",
      "\n",
      "Step 5\n",
      "Current state: 2\n",
      "Q-values for current state: [-0.00935338 -0.00570619 -0.00766465 -0.00298503]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 2\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.0029850250000000005\n",
      "New Q-value: -0.003970099875000001\n",
      "\n",
      "Step 6\n",
      "Current state: 2\n",
      "Q-values for current state: [-0.00935338 -0.00570619 -0.00766465 -0.0039701 ]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 3\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.007664654964625001\n",
      "New Q-value: -0.008297877980662501\n",
      "\n",
      "Step 7\n",
      "Current state: 3\n",
      "Q-values for current state: [-0.00512264 -0.00420725 -0.00445605 -0.00495025]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 11\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.0042072475000000005\n",
      "New Q-value: -0.004898670250000001\n",
      "\n",
      "Step 8\n",
      "Current state: 11\n",
      "Q-values for current state: [-0.0011805  -0.00468559 -0.00371403 -0.00289953]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 10\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.0011805000000000001\n",
      "New Q-value: -0.0023379048750000004\n",
      "\n",
      "Step 9\n",
      "Current state: 10\n",
      "Q-values for current state: [-0.00539021 -0.0041901  -0.00439902 -0.00289953]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 18\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.0041901\n",
      "New Q-value: -0.0048832375\n",
      "\n",
      "Step 10\n",
      "Current state: 18\n",
      "Q-values for current state: [-0.002805  -0.00271   -0.0019    -0.0011805]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 26\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.00271\n",
      "New Q-value: -0.003439\n",
      "\n",
      "Step 11\n",
      "Current state: 26\n",
      "Q-values for current state: [-0.0019  0.     -0.001  -0.0019]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 27\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.001\n",
      "New Q-value: -0.0019000000000000002\n",
      "\n",
      "Step 12\n",
      "Current state: 27\n",
      "Q-values for current state: [ 0.    -0.001  0.     0.   ]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 19\n",
      "Reward: -0.01\n",
      "Terminated: True\n",
      "Old Q-value: 0.0\n",
      "New Q-value: -0.001\n",
      "\n",
      "Episode 18\n",
      "Starting state: 0\n",
      "\n",
      "Step 0\n",
      "Current state: 0\n",
      "Q-values for current state: [-0.01072908 -0.01324269 -0.01484923 -0.01637605]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 0\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.01637604932108041\n",
      "New Q-value: -0.01675770737251695\n",
      "\n",
      "Step 1\n",
      "Current state: 0\n",
      "Q-values for current state: [-0.01072908 -0.01324269 -0.01484923 -0.01675771]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 1\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.014849230864808443\n",
      "New Q-value: -0.015128489799922849\n",
      "\n",
      "Step 2\n",
      "Current state: 1\n",
      "Q-values for current state: [-0.01250845 -0.00804402 -0.00978662 -0.01070128]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 9\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.008044021279949999\n",
      "New Q-value: -0.008637678651955\n",
      "\n",
      "Step 3\n",
      "Current state: 9\n",
      "Q-values for current state: [-0.00915621 -0.0041901  -0.00715965 -0.00698999]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 17\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.0041901\n",
      "New Q-value: -0.004960615000000001\n",
      "\n",
      "Step 4\n",
      "Current state: 17\n",
      "Q-values for current state: [-0.00596337 -0.00271    -0.001995   -0.00630534]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 9\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.006305340550000001\n",
      "New Q-value: -0.007146064920000001\n",
      "\n",
      "Step 5\n",
      "Current state: 9\n",
      "Q-values for current state: [-0.00915621 -0.00496062 -0.00715965 -0.00698999]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 8\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.009156207414260155\n",
      "New Q-value: -0.00991838814142914\n",
      "\n",
      "Step 6\n",
      "Current state: 8\n",
      "Q-values for current state: [-0.01426596 -0.01094741 -0.00713475 -0.01404268]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 9\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.007134752301\n",
      "New Q-value: -0.0078925354959\n",
      "\n",
      "Step 7\n",
      "Current state: 9\n",
      "Q-values for current state: [-0.00991839 -0.00496062 -0.00715965 -0.00698999]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 8\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.00991838814142914\n",
      "New Q-value: -0.010676340199396726\n",
      "\n",
      "Step 8\n",
      "Current state: 8\n",
      "Q-values for current state: [-0.01426596 -0.01094741 -0.00789254 -0.01404268]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 9\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.0078925354959\n",
      "New Q-value: -0.00857454037131\n",
      "\n",
      "Step 9\n",
      "Current state: 9\n",
      "Q-values for current state: [-0.01067634 -0.00496062 -0.00715965 -0.00698999]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 10\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.007159654973999999\n",
      "New Q-value: -0.0077191443516\n",
      "\n",
      "Step 10\n",
      "Current state: 10\n",
      "Q-values for current state: [-0.00539021 -0.00488324 -0.00439902 -0.00289953]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 11\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.004399019725\n",
      "New Q-value: -0.005181218715625\n",
      "\n",
      "Step 11\n",
      "Current state: 11\n",
      "Q-values for current state: [-0.0023379  -0.00468559 -0.00371403 -0.00289953]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 12\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.003714025\n",
      "New Q-value: -0.0046000725\n",
      "\n",
      "Step 12\n",
      "Current state: 12\n",
      "Q-values for current state: [-0.00365208 -0.00271    -0.00517475 -0.002994  ]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 20\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.00271\n",
      "New Q-value: -0.003439\n",
      "\n",
      "Step 13\n",
      "Current state: 20\n",
      "Q-values for current state: [-0.001       0.          0.         -0.00305295]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 12\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.00305295\n",
      "New Q-value: -0.004032085463125\n",
      "\n",
      "Step 14\n",
      "Current state: 12\n",
      "Q-values for current state: [-0.00365208 -0.003439   -0.00517475 -0.002994  ]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 11\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.00365208025\n",
      "New Q-value: -0.004508973188125\n",
      "\n",
      "Step 15\n",
      "Current state: 11\n",
      "Q-values for current state: [-0.0023379  -0.00468559 -0.00460007 -0.00289953]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 12\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.0046000725\n",
      "New Q-value: -0.005424495713125\n",
      "\n",
      "Step 16\n",
      "Current state: 12\n",
      "Q-values for current state: [-0.00450897 -0.003439   -0.00517475 -0.002994  ]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 13\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.0051747450000000006\n",
      "New Q-value: -0.005914720500000001\n",
      "\n",
      "Step 17\n",
      "Current state: 13\n",
      "Q-values for current state: [-0.002805   -0.003439   -0.00271    -0.00510682]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 5\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.00510682\n",
      "New Q-value: -0.005853588\n",
      "\n",
      "Step 18\n",
      "Current state: 5\n",
      "Q-values for current state: [-0.002994  -0.0036195 -0.00271   -0.0044552]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 6\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.00271\n",
      "New Q-value: -0.0035340000000000002\n",
      "\n",
      "Step 19\n",
      "Current state: 6\n",
      "Q-values for current state: [-0.0019    -0.0019    -0.001     -0.0028905]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 14\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.0019000000000000002\n",
      "New Q-value: -0.00271\n",
      "\n",
      "Step 20\n",
      "Current state: 14\n",
      "Q-values for current state: [-0.004515 -0.0019   -0.001     0.      ]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 15\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.001\n",
      "New Q-value: -0.0019000000000000002\n",
      "\n",
      "Step 21\n",
      "Current state: 15\n",
      "Q-values for current state: [-0.001     0.       -0.001    -0.003439]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 15\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.001\n",
      "New Q-value: -0.0019000000000000002\n",
      "\n",
      "Step 22\n",
      "Current state: 15\n",
      "Q-values for current state: [-0.001     0.       -0.0019   -0.003439]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 23\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: -0.001\n",
      "\n",
      "Step 23\n",
      "Current state: 23\n",
      "Q-values for current state: [ 0.     0.     0.    -0.001]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 15\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.001\n",
      "New Q-value: -0.0019950000000000002\n",
      "\n",
      "Step 24\n",
      "Current state: 15\n",
      "Q-values for current state: [-0.001    -0.001    -0.0019   -0.003439]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 14\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.001\n",
      "New Q-value: -0.0019000000000000002\n",
      "\n",
      "Step 25\n",
      "Current state: 14\n",
      "Q-values for current state: [-0.004515 -0.0019   -0.0019    0.      ]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 22\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.0019000000000000002\n",
      "New Q-value: -0.002805\n",
      "\n",
      "Step 26\n",
      "Current state: 22\n",
      "Q-values for current state: [-0.0019   -0.003439 -0.001    -0.0019  ]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 14\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.0019000000000000002\n",
      "New Q-value: -0.00271\n",
      "\n",
      "Step 27\n",
      "Current state: 14\n",
      "Q-values for current state: [-0.004515 -0.002805 -0.0019    0.      ]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 22\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.002805\n",
      "New Q-value: -0.0036195000000000003\n",
      "\n",
      "Step 28\n",
      "Current state: 22\n",
      "Q-values for current state: [-0.0019   -0.003439 -0.001    -0.00271 ]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 14\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.00271\n",
      "New Q-value: -0.003439\n",
      "\n",
      "Step 29\n",
      "Current state: 14\n",
      "Q-values for current state: [-0.004515  -0.0036195 -0.0019     0.       ]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 15\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.0019000000000000002\n",
      "New Q-value: -0.002805\n",
      "\n",
      "Step 30\n",
      "Current state: 15\n",
      "Q-values for current state: [-0.0019   -0.001    -0.0019   -0.003439]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 14\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.0019000000000000002\n",
      "New Q-value: -0.00271\n",
      "\n",
      "Step 31\n",
      "Current state: 14\n",
      "Q-values for current state: [-0.004515  -0.0036195 -0.002805   0.       ]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 22\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.0036195000000000003\n",
      "New Q-value: -0.00435255\n",
      "\n",
      "Step 32\n",
      "Current state: 22\n",
      "Q-values for current state: [-0.0019   -0.003439 -0.001    -0.003439]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 30\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.003439\n",
      "New Q-value: -0.0040951\n",
      "\n",
      "Step 33\n",
      "Current state: 30\n",
      "Q-values for current state: [ 0.         0.        -0.001     -0.0036195]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 38\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: -0.001\n",
      "\n",
      "Step 34\n",
      "Current state: 38\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 30\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: -0.001\n",
      "\n",
      "Step 35\n",
      "Current state: 30\n",
      "Q-values for current state: [ 0.        -0.001     -0.001     -0.0036195]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 38\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.001\n",
      "New Q-value: -0.0019000000000000002\n",
      "\n",
      "Step 36\n",
      "Current state: 38\n",
      "Q-values for current state: [ 0.     0.     0.    -0.001]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 39\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: -0.001\n",
      "\n",
      "Step 37\n",
      "Current state: 39\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 39\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: -0.001\n",
      "\n",
      "Step 38\n",
      "Current state: 39\n",
      "Q-values for current state: [ 0.     0.    -0.001  0.   ]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 39\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.001\n",
      "New Q-value: -0.0019000000000000002\n",
      "\n",
      "Step 39\n",
      "Current state: 39\n",
      "Q-values for current state: [ 0.      0.     -0.0019  0.    ]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 31\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: -0.001\n",
      "\n",
      "Step 40\n",
      "Current state: 31\n",
      "Q-values for current state: [-0.001  0.     0.     0.   ]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 30\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.001\n",
      "New Q-value: -0.0019000000000000002\n",
      "\n",
      "Step 41\n",
      "Current state: 30\n",
      "Q-values for current state: [ 0.        -0.0019    -0.001     -0.0036195]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 22\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.0036195000000000003\n",
      "New Q-value: -0.00435255\n",
      "\n",
      "Step 42\n",
      "Current state: 22\n",
      "Q-values for current state: [-0.0019    -0.0040951 -0.001     -0.003439 ]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 23\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.001\n",
      "New Q-value: -0.0019000000000000002\n",
      "\n",
      "Step 43\n",
      "Current state: 23\n",
      "Q-values for current state: [ 0.        0.        0.       -0.001995]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 23\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: -0.001\n",
      "\n",
      "Step 44\n",
      "Current state: 23\n",
      "Q-values for current state: [ 0.        0.       -0.001    -0.001995]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 15\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.0019950000000000002\n",
      "New Q-value: -0.0028905000000000003\n",
      "\n",
      "Step 45\n",
      "Current state: 15\n",
      "Q-values for current state: [-0.00271  -0.001    -0.0019   -0.003439]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 23\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.001\n",
      "New Q-value: -0.0019000000000000002\n",
      "\n",
      "Step 46\n",
      "Current state: 23\n",
      "Q-values for current state: [ 0.         0.        -0.001     -0.0028905]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 31\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: -0.001\n",
      "\n",
      "Step 47\n",
      "Current state: 31\n",
      "Q-values for current state: [-0.0019  0.      0.      0.    ]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 23\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: -0.001\n",
      "\n",
      "Step 48\n",
      "Current state: 23\n",
      "Q-values for current state: [ 0.        -0.001     -0.001     -0.0028905]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 31\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.001\n",
      "New Q-value: -0.0019000000000000002\n",
      "\n",
      "Step 49\n",
      "Current state: 31\n",
      "Q-values for current state: [-0.0019  0.      0.     -0.001 ]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 39\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: -0.001\n",
      "\n",
      "Step 50\n",
      "Current state: 39\n",
      "Q-values for current state: [ 0.      0.     -0.0019 -0.001 ]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 47\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: -0.001\n",
      "\n",
      "Step 51\n",
      "Current state: 47\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 47\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: -0.001\n",
      "\n",
      "Step 52\n",
      "Current state: 47\n",
      "Q-values for current state: [ 0.     0.    -0.001  0.   ]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 55\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: -0.001\n",
      "\n",
      "Step 53\n",
      "Current state: 55\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 47\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: -0.001\n",
      "\n",
      "Step 54\n",
      "Current state: 47\n",
      "Q-values for current state: [ 0.    -0.001 -0.001  0.   ]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 47\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.001\n",
      "New Q-value: -0.0019000000000000002\n",
      "\n",
      "Step 55\n",
      "Current state: 47\n",
      "Q-values for current state: [ 0.     -0.001  -0.0019  0.    ]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 39\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: -0.001\n",
      "\n",
      "Step 56\n",
      "Current state: 39\n",
      "Q-values for current state: [ 0.     -0.001  -0.0019 -0.001 ]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 39\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.0019000000000000002\n",
      "New Q-value: -0.00271\n",
      "\n",
      "Step 57\n",
      "Current state: 39\n",
      "Q-values for current state: [ 0.      -0.001   -0.00271 -0.001  ]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 38\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: -0.001\n",
      "\n",
      "Step 58\n",
      "Current state: 38\n",
      "Q-values for current state: [ 0.     0.    -0.001 -0.001]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 39\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.001\n",
      "New Q-value: -0.0019950000000000002\n",
      "\n",
      "Step 59\n",
      "Current state: 39\n",
      "Q-values for current state: [-0.001   -0.001   -0.00271 -0.001  ]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 47\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.001\n",
      "New Q-value: -0.0019000000000000002\n",
      "\n",
      "Step 60\n",
      "Current state: 47\n",
      "Q-values for current state: [ 0.     -0.001  -0.0019 -0.001 ]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 47\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.0019000000000000002\n",
      "New Q-value: -0.00271\n",
      "\n",
      "Step 61\n",
      "Current state: 47\n",
      "Q-values for current state: [ 0.      -0.001   -0.00271 -0.001  ]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 55\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.001\n",
      "New Q-value: -0.0019000000000000002\n",
      "\n",
      "Step 62\n",
      "Current state: 55\n",
      "Q-values for current state: [ 0.     0.     0.    -0.001]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 47\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.001\n",
      "New Q-value: -0.0019000000000000002\n",
      "\n",
      "Step 63\n",
      "Current state: 47\n",
      "Q-values for current state: [ 0.      -0.0019  -0.00271 -0.001  ]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 46\n",
      "Reward: -0.01\n",
      "Terminated: True\n",
      "Old Q-value: 0.0\n",
      "New Q-value: -0.001\n",
      "\n",
      "Episode 19\n",
      "Starting state: 0\n",
      "\n",
      "Step 0\n",
      "Current state: 0\n",
      "Q-values for current state: [-0.01072908 -0.01324269 -0.01512849 -0.01675771]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 0\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.010729084037311395\n",
      "New Q-value: -0.011675438617124839\n",
      "\n",
      "Step 1\n",
      "Current state: 0\n",
      "Q-values for current state: [-0.01167544 -0.01324269 -0.01512849 -0.01675771]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 1\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.015128489799922849\n",
      "New Q-value: -0.015436220291866289\n",
      "\n",
      "Step 2\n",
      "Current state: 1\n",
      "Q-values for current state: [-0.01250845 -0.00863768 -0.00978662 -0.01070128]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 1\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.010701277548438887\n",
      "New Q-value: -0.011451729265530723\n",
      "\n",
      "Step 3\n",
      "Current state: 1\n",
      "Q-values for current state: [-0.01250845 -0.00863768 -0.00978662 -0.01145173]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 9\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.008637678651955\n",
      "New Q-value: -0.0092451692117595\n",
      "\n",
      "Step 4\n",
      "Current state: 9\n",
      "Q-values for current state: [-0.01067634 -0.00496062 -0.00771914 -0.00698999]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 8\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.010676340199396726\n",
      "New Q-value: -0.011423287514731503\n",
      "\n",
      "Step 5\n",
      "Current state: 8\n",
      "Q-values for current state: [-0.01426596 -0.01094741 -0.00857454 -0.01404268]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 8\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.014265955889535744\n",
      "New Q-value: -0.01465394163585662\n",
      "\n",
      "Step 6\n",
      "Current state: 8\n",
      "Q-values for current state: [-0.01465394 -0.01094741 -0.00857454 -0.01404268]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 16\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.010947408007278621\n",
      "New Q-value: -0.011459075492000759\n",
      "\n",
      "Step 7\n",
      "Current state: 16\n",
      "Q-values for current state: [-0.00745682 -0.00682348 -0.00638325 -0.00945618]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 17\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.00638324511\n",
      "New Q-value: -0.006934445599\n",
      "\n",
      "Step 8\n",
      "Current state: 17\n",
      "Q-values for current state: [-0.00596337 -0.00271    -0.001995   -0.00714606]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 18\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.0019950000000000002\n",
      "New Q-value: -0.0029076475000000004\n",
      "\n",
      "Step 9\n",
      "Current state: 18\n",
      "Q-values for current state: [-0.002805  -0.003439  -0.0019    -0.0011805]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 17\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.002805\n",
      "New Q-value: -0.00378195\n",
      "\n",
      "Step 10\n",
      "Current state: 17\n",
      "Q-values for current state: [-0.00596337 -0.00271    -0.00290765 -0.00714606]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 18\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.0029076475000000004\n",
      "New Q-value: -0.0037290302500000002\n",
      "\n",
      "Step 11\n",
      "Current state: 18\n",
      "Q-values for current state: [-0.00378195 -0.003439   -0.0019     -0.0011805 ]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 26\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.003439\n",
      "New Q-value: -0.0040951\n",
      "\n",
      "Step 12\n",
      "Current state: 26\n",
      "Q-values for current state: [-0.0019  0.     -0.0019 -0.0019]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 25\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.0019000000000000002\n",
      "New Q-value: -0.00271\n",
      "\n",
      "Step 13\n",
      "Current state: 25\n",
      "Q-values for current state: [-0.00531402  0.         -0.0019     -0.001     ]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 26\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.0019000000000000002\n",
      "New Q-value: -0.00271\n",
      "\n",
      "Step 14\n",
      "Current state: 26\n",
      "Q-values for current state: [-0.00271  0.      -0.0019  -0.0019 ]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 34\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: -0.001\n",
      "\n",
      "Step 15\n",
      "Current state: 34\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 33\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: -0.001\n",
      "\n",
      "Step 16\n",
      "Current state: 33\n",
      "Q-values for current state: [-0.001095 -0.001     0.       -0.001   ]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 34\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: -0.001\n",
      "\n",
      "Step 17\n",
      "Current state: 34\n",
      "Q-values for current state: [-0.001  0.     0.     0.   ]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 42\n",
      "Reward: -0.01\n",
      "Terminated: True\n",
      "Old Q-value: 0.0\n",
      "New Q-value: -0.001\n",
      "\n",
      "Episode 20\n",
      "Starting state: 0\n",
      "\n",
      "Step 0\n",
      "Current state: 0\n",
      "Q-values for current state: [-0.01167544 -0.01324269 -0.01543622 -0.01675771]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 8\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.01324268596949959\n",
      "New Q-value: -0.013732998707824082\n",
      "\n",
      "Step 1\n",
      "Current state: 8\n",
      "Q-values for current state: [-0.01465394 -0.01145908 -0.00857454 -0.01404268]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 16\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.011459075492000759\n",
      "New Q-value: -0.011961398980750684\n",
      "\n",
      "Step 2\n",
      "Current state: 16\n",
      "Q-values for current state: [-0.00745682 -0.00682348 -0.00693445 -0.00945618]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 16\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.0074568155798625\n",
      "New Q-value: -0.00835936505982625\n",
      "\n",
      "Step 3\n",
      "Current state: 16\n",
      "Q-values for current state: [-0.00835937 -0.00682348 -0.00693445 -0.00945618]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 24\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.00682348461\n",
      "New Q-value: -0.007398586148999999\n",
      "\n",
      "Step 4\n",
      "Current state: 24\n",
      "Q-values for current state: [-0.00690225 -0.00681978 -0.00271    -0.00618237]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 32\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.00681977866\n",
      "New Q-value: -0.007395250794\n",
      "\n",
      "Step 5\n",
      "Current state: 32\n",
      "Q-values for current state: [-0.00583364 -0.00494304 -0.00271    -0.0028905 ]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 32\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.005833637050000001\n",
      "New Q-value: -0.006507723345000001\n",
      "\n",
      "Step 6\n",
      "Current state: 32\n",
      "Q-values for current state: [-0.00650772 -0.00494304 -0.00271    -0.0028905 ]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 33\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.00271\n",
      "New Q-value: -0.0035340000000000002\n",
      "\n",
      "Step 7\n",
      "Current state: 33\n",
      "Q-values for current state: [-0.001095 -0.001    -0.001    -0.001   ]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 41\n",
      "Reward: -0.01\n",
      "Terminated: True\n",
      "Old Q-value: -0.001\n",
      "New Q-value: -0.0019000000000000002\n",
      "\n",
      "Episode 20 summary:\n",
      "Total reward: -0.08\n",
      "Successful episodes so far: []\n",
      "Average reward over last 10 episodes: -0.301\n",
      "\n",
      "Episode 21\n",
      "Starting state: 0\n",
      "\n",
      "Step 0\n",
      "Current state: 0\n",
      "Q-values for current state: [-0.01167544 -0.013733   -0.01543622 -0.01675771]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 0\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.011675438617124839\n",
      "New Q-value: -0.012617061424039214\n",
      "\n",
      "Step 1\n",
      "Current state: 0\n",
      "Q-values for current state: [-0.01261706 -0.013733   -0.01543622 -0.01675771]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 8\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.013732998707824082\n",
      "New Q-value: -0.014174280172316124\n",
      "\n",
      "Step 2\n",
      "Current state: 8\n",
      "Q-values for current state: [-0.01465394 -0.0119614  -0.00857454 -0.01404268]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 16\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.011961398980750684\n",
      "New Q-value: -0.012424031414580615\n",
      "\n",
      "Step 3\n",
      "Current state: 16\n",
      "Q-values for current state: [-0.00835937 -0.00739859 -0.00693445 -0.00945618]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 17\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.006934445599\n",
      "New Q-value: -0.0074984510391\n",
      "\n",
      "Step 4\n",
      "Current state: 17\n",
      "Q-values for current state: [-0.00596337 -0.00271    -0.00372903 -0.00714606]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 16\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.0059633659254999995\n",
      "New Q-value: -0.007069895017104999\n",
      "\n",
      "Step 5\n",
      "Current state: 16\n",
      "Q-values for current state: [-0.00835937 -0.00739859 -0.00749845 -0.00945618]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 17\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.0074984510391\n",
      "New Q-value: -0.00800605593519\n",
      "\n",
      "Step 6\n",
      "Current state: 17\n",
      "Q-values for current state: [-0.0070699  -0.00271    -0.00372903 -0.00714606]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 18\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.0037290302500000002\n",
      "New Q-value: -0.0044682747250000005\n",
      "\n",
      "Step 7\n",
      "Current state: 18\n",
      "Q-values for current state: [-0.00378195 -0.0040951  -0.0019     -0.0011805 ]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 10\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.0011805000000000001\n",
      "New Q-value: -0.0023379048750000004\n",
      "\n",
      "Step 8\n",
      "Current state: 10\n",
      "Q-values for current state: [-0.00539021 -0.00488324 -0.00518122 -0.00289953]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 2\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.002899525\n",
      "New Q-value: -0.0039867319881250005\n",
      "\n",
      "Step 9\n",
      "Current state: 2\n",
      "Q-values for current state: [-0.00935338 -0.00570619 -0.00829788 -0.0039701 ]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 3\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.008297877980662501\n",
      "New Q-value: -0.00889141539572125\n",
      "\n",
      "Step 10\n",
      "Current state: 3\n",
      "Q-values for current state: [-0.00512264 -0.00489867 -0.00445605 -0.00495025]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 3\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.004950249375625001\n",
      "New Q-value: -0.005878549651187501\n",
      "\n",
      "Step 11\n",
      "Current state: 3\n",
      "Q-values for current state: [-0.00512264 -0.00489867 -0.00445605 -0.00587855]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 2\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.0051226375\n",
      "New Q-value: -0.005987533238125\n",
      "\n",
      "Step 12\n",
      "Current state: 2\n",
      "Q-values for current state: [-0.00935338 -0.00570619 -0.00889142 -0.0039701 ]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 3\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.00889141539572125\n",
      "New Q-value: -0.009425599069274126\n",
      "\n",
      "Step 13\n",
      "Current state: 3\n",
      "Q-values for current state: [-0.00598753 -0.00489867 -0.00445605 -0.00587855]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 2\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.005987533238125\n",
      "New Q-value: -0.0067659394024375\n",
      "\n",
      "Step 14\n",
      "Current state: 2\n",
      "Q-values for current state: [-0.00935338 -0.00570619 -0.0094256  -0.0039701 ]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 10\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.005706186\n",
      "New Q-value: -0.006514306938871875\n",
      "\n",
      "Step 15\n",
      "Current state: 10\n",
      "Q-values for current state: [-0.00539021 -0.00488324 -0.00518122 -0.00398673]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 18\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.0048832375\n",
      "New Q-value: -0.00557541375\n",
      "\n",
      "Step 16\n",
      "Current state: 18\n",
      "Q-values for current state: [-0.00378195 -0.0040951  -0.0019     -0.0023379 ]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 26\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.0040951\n",
      "New Q-value: -0.0047805899999999995\n",
      "\n",
      "Step 17\n",
      "Current state: 26\n",
      "Q-values for current state: [-0.00271 -0.001   -0.0019  -0.0019 ]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 18\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.0019000000000000002\n",
      "New Q-value: -0.0028905\n",
      "\n",
      "Step 18\n",
      "Current state: 18\n",
      "Q-values for current state: [-0.00378195 -0.00478059 -0.0019     -0.0023379 ]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 17\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.00378195\n",
      "New Q-value: -0.004661205\n",
      "\n",
      "Step 19\n",
      "Current state: 17\n",
      "Q-values for current state: [-0.0070699  -0.00271    -0.00446827 -0.00714606]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 25\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.00271\n",
      "New Q-value: -0.003439\n",
      "\n",
      "Step 20\n",
      "Current state: 25\n",
      "Q-values for current state: [-0.00531402  0.         -0.00271    -0.001     ]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 17\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.001\n",
      "New Q-value: -0.0022267050000000003\n",
      "\n",
      "Step 21\n",
      "Current state: 17\n",
      "Q-values for current state: [-0.0070699  -0.003439   -0.00446827 -0.00714606]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 18\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.0044682747250000005\n",
      "New Q-value: -0.0052019472525\n",
      "\n",
      "Step 22\n",
      "Current state: 18\n",
      "Q-values for current state: [-0.0046612  -0.00478059 -0.0019     -0.0023379 ]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 19\n",
      "Reward: -0.01\n",
      "Terminated: True\n",
      "Old Q-value: -0.0019000000000000002\n",
      "New Q-value: -0.00271\n",
      "\n",
      "Episode 22\n",
      "Starting state: 0\n",
      "\n",
      "Step 0\n",
      "Current state: 0\n",
      "Q-values for current state: [-0.01261706 -0.01417428 -0.01543622 -0.01675771]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 0\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.01675770737251695\n",
      "New Q-value: -0.01728055747054898\n",
      "\n",
      "Step 1\n",
      "Current state: 0\n",
      "Q-values for current state: [-0.01261706 -0.01417428 -0.01543622 -0.01728056]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 1\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.015436220291866289\n",
      "New Q-value: -0.01577088933779681\n",
      "\n",
      "Step 2\n",
      "Current state: 1\n",
      "Q-values for current state: [-0.01250845 -0.00924517 -0.00978662 -0.01145173]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 0\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.01250844630633067\n",
      "New Q-value: -0.013456222510981329\n",
      "\n",
      "Step 3\n",
      "Current state: 0\n",
      "Q-values for current state: [-0.01261706 -0.01417428 -0.01577089 -0.01728056]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 0\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.01728055747054898\n",
      "New Q-value: -0.017751122558777806\n",
      "\n",
      "Step 4\n",
      "Current state: 0\n",
      "Q-values for current state: [-0.01261706 -0.01417428 -0.01577089 -0.01775112]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 1\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.01577088933779681\n",
      "New Q-value: -0.01607209147913428\n",
      "\n",
      "Step 5\n",
      "Current state: 1\n",
      "Q-values for current state: [-0.01345622 -0.00924517 -0.00978662 -0.01145173]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 2\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.009786617176111503\n",
      "New Q-value: -0.010185114946625353\n",
      "\n",
      "Step 6\n",
      "Current state: 2\n",
      "Q-values for current state: [-0.00935338 -0.00651431 -0.0094256  -0.0039701 ]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 3\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.009425599069274126\n",
      "New Q-value: -0.009906364375471714\n",
      "\n",
      "Step 7\n",
      "Current state: 3\n",
      "Q-values for current state: [-0.00676594 -0.00489867 -0.00445605 -0.00587855]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 2\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.0067659394024375\n",
      "New Q-value: -0.0074665049503187505\n",
      "\n",
      "Step 8\n",
      "Current state: 2\n",
      "Q-values for current state: [-0.00935338 -0.00651431 -0.00990636 -0.0039701 ]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 3\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.009906364375471714\n",
      "New Q-value: -0.010339053151049542\n",
      "\n",
      "Step 9\n",
      "Current state: 3\n",
      "Q-values for current state: [-0.0074665  -0.00489867 -0.00445605 -0.00587855]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 11\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.004898670250000001\n",
      "New Q-value: -0.005630904188125001\n",
      "\n",
      "Step 10\n",
      "Current state: 11\n",
      "Q-values for current state: [-0.0023379  -0.00468559 -0.0054245  -0.00289953]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 3\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.002899525\n",
      "New Q-value: -0.004032897713125001\n",
      "\n",
      "Step 11\n",
      "Current state: 3\n",
      "Q-values for current state: [-0.0074665  -0.0056309  -0.00445605 -0.00587855]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 3\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.005878549651187501\n",
      "New Q-value: -0.006714019899193751\n",
      "\n",
      "Step 12\n",
      "Current state: 3\n",
      "Q-values for current state: [-0.0074665  -0.0056309  -0.00445605 -0.00671402]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 4\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.0044560548750000005\n",
      "New Q-value: -0.005208954262500001\n",
      "\n",
      "Step 13\n",
      "Current state: 4\n",
      "Q-values for current state: [-0.00271    -0.00494304 -0.00208953 -0.00379995]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 4\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.0037999548750000003\n",
      "New Q-value: -0.0046184642625\n",
      "\n",
      "Step 14\n",
      "Current state: 4\n",
      "Q-values for current state: [-0.00271    -0.00494304 -0.00208953 -0.00461846]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 3\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.00271\n",
      "New Q-value: -0.0039338506549375\n",
      "\n",
      "Step 15\n",
      "Current state: 3\n",
      "Q-values for current state: [-0.0074665  -0.0056309  -0.00520895 -0.00671402]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 11\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.005630904188125001\n",
      "New Q-value: -0.006289914732437501\n",
      "\n",
      "Step 16\n",
      "Current state: 11\n",
      "Q-values for current state: [-0.0023379  -0.00468559 -0.0054245  -0.0040329 ]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 3\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.004032897713125001\n",
      "New Q-value: -0.00512445859675\n",
      "\n",
      "Step 17\n",
      "Current state: 3\n",
      "Q-values for current state: [-0.0074665  -0.00628991 -0.00520895 -0.00671402]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 4\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.005208954262500001\n",
      "New Q-value: -0.005886563711250001\n",
      "\n",
      "Step 18\n",
      "Current state: 4\n",
      "Q-values for current state: [-0.00393385 -0.00494304 -0.00208953 -0.00461846]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 3\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.0039338506549375\n",
      "New Q-value: -0.0050996891420125\n",
      "\n",
      "Step 19\n",
      "Current state: 3\n",
      "Q-values for current state: [-0.0074665  -0.00628991 -0.00588656 -0.00671402]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 2\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.0074665049503187505\n",
      "New Q-value: -0.008097013943411877\n",
      "\n",
      "Step 20\n",
      "Current state: 2\n",
      "Q-values for current state: [-0.00935338 -0.00651431 -0.01033905 -0.0039701 ]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 2\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.003970099875000001\n",
      "New Q-value: -0.004950249375625001\n",
      "\n",
      "Step 21\n",
      "Current state: 2\n",
      "Q-values for current state: [-0.00935338 -0.00651431 -0.01033905 -0.00495025]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 3\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.010339053151049542\n",
      "New Q-value: -0.010864371388513337\n",
      "\n",
      "Step 22\n",
      "Current state: 3\n",
      "Q-values for current state: [-0.00809701 -0.00628991 -0.00588656 -0.00671402]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 3\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.006714019899193751\n",
      "New Q-value: -0.007601841461843126\n",
      "\n",
      "Step 23\n",
      "Current state: 3\n",
      "Q-values for current state: [-0.00809701 -0.00628991 -0.00588656 -0.00760184]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 2\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.008097013943411877\n",
      "New Q-value: -0.008757586239755064\n",
      "\n",
      "Step 24\n",
      "Current state: 2\n",
      "Q-values for current state: [-0.00935338 -0.00651431 -0.01086437 -0.00495025]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 1\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.009353375196193761\n",
      "New Q-value: -0.010296328751691538\n",
      "\n",
      "Step 25\n",
      "Current state: 1\n",
      "Q-values for current state: [-0.01345622 -0.00924517 -0.01018511 -0.01145173]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 0\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.013456222510981329\n",
      "New Q-value: -0.014309221095166922\n",
      "\n",
      "Step 26\n",
      "Current state: 0\n",
      "Q-values for current state: [-0.01261706 -0.01417428 -0.01607209 -0.01775112]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 0\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.012617061424039214\n",
      "New Q-value: -0.013553976116919018\n",
      "\n",
      "Step 27\n",
      "Current state: 0\n",
      "Q-values for current state: [-0.01355398 -0.01417428 -0.01607209 -0.01775112]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 1\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.01607209147913428\n",
      "New Q-value: -0.016343173406338004\n",
      "\n",
      "Step 28\n",
      "Current state: 1\n",
      "Q-values for current state: [-0.01430922 -0.00924517 -0.01018511 -0.01145173]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 0\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.014309221095166922\n",
      "New Q-value: -0.015165926716757537\n",
      "\n",
      "Step 29\n",
      "Current state: 0\n",
      "Q-values for current state: [-0.01355398 -0.01417428 -0.01634317 -0.01775112]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 1\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.016343173406338004\n",
      "New Q-value: -0.016587147140821355\n",
      "\n",
      "Step 30\n",
      "Current state: 1\n",
      "Q-values for current state: [-0.01516593 -0.00924517 -0.01018511 -0.01145173]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 9\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.0092451692117595\n",
      "New Q-value: -0.00979191071558355\n",
      "\n",
      "Step 31\n",
      "Current state: 9\n",
      "Q-values for current state: [-0.01142329 -0.00496062 -0.00771914 -0.00698999]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 8\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.011423287514731503\n",
      "New Q-value: -0.012095540098532802\n",
      "\n",
      "Step 32\n",
      "Current state: 8\n",
      "Q-values for current state: [-0.01465394 -0.01242403 -0.00857454 -0.01404268]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 8\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.01465394163585662\n",
      "New Q-value: -0.015003128807545407\n",
      "\n",
      "Step 33\n",
      "Current state: 8\n",
      "Q-values for current state: [-0.01500313 -0.01242403 -0.00857454 -0.01404268]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 8\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.015003128807545407\n",
      "New Q-value: -0.015317397262065316\n",
      "\n",
      "Step 34\n",
      "Current state: 8\n",
      "Q-values for current state: [-0.0153174  -0.01242403 -0.00857454 -0.01404268]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 9\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.00857454037131\n",
      "New Q-value: -0.009188344759179\n",
      "\n",
      "Step 35\n",
      "Current state: 9\n",
      "Q-values for current state: [-0.01209554 -0.00496062 -0.00771914 -0.00698999]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 1\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.006989985178301678\n",
      "New Q-value: -0.008221218178451947\n",
      "\n",
      "Step 36\n",
      "Current state: 1\n",
      "Q-values for current state: [-0.01516593 -0.00979191 -0.01018511 -0.01145173]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 1\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.011451729265530723\n",
      "New Q-value: -0.012236787856958089\n",
      "\n",
      "Step 37\n",
      "Current state: 1\n",
      "Q-values for current state: [-0.01516593 -0.00979191 -0.01018511 -0.01223679]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 0\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.015165926716757537\n",
      "New Q-value: -0.015936961776189088\n",
      "\n",
      "Step 38\n",
      "Current state: 0\n",
      "Q-values for current state: [-0.01355398 -0.01417428 -0.01658715 -0.01775112]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 0\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.017751122558777806\n",
      "New Q-value: -0.018263638034007334\n",
      "\n",
      "Step 39\n",
      "Current state: 0\n",
      "Q-values for current state: [-0.01355398 -0.01417428 -0.01658715 -0.01826364]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 0\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.018263638034007334\n",
      "New Q-value: -0.01872490196171391\n",
      "\n",
      "Step 40\n",
      "Current state: 0\n",
      "Q-values for current state: [-0.01355398 -0.01417428 -0.01658715 -0.0187249 ]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 8\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.014174280172316124\n",
      "New Q-value: -0.014629744907206517\n",
      "\n",
      "Step 41\n",
      "Current state: 8\n",
      "Q-values for current state: [-0.0153174  -0.01242403 -0.00918834 -0.01404268]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 0\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.014042683501124975\n",
      "New Q-value: -0.014926042882119784\n",
      "\n",
      "Step 42\n",
      "Current state: 0\n",
      "Q-values for current state: [-0.01355398 -0.01462974 -0.01658715 -0.0187249 ]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 0\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.013553976116919018\n",
      "New Q-value: -0.014486206236334424\n",
      "\n",
      "Step 43\n",
      "Current state: 0\n",
      "Q-values for current state: [-0.01448621 -0.01462974 -0.01658715 -0.0187249 ]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 1\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.016587147140821355\n",
      "New Q-value: -0.016858663944719656\n",
      "\n",
      "Step 44\n",
      "Current state: 1\n",
      "Q-values for current state: [-0.01593696 -0.00979191 -0.01018511 -0.01223679]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 0\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.015936961776189088\n",
      "New Q-value: -0.01671945519102195\n",
      "\n",
      "Step 45\n",
      "Current state: 0\n",
      "Q-values for current state: [-0.01448621 -0.01462974 -0.01685866 -0.0187249 ]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 8\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.014629744907206517\n",
      "New Q-value: -0.01503966316860787\n",
      "\n",
      "Step 46\n",
      "Current state: 8\n",
      "Q-values for current state: [-0.0153174  -0.01242403 -0.00918834 -0.01492604]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 9\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.009188344759179\n",
      "New Q-value: -0.0097407687082611\n",
      "\n",
      "Step 47\n",
      "Current state: 9\n",
      "Q-values for current state: [-0.01209554 -0.00496062 -0.00771914 -0.00822122]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 10\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.0077191443516\n",
      "New Q-value: -0.008325969455311874\n",
      "\n",
      "Step 48\n",
      "Current state: 10\n",
      "Q-values for current state: [-0.00539021 -0.00557541 -0.00518122 -0.00398673]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 11\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.005181218715625\n",
      "New Q-value: -0.0058851978071875\n",
      "\n",
      "Step 49\n",
      "Current state: 11\n",
      "Q-values for current state: [-0.0023379  -0.00468559 -0.0054245  -0.00512446]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 12\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.005424495713125\n",
      "New Q-value: -0.0061664766049375\n",
      "\n",
      "Step 50\n",
      "Current state: 12\n",
      "Q-values for current state: [-0.00450897 -0.003439   -0.00591472 -0.002994  ]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 13\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.005914720500000001\n",
      "New Q-value: -0.00658069845\n",
      "\n",
      "Step 51\n",
      "Current state: 13\n",
      "Q-values for current state: [-0.002805   -0.003439   -0.00271    -0.00585359]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 5\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.005853588\n",
      "New Q-value: -0.006552659663125\n",
      "\n",
      "Step 52\n",
      "Current state: 5\n",
      "Q-values for current state: [-0.002994  -0.0036195 -0.003534  -0.0044552]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 4\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.002994004875\n",
      "New Q-value: -0.0038931092625\n",
      "\n",
      "Step 53\n",
      "Current state: 4\n",
      "Q-values for current state: [-0.00509969 -0.00494304 -0.00208953 -0.00461846]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 5\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.002089525\n",
      "New Q-value: -0.0032163025000000005\n",
      "\n",
      "Step 54\n",
      "Current state: 5\n",
      "Q-values for current state: [-0.00389311 -0.0036195  -0.003534   -0.0044552 ]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 6\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.0035340000000000002\n",
      "New Q-value: -0.0042756\n",
      "\n",
      "Step 55\n",
      "Current state: 6\n",
      "Q-values for current state: [-0.0019    -0.00271   -0.001     -0.0028905]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 5\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.0019000000000000002\n",
      "New Q-value: -0.0030538525\n",
      "\n",
      "Step 56\n",
      "Current state: 5\n",
      "Q-values for current state: [-0.00389311 -0.0036195  -0.0042756  -0.0044552 ]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 6\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.0042756\n",
      "New Q-value: -0.00494304\n",
      "\n",
      "Step 57\n",
      "Current state: 6\n",
      "Q-values for current state: [-0.00305385 -0.00271    -0.001      -0.0028905 ]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 6\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.0028905000000000003\n",
      "New Q-value: -0.0036964500000000004\n",
      "\n",
      "Step 58\n",
      "Current state: 6\n",
      "Q-values for current state: [-0.00305385 -0.00271    -0.001      -0.00369645]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 7\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.001\n",
      "New Q-value: -0.0019950000000000002\n",
      "\n",
      "Step 59\n",
      "Current state: 7\n",
      "Q-values for current state: [-0.001995 -0.00271  -0.001    -0.0019  ]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 7\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.001\n",
      "New Q-value: -0.0019950000000000002\n",
      "\n",
      "Step 60\n",
      "Current state: 7\n",
      "Q-values for current state: [-0.001995 -0.00271  -0.001995 -0.0019  ]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 6\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.0019950000000000002\n",
      "New Q-value: -0.0029850250000000005\n",
      "\n",
      "Step 61\n",
      "Current state: 6\n",
      "Q-values for current state: [-0.00305385 -0.00271    -0.001995   -0.00369645]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 7\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.0019950000000000002\n",
      "New Q-value: -0.0029760000000000003\n",
      "\n",
      "Step 62\n",
      "Current state: 7\n",
      "Q-values for current state: [-0.00298503 -0.00271    -0.001995   -0.0019    ]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 7\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.0019950000000000002\n",
      "New Q-value: -0.0029760000000000003\n",
      "\n",
      "Step 63\n",
      "Current state: 7\n",
      "Q-values for current state: [-0.00298503 -0.00271    -0.002976   -0.0019    ]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 7\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.0019000000000000002\n",
      "New Q-value: -0.0028905\n",
      "\n",
      "Step 64\n",
      "Current state: 7\n",
      "Q-values for current state: [-0.00298503 -0.00271    -0.002976   -0.0028905 ]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 7\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.0029760000000000003\n",
      "New Q-value: -0.00393585\n",
      "\n",
      "Step 65\n",
      "Current state: 7\n",
      "Q-values for current state: [-0.00298503 -0.00271    -0.00393585 -0.0028905 ]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 7\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.00393585\n",
      "New Q-value: -0.004799715\n",
      "\n",
      "Step 66\n",
      "Current state: 7\n",
      "Q-values for current state: [-0.00298503 -0.00271    -0.00479972 -0.0028905 ]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 7\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.0028905\n",
      "New Q-value: -0.0038589\n",
      "\n",
      "Step 67\n",
      "Current state: 7\n",
      "Q-values for current state: [-0.00298503 -0.00271    -0.00479972 -0.0038589 ]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 6\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.0029850250000000005\n",
      "New Q-value: -0.0039439725\n",
      "\n",
      "Step 68\n",
      "Current state: 6\n",
      "Q-values for current state: [-0.00305385 -0.00271    -0.002976   -0.00369645]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 6\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.0036964500000000004\n",
      "New Q-value: -0.004584255000000001\n",
      "\n",
      "Step 69\n",
      "Current state: 6\n",
      "Q-values for current state: [-0.00305385 -0.00271    -0.002976   -0.00458426]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 7\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.0029760000000000003\n",
      "New Q-value: -0.00393585\n",
      "\n",
      "Step 70\n",
      "Current state: 7\n",
      "Q-values for current state: [-0.00394397 -0.00271    -0.00479972 -0.0038589 ]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 7\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.004799715\n",
      "New Q-value: -0.005577193500000001\n",
      "\n",
      "Step 71\n",
      "Current state: 7\n",
      "Q-values for current state: [-0.00394397 -0.00271    -0.00557719 -0.0038589 ]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 7\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.005577193500000001\n",
      "New Q-value: -0.006276924150000001\n",
      "\n",
      "Step 72\n",
      "Current state: 7\n",
      "Q-values for current state: [-0.00394397 -0.00271    -0.00627692 -0.0038589 ]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 15\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.00271\n",
      "New Q-value: -0.0036195000000000003\n",
      "\n",
      "Step 73\n",
      "Current state: 15\n",
      "Q-values for current state: [-0.00271  -0.0019   -0.0019   -0.003439]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 7\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.003439\n",
      "New Q-value: -0.0044389525\n",
      "\n",
      "Step 74\n",
      "Current state: 7\n",
      "Q-values for current state: [-0.00394397 -0.0036195  -0.00627692 -0.0038589 ]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 6\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.0039439725\n",
      "New Q-value: -0.00480702525\n",
      "\n",
      "Step 75\n",
      "Current state: 6\n",
      "Q-values for current state: [-0.00305385 -0.00271    -0.00393585 -0.00458426]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 7\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.00393585\n",
      "New Q-value: -0.0048861175\n",
      "\n",
      "Step 76\n",
      "Current state: 7\n",
      "Q-values for current state: [-0.00480703 -0.0036195  -0.00627692 -0.0038589 ]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 15\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.0036195000000000003\n",
      "New Q-value: -0.004438050000000001\n",
      "\n",
      "Step 77\n",
      "Current state: 15\n",
      "Q-values for current state: [-0.00271    -0.0019     -0.0019     -0.00443895]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 23\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.0019000000000000002\n",
      "New Q-value: -0.00271\n",
      "\n",
      "Step 78\n",
      "Current state: 23\n",
      "Q-values for current state: [ 0.        -0.0019    -0.001     -0.0028905]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 31\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.0019000000000000002\n",
      "New Q-value: -0.00271\n",
      "\n",
      "Step 79\n",
      "Current state: 31\n",
      "Q-values for current state: [-0.0019 -0.001   0.     -0.001 ]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 30\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.0019000000000000002\n",
      "New Q-value: -0.00271\n",
      "\n",
      "Step 80\n",
      "Current state: 30\n",
      "Q-values for current state: [ 0.         -0.0019     -0.001      -0.00435255]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 38\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.0019000000000000002\n",
      "New Q-value: -0.00271\n",
      "\n",
      "Step 81\n",
      "Current state: 38\n",
      "Q-values for current state: [ 0.        0.       -0.001995 -0.001   ]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 37\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: -0.001\n",
      "\n",
      "Step 82\n",
      "Current state: 37\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 29\n",
      "Reward: -0.01\n",
      "Terminated: True\n",
      "Old Q-value: 0.0\n",
      "New Q-value: -0.001\n",
      "\n",
      "Episode 23\n",
      "Starting state: 0\n",
      "\n",
      "Step 0\n",
      "Current state: 0\n",
      "Q-values for current state: [-0.01448621 -0.01503966 -0.01685866 -0.0187249 ]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 8\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.01503966316860787\n",
      "New Q-value: -0.015461069879031888\n",
      "\n",
      "Step 1\n",
      "Current state: 8\n",
      "Q-values for current state: [-0.0153174  -0.01242403 -0.00974077 -0.01492604]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 0\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.014926042882119784\n",
      "New Q-value: -0.015809628186359576\n",
      "\n",
      "Step 2\n",
      "Current state: 0\n",
      "Q-values for current state: [-0.01448621 -0.01546107 -0.01685866 -0.0187249 ]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 0\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.014486206236334424\n",
      "New Q-value: -0.015413775205152752\n",
      "\n",
      "Step 3\n",
      "Current state: 0\n",
      "Q-values for current state: [-0.01541378 -0.01546107 -0.01685866 -0.0187249 ]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 8\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.015461069879031888\n",
      "New Q-value: -0.015840335918413505\n",
      "\n",
      "Step 4\n",
      "Current state: 8\n",
      "Q-values for current state: [-0.0153174  -0.01242403 -0.00974077 -0.01580963]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 8\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.015317397262065316\n",
      "New Q-value: -0.015711030563143588\n",
      "\n",
      "Step 5\n",
      "Current state: 8\n",
      "Q-values for current state: [-0.01571103 -0.01242403 -0.00974077 -0.01580963]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 0\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.015809628186359576\n",
      "New Q-value: -0.01669297401221313\n",
      "\n",
      "Step 6\n",
      "Current state: 0\n",
      "Q-values for current state: [-0.01541378 -0.01584034 -0.01685866 -0.0187249 ]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 8\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.015840335918413505\n",
      "New Q-value: -0.01618167535385696\n",
      "\n",
      "Step 7\n",
      "Current state: 8\n",
      "Q-values for current state: [-0.01571103 -0.01242403 -0.00974077 -0.01669297]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 9\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.0097407687082611\n",
      "New Q-value: -0.01023795026243499\n",
      "\n",
      "Step 8\n",
      "Current state: 9\n",
      "Q-values for current state: [-0.01209554 -0.00496062 -0.00832597 -0.00822122]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 17\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.004960615000000001\n",
      "New Q-value: -0.005791258500000001\n",
      "\n",
      "Step 9\n",
      "Current state: 17\n",
      "Q-values for current state: [-0.0070699  -0.003439   -0.00520195 -0.00714606]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 9\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.007146064920000001\n",
      "New Q-value: -0.007981627985500001\n",
      "\n",
      "Step 10\n",
      "Current state: 9\n",
      "Q-values for current state: [-0.01209554 -0.00579126 -0.00832597 -0.00822122]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 10\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.008325969455311874\n",
      "New Q-value: -0.008872112048652563\n",
      "\n",
      "Step 11\n",
      "Current state: 10\n",
      "Q-values for current state: [-0.00539021 -0.00557541 -0.0058852  -0.00398673]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 2\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.0039867319881250005\n",
      "New Q-value: -0.005058332479996876\n",
      "\n",
      "Step 12\n",
      "Current state: 2\n",
      "Q-values for current state: [-0.01029633 -0.00651431 -0.01086437 -0.00495025]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 10\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.006514306938871875\n",
      "New Q-value: -0.007343417830584391\n",
      "\n",
      "Step 13\n",
      "Current state: 10\n",
      "Q-values for current state: [-0.00539021 -0.00557541 -0.0058852  -0.00505833]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 2\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.005058332479996876\n",
      "New Q-value: -0.006022772922681564\n",
      "\n",
      "Step 14\n",
      "Current state: 2\n",
      "Q-values for current state: [-0.01029633 -0.00734342 -0.01086437 -0.00495025]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 10\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.007343417830584391\n",
      "New Q-value: -0.008121145522525951\n",
      "\n",
      "Step 15\n",
      "Current state: 10\n",
      "Q-values for current state: [-0.00539021 -0.00557541 -0.0058852  -0.00602277]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 9\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.005390205\n",
      "New Q-value: -0.006401354057500001\n",
      "\n",
      "Step 16\n",
      "Current state: 9\n",
      "Q-values for current state: [-0.01209554 -0.00579126 -0.00887211 -0.00822122]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 8\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.012095540098532802\n",
      "New Q-value: -0.012858591363610845\n",
      "\n",
      "Step 17\n",
      "Current state: 8\n",
      "Q-values for current state: [-0.01571103 -0.01242403 -0.01023795 -0.01669297]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 16\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.012424031414580615\n",
      "New Q-value: -0.012884493957277553\n",
      "\n",
      "Step 18\n",
      "Current state: 16\n",
      "Q-values for current state: [-0.00835937 -0.00739859 -0.00800606 -0.00945618]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 16\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.00835936505982625\n",
      "New Q-value: -0.009226294237998625\n",
      "\n",
      "Step 19\n",
      "Current state: 16\n",
      "Q-values for current state: [-0.00922629 -0.00739859 -0.00800606 -0.00945618]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 8\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.009456183137000201\n",
      "New Q-value: -0.010483170098231505\n",
      "\n",
      "Step 20\n",
      "Current state: 8\n",
      "Q-values for current state: [-0.01571103 -0.01288449 -0.01023795 -0.01669297]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 16\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.012884493957277553\n",
      "New Q-value: -0.013298910245704797\n",
      "\n",
      "Step 21\n",
      "Current state: 16\n",
      "Q-values for current state: [-0.00922629 -0.00739859 -0.00800606 -0.01048317]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 8\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.010483170098231505\n",
      "New Q-value: -0.011407458363339678\n",
      "\n",
      "Step 22\n",
      "Current state: 8\n",
      "Q-values for current state: [-0.01571103 -0.01329891 -0.01023795 -0.01669297]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 16\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.013298910245704797\n",
      "New Q-value: -0.013671884905289318\n",
      "\n",
      "Step 23\n",
      "Current state: 16\n",
      "Q-values for current state: [-0.00922629 -0.00739859 -0.00800606 -0.01140746]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 8\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.011407458363339678\n",
      "New Q-value: -0.012239317801937034\n",
      "\n",
      "Step 24\n",
      "Current state: 8\n",
      "Q-values for current state: [-0.01571103 -0.01367188 -0.01023795 -0.01669297]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 16\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.013671884905289318\n",
      "New Q-value: -0.014007562098915386\n",
      "\n",
      "Step 25\n",
      "Current state: 16\n",
      "Q-values for current state: [-0.00922629 -0.00739859 -0.00800606 -0.01223932]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 16\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.009226294237998625\n",
      "New Q-value: -0.010006530498353762\n",
      "\n",
      "Step 26\n",
      "Current state: 16\n",
      "Q-values for current state: [-0.01000653 -0.00739859 -0.00800606 -0.01223932]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 8\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.012239317801937034\n",
      "New Q-value: -0.012987991296674654\n",
      "\n",
      "Step 27\n",
      "Current state: 8\n",
      "Q-values for current state: [-0.01571103 -0.01400756 -0.01023795 -0.01669297]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 16\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.014007562098915386\n",
      "New Q-value: -0.014309671573178848\n",
      "\n",
      "Step 28\n",
      "Current state: 16\n",
      "Q-values for current state: [-0.01000653 -0.00739859 -0.00800606 -0.01298799]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 8\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.012987991296674654\n",
      "New Q-value: -0.013661797441938513\n",
      "\n",
      "Step 29\n",
      "Current state: 8\n",
      "Q-values for current state: [-0.01571103 -0.01430967 -0.01023795 -0.01669297]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 9\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.01023795026243499\n",
      "New Q-value: -0.010764324793691491\n",
      "\n",
      "Step 30\n",
      "Current state: 9\n",
      "Q-values for current state: [-0.01285859 -0.00579126 -0.00887211 -0.00822122]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 10\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.008872112048652563\n",
      "New Q-value: -0.009514565150037306\n",
      "\n",
      "Step 31\n",
      "Current state: 10\n",
      "Q-values for current state: [-0.00640135 -0.00557541 -0.0058852  -0.00602277]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 18\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.00557541375\n",
      "New Q-value: -0.006239973338125\n",
      "\n",
      "Step 32\n",
      "Current state: 18\n",
      "Q-values for current state: [-0.0046612  -0.00478059 -0.00271    -0.0023379 ]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 19\n",
      "Reward: -0.01\n",
      "Terminated: True\n",
      "Old Q-value: -0.00271\n",
      "New Q-value: -0.003439\n",
      "\n",
      "Episode 24\n",
      "Starting state: 0\n",
      "\n",
      "Step 0\n",
      "Current state: 0\n",
      "Q-values for current state: [-0.01541378 -0.01618168 -0.01685866 -0.0187249 ]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 0\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.015413775205152752\n",
      "New Q-value: -0.01633670632912699\n",
      "\n",
      "Step 1\n",
      "Current state: 0\n",
      "Q-values for current state: [-0.01633671 -0.01618168 -0.01685866 -0.0187249 ]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 0\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.01633670632912699\n",
      "New Q-value: -0.017240294854830703\n",
      "\n",
      "Step 2\n",
      "Current state: 0\n",
      "Q-values for current state: [-0.01724029 -0.01618168 -0.01685866 -0.0187249 ]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 0\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.017240294854830703\n",
      "New Q-value: -0.018053524527964042\n",
      "\n",
      "Step 3\n",
      "Current state: 0\n",
      "Q-values for current state: [-0.01805352 -0.01618168 -0.01685866 -0.0187249 ]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 8\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.01618167535385696\n",
      "New Q-value: -0.016586118673871955\n",
      "\n",
      "Step 4\n",
      "Current state: 8\n",
      "Q-values for current state: [-0.01571103 -0.01430967 -0.01076432 -0.01669297]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 9\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.010764324793691491\n",
      "New Q-value: -0.011238061871822342\n",
      "\n",
      "Step 5\n",
      "Current state: 9\n",
      "Q-values for current state: [-0.01285859 -0.00579126 -0.00951457 -0.00822122]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 8\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.012858591363610845\n",
      "New Q-value: -0.013640348105072883\n",
      "\n",
      "Step 6\n",
      "Current state: 8\n",
      "Q-values for current state: [-0.01571103 -0.01430967 -0.01123806 -0.01669297]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 16\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.014309671573178848\n",
      "New Q-value: -0.014581570100015963\n",
      "\n",
      "Step 7\n",
      "Current state: 16\n",
      "Q-values for current state: [-0.01000653 -0.00739859 -0.00800606 -0.0136618 ]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 8\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.013661797441938513\n",
      "New Q-value: -0.014363233575567784\n",
      "\n",
      "Step 8\n",
      "Current state: 8\n",
      "Q-values for current state: [-0.01571103 -0.01458157 -0.01123806 -0.01669297]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 16\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.014581570100015963\n",
      "New Q-value: -0.014826278774169366\n",
      "\n",
      "Step 9\n",
      "Current state: 16\n",
      "Q-values for current state: [-0.01000653 -0.00739859 -0.00800606 -0.01436323]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 17\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.00800605593519\n",
      "New Q-value: -0.008532155341671\n",
      "\n",
      "Step 10\n",
      "Current state: 17\n",
      "Q-values for current state: [-0.0070699  -0.003439   -0.00520195 -0.00798163]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 9\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.007981627985500001\n",
      "New Q-value: -0.00873363474445\n",
      "\n",
      "Step 11\n",
      "Current state: 9\n",
      "Q-values for current state: [-0.01364035 -0.00579126 -0.00951457 -0.00822122]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 1\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.008221218178451947\n",
      "New Q-value: -0.00932932787858719\n",
      "\n",
      "Step 12\n",
      "Current state: 1\n",
      "Q-values for current state: [-0.01671946 -0.00979191 -0.01018511 -0.01223679]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 2\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.010185114946625353\n",
      "New Q-value: -0.010636877142647192\n",
      "\n",
      "Step 13\n",
      "Current state: 2\n",
      "Q-values for current state: [-0.01029633 -0.00812115 -0.01086437 -0.00495025]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 1\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.010296328751691538\n",
      "New Q-value: -0.011196927394502822\n",
      "\n",
      "Step 14\n",
      "Current state: 1\n",
      "Q-values for current state: [-0.01671946 -0.00979191 -0.01063688 -0.01223679]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 1\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.012236787856958089\n",
      "New Q-value: -0.012943340589242716\n",
      "\n",
      "Step 15\n",
      "Current state: 1\n",
      "Q-values for current state: [-0.01671946 -0.00979191 -0.01063688 -0.01294334]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 9\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.00979191071558355\n",
      "New Q-value: -0.010362889201525196\n",
      "\n",
      "Step 16\n",
      "Current state: 9\n",
      "Q-values for current state: [-0.01364035 -0.00579126 -0.00951457 -0.00932933]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 10\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.009514565150037306\n",
      "New Q-value: -0.010122202426716388\n",
      "\n",
      "Step 17\n",
      "Current state: 10\n",
      "Q-values for current state: [-0.00640135 -0.00623997 -0.0058852  -0.00602277]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 18\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.006239973338125\n",
      "New Q-value: -0.0068380769674375\n",
      "\n",
      "Step 18\n",
      "Current state: 18\n",
      "Q-values for current state: [-0.0046612  -0.00478059 -0.003439   -0.0023379 ]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 19\n",
      "Reward: -0.01\n",
      "Terminated: True\n",
      "Old Q-value: -0.003439\n",
      "New Q-value: -0.0040951\n",
      "\n",
      "Episode 25\n",
      "Starting state: 0\n",
      "\n",
      "Step 0\n",
      "Current state: 0\n",
      "Q-values for current state: [-0.01805352 -0.01658612 -0.01685866 -0.0187249 ]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 0\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.018053524527964042\n",
      "New Q-value: -0.018823853349185474\n",
      "\n",
      "Step 1\n",
      "Current state: 0\n",
      "Q-values for current state: [-0.01882385 -0.01658612 -0.01685866 -0.0187249 ]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 0\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.018823853349185474\n",
      "New Q-value: -0.01951714928828476\n",
      "\n",
      "Step 2\n",
      "Current state: 0\n",
      "Q-values for current state: [-0.01951715 -0.01658612 -0.01685866 -0.0187249 ]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 1\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.016858663944719656\n",
      "New Q-value: -0.017157272024392583\n",
      "\n",
      "Step 3\n",
      "Current state: 1\n",
      "Q-values for current state: [-0.01671946 -0.01036289 -0.01063688 -0.01294334]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 9\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.010362889201525196\n",
      "New Q-value: -0.010876769838872676\n",
      "\n",
      "Step 4\n",
      "Current state: 9\n",
      "Q-values for current state: [-0.01364035 -0.00579126 -0.0101222  -0.00932933]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 8\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.013640348105072883\n",
      "New Q-value: -0.014343929172388716\n",
      "\n",
      "Step 5\n",
      "Current state: 8\n",
      "Q-values for current state: [-0.01571103 -0.01482628 -0.01123806 -0.01669297]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 9\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.011238061871822342\n",
      "New Q-value: -0.01166442524214011\n",
      "\n",
      "Step 6\n",
      "Current state: 9\n",
      "Q-values for current state: [-0.01434393 -0.00579126 -0.0101222  -0.00932933]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 10\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.010122202426716388\n",
      "New Q-value: -0.010669075975727561\n",
      "\n",
      "Step 7\n",
      "Current state: 10\n",
      "Q-values for current state: [-0.00640135 -0.00683808 -0.0058852  -0.00602277]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 18\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.0068380769674375\n",
      "New Q-value: -0.00737637023381875\n",
      "\n",
      "Step 8\n",
      "Current state: 18\n",
      "Q-values for current state: [-0.0046612  -0.00478059 -0.0040951  -0.0023379 ]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 17\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.004661205\n",
      "New Q-value: -0.0055217895\n",
      "\n",
      "Step 9\n",
      "Current state: 17\n",
      "Q-values for current state: [-0.0070699  -0.003439   -0.00520195 -0.00873363]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 9\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.00873363474445\n",
      "New Q-value: -0.009410440827505001\n",
      "\n",
      "Step 10\n",
      "Current state: 9\n",
      "Q-values for current state: [-0.01434393 -0.00579126 -0.01066908 -0.00932933]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 10\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.010669075975727561\n",
      "New Q-value: -0.011161262169837618\n",
      "\n",
      "Step 11\n",
      "Current state: 10\n",
      "Q-values for current state: [-0.00640135 -0.00737637 -0.0058852  -0.00602277]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 11\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.0058851978071875\n",
      "New Q-value: -0.00651877898959375\n",
      "\n",
      "Step 12\n",
      "Current state: 11\n",
      "Q-values for current state: [-0.0023379  -0.00468559 -0.00616648 -0.00512446]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 3\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.00512445859675\n",
      "New Q-value: -0.00617123628964375\n",
      "\n",
      "Step 13\n",
      "Current state: 3\n",
      "Q-values for current state: [-0.00875759 -0.00628991 -0.00588656 -0.00760184]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 11\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.006289914732437501\n",
      "New Q-value: -0.006883024222318751\n",
      "\n",
      "Step 14\n",
      "Current state: 11\n",
      "Q-values for current state: [-0.0023379  -0.00468559 -0.00616648 -0.00617124]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 19\n",
      "Reward: -0.01\n",
      "Terminated: True\n",
      "Old Q-value: -0.00468559\n",
      "New Q-value: -0.005217031\n",
      "\n",
      "Episode 26\n",
      "Starting state: 0\n",
      "\n",
      "Step 0\n",
      "Current state: 0\n",
      "Q-values for current state: [-0.01951715 -0.01658612 -0.01715727 -0.0187249 ]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 1\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.017157272024392583\n",
      "New Q-value: -0.017452048150504808\n",
      "\n",
      "Step 1\n",
      "Current state: 1\n",
      "Q-values for current state: [-0.01671946 -0.01087677 -0.01063688 -0.01294334]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 1\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.012943340589242716\n",
      "New Q-value: -0.013659509858869929\n",
      "\n",
      "Step 2\n",
      "Current state: 1\n",
      "Q-values for current state: [-0.01671946 -0.01087677 -0.01063688 -0.01365951]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 0\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.01671945519102195\n",
      "New Q-value: -0.01762319094593759\n",
      "\n",
      "Step 3\n",
      "Current state: 0\n",
      "Q-values for current state: [-0.01951715 -0.01658612 -0.01745205 -0.0187249 ]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 8\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.016586118673871955\n",
      "New Q-value: -0.01703562720448807\n",
      "\n",
      "Step 4\n",
      "Current state: 8\n",
      "Q-values for current state: [-0.01571103 -0.01482628 -0.01166443 -0.01669297]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 0\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.01669297401221313\n",
      "New Q-value: -0.017642061195418182\n",
      "\n",
      "Step 5\n",
      "Current state: 0\n",
      "Q-values for current state: [-0.01951715 -0.01703563 -0.01745205 -0.0187249 ]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 0\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.01872490196171391\n",
      "New Q-value: -0.019470796349968886\n",
      "\n",
      "Step 6\n",
      "Current state: 0\n",
      "Q-values for current state: [-0.01951715 -0.01703563 -0.01745205 -0.0194708 ]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 0\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.019470796349968886\n",
      "New Q-value: -0.020142101299398363\n",
      "\n",
      "Step 7\n",
      "Current state: 0\n",
      "Q-values for current state: [-0.01951715 -0.01703563 -0.01745205 -0.0201421 ]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 0\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.01951714928828476\n",
      "New Q-value: -0.020183818943882652\n",
      "\n",
      "Step 8\n",
      "Current state: 0\n",
      "Q-values for current state: [-0.02018382 -0.01703563 -0.01745205 -0.0201421 ]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 0\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.020183818943882652\n",
      "New Q-value: -0.020783821633920754\n",
      "\n",
      "Step 9\n",
      "Current state: 0\n",
      "Q-values for current state: [-0.02078382 -0.01703563 -0.01745205 -0.0201421 ]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 1\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.017452048150504808\n",
      "New Q-value: -0.01771734666400581\n",
      "\n",
      "Step 10\n",
      "Current state: 1\n",
      "Q-values for current state: [-0.01762319 -0.01087677 -0.01063688 -0.01365951]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 1\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.013659509858869929\n",
      "New Q-value: -0.014304062201534419\n",
      "\n",
      "Step 11\n",
      "Current state: 1\n",
      "Q-values for current state: [-0.01762319 -0.01087677 -0.01063688 -0.01430406]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 0\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.01762319094593759\n",
      "New Q-value: -0.018479256435770197\n",
      "\n",
      "Step 12\n",
      "Current state: 0\n",
      "Q-values for current state: [-0.02078382 -0.01703563 -0.01771735 -0.0201421 ]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 0\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.020783821633920754\n",
      "New Q-value: -0.021323824054955046\n",
      "\n",
      "Step 13\n",
      "Current state: 0\n",
      "Q-values for current state: [-0.02132382 -0.01703563 -0.01771735 -0.0201421 ]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 0\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.021323824054955046\n",
      "New Q-value: -0.021809826233885908\n",
      "\n",
      "Step 14\n",
      "Current state: 0\n",
      "Q-values for current state: [-0.02180983 -0.01703563 -0.01771735 -0.0201421 ]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 1\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.01771734666400581\n",
      "New Q-value: -0.017956115326156714\n",
      "\n",
      "Step 15\n",
      "Current state: 1\n",
      "Q-values for current state: [-0.01847926 -0.01087677 -0.01063688 -0.01430406]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 9\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.010876769838872676\n",
      "New Q-value: -0.011339262412485408\n",
      "\n",
      "Step 16\n",
      "Current state: 9\n",
      "Q-values for current state: [-0.01434393 -0.00579126 -0.01116126 -0.00932933]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 10\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.011161262169837618\n",
      "New Q-value: -0.011617299380508605\n",
      "\n",
      "Step 17\n",
      "Current state: 10\n",
      "Q-values for current state: [-0.00640135 -0.00737637 -0.00651878 -0.00602277]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 18\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.00737637023381875\n",
      "New Q-value: -0.007860834173561876\n",
      "\n",
      "Step 18\n",
      "Current state: 18\n",
      "Q-values for current state: [-0.00552179 -0.00478059 -0.0040951  -0.0023379 ]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 19\n",
      "Reward: -0.01\n",
      "Terminated: True\n",
      "Old Q-value: -0.0040951\n",
      "New Q-value: -0.00468559\n",
      "\n",
      "Episode 27\n",
      "Starting state: 0\n",
      "\n",
      "Step 0\n",
      "Current state: 0\n",
      "Q-values for current state: [-0.02180983 -0.01703563 -0.01795612 -0.0201421 ]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 0\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.020142101299398363\n",
      "New Q-value: -0.020746275753884894\n",
      "\n",
      "Step 1\n",
      "Current state: 0\n",
      "Q-values for current state: [-0.02180983 -0.01703563 -0.01795612 -0.02074628]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 8\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.01703562720448807\n",
      "New Q-value: -0.017440184882042572\n",
      "\n",
      "Step 2\n",
      "Current state: 8\n",
      "Q-values for current state: [-0.01571103 -0.01482628 -0.01166443 -0.01764206]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 8\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.015711030563143588\n",
      "New Q-value: -0.01624804790483254\n",
      "\n",
      "Step 3\n",
      "Current state: 8\n",
      "Q-values for current state: [-0.01624805 -0.01482628 -0.01166443 -0.01764206]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 9\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.01166442524214011\n",
      "New Q-value: -0.012048152275426098\n",
      "\n",
      "Step 4\n",
      "Current state: 9\n",
      "Q-values for current state: [-0.01434393 -0.00579126 -0.0116173  -0.00932933]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 8\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.014343929172388716\n",
      "New Q-value: -0.015054110721315324\n",
      "\n",
      "Step 5\n",
      "Current state: 8\n",
      "Q-values for current state: [-0.01624805 -0.01482628 -0.01204815 -0.01764206]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 0\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.017642061195418182\n",
      "New Q-value: -0.01853467263967041\n",
      "\n",
      "Step 6\n",
      "Current state: 0\n",
      "Q-values for current state: [-0.02180983 -0.01744018 -0.01795612 -0.02074628]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 8\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.017440184882042572\n",
      "New Q-value: -0.017840740860003795\n",
      "\n",
      "Step 7\n",
      "Current state: 8\n",
      "Q-values for current state: [-0.01624805 -0.01482628 -0.01204815 -0.01853467]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 9\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.012048152275426098\n",
      "New Q-value: -0.012393506605383488\n",
      "\n",
      "Step 8\n",
      "Current state: 9\n",
      "Q-values for current state: [-0.01505411 -0.00579126 -0.0116173  -0.00932933]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 8\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.015054110721315324\n",
      "New Q-value: -0.015726082776695224\n",
      "\n",
      "Step 9\n",
      "Current state: 8\n",
      "Q-values for current state: [-0.01624805 -0.01482628 -0.01239351 -0.01853467]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 8\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.01624804790483254\n",
      "New Q-value: -0.016800626241860718\n",
      "\n",
      "Step 10\n",
      "Current state: 8\n",
      "Q-values for current state: [-0.01680063 -0.01482628 -0.01239351 -0.01853467]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 9\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.012393506605383488\n",
      "New Q-value: -0.01270432550234514\n",
      "\n",
      "Step 11\n",
      "Current state: 9\n",
      "Q-values for current state: [-0.01572608 -0.00579126 -0.0116173  -0.00932933]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 17\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.005791258500000001\n",
      "New Q-value: -0.006538837650000001\n",
      "\n",
      "Step 12\n",
      "Current state: 17\n",
      "Q-values for current state: [-0.0070699  -0.003439   -0.00520195 -0.00941044]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 25\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.003439\n",
      "New Q-value: -0.0040951\n",
      "\n",
      "Step 13\n",
      "Current state: 25\n",
      "Q-values for current state: [-0.00531402  0.         -0.00271    -0.00222671]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 33\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: -0.001095\n",
      "\n",
      "Step 14\n",
      "Current state: 33\n",
      "Q-values for current state: [-0.001095 -0.0019   -0.001    -0.001   ]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 34\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.001\n",
      "New Q-value: -0.0019000000000000002\n",
      "\n",
      "Step 15\n",
      "Current state: 34\n",
      "Q-values for current state: [-0.001 -0.001  0.     0.   ]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 42\n",
      "Reward: -0.01\n",
      "Terminated: True\n",
      "Old Q-value: -0.001\n",
      "New Q-value: -0.0019000000000000002\n",
      "\n",
      "Episode 28\n",
      "Starting state: 0\n",
      "\n",
      "Step 0\n",
      "Current state: 0\n",
      "Q-values for current state: [-0.02180983 -0.01784074 -0.01795612 -0.02074628]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 1\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.017956115326156714\n",
      "New Q-value: -0.018171007122092526\n",
      "\n",
      "Step 1\n",
      "Current state: 1\n",
      "Q-values for current state: [-0.01847926 -0.01133926 -0.01063688 -0.01430406]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 2\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.010636877142647192\n",
      "New Q-value: -0.011043463119066848\n",
      "\n",
      "Step 2\n",
      "Current state: 2\n",
      "Q-values for current state: [-0.01119693 -0.00812115 -0.01086437 -0.00495025]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 3\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.010864371388513337\n",
      "New Q-value: -0.011337157802230753\n",
      "\n",
      "Step 3\n",
      "Current state: 3\n",
      "Q-values for current state: [-0.00875759 -0.00688302 -0.00588656 -0.00760184]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 4\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.005886563711250001\n",
      "New Q-value: -0.006603456077625001\n",
      "\n",
      "Step 4\n",
      "Current state: 4\n",
      "Q-values for current state: [-0.00509969 -0.00494304 -0.0032163  -0.00461846]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 4\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.0046184642625\n",
      "New Q-value: -0.00546216657375\n",
      "\n",
      "Step 5\n",
      "Current state: 4\n",
      "Q-values for current state: [-0.00509969 -0.00494304 -0.0032163  -0.00546217]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 5\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.0032163025000000005\n",
      "New Q-value: -0.00423852475\n",
      "\n",
      "Step 6\n",
      "Current state: 5\n",
      "Q-values for current state: [-0.00389311 -0.0036195  -0.00494304 -0.0044552 ]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 13\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.0036195000000000003\n",
      "New Q-value: -0.004515000000000001\n",
      "\n",
      "Step 7\n",
      "Current state: 13\n",
      "Q-values for current state: [-0.002805   -0.003439   -0.00271    -0.00655266]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 14\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.00271\n",
      "New Q-value: -0.003439\n",
      "\n",
      "Step 8\n",
      "Current state: 14\n",
      "Q-values for current state: [-0.004515   -0.00435255 -0.002805    0.        ]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 15\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.002805\n",
      "New Q-value: -0.003705\n",
      "\n",
      "Step 9\n",
      "Current state: 15\n",
      "Q-values for current state: [-0.00271    -0.00271    -0.0019     -0.00443895]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 23\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.00271\n",
      "New Q-value: -0.003439\n",
      "\n",
      "Step 10\n",
      "Current state: 23\n",
      "Q-values for current state: [ 0.        -0.00271   -0.001     -0.0028905]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 22\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: -0.0011805000000000001\n",
      "\n",
      "Step 11\n",
      "Current state: 22\n",
      "Q-values for current state: [-0.0019    -0.0040951 -0.0019    -0.003439 ]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 21\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.0019000000000000002\n",
      "New Q-value: -0.002805\n",
      "\n",
      "Step 12\n",
      "Current state: 21\n",
      "Q-values for current state: [-0.001     -0.001     -0.0028905 -0.001    ]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 20\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.001\n",
      "New Q-value: -0.0019000000000000002\n",
      "\n",
      "Step 13\n",
      "Current state: 20\n",
      "Q-values for current state: [-0.001       0.          0.         -0.00403209]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 21\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: -0.001095\n",
      "\n",
      "Step 14\n",
      "Current state: 21\n",
      "Q-values for current state: [-0.0019    -0.001     -0.0028905 -0.001    ]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 20\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.0019000000000000002\n",
      "New Q-value: -0.00271\n",
      "\n",
      "Step 15\n",
      "Current state: 20\n",
      "Q-values for current state: [-0.001       0.         -0.001095   -0.00403209]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 21\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.001095\n",
      "New Q-value: -0.0020805\n",
      "\n",
      "Step 16\n",
      "Current state: 21\n",
      "Q-values for current state: [-0.00271   -0.001     -0.0028905 -0.001    ]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 29\n",
      "Reward: -0.01\n",
      "Terminated: True\n",
      "Old Q-value: -0.001\n",
      "New Q-value: -0.0019000000000000002\n",
      "\n",
      "Episode 29\n",
      "Starting state: 0\n",
      "\n",
      "Step 0\n",
      "Current state: 0\n",
      "Q-values for current state: [-0.02180983 -0.01784074 -0.01817101 -0.02074628]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 0\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.021809826233885908\n",
      "New Q-value: -0.022323713992197677\n",
      "\n",
      "Step 1\n",
      "Current state: 0\n",
      "Q-values for current state: [-0.02232371 -0.01784074 -0.01817101 -0.02074628]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 0\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.022323713992197677\n",
      "New Q-value: -0.02278621297467827\n",
      "\n",
      "Step 2\n",
      "Current state: 0\n",
      "Q-values for current state: [-0.02278621 -0.01784074 -0.01817101 -0.02074628]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 1\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.018171007122092526\n",
      "New Q-value: -0.018403035406194625\n",
      "\n",
      "Step 3\n",
      "Current state: 1\n",
      "Q-values for current state: [-0.01847926 -0.01133926 -0.01104346 -0.01430406]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 2\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.011043463119066848\n",
      "New Q-value: -0.011409390497844538\n",
      "\n",
      "Step 4\n",
      "Current state: 2\n",
      "Q-values for current state: [-0.01119693 -0.00812115 -0.01133716 -0.00495025]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 1\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.011196927394502822\n",
      "New Q-value: -0.012154464584238652\n",
      "\n",
      "Step 5\n",
      "Current state: 1\n",
      "Q-values for current state: [-0.01847926 -0.01133926 -0.01140939 -0.01430406]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 2\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.011409390497844538\n",
      "New Q-value: -0.011738725138744459\n",
      "\n",
      "Step 6\n",
      "Current state: 2\n",
      "Q-values for current state: [-0.01215446 -0.00812115 -0.01133716 -0.00495025]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 10\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.008121145522525951\n",
      "New Q-value: -0.008881194397928104\n",
      "\n",
      "Step 7\n",
      "Current state: 10\n",
      "Q-values for current state: [-0.00640135 -0.00786083 -0.00651878 -0.00602277]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 18\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.007860834173561876\n",
      "New Q-value: -0.008296851719330688\n",
      "\n",
      "Step 8\n",
      "Current state: 18\n",
      "Q-values for current state: [-0.00552179 -0.00478059 -0.00468559 -0.0023379 ]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 26\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.0047805899999999995\n",
      "New Q-value: -0.005397531\n",
      "\n",
      "Step 9\n",
      "Current state: 26\n",
      "Q-values for current state: [-0.00271   -0.001     -0.0019    -0.0028905]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 34\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.001\n",
      "New Q-value: -0.0019000000000000002\n",
      "\n",
      "Step 10\n",
      "Current state: 34\n",
      "Q-values for current state: [-0.001  -0.0019  0.      0.    ]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 33\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.001\n",
      "New Q-value: -0.0019950000000000002\n",
      "\n",
      "Step 11\n",
      "Current state: 33\n",
      "Q-values for current state: [-0.001095 -0.0019   -0.0019   -0.001   ]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 41\n",
      "Reward: -0.01\n",
      "Terminated: True\n",
      "Old Q-value: -0.0019000000000000002\n",
      "New Q-value: -0.00271\n",
      "\n",
      "Episode 30\n",
      "Starting state: 0\n",
      "\n",
      "Step 0\n",
      "Current state: 0\n",
      "Q-values for current state: [-0.02278621 -0.01784074 -0.01840304 -0.02074628]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 0\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.02278621297467827\n",
      "New Q-value: -0.023202462058910803\n",
      "\n",
      "Step 1\n",
      "Current state: 0\n",
      "Q-values for current state: [-0.02320246 -0.01784074 -0.01840304 -0.02074628]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 8\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.017840740860003795\n",
      "New Q-value: -0.018263577696726204\n",
      "\n",
      "Step 2\n",
      "Current state: 8\n",
      "Q-values for current state: [-0.01680063 -0.01482628 -0.01270433 -0.01853467]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 8\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.016800626241860718\n",
      "New Q-value: -0.017327474540397434\n",
      "\n",
      "Step 3\n",
      "Current state: 8\n",
      "Q-values for current state: [-0.01732747 -0.01482628 -0.01270433 -0.01853467]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 8\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.017327474540397434\n",
      "New Q-value: -0.017801638009080478\n",
      "\n",
      "Step 4\n",
      "Current state: 8\n",
      "Q-values for current state: [-0.01780164 -0.01482628 -0.01270433 -0.01853467]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 0\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.01853467263967041\n",
      "New Q-value: -0.019416245256892357\n",
      "\n",
      "Step 5\n",
      "Current state: 0\n",
      "Q-values for current state: [-0.02320246 -0.01826358 -0.01840304 -0.02074628]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 0\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.020746275753884894\n",
      "New Q-value: -0.021406688059685392\n",
      "\n",
      "Step 6\n",
      "Current state: 0\n",
      "Q-values for current state: [-0.02320246 -0.01826358 -0.01840304 -0.02140669]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 0\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.021406688059685392\n",
      "New Q-value: -0.022001059134905843\n",
      "\n",
      "Step 7\n",
      "Current state: 0\n",
      "Q-values for current state: [-0.02320246 -0.01826358 -0.01840304 -0.02200106]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 8\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.018263577696726204\n",
      "New Q-value: -0.018644130849776373\n",
      "\n",
      "Step 8\n",
      "Current state: 8\n",
      "Q-values for current state: [-0.01780164 -0.01482628 -0.01270433 -0.01941625]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 16\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.014826278774169366\n",
      "New Q-value: -0.01504651658090743\n",
      "\n",
      "Step 9\n",
      "Current state: 16\n",
      "Q-values for current state: [-0.01000653 -0.00739859 -0.00853216 -0.01436323]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 8\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.014363233575567784\n",
      "New Q-value: -0.015133821140733793\n",
      "\n",
      "Step 10\n",
      "Current state: 8\n",
      "Q-values for current state: [-0.01780164 -0.01504652 -0.01270433 -0.01941625]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 8\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.017801638009080478\n",
      "New Q-value: -0.018228385130895218\n",
      "\n",
      "Step 11\n",
      "Current state: 8\n",
      "Q-values for current state: [-0.01822839 -0.01504652 -0.01270433 -0.01941625]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 8\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.018228385130895218\n",
      "New Q-value: -0.018612457540528485\n",
      "\n",
      "Step 12\n",
      "Current state: 8\n",
      "Q-values for current state: [-0.01861246 -0.01504652 -0.01270433 -0.01941625]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 9\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.01270432550234514\n",
      "New Q-value: -0.013055082528860626\n",
      "\n",
      "Step 13\n",
      "Current state: 9\n",
      "Q-values for current state: [-0.01572608 -0.00653884 -0.0116173  -0.00932933]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 17\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.006538837650000001\n",
      "New Q-value: -0.007273988385000001\n",
      "\n",
      "Step 14\n",
      "Current state: 17\n",
      "Q-values for current state: [-0.0070699  -0.0040951  -0.00520195 -0.00941044]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 9\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.009410440827505001\n",
      "New Q-value: -0.0101604256413295\n",
      "\n",
      "Step 15\n",
      "Current state: 9\n",
      "Q-values for current state: [-0.01572608 -0.00727399 -0.0116173  -0.00932933]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 1\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.00932932787858719\n",
      "New Q-value: -0.010473625019914584\n",
      "\n",
      "Step 16\n",
      "Current state: 1\n",
      "Q-values for current state: [-0.01847926 -0.01133926 -0.01173873 -0.01430406]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 1\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.014304062201534419\n",
      "New Q-value: -0.014950885910567091\n",
      "\n",
      "Step 17\n",
      "Current state: 1\n",
      "Q-values for current state: [-0.01847926 -0.01133926 -0.01173873 -0.01495089]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 2\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.011738725138744459\n",
      "New Q-value: -0.012035126315554388\n",
      "\n",
      "Step 18\n",
      "Current state: 2\n",
      "Q-values for current state: [-0.01215446 -0.00888119 -0.01133716 -0.00495025]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 3\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.011337157802230753\n",
      "New Q-value: -0.011830770349382052\n",
      "\n",
      "Step 19\n",
      "Current state: 3\n",
      "Q-values for current state: [-0.00875759 -0.00688302 -0.00660346 -0.00760184]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 2\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.008757586239755064\n",
      "New Q-value: -0.009352101306463932\n",
      "\n",
      "Step 20\n",
      "Current state: 2\n",
      "Q-values for current state: [-0.01215446 -0.00888119 -0.01183077 -0.00495025]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 2\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.004950249375625001\n",
      "New Q-value: -0.005925498128746876\n",
      "\n",
      "Step 21\n",
      "Current state: 2\n",
      "Q-values for current state: [-0.01215446 -0.00888119 -0.01183077 -0.0059255 ]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 1\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.012154464584238652\n",
      "New Q-value: -0.0130162480550009\n",
      "\n",
      "Step 22\n",
      "Current state: 1\n",
      "Q-values for current state: [-0.01847926 -0.01133926 -0.01203513 -0.01495089]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 1\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.014950885910567091\n",
      "New Q-value: -0.015533027248696495\n",
      "\n",
      "Step 23\n",
      "Current state: 1\n",
      "Q-values for current state: [-0.01847926 -0.01133926 -0.01203513 -0.01553303]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 1\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.015533027248696495\n",
      "New Q-value: -0.01605695445301296\n",
      "\n",
      "Step 24\n",
      "Current state: 1\n",
      "Q-values for current state: [-0.01847926 -0.01133926 -0.01203513 -0.01605695]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 1\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.01605695445301296\n",
      "New Q-value: -0.016528488936897776\n",
      "\n",
      "Step 25\n",
      "Current state: 1\n",
      "Q-values for current state: [-0.01847926 -0.01133926 -0.01203513 -0.01652849]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 9\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.011339262412485408\n",
      "New Q-value: -0.011896365067811866\n",
      "\n",
      "Step 26\n",
      "Current state: 9\n",
      "Q-values for current state: [-0.01572608 -0.00727399 -0.0116173  -0.01047363]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 10\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.011617299380508605\n",
      "New Q-value: -0.012027732870112494\n",
      "\n",
      "Step 27\n",
      "Current state: 10\n",
      "Q-values for current state: [-0.00640135 -0.00829685 -0.00651878 -0.00602277]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 11\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.00651877898959375\n",
      "New Q-value: -0.007089002053759375\n",
      "\n",
      "Step 28\n",
      "Current state: 11\n",
      "Q-values for current state: [-0.0023379  -0.00521703 -0.00616648 -0.00617124]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 10\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.0023379048750000004\n",
      "New Q-value: -0.0036762778151547488\n",
      "\n",
      "Step 29\n",
      "Current state: 10\n",
      "Q-values for current state: [-0.00640135 -0.00829685 -0.007089   -0.00602277]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 9\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.006401354057500001\n",
      "New Q-value: -0.007452247548325001\n",
      "\n",
      "Step 30\n",
      "Current state: 9\n",
      "Q-values for current state: [-0.01572608 -0.00727399 -0.01202773 -0.01047363]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 10\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.012027732870112494\n",
      "New Q-value: -0.012397123010755993\n",
      "\n",
      "Step 31\n",
      "Current state: 10\n",
      "Q-values for current state: [-0.00745225 -0.00829685 -0.007089   -0.00602277]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 2\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.006022772922681564\n",
      "New Q-value: -0.006983417952644361\n",
      "\n",
      "Step 32\n",
      "Current state: 2\n",
      "Q-values for current state: [-0.01301625 -0.00888119 -0.01183077 -0.0059255 ]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 3\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.011830770349382052\n",
      "New Q-value: -0.012275021641818222\n",
      "\n",
      "Step 33\n",
      "Current state: 3\n",
      "Q-values for current state: [-0.0093521  -0.00688302 -0.00660346 -0.00760184]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 2\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.009352101306463932\n",
      "New Q-value: -0.009979813498048492\n",
      "\n",
      "Step 34\n",
      "Current state: 2\n",
      "Q-values for current state: [-0.01301625 -0.00888119 -0.01227502 -0.0059255 ]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 3\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.012275021641818222\n",
      "New Q-value: -0.012674847805010775\n",
      "\n",
      "Step 35\n",
      "Current state: 3\n",
      "Q-values for current state: [-0.00997981 -0.00688302 -0.00660346 -0.00760184]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 11\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.006883024222318751\n",
      "New Q-value: -0.007543968192526577\n",
      "\n",
      "Step 36\n",
      "Current state: 11\n",
      "Q-values for current state: [-0.00367628 -0.00521703 -0.00616648 -0.00617124]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 3\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.00617123628964375\n",
      "New Q-value: -0.00718144098805375\n",
      "\n",
      "Step 37\n",
      "Current state: 3\n",
      "Q-values for current state: [-0.00997981 -0.00754397 -0.00660346 -0.00760184]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 3\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.007601841461843126\n",
      "New Q-value: -0.008468985643033188\n",
      "\n",
      "Step 38\n",
      "Current state: 3\n",
      "Q-values for current state: [-0.00997981 -0.00754397 -0.00660346 -0.00846899]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 4\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.006603456077625001\n",
      "New Q-value: -0.007345770321112501\n",
      "\n",
      "Step 39\n",
      "Current state: 4\n",
      "Q-values for current state: [-0.00509969 -0.00494304 -0.00423852 -0.00546217]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 4\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.00546216657375\n",
      "New Q-value: -0.006318609767625\n",
      "\n",
      "Step 40\n",
      "Current state: 4\n",
      "Q-values for current state: [-0.00509969 -0.00494304 -0.00423852 -0.00631861]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 3\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.0050996891420125\n",
      "New Q-value: -0.006287568408316938\n",
      "\n",
      "Step 41\n",
      "Current state: 3\n",
      "Q-values for current state: [-0.00997981 -0.00754397 -0.00734577 -0.00846899]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 4\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.007345770321112501\n",
      "New Q-value: -0.008013853140251251\n",
      "\n",
      "Step 42\n",
      "Current state: 4\n",
      "Q-values for current state: [-0.00628757 -0.00494304 -0.00423852 -0.00631861]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 3\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.006287568408316938\n",
      "New Q-value: -0.007375488545775269\n",
      "\n",
      "Step 43\n",
      "Current state: 3\n",
      "Q-values for current state: [-0.00997981 -0.00754397 -0.00801385 -0.00846899]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 3\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.008468985643033188\n",
      "New Q-value: -0.009338764057019893\n",
      "\n",
      "Step 44\n",
      "Current state: 3\n",
      "Q-values for current state: [-0.00997981 -0.00754397 -0.00801385 -0.00933876]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 3\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.009338764057019893\n",
      "New Q-value: -0.01012156462960793\n",
      "\n",
      "Step 45\n",
      "Current state: 3\n",
      "Q-values for current state: [-0.00997981 -0.00754397 -0.00801385 -0.01012156]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 3\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.01012156462960793\n",
      "New Q-value: -0.010826085144937162\n",
      "\n",
      "Step 46\n",
      "Current state: 3\n",
      "Q-values for current state: [-0.00997981 -0.00754397 -0.00801385 -0.01082609]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 2\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.009979813498048492\n",
      "New Q-value: -0.010544754470474597\n",
      "\n",
      "Step 47\n",
      "Current state: 2\n",
      "Q-values for current state: [-0.01301625 -0.00888119 -0.01267485 -0.0059255 ]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 1\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.0130162480550009\n",
      "New Q-value: -0.013844777930942938\n",
      "\n",
      "Step 48\n",
      "Current state: 1\n",
      "Q-values for current state: [-0.01847926 -0.01189637 -0.01203513 -0.01652849]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 9\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.011896365067811866\n",
      "New Q-value: -0.01239775745760568\n",
      "\n",
      "Step 49\n",
      "Current state: 9\n",
      "Q-values for current state: [-0.01572608 -0.00727399 -0.01239712 -0.01047363]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 17\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.007273988385000001\n",
      "New Q-value: -0.0079356240465\n",
      "\n",
      "Step 50\n",
      "Current state: 17\n",
      "Q-values for current state: [-0.0070699  -0.0040951  -0.00520195 -0.01016043]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 16\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.007069895017104999\n",
      "New Q-value: -0.0080657711995495\n",
      "\n",
      "Step 51\n",
      "Current state: 16\n",
      "Q-values for current state: [-0.01000653 -0.00739859 -0.00853216 -0.01513382]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 8\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.015133821140733793\n",
      "New Q-value: -0.015860671866902175\n",
      "\n",
      "Step 52\n",
      "Current state: 8\n",
      "Q-values for current state: [-0.01861246 -0.01504652 -0.01305508 -0.01941625]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 9\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.013055082528860626\n",
      "New Q-value: -0.013503458560392064\n",
      "\n",
      "Step 53\n",
      "Current state: 9\n",
      "Q-values for current state: [-0.01572608 -0.00793562 -0.01239712 -0.01047363]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 17\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.0079356240465\n",
      "New Q-value: -0.00853109614185\n",
      "\n",
      "Step 54\n",
      "Current state: 17\n",
      "Q-values for current state: [-0.00806577 -0.0040951  -0.00520195 -0.01016043]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 16\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.0080657711995495\n",
      "New Q-value: -0.00896205976374955\n",
      "\n",
      "Step 55\n",
      "Current state: 16\n",
      "Q-values for current state: [-0.01000653 -0.00739859 -0.00853216 -0.01586067]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 24\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.007398586148999999\n",
      "New Q-value: -0.0079161775341\n",
      "\n",
      "Step 56\n",
      "Current state: 24\n",
      "Q-values for current state: [-0.00690225 -0.00739525 -0.00271    -0.00618237]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 16\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.0061823693875\n",
      "New Q-value: -0.0073161693144895\n",
      "\n",
      "Step 57\n",
      "Current state: 16\n",
      "Q-values for current state: [-0.01000653 -0.00791618 -0.00853216 -0.01586067]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 17\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.008532155341671\n",
      "New Q-value: -0.0090679743075039\n",
      "\n",
      "Step 58\n",
      "Current state: 17\n",
      "Q-values for current state: [-0.00896206 -0.0040951  -0.00520195 -0.01016043]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 9\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.0101604256413295\n",
      "New Q-value: -0.010954837210672301\n",
      "\n",
      "Step 59\n",
      "Current state: 9\n",
      "Q-values for current state: [-0.01572608 -0.0085311  -0.01239712 -0.01047363]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 17\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.00853109614185\n",
      "New Q-value: -0.009067021027665\n",
      "\n",
      "Step 60\n",
      "Current state: 17\n",
      "Q-values for current state: [-0.00896206 -0.0040951  -0.00520195 -0.01095484]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 18\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.0052019472525\n",
      "New Q-value: -0.005903853490375\n",
      "\n",
      "Step 61\n",
      "Current state: 18\n",
      "Q-values for current state: [-0.00552179 -0.00539753 -0.00468559 -0.0023379 ]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 10\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.0023379048750000004\n",
      "New Q-value: -0.0037675390930012147\n",
      "\n",
      "Step 62\n",
      "Current state: 10\n",
      "Q-values for current state: [-0.00745225 -0.00829685 -0.007089   -0.00698342]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 2\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.006983417952644361\n",
      "New Q-value: -0.007847998479610879\n",
      "\n",
      "Step 63\n",
      "Current state: 2\n",
      "Q-values for current state: [-0.01384478 -0.00888119 -0.01267485 -0.0059255 ]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 1\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.013844777930942938\n",
      "New Q-value: -0.014603637137826312\n",
      "\n",
      "Step 64\n",
      "Current state: 1\n",
      "Q-values for current state: [-0.01847926 -0.01239776 -0.01203513 -0.01652849]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 1\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.016528488936897776\n",
      "New Q-value: -0.017018977043185664\n",
      "\n",
      "Step 65\n",
      "Current state: 1\n",
      "Q-values for current state: [-0.01847926 -0.01239776 -0.01203513 -0.01701898]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 1\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.017018977043185664\n",
      "New Q-value: -0.017460416338844764\n",
      "\n",
      "Step 66\n",
      "Current state: 1\n",
      "Q-values for current state: [-0.01847926 -0.01239776 -0.01203513 -0.01746042]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 1\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.017460416338844764\n",
      "New Q-value: -0.017857711704937954\n",
      "\n",
      "Step 67\n",
      "Current state: 1\n",
      "Q-values for current state: [-0.01847926 -0.01239776 -0.01203513 -0.01785771]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 9\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.01239775745760568\n",
      "New Q-value: -0.013019348709473286\n",
      "\n",
      "Step 68\n",
      "Current state: 9\n",
      "Q-values for current state: [-0.01572608 -0.00906702 -0.01239712 -0.01047363]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 1\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.010473625019914584\n",
      "New Q-value: -0.011569599517900793\n",
      "\n",
      "Step 69\n",
      "Current state: 1\n",
      "Q-values for current state: [-0.01847926 -0.01301935 -0.01203513 -0.01785771]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 0\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.018479256435770197\n",
      "New Q-value: -0.019379619155781666\n",
      "\n",
      "Step 70\n",
      "Current state: 0\n",
      "Q-values for current state: [-0.02320246 -0.01864413 -0.01840304 -0.02200106]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 0\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.023202462058910803\n",
      "New Q-value: -0.023630504216608214\n",
      "\n",
      "Step 71\n",
      "Current state: 0\n",
      "Q-values for current state: [-0.0236305  -0.01864413 -0.01840304 -0.02200106]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 0\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.023630504216608214\n",
      "New Q-value: -0.024015742158535882\n",
      "\n",
      "Step 72\n",
      "Current state: 0\n",
      "Q-values for current state: [-0.02401574 -0.01864413 -0.01840304 -0.02200106]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 0\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.024015742158535882\n",
      "New Q-value: -0.024362456306270784\n",
      "\n",
      "Step 73\n",
      "Current state: 0\n",
      "Q-values for current state: [-0.02436246 -0.01864413 -0.01840304 -0.02200106]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 8\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.018644130849776373\n",
      "New Q-value: -0.01906254632803598\n",
      "\n",
      "Step 74\n",
      "Current state: 8\n",
      "Q-values for current state: [-0.01861246 -0.01504652 -0.01350346 -0.01941625]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 9\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.013503458560392064\n",
      "New Q-value: -0.014014479701981032\n",
      "\n",
      "Step 75\n",
      "Current state: 9\n",
      "Q-values for current state: [-0.01572608 -0.00906702 -0.01239712 -0.0115696 ]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 17\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.009067021027665\n",
      "New Q-value: -0.0095493534248985\n",
      "\n",
      "Step 76\n",
      "Current state: 17\n",
      "Q-values for current state: [-0.00896206 -0.0040951  -0.00590385 -0.01095484]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 16\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.00896205976374955\n",
      "New Q-value: -0.009817890653114095\n",
      "\n",
      "Step 77\n",
      "Current state: 16\n",
      "Q-values for current state: [-0.01000653 -0.00791618 -0.00906797 -0.01586067]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 8\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.015860671866902175\n",
      "New Q-value: -0.016605980251900156\n",
      "\n",
      "Step 78\n",
      "Current state: 8\n",
      "Q-values for current state: [-0.01861246 -0.01504652 -0.01401448 -0.01941625]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 0\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.019416245256892357\n",
      "New Q-value: -0.02022290909479161\n",
      "\n",
      "Step 79\n",
      "Current state: 0\n",
      "Q-values for current state: [-0.02436246 -0.01906255 -0.01840304 -0.02200106]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 0\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.022001059134905843\n",
      "New Q-value: -0.022549241585003748\n",
      "\n",
      "Step 80\n",
      "Current state: 0\n",
      "Q-values for current state: [-0.02436246 -0.01906255 -0.01840304 -0.02254924]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 8\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.01906254632803598\n",
      "New Q-value: -0.01948766726692058\n",
      "\n",
      "Step 81\n",
      "Current state: 8\n",
      "Q-values for current state: [-0.01861246 -0.01504652 -0.01401448 -0.02022291]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 16\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.01504651658090743\n",
      "New Q-value: -0.015293901788556186\n",
      "\n",
      "Step 82\n",
      "Current state: 16\n",
      "Q-values for current state: [-0.01000653 -0.00791618 -0.00906797 -0.01660598]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 17\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.0090679743075039\n",
      "New Q-value: -0.00955021137675351\n",
      "\n",
      "Step 83\n",
      "Current state: 17\n",
      "Q-values for current state: [-0.00981789 -0.0040951  -0.00590385 -0.01095484]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 9\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.010954837210672301\n",
      "New Q-value: -0.011766542064970428\n",
      "\n",
      "Step 84\n",
      "Current state: 9\n",
      "Q-values for current state: [-0.01572608 -0.00954935 -0.01239712 -0.0115696 ]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 8\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.015726082776695224\n",
      "New Q-value: -0.0164848500707139\n",
      "\n",
      "Step 85\n",
      "Current state: 8\n",
      "Q-values for current state: [-0.01861246 -0.0152939  -0.01401448 -0.02022291]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 8\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.018612457540528485\n",
      "New Q-value: -0.019082587358163833\n",
      "\n",
      "Step 86\n",
      "Current state: 8\n",
      "Q-values for current state: [-0.01908259 -0.0152939  -0.01401448 -0.02022291]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 8\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.019082587358163833\n",
      "New Q-value: -0.019505704194035647\n",
      "\n",
      "Step 87\n",
      "Current state: 8\n",
      "Q-values for current state: [-0.0195057  -0.0152939  -0.01401448 -0.02022291]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 9\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.014014479701981032\n",
      "New Q-value: -0.014520220307148286\n",
      "\n",
      "Step 88\n",
      "Current state: 9\n",
      "Q-values for current state: [-0.01648485 -0.00954935 -0.01239712 -0.0115696 ]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 8\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.0164848500707139\n",
      "New Q-value: -0.017215785992821597\n",
      "\n",
      "Step 89\n",
      "Current state: 8\n",
      "Q-values for current state: [-0.0195057  -0.0152939  -0.01452022 -0.02022291]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 16\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.015293901788556186\n",
      "New Q-value: -0.015516548475440067\n",
      "\n",
      "Step 90\n",
      "Current state: 16\n",
      "Q-values for current state: [-0.01000653 -0.00791618 -0.00955021 -0.01660598]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 17\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.00955021137675351\n",
      "New Q-value: -0.00998422473907816\n",
      "\n",
      "Step 91\n",
      "Current state: 17\n",
      "Q-values for current state: [-0.00981789 -0.0040951  -0.00590385 -0.01176654]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 16\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.009817890653114095\n",
      "New Q-value: -0.010588138453542185\n",
      "\n",
      "Step 92\n",
      "Current state: 16\n",
      "Q-values for current state: [-0.01000653 -0.00791618 -0.00998422 -0.01660598]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 17\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.00998422473907816\n",
      "New Q-value: -0.010374836765170344\n",
      "\n",
      "Step 93\n",
      "Current state: 17\n",
      "Q-values for current state: [-0.01058814 -0.0040951  -0.00590385 -0.01176654]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 16\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.010588138453542185\n",
      "New Q-value: -0.011281361473927467\n",
      "\n",
      "Step 94\n",
      "Current state: 16\n",
      "Q-values for current state: [-0.01000653 -0.00791618 -0.01037484 -0.01660598]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 8\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.016605980251900156\n",
      "New Q-value: -0.01732480315588923\n",
      "\n",
      "Step 95\n",
      "Current state: 8\n",
      "Q-values for current state: [-0.0195057  -0.01551655 -0.01452022 -0.02022291]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 8\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.019505704194035647\n",
      "New Q-value: -0.01993455470381117\n",
      "\n",
      "Step 96\n",
      "Current state: 8\n",
      "Q-values for current state: [-0.01993455 -0.01551655 -0.01452022 -0.02022291]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 8\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.01993455470381117\n",
      "New Q-value: -0.02032052016260914\n",
      "\n",
      "Step 97\n",
      "Current state: 8\n",
      "Q-values for current state: [-0.02032052 -0.01551655 -0.01452022 -0.02022291]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 8\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.02032052016260914\n",
      "New Q-value: -0.020667889075527312\n",
      "\n",
      "Step 98\n",
      "Current state: 8\n",
      "Q-values for current state: [-0.02066789 -0.01551655 -0.01452022 -0.02022291]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 0\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.02022290909479161\n",
      "New Q-value: -0.02094890654890094\n",
      "\n",
      "Step 99\n",
      "Current state: 0\n",
      "Q-values for current state: [-0.02436246 -0.01948767 -0.01840304 -0.02254924]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 1\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.018403035406194625\n",
      "New Q-value: -0.01870606886555283\n",
      "\n",
      "Episode 30 summary:\n",
      "Total reward: -1.0000000000000007\n",
      "Successful episodes so far: []\n",
      "Average reward over last 10 episodes: -0.337\n",
      "\n",
      "Episode 31\n",
      "Starting state: 0\n",
      "\n",
      "Step 0\n",
      "Current state: 0\n",
      "Q-values for current state: [-0.02436246 -0.01948767 -0.01870607 -0.02254924]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 8\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.01948766726692058\n",
      "New Q-value: -0.01991832146940761\n",
      "\n",
      "Step 1\n",
      "Current state: 8\n",
      "Q-values for current state: [-0.02066789 -0.01551655 -0.01452022 -0.02094891]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 9\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.014520220307148286\n",
      "New Q-value: -0.014975386851798816\n",
      "\n",
      "Step 2\n",
      "Current state: 9\n",
      "Q-values for current state: [-0.01721579 -0.00954935 -0.01239712 -0.0115696 ]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 8\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.017215785992821597\n",
      "New Q-value: -0.017916869144460324\n",
      "\n",
      "Step 3\n",
      "Current state: 8\n",
      "Q-values for current state: [-0.02066789 -0.01551655 -0.01497539 -0.02094891]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 9\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.014975386851798816\n",
      "New Q-value: -0.015385036741984291\n",
      "\n",
      "Step 4\n",
      "Current state: 9\n",
      "Q-values for current state: [-0.01791687 -0.00954935 -0.01239712 -0.0115696 ]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 10\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.012397123010755993\n",
      "New Q-value: -0.012830865904787533\n",
      "\n",
      "Step 5\n",
      "Current state: 10\n",
      "Q-values for current state: [-0.00745225 -0.00829685 -0.007089   -0.007848  ]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 9\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.007452247548325001\n",
      "New Q-value: -0.008614211368857858\n",
      "\n",
      "Step 6\n",
      "Current state: 9\n",
      "Q-values for current state: [-0.01791687 -0.00954935 -0.01283087 -0.0115696 ]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 8\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.017916869144460324\n",
      "New Q-value: -0.0185867607205028\n",
      "\n",
      "Step 7\n",
      "Current state: 8\n",
      "Q-values for current state: [-0.02066789 -0.01551655 -0.01538504 -0.02094891]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 8\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.020667889075527312\n",
      "New Q-value: -0.021062678658463087\n",
      "\n",
      "Step 8\n",
      "Current state: 8\n",
      "Q-values for current state: [-0.02106268 -0.01551655 -0.01538504 -0.02094891]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 16\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.015516548475440067\n",
      "New Q-value: -0.01571693049363556\n",
      "\n",
      "Step 9\n",
      "Current state: 16\n",
      "Q-values for current state: [-0.01000653 -0.00791618 -0.01037484 -0.0173248 ]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 24\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.0079161775341\n",
      "New Q-value: -0.00838200978069\n",
      "\n",
      "Step 10\n",
      "Current state: 24\n",
      "Q-values for current state: [-0.00690225 -0.00739525 -0.00271    -0.00731617]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 24\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.006902250099\n",
      "New Q-value: -0.0074694750891\n",
      "\n",
      "Step 11\n",
      "Current state: 24\n",
      "Q-values for current state: [-0.00746948 -0.00739525 -0.00271    -0.00731617]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 32\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.007395250794\n",
      "New Q-value: -0.0079303232146\n",
      "\n",
      "Step 12\n",
      "Current state: 32\n",
      "Q-values for current state: [-0.00650772 -0.00494304 -0.003534   -0.0028905 ]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 40\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.00494304\n",
      "New Q-value: -0.0055437360000000005\n",
      "\n",
      "Step 13\n",
      "Current state: 40\n",
      "Q-values for current state: [-0.0028905  -0.003534   -0.001      -0.00217502]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 40\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.0028905000000000003\n",
      "New Q-value: -0.0036964500000000004\n",
      "\n",
      "Step 14\n",
      "Current state: 40\n",
      "Q-values for current state: [-0.00369645 -0.003534   -0.001      -0.00217502]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 32\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.0021750249999999997\n",
      "New Q-value: -0.00323212\n",
      "\n",
      "Step 15\n",
      "Current state: 32\n",
      "Q-values for current state: [-0.00650772 -0.00554374 -0.003534   -0.0028905 ]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 32\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.006507723345000001\n",
      "New Q-value: -0.0071315485105000005\n",
      "\n",
      "Step 16\n",
      "Current state: 32\n",
      "Q-values for current state: [-0.00713155 -0.00554374 -0.003534   -0.0028905 ]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 24\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.0028905000000000003\n",
      "New Q-value: -0.0038589\n",
      "\n",
      "Step 17\n",
      "Current state: 24\n",
      "Q-values for current state: [-0.00746948 -0.00793032 -0.00271    -0.00731617]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 24\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.0074694750891\n",
      "New Q-value: -0.00797997758019\n",
      "\n",
      "Step 18\n",
      "Current state: 24\n",
      "Q-values for current state: [-0.00797998 -0.00793032 -0.00271    -0.00731617]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 25\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.00271\n",
      "New Q-value: -0.003543025\n",
      "\n",
      "Step 19\n",
      "Current state: 25\n",
      "Q-values for current state: [-0.00531402 -0.001095   -0.00271    -0.00222671]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 26\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.00271\n",
      "New Q-value: -0.0036195000000000003\n",
      "\n",
      "Step 20\n",
      "Current state: 26\n",
      "Q-values for current state: [-0.00271   -0.0019    -0.0019    -0.0028905]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 27\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.0019000000000000002\n",
      "New Q-value: -0.00271\n",
      "\n",
      "Step 21\n",
      "Current state: 27\n",
      "Q-values for current state: [ 0.    -0.001  0.    -0.001]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 28\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: -0.001\n",
      "\n",
      "Step 22\n",
      "Current state: 28\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 29\n",
      "Reward: -0.01\n",
      "Terminated: True\n",
      "Old Q-value: 0.0\n",
      "New Q-value: -0.001\n",
      "\n",
      "Episode 32\n",
      "Starting state: 0\n",
      "\n",
      "Step 0\n",
      "Current state: 0\n",
      "Q-values for current state: [-0.02436246 -0.01991832 -0.01870607 -0.02254924]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 0\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.024362456306270784\n",
      "New Q-value: -0.024703287217871225\n",
      "\n",
      "Step 1\n",
      "Current state: 0\n",
      "Q-values for current state: [-0.02470329 -0.01991832 -0.01870607 -0.02254924]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 0\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.024703287217871225\n",
      "New Q-value: -0.02501003503831162\n",
      "\n",
      "Step 2\n",
      "Current state: 0\n",
      "Q-values for current state: [-0.02501004 -0.01991832 -0.01870607 -0.02254924]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 8\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.01991832146940761\n",
      "New Q-value: -0.020388067812955355\n",
      "\n",
      "Step 3\n",
      "Current state: 8\n",
      "Q-values for current state: [-0.02106268 -0.01571693 -0.01538504 -0.02094891]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 8\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.021062678658463087\n",
      "New Q-value: -0.021417989283105288\n",
      "\n",
      "Step 4\n",
      "Current state: 8\n",
      "Q-values for current state: [-0.02141799 -0.01571693 -0.01538504 -0.02094891]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 9\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.015385036741984291\n",
      "New Q-value: -0.015753721643151218\n",
      "\n",
      "Step 5\n",
      "Current state: 9\n",
      "Q-values for current state: [-0.01858676 -0.00954935 -0.01283087 -0.0115696 ]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 8\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.0185867607205028\n",
      "New Q-value: -0.0192211930453479\n",
      "\n",
      "Step 6\n",
      "Current state: 8\n",
      "Q-values for current state: [-0.02141799 -0.01571693 -0.01575372 -0.02094891]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 0\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.02094890654890094\n",
      "New Q-value: -0.021631092436238362\n",
      "\n",
      "Step 7\n",
      "Current state: 0\n",
      "Q-values for current state: [-0.02501004 -0.02038807 -0.01870607 -0.02254924]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 0\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.02501003503831162\n",
      "New Q-value: -0.02528610807670798\n",
      "\n",
      "Step 8\n",
      "Current state: 0\n",
      "Q-values for current state: [-0.02528611 -0.02038807 -0.01870607 -0.02254924]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 8\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.020388067812955355\n",
      "New Q-value: -0.020842369428555198\n",
      "\n",
      "Step 9\n",
      "Current state: 8\n",
      "Q-values for current state: [-0.02141799 -0.01571693 -0.01575372 -0.02163109]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 0\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.021631092436238362\n",
      "New Q-value: -0.022245059734842046\n",
      "\n",
      "Step 10\n",
      "Current state: 0\n",
      "Q-values for current state: [-0.02528611 -0.02084237 -0.01870607 -0.02254924]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 1\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.01870606886555283\n",
      "New Q-value: -0.018978798978975215\n",
      "\n",
      "Step 11\n",
      "Current state: 1\n",
      "Q-values for current state: [-0.01937962 -0.01301935 -0.01203513 -0.01785771]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 1\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.017857711704937954\n",
      "New Q-value: -0.018215277534421825\n",
      "\n",
      "Step 12\n",
      "Current state: 1\n",
      "Q-values for current state: [-0.01937962 -0.01301935 -0.01203513 -0.01821528]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 0\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.019379619155781666\n",
      "New Q-value: -0.020244643143206145\n",
      "\n",
      "Step 13\n",
      "Current state: 0\n",
      "Q-values for current state: [-0.02528611 -0.02084237 -0.0189788  -0.02254924]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 1\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.018978798978975215\n",
      "New Q-value: -0.01922425608105536\n",
      "\n",
      "Step 14\n",
      "Current state: 1\n",
      "Q-values for current state: [-0.02024464 -0.01301935 -0.01203513 -0.01821528]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 2\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.012035126315554388\n",
      "New Q-value: -0.012394536006229903\n",
      "\n",
      "Step 15\n",
      "Current state: 2\n",
      "Q-values for current state: [-0.01460364 -0.00888119 -0.01267485 -0.0059255 ]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 3\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.012674847805010775\n",
      "New Q-value: -0.013124040002799722\n",
      "\n",
      "Step 16\n",
      "Current state: 3\n",
      "Q-values for current state: [-0.01054475 -0.00754397 -0.00801385 -0.01082609]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 11\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.007543968192526577\n",
      "New Q-value: -0.00813881776571362\n",
      "\n",
      "Step 17\n",
      "Current state: 11\n",
      "Q-values for current state: [-0.00367628 -0.00521703 -0.00616648 -0.00718144]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 12\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.0061664766049375\n",
      "New Q-value: -0.00683425940756875\n",
      "\n",
      "Step 18\n",
      "Current state: 12\n",
      "Q-values for current state: [-0.00450897 -0.003439   -0.0065807  -0.002994  ]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 13\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.00658069845\n",
      "New Q-value: -0.007189103605\n",
      "\n",
      "Step 19\n",
      "Current state: 13\n",
      "Q-values for current state: [-0.002805   -0.003439   -0.003439   -0.00655266]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 21\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.003439\n",
      "New Q-value: -0.0041901\n",
      "\n",
      "Step 20\n",
      "Current state: 21\n",
      "Q-values for current state: [-0.00271   -0.0019    -0.0028905 -0.001    ]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 22\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.0028905000000000003\n",
      "New Q-value: -0.00378195\n",
      "\n",
      "Step 21\n",
      "Current state: 22\n",
      "Q-values for current state: [-0.002805  -0.0040951 -0.0019    -0.003439 ]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 14\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.003439\n",
      "New Q-value: -0.0040951\n",
      "\n",
      "Step 22\n",
      "Current state: 14\n",
      "Q-values for current state: [-0.004515   -0.00435255 -0.003705    0.        ]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 13\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.004515\n",
      "New Q-value: -0.005329975\n",
      "\n",
      "Step 23\n",
      "Current state: 13\n",
      "Q-values for current state: [-0.002805   -0.0041901  -0.003439   -0.00655266]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 5\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.006552659663125\n",
      "New Q-value: -0.0072672390767500004\n",
      "\n",
      "Step 24\n",
      "Current state: 5\n",
      "Q-values for current state: [-0.00389311 -0.004515   -0.00494304 -0.0044552 ]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 13\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.004515000000000001\n",
      "New Q-value: -0.005329975000000001\n",
      "\n",
      "Step 25\n",
      "Current state: 13\n",
      "Q-values for current state: [-0.002805   -0.0041901  -0.003439   -0.00726724]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 5\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.0072672390767500004\n",
      "New Q-value: -0.0079103605490125\n",
      "\n",
      "Step 26\n",
      "Current state: 5\n",
      "Q-values for current state: [-0.00389311 -0.00532998 -0.00494304 -0.0044552 ]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 4\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.0038931092625\n",
      "New Q-value: -0.004906458187500001\n",
      "\n",
      "Step 27\n",
      "Current state: 4\n",
      "Q-values for current state: [-0.00737549 -0.00494304 -0.00423852 -0.00631861]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 4\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.006318609767625\n",
      "New Q-value: -0.0070894086421125005\n",
      "\n",
      "Step 28\n",
      "Current state: 4\n",
      "Q-values for current state: [-0.00737549 -0.00494304 -0.00423852 -0.00708941]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 4\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.0070894086421125005\n",
      "New Q-value: -0.007783127629151251\n",
      "\n",
      "Step 29\n",
      "Current state: 4\n",
      "Q-values for current state: [-0.00737549 -0.00494304 -0.00423852 -0.00778313]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 4\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.007783127629151251\n",
      "New Q-value: -0.008407474717486126\n",
      "\n",
      "Step 30\n",
      "Current state: 4\n",
      "Q-values for current state: [-0.00737549 -0.00494304 -0.00423852 -0.00840747]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 3\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.007375488545775269\n",
      "New Q-value: -0.008399255739521612\n",
      "\n",
      "Step 31\n",
      "Current state: 3\n",
      "Q-values for current state: [-0.01054475 -0.00813882 -0.00801385 -0.01082609]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 11\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.00813881776571362\n",
      "New Q-value: -0.00867418238158196\n",
      "\n",
      "Step 32\n",
      "Current state: 11\n",
      "Q-values for current state: [-0.00367628 -0.00521703 -0.00683426 -0.00718144]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 19\n",
      "Reward: -0.01\n",
      "Terminated: True\n",
      "Old Q-value: -0.005217031\n",
      "New Q-value: -0.0056953279\n",
      "\n",
      "Episode 33\n",
      "Starting state: 0\n",
      "\n",
      "Step 0\n",
      "Current state: 0\n",
      "Q-values for current state: [-0.02528611 -0.02084237 -0.01922426 -0.02254924]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 0\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.022549241585003748\n",
      "New Q-value: -0.02312062175420363\n",
      "\n",
      "Step 1\n",
      "Current state: 0\n",
      "Q-values for current state: [-0.02528611 -0.02084237 -0.01922426 -0.02312062]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 8\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.020842369428555198\n",
      "New Q-value: -0.021251240882595057\n",
      "\n",
      "Step 2\n",
      "Current state: 8\n",
      "Q-values for current state: [-0.02141799 -0.01571693 -0.01575372 -0.02224506]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 0\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.022245059734842046\n",
      "New Q-value: -0.0228468580890581\n",
      "\n",
      "Step 3\n",
      "Current state: 0\n",
      "Q-values for current state: [-0.02528611 -0.02125124 -0.01922426 -0.02312062]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 1\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.01922425608105536\n",
      "New Q-value: -0.019479311393541664\n",
      "\n",
      "Step 4\n",
      "Current state: 1\n",
      "Q-values for current state: [-0.02024464 -0.01301935 -0.01239454 -0.01821528]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 0\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.020244643143206145\n",
      "New Q-value: -0.021070713411271987\n",
      "\n",
      "Step 5\n",
      "Current state: 0\n",
      "Q-values for current state: [-0.02528611 -0.02125124 -0.01947931 -0.02312062]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 0\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.02528610807670798\n",
      "New Q-value: -0.02560803185142364\n",
      "\n",
      "Step 6\n",
      "Current state: 0\n",
      "Q-values for current state: [-0.02560803 -0.02125124 -0.01947931 -0.02312062]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 1\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.019479311393541664\n",
      "New Q-value: -0.019708861174779338\n",
      "\n",
      "Step 7\n",
      "Current state: 1\n",
      "Q-values for current state: [-0.02107071 -0.01301935 -0.01239454 -0.01821528]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 1\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.018215277534421825\n",
      "New Q-value: -0.018571230701571483\n",
      "\n",
      "Step 8\n",
      "Current state: 1\n",
      "Q-values for current state: [-0.02107071 -0.01301935 -0.01239454 -0.01857123]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 0\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.021070713411271987\n",
      "New Q-value: -0.021835983881748827\n",
      "\n",
      "Step 9\n",
      "Current state: 0\n",
      "Q-values for current state: [-0.02560803 -0.02125124 -0.01970886 -0.02312062]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 1\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.019708861174779338\n",
      "New Q-value: -0.019915455977893244\n",
      "\n",
      "Step 10\n",
      "Current state: 1\n",
      "Q-values for current state: [-0.02183598 -0.01301935 -0.01239454 -0.01857123]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 2\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.012394536006229903\n",
      "New Q-value: -0.012718004727837866\n",
      "\n",
      "Step 11\n",
      "Current state: 2\n",
      "Q-values for current state: [-0.01460364 -0.00888119 -0.01312404 -0.0059255 ]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 2\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.005925498128746876\n",
      "New Q-value: -0.0068958706381031425\n",
      "\n",
      "Step 12\n",
      "Current state: 2\n",
      "Q-values for current state: [-0.01460364 -0.00888119 -0.01312404 -0.00689587]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 1\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.014603637137826312\n",
      "New Q-value: -0.015351483873188278\n",
      "\n",
      "Step 13\n",
      "Current state: 1\n",
      "Q-values for current state: [-0.02183598 -0.01301935 -0.012718   -0.01857123]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 1\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.018571230701571483\n",
      "New Q-value: -0.01892231808055893\n",
      "\n",
      "Step 14\n",
      "Current state: 1\n",
      "Q-values for current state: [-0.02183598 -0.01301935 -0.012718   -0.01892232]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 2\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.012718004727837866\n",
      "New Q-value: -0.013101311965673879\n",
      "\n",
      "Step 15\n",
      "Current state: 2\n",
      "Q-values for current state: [-0.01535148 -0.00888119 -0.01312404 -0.00689587]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 3\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.013124040002799722\n",
      "New Q-value: -0.013572952050843618\n",
      "\n",
      "Step 16\n",
      "Current state: 3\n",
      "Q-values for current state: [-0.01054475 -0.00867418 -0.00801385 -0.01082609]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 11\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.00867418238158196\n",
      "New Q-value: -0.009156010535863464\n",
      "\n",
      "Step 17\n",
      "Current state: 11\n",
      "Q-values for current state: [-0.00367628 -0.00569533 -0.00683426 -0.00718144]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 12\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.00683425940756875\n",
      "New Q-value: -0.007435263929936875\n",
      "\n",
      "Step 18\n",
      "Current state: 12\n",
      "Q-values for current state: [-0.00450897 -0.003439   -0.0071891  -0.002994  ]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 20\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.003439\n",
      "New Q-value: -0.0040951\n",
      "\n",
      "Step 19\n",
      "Current state: 20\n",
      "Q-values for current state: [-0.001       0.         -0.0020805  -0.00403209]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 19\n",
      "Reward: -0.01\n",
      "Terminated: True\n",
      "Old Q-value: -0.001\n",
      "New Q-value: -0.0019000000000000002\n",
      "\n",
      "Episode 34\n",
      "Starting state: 0\n",
      "\n",
      "Step 0\n",
      "Current state: 0\n",
      "Q-values for current state: [-0.02560803 -0.02125124 -0.01991546 -0.02312062]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 0\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.02312062175420363\n",
      "New Q-value: -0.023700527896683126\n",
      "\n",
      "Step 1\n",
      "Current state: 0\n",
      "Q-values for current state: [-0.02560803 -0.02125124 -0.01991546 -0.02370053]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 0\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.02560803185142364\n",
      "New Q-value: -0.025939196984181134\n",
      "\n",
      "Step 2\n",
      "Current state: 0\n",
      "Q-values for current state: [-0.0259392  -0.02125124 -0.01991546 -0.02370053]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 0\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.023700527896683126\n",
      "New Q-value: -0.024222443424914673\n",
      "\n",
      "Step 3\n",
      "Current state: 0\n",
      "Q-values for current state: [-0.0259392  -0.02125124 -0.01991546 -0.02422244]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 0\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.025939196984181134\n",
      "New Q-value: -0.02623724560366288\n",
      "\n",
      "Step 4\n",
      "Current state: 0\n",
      "Q-values for current state: [-0.02623725 -0.02125124 -0.01991546 -0.02422244]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 0\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.024222443424914673\n",
      "New Q-value: -0.024692167400323064\n",
      "\n",
      "Step 5\n",
      "Current state: 0\n",
      "Q-values for current state: [-0.02623725 -0.02125124 -0.01991546 -0.02469217]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 0\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.024692167400323064\n",
      "New Q-value: -0.025114918978190615\n",
      "\n",
      "Step 6\n",
      "Current state: 0\n",
      "Q-values for current state: [-0.02623725 -0.02125124 -0.01991546 -0.02511492]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 0\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.02623724560366288\n",
      "New Q-value: -0.026505489361196448\n",
      "\n",
      "Step 7\n",
      "Current state: 0\n",
      "Q-values for current state: [-0.02650549 -0.02125124 -0.01991546 -0.02511492]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 0\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.025114918978190615\n",
      "New Q-value: -0.025495395398271413\n",
      "\n",
      "Step 8\n",
      "Current state: 0\n",
      "Q-values for current state: [-0.02650549 -0.02125124 -0.01991546 -0.0254954 ]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 1\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.019915455977893244\n",
      "New Q-value: -0.020160748507503882\n",
      "\n",
      "Step 9\n",
      "Current state: 1\n",
      "Q-values for current state: [-0.02183598 -0.01301935 -0.01310131 -0.01892232]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 2\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.013101311965673879\n",
      "New Q-value: -0.013446288479726289\n",
      "\n",
      "Step 10\n",
      "Current state: 2\n",
      "Q-values for current state: [-0.01535148 -0.00888119 -0.01357295 -0.00689587]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 3\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.013572952050843618\n",
      "New Q-value: -0.013976972894083126\n",
      "\n",
      "Step 11\n",
      "Current state: 3\n",
      "Q-values for current state: [-0.01054475 -0.00915601 -0.00801385 -0.01082609]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 11\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.009156010535863464\n",
      "New Q-value: -0.009589655874716819\n",
      "\n",
      "Step 12\n",
      "Current state: 11\n",
      "Q-values for current state: [-0.00367628 -0.00569533 -0.00743526 -0.00718144]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 10\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.0036762778151547488\n",
      "New Q-value: -0.004982105228746415\n",
      "\n",
      "Step 13\n",
      "Current state: 10\n",
      "Q-values for current state: [-0.00861421 -0.00829685 -0.007089   -0.007848  ]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 9\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.008614211368857858\n",
      "New Q-value: -0.00965997880733743\n",
      "\n",
      "Step 14\n",
      "Current state: 9\n",
      "Q-values for current state: [-0.01922119 -0.00954935 -0.01283087 -0.0115696 ]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 17\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.0095493534248985\n",
      "New Q-value: -0.00998345258240865\n",
      "\n",
      "Step 15\n",
      "Current state: 17\n",
      "Q-values for current state: [-0.01128136 -0.0040951  -0.00590385 -0.01176654]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 25\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.0040951\n",
      "New Q-value: -0.004789615\n",
      "\n",
      "Step 16\n",
      "Current state: 25\n",
      "Q-values for current state: [-0.00531402 -0.001095   -0.0036195  -0.00222671]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 26\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.0036195000000000003\n",
      "New Q-value: -0.004438050000000001\n",
      "\n",
      "Step 17\n",
      "Current state: 26\n",
      "Q-values for current state: [-0.00271   -0.0019    -0.00271   -0.0028905]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 27\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.00271\n",
      "New Q-value: -0.003439\n",
      "\n",
      "Step 18\n",
      "Current state: 27\n",
      "Q-values for current state: [ 0.    -0.001 -0.001 -0.001]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 28\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.001\n",
      "New Q-value: -0.0019000000000000002\n",
      "\n",
      "Step 19\n",
      "Current state: 28\n",
      "Q-values for current state: [ 0.     0.    -0.001  0.   ]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 27\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: -0.001\n",
      "\n",
      "Step 20\n",
      "Current state: 27\n",
      "Q-values for current state: [ 0.     -0.001  -0.0019 -0.001 ]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 19\n",
      "Reward: -0.01\n",
      "Terminated: True\n",
      "Old Q-value: -0.001\n",
      "New Q-value: -0.0019000000000000002\n",
      "\n",
      "Episode 35\n",
      "Starting state: 0\n",
      "\n",
      "Step 0\n",
      "Current state: 0\n",
      "Q-values for current state: [-0.02650549 -0.02125124 -0.02016075 -0.0254954 ]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 8\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.021251240882595057\n",
      "New Q-value: -0.02161922519123093\n",
      "\n",
      "Step 1\n",
      "Current state: 8\n",
      "Q-values for current state: [-0.02141799 -0.01571693 -0.01575372 -0.02284686]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 16\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.01571693049363556\n",
      "New Q-value: -0.015941528373437555\n",
      "\n",
      "Step 2\n",
      "Current state: 16\n",
      "Q-values for current state: [-0.01000653 -0.00838201 -0.01037484 -0.0173248 ]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 16\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.010006530498353762\n",
      "New Q-value: -0.010802168377683935\n",
      "\n",
      "Step 3\n",
      "Current state: 16\n",
      "Q-values for current state: [-0.01080217 -0.00838201 -0.01037484 -0.0173248 ]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 16\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.010802168377683935\n",
      "New Q-value: -0.011518242469081091\n",
      "\n",
      "Step 4\n",
      "Current state: 16\n",
      "Q-values for current state: [-0.01151824 -0.00838201 -0.01037484 -0.0173248 ]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 17\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.010374836765170344\n",
      "New Q-value: -0.010792366513653309\n",
      "\n",
      "Step 5\n",
      "Current state: 17\n",
      "Q-values for current state: [-0.01128136 -0.00478961 -0.00590385 -0.01176654]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 25\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.004789615\n",
      "New Q-value: -0.0054146785\n",
      "\n",
      "Step 6\n",
      "Current state: 25\n",
      "Q-values for current state: [-0.00531402 -0.001095   -0.00443805 -0.00222671]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 33\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.001095\n",
      "New Q-value: -0.0020805\n",
      "\n",
      "Step 7\n",
      "Current state: 33\n",
      "Q-values for current state: [-0.001095 -0.00271  -0.0019   -0.001   ]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 25\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.001\n",
      "New Q-value: -0.0020976475\n",
      "\n",
      "Step 8\n",
      "Current state: 25\n",
      "Q-values for current state: [-0.00531402 -0.0020805  -0.00443805 -0.00222671]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 24\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.0053140245\n",
      "New Q-value: -0.006119209425\n",
      "\n",
      "Step 9\n",
      "Current state: 24\n",
      "Q-values for current state: [-0.00797998 -0.00793032 -0.00354302 -0.00731617]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 25\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.003543025\n",
      "New Q-value: -0.0043863700000000005\n",
      "\n",
      "Step 10\n",
      "Current state: 25\n",
      "Q-values for current state: [-0.00611921 -0.0020805  -0.00443805 -0.00222671]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 17\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.0022267050000000003\n",
      "New Q-value: -0.0035184289575\n",
      "\n",
      "Step 11\n",
      "Current state: 17\n",
      "Q-values for current state: [-0.01128136 -0.00541468 -0.00590385 -0.01176654]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 25\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.0054146785\n",
      "New Q-value: -0.0060708581499999996\n",
      "\n",
      "Step 12\n",
      "Current state: 25\n",
      "Q-values for current state: [-0.00611921 -0.0020805  -0.00443805 -0.00351843]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 26\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.004438050000000001\n",
      "New Q-value: -0.0051747450000000006\n",
      "\n",
      "Step 13\n",
      "Current state: 26\n",
      "Q-values for current state: [-0.00271   -0.0019    -0.003439  -0.0028905]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 18\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.0028905\n",
      "New Q-value: -0.003959366213835115\n",
      "\n",
      "Step 14\n",
      "Current state: 18\n",
      "Q-values for current state: [-0.00552179 -0.00539753 -0.00468559 -0.00376754]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 17\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.0055217895\n",
      "New Q-value: -0.006530476631585625\n",
      "\n",
      "Step 15\n",
      "Current state: 17\n",
      "Q-values for current state: [-0.01128136 -0.00607086 -0.00590385 -0.01176654]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 25\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.0060708581499999996\n",
      "New Q-value: -0.006661419835\n",
      "\n",
      "Step 16\n",
      "Current state: 25\n",
      "Q-values for current state: [-0.00611921 -0.0020805  -0.00517475 -0.00351843]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 17\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.0035184289575\n",
      "New Q-value: -0.004727452143335625\n",
      "\n",
      "Step 17\n",
      "Current state: 17\n",
      "Q-values for current state: [-0.01128136 -0.00666142 -0.00590385 -0.01176654]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 9\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.011766542064970428\n",
      "New Q-value: -0.012538315853802207\n",
      "\n",
      "Step 18\n",
      "Current state: 9\n",
      "Q-values for current state: [-0.01922119 -0.00998345 -0.01283087 -0.0115696 ]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 10\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.012830865904787533\n",
      "New Q-value: -0.013221234509415921\n",
      "\n",
      "Step 19\n",
      "Current state: 10\n",
      "Q-values for current state: [-0.00965998 -0.00829685 -0.007089   -0.007848  ]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 11\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.007089002053759375\n",
      "New Q-value: -0.007853401845114348\n",
      "\n",
      "Step 20\n",
      "Current state: 11\n",
      "Q-values for current state: [-0.00498211 -0.00569533 -0.00743526 -0.00718144]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 10\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.004982105228746415\n",
      "New Q-value: -0.006229454561434807\n",
      "\n",
      "Step 21\n",
      "Current state: 10\n",
      "Q-values for current state: [-0.00965998 -0.00829685 -0.0078534  -0.007848  ]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 11\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.007853401845114348\n",
      "New Q-value: -0.008609117811102913\n",
      "\n",
      "Step 22\n",
      "Current state: 11\n",
      "Q-values for current state: [-0.00622945 -0.00569533 -0.00743526 -0.00718144]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 3\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.00718144098805375\n",
      "New Q-value: -0.008224612937572243\n",
      "\n",
      "Step 23\n",
      "Current state: 3\n",
      "Q-values for current state: [-0.01054475 -0.00958966 -0.00801385 -0.01082609]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 11\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.009589655874716819\n",
      "New Q-value: -0.010171746437745138\n",
      "\n",
      "Step 24\n",
      "Current state: 11\n",
      "Q-values for current state: [-0.00622945 -0.00569533 -0.00743526 -0.00822461]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 12\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.007435263929936875\n",
      "New Q-value: -0.007976168000068187\n",
      "\n",
      "Step 25\n",
      "Current state: 12\n",
      "Q-values for current state: [-0.00450897 -0.0040951  -0.0071891  -0.002994  ]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 13\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.007189103605\n",
      "New Q-value: -0.0077366682445000005\n",
      "\n",
      "Step 26\n",
      "Current state: 13\n",
      "Q-values for current state: [-0.002805   -0.0041901  -0.003439   -0.00791036]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 21\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.0041901\n",
      "New Q-value: -0.00486609\n",
      "\n",
      "Step 27\n",
      "Current state: 21\n",
      "Q-values for current state: [-0.00271    -0.0019     -0.00378195 -0.001     ]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 29\n",
      "Reward: -0.01\n",
      "Terminated: True\n",
      "Old Q-value: -0.0019000000000000002\n",
      "New Q-value: -0.00271\n",
      "\n",
      "Episode 36\n",
      "Starting state: 0\n",
      "\n",
      "Step 0\n",
      "Current state: 0\n",
      "Q-values for current state: [-0.02650549 -0.02161923 -0.02016075 -0.0254954 ]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 8\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.02161922519123093\n",
      "New Q-value: -0.021953906228207202\n",
      "\n",
      "Step 1\n",
      "Current state: 8\n",
      "Q-values for current state: [-0.02141799 -0.01594153 -0.01575372 -0.02284686]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 0\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.0228468580890581\n",
      "New Q-value: -0.023477443388365157\n",
      "\n",
      "Step 2\n",
      "Current state: 0\n",
      "Q-values for current state: [-0.02650549 -0.02195391 -0.02016075 -0.0254954 ]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 0\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.026505489361196448\n",
      "New Q-value: -0.02677021153328967\n",
      "\n",
      "Step 3\n",
      "Current state: 0\n",
      "Q-values for current state: [-0.02677021 -0.02195391 -0.02016075 -0.0254954 ]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 0\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.025495395398271413\n",
      "New Q-value: -0.02586112696665714\n",
      "\n",
      "Step 4\n",
      "Current state: 0\n",
      "Q-values for current state: [-0.02677021 -0.02195391 -0.02016075 -0.02586113]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 8\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.021953906228207202\n",
      "New Q-value: -0.02225511916148585\n",
      "\n",
      "Step 5\n",
      "Current state: 8\n",
      "Q-values for current state: [-0.02141799 -0.01594153 -0.01575372 -0.02347744]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 16\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.015941528373437555\n",
      "New Q-value: -0.01614366646525935\n",
      "\n",
      "Step 6\n",
      "Current state: 16\n",
      "Q-values for current state: [-0.01151824 -0.00838201 -0.01079237 -0.0173248 ]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 8\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.01732480315588923\n",
      "New Q-value: -0.01808892639639967\n",
      "\n",
      "Step 7\n",
      "Current state: 8\n",
      "Q-values for current state: [-0.02141799 -0.01614367 -0.01575372 -0.02347744]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 16\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.01614366646525935\n",
      "New Q-value: -0.016325590747898966\n",
      "\n",
      "Step 8\n",
      "Current state: 16\n",
      "Q-values for current state: [-0.01151824 -0.00838201 -0.01079237 -0.01808893]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 24\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.00838200978069\n",
      "New Q-value: -0.008960513952621\n",
      "\n",
      "Step 9\n",
      "Current state: 24\n",
      "Q-values for current state: [-0.00797998 -0.00793032 -0.00438637 -0.00731617]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 25\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.0043863700000000005\n",
      "New Q-value: -0.0051453805\n",
      "\n",
      "Step 10\n",
      "Current state: 25\n",
      "Q-values for current state: [-0.00611921 -0.0020805  -0.00517475 -0.00472745]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 24\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.006119209425\n",
      "New Q-value: -0.0069960996300000005\n",
      "\n",
      "Step 11\n",
      "Current state: 24\n",
      "Q-values for current state: [-0.00797998 -0.00793032 -0.00514538 -0.00731617]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 25\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.0051453805\n",
      "New Q-value: -0.005828489950000001\n",
      "\n",
      "Step 12\n",
      "Current state: 25\n",
      "Q-values for current state: [-0.0069961  -0.0020805  -0.00517475 -0.00472745]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 26\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.0051747450000000006\n",
      "New Q-value: -0.0058377705000000005\n",
      "\n",
      "Step 13\n",
      "Current state: 26\n",
      "Q-values for current state: [-0.00271    -0.0019     -0.003439   -0.00395937]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 25\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.00271\n",
      "New Q-value: -0.0036366475000000004\n",
      "\n",
      "Step 14\n",
      "Current state: 25\n",
      "Q-values for current state: [-0.0069961  -0.0020805  -0.00583777 -0.00472745]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 26\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.0058377705000000005\n",
      "New Q-value: -0.00643449345\n",
      "\n",
      "Step 15\n",
      "Current state: 26\n",
      "Q-values for current state: [-0.00363665 -0.0019     -0.003439   -0.00395937]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 34\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.0019000000000000002\n",
      "New Q-value: -0.00271\n",
      "\n",
      "Step 16\n",
      "Current state: 34\n",
      "Q-values for current state: [-0.001995 -0.0019    0.        0.      ]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 33\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.0019950000000000002\n",
      "New Q-value: -0.002899525\n",
      "\n",
      "Step 17\n",
      "Current state: 33\n",
      "Q-values for current state: [-0.001095   -0.00271    -0.0019     -0.00209765]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 41\n",
      "Reward: -0.01\n",
      "Terminated: True\n",
      "Old Q-value: -0.00271\n",
      "New Q-value: -0.003439\n",
      "\n",
      "Episode 37\n",
      "Starting state: 0\n",
      "\n",
      "Step 0\n",
      "Current state: 0\n",
      "Q-values for current state: [-0.02677021 -0.02225512 -0.02016075 -0.02586113]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 8\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.02225511916148585\n",
      "New Q-value: -0.02252621080143663\n",
      "\n",
      "Step 1\n",
      "Current state: 8\n",
      "Q-values for current state: [-0.02141799 -0.01632559 -0.01575372 -0.02347744]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 8\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.021417989283105288\n",
      "New Q-value: -0.021772793910894126\n",
      "\n",
      "Step 2\n",
      "Current state: 8\n",
      "Q-values for current state: [-0.02177279 -0.01632559 -0.01575372 -0.02347744]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 16\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.016325590747898966\n",
      "New Q-value: -0.016544280498608066\n",
      "\n",
      "Step 3\n",
      "Current state: 16\n",
      "Q-values for current state: [-0.01151824 -0.00896051 -0.01079237 -0.01808893]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 8\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.01808892639639967\n",
      "New Q-value: -0.01877663731285907\n",
      "\n",
      "Step 4\n",
      "Current state: 8\n",
      "Q-values for current state: [-0.02177279 -0.01654428 -0.01575372 -0.02347744]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 9\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.015753721643151218\n",
      "New Q-value: -0.016126777474164918\n",
      "\n",
      "Step 5\n",
      "Current state: 9\n",
      "Q-values for current state: [-0.01922119 -0.00998345 -0.01322123 -0.0115696 ]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 1\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.011569599517900793\n",
      "New Q-value: -0.012649477693510675\n",
      "\n",
      "Step 6\n",
      "Current state: 1\n",
      "Q-values for current state: [-0.02183598 -0.01301935 -0.01344629 -0.01892232]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 0\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.021835983881748827\n",
      "New Q-value: -0.022567656601786813\n",
      "\n",
      "Step 7\n",
      "Current state: 0\n",
      "Q-values for current state: [-0.02677021 -0.02252621 -0.02016075 -0.02586113]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 0\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.02677021153328967\n",
      "New Q-value: -0.027008461488173572\n",
      "\n",
      "Step 8\n",
      "Current state: 0\n",
      "Q-values for current state: [-0.02700846 -0.02252621 -0.02016075 -0.02586113]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 0\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.02586112696665714\n",
      "New Q-value: -0.026190285378204295\n",
      "\n",
      "Step 9\n",
      "Current state: 0\n",
      "Q-values for current state: [-0.02700846 -0.02252621 -0.02016075 -0.02619029]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 8\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.02252621080143663\n",
      "New Q-value: -0.022805633581338636\n",
      "\n",
      "Step 10\n",
      "Current state: 8\n",
      "Q-values for current state: [-0.02177279 -0.01654428 -0.01612678 -0.02347744]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 8\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.021772793910894126\n",
      "New Q-value: -0.02212755837985038\n",
      "\n",
      "Step 11\n",
      "Current state: 8\n",
      "Q-values for current state: [-0.02212756 -0.01654428 -0.01612678 -0.02347744]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 9\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.016126777474164918\n",
      "New Q-value: -0.016462527722077246\n",
      "\n",
      "Step 12\n",
      "Current state: 9\n",
      "Q-values for current state: [-0.01922119 -0.00998345 -0.01322123 -0.01264948]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 17\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.00998345258240865\n",
      "New Q-value: -0.01054597340575341\n",
      "\n",
      "Step 13\n",
      "Current state: 17\n",
      "Q-values for current state: [-0.01128136 -0.00666142 -0.00590385 -0.01253832]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 18\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.005903853490375\n",
      "New Q-value: -0.006671384355172615\n",
      "\n",
      "Step 14\n",
      "Current state: 18\n",
      "Q-values for current state: [-0.00653048 -0.00539753 -0.00468559 -0.00376754]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 26\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.005397531\n",
      "New Q-value: -0.0061152279\n",
      "\n",
      "Step 15\n",
      "Current state: 26\n",
      "Q-values for current state: [-0.00363665 -0.00271    -0.003439   -0.00395937]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 27\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.003439\n",
      "New Q-value: -0.0040951\n",
      "\n",
      "Step 16\n",
      "Current state: 27\n",
      "Q-values for current state: [ 0.     -0.001  -0.0019 -0.0019]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 28\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.0019000000000000002\n",
      "New Q-value: -0.00271\n",
      "\n",
      "Step 17\n",
      "Current state: 28\n",
      "Q-values for current state: [-0.001  0.    -0.001  0.   ]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 36\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: -0.001\n",
      "\n",
      "Step 18\n",
      "Current state: 36\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 37\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: -0.001\n",
      "\n",
      "Step 19\n",
      "Current state: 37\n",
      "Q-values for current state: [ 0.     0.     0.    -0.001]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 29\n",
      "Reward: -0.01\n",
      "Terminated: True\n",
      "Old Q-value: -0.001\n",
      "New Q-value: -0.0019000000000000002\n",
      "\n",
      "Episode 38\n",
      "Starting state: 0\n",
      "\n",
      "Step 0\n",
      "Current state: 0\n",
      "Q-values for current state: [-0.02700846 -0.02280563 -0.02016075 -0.02619029]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 8\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.022805633581338636\n",
      "New Q-value: -0.02308901035680211\n",
      "\n",
      "Step 1\n",
      "Current state: 8\n",
      "Q-values for current state: [-0.02212756 -0.01654428 -0.01646253 -0.02347744]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 16\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.016544280498608066\n",
      "New Q-value: -0.016741101274246255\n",
      "\n",
      "Step 2\n",
      "Current state: 16\n",
      "Q-values for current state: [-0.01151824 -0.00896051 -0.01079237 -0.01877664]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 8\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.01877663731285907\n",
      "New Q-value: -0.0194629137151705\n",
      "\n",
      "Step 3\n",
      "Current state: 8\n",
      "Q-values for current state: [-0.02212756 -0.0167411  -0.01646253 -0.02347744]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 9\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.016462527722077246\n",
      "New Q-value: -0.016818142423416095\n",
      "\n",
      "Step 4\n",
      "Current state: 9\n",
      "Q-values for current state: [-0.01922119 -0.01054597 -0.01322123 -0.01264948]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 17\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.01054597340575341\n",
      "New Q-value: -0.011124210949503069\n",
      "\n",
      "Step 5\n",
      "Current state: 17\n",
      "Q-values for current state: [-0.01128136 -0.00666142 -0.00667138 -0.01253832]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 16\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.011281361473927467\n",
      "New Q-value: -0.012004474152033715\n",
      "\n",
      "Step 6\n",
      "Current state: 16\n",
      "Q-values for current state: [-0.01151824 -0.00896051 -0.01079237 -0.01946291]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 8\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.0194629137151705\n",
      "New Q-value: -0.020107026964706846\n",
      "\n",
      "Step 7\n",
      "Current state: 8\n",
      "Q-values for current state: [-0.02212756 -0.0167411  -0.01681814 -0.02347744]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 0\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.023477443388365157\n",
      "New Q-value: -0.02404497015774151\n",
      "\n",
      "Step 8\n",
      "Current state: 0\n",
      "Q-values for current state: [-0.02700846 -0.02308901 -0.02016075 -0.02619029]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 0\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.026190285378204295\n",
      "New Q-value: -0.026486527948596735\n",
      "\n",
      "Step 9\n",
      "Current state: 0\n",
      "Q-values for current state: [-0.02700846 -0.02308901 -0.02016075 -0.02648653]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 0\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.027008461488173572\n",
      "New Q-value: -0.027222886447569084\n",
      "\n",
      "Step 10\n",
      "Current state: 0\n",
      "Q-values for current state: [-0.02722289 -0.02308901 -0.02016075 -0.02648653]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 0\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.026486527948596735\n",
      "New Q-value: -0.02675314626194993\n",
      "\n",
      "Step 11\n",
      "Current state: 0\n",
      "Q-values for current state: [-0.02722289 -0.02308901 -0.02016075 -0.02675315]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 0\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.027222886447569084\n",
      "New Q-value: -0.027415868911025043\n",
      "\n",
      "Step 12\n",
      "Current state: 0\n",
      "Q-values for current state: [-0.02741587 -0.02308901 -0.02016075 -0.02675315]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 1\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.020160748507503882\n",
      "New Q-value: -0.020381511784153455\n",
      "\n",
      "Step 13\n",
      "Current state: 1\n",
      "Q-values for current state: [-0.02256766 -0.01301935 -0.01344629 -0.01892232]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 1\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.01892231808055893\n",
      "New Q-value: -0.019266924399903\n",
      "\n",
      "Step 14\n",
      "Current state: 1\n",
      "Q-values for current state: [-0.02256766 -0.01301935 -0.01344629 -0.01926692]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 0\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.022567656601786813\n",
      "New Q-value: -0.02324713456110271\n",
      "\n",
      "Step 15\n",
      "Current state: 0\n",
      "Q-values for current state: [-0.02741587 -0.02308901 -0.02038151 -0.02675315]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 1\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.020381511784153455\n",
      "New Q-value: -0.020580198733138072\n",
      "\n",
      "Step 16\n",
      "Current state: 1\n",
      "Q-values for current state: [-0.02324713 -0.01301935 -0.01344629 -0.01926692]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 0\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.02324713456110271\n",
      "New Q-value: -0.023877539984640553\n",
      "\n",
      "Step 17\n",
      "Current state: 0\n",
      "Q-values for current state: [-0.02741587 -0.02308901 -0.0205802  -0.02675315]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 8\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.02308901035680211\n",
      "New Q-value: -0.02337051394217529\n",
      "\n",
      "Step 18\n",
      "Current state: 8\n",
      "Q-values for current state: [-0.02212756 -0.0167411  -0.01681814 -0.02404497]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 0\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.02404497015774151\n",
      "New Q-value: -0.024595592021615476\n",
      "\n",
      "Step 19\n",
      "Current state: 0\n",
      "Q-values for current state: [-0.02741587 -0.02337051 -0.0205802  -0.02675315]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 0\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.027415868911025043\n",
      "New Q-value: -0.027629400899570656\n",
      "\n",
      "Step 20\n",
      "Current state: 0\n",
      "Q-values for current state: [-0.0276294  -0.02337051 -0.0205802  -0.02675315]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 8\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.02337051394217529\n",
      "New Q-value: -0.023623867169011156\n",
      "\n",
      "Step 21\n",
      "Current state: 8\n",
      "Q-values for current state: [-0.02212756 -0.0167411  -0.01681814 -0.02459559]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 9\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.016818142423416095\n",
      "New Q-value: -0.017193128221277276\n",
      "\n",
      "Step 22\n",
      "Current state: 9\n",
      "Q-values for current state: [-0.01922119 -0.01112421 -0.01322123 -0.01264948]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 8\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.0192211930453479\n",
      "New Q-value: -0.019889478361866504\n",
      "\n",
      "Step 23\n",
      "Current state: 8\n",
      "Q-values for current state: [-0.02212756 -0.0167411  -0.01719313 -0.02459559]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 9\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.017193128221277276\n",
      "New Q-value: -0.01753061543935234\n",
      "\n",
      "Step 24\n",
      "Current state: 9\n",
      "Q-values for current state: [-0.01988948 -0.01112421 -0.01322123 -0.01264948]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 10\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.013221234509415921\n",
      "New Q-value: -0.013644670914037363\n",
      "\n",
      "Step 25\n",
      "Current state: 10\n",
      "Q-values for current state: [-0.00965998 -0.00829685 -0.00860912 -0.007848  ]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 11\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.008609117811102913\n",
      "New Q-value: -0.009289262180492622\n",
      "\n",
      "Step 26\n",
      "Current state: 11\n",
      "Q-values for current state: [-0.00622945 -0.00569533 -0.00797617 -0.00822461]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 3\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.008224612937572243\n",
      "New Q-value: -0.009163467692138888\n",
      "\n",
      "Step 27\n",
      "Current state: 3\n",
      "Q-values for current state: [-0.01054475 -0.01017175 -0.00801385 -0.01082609]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 4\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.008013853140251251\n",
      "New Q-value: -0.008615127677476126\n",
      "\n",
      "Step 28\n",
      "Current state: 4\n",
      "Q-values for current state: [-0.00839926 -0.00494304 -0.00423852 -0.00840747]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 5\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.00423852475\n",
      "New Q-value: -0.0052379160375\n",
      "\n",
      "Step 29\n",
      "Current state: 5\n",
      "Q-values for current state: [-0.00490646 -0.00532998 -0.00494304 -0.0044552 ]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 13\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.005329975000000001\n",
      "New Q-value: -0.006063452500000001\n",
      "\n",
      "Step 30\n",
      "Current state: 13\n",
      "Q-values for current state: [-0.002805   -0.00486609 -0.003439   -0.00791036]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 21\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.00486609\n",
      "New Q-value: -0.005474481\n",
      "\n",
      "Step 31\n",
      "Current state: 21\n",
      "Q-values for current state: [-0.00271    -0.00271    -0.00378195 -0.001     ]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 13\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.001\n",
      "New Q-value: -0.002166475\n",
      "\n",
      "Step 32\n",
      "Current state: 13\n",
      "Q-values for current state: [-0.002805   -0.00547448 -0.003439   -0.00791036]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 12\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.002805\n",
      "New Q-value: -0.0038089304631250005\n",
      "\n",
      "Step 33\n",
      "Current state: 12\n",
      "Q-values for current state: [-0.00450897 -0.0040951  -0.00773667 -0.002994  ]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 4\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.002994004875\n",
      "New Q-value: -0.0041641931875\n",
      "\n",
      "Step 34\n",
      "Current state: 4\n",
      "Q-values for current state: [-0.00839926 -0.00494304 -0.00523792 -0.00840747]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 3\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.008399255739521612\n",
      "New Q-value: -0.009377767294929684\n",
      "\n",
      "Step 35\n",
      "Current state: 3\n",
      "Q-values for current state: [-0.01054475 -0.01017175 -0.00861513 -0.01082609]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 4\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.008615127677476126\n",
      "New Q-value: -0.009223203709728513\n",
      "\n",
      "Step 36\n",
      "Current state: 4\n",
      "Q-values for current state: [-0.00937777 -0.00494304 -0.00523792 -0.00840747]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 3\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.009377767294929684\n",
      "New Q-value: -0.010316194917860923\n",
      "\n",
      "Step 37\n",
      "Current state: 3\n",
      "Q-values for current state: [-0.01054475 -0.01017175 -0.0092232  -0.01082609]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 4\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.009223203709728513\n",
      "New Q-value: -0.009770472138755662\n",
      "\n",
      "Step 38\n",
      "Current state: 4\n",
      "Q-values for current state: [-0.01031619 -0.00494304 -0.00523792 -0.00840747]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 12\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.00494304\n",
      "New Q-value: -0.0058377705000000005\n",
      "\n",
      "Step 39\n",
      "Current state: 12\n",
      "Q-values for current state: [-0.00450897 -0.0040951  -0.00773667 -0.00416419]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 20\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.0040951\n",
      "New Q-value: -0.00468559\n",
      "\n",
      "Step 40\n",
      "Current state: 20\n",
      "Q-values for current state: [-0.0019      0.         -0.0020805  -0.00403209]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 19\n",
      "Reward: -0.01\n",
      "Terminated: True\n",
      "Old Q-value: -0.0019000000000000002\n",
      "New Q-value: -0.00271\n",
      "\n",
      "Episode 39\n",
      "Starting state: 0\n",
      "\n",
      "Step 0\n",
      "Current state: 0\n",
      "Q-values for current state: [-0.0276294  -0.02362387 -0.0205802  -0.02675315]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 0\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.027629400899570656\n",
      "New Q-value: -0.027821579689261706\n",
      "\n",
      "Step 1\n",
      "Current state: 0\n",
      "Q-values for current state: [-0.02782158 -0.02362387 -0.0205802  -0.02675315]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 0\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.02675314626194993\n",
      "New Q-value: -0.027032950515403054\n",
      "\n",
      "Step 2\n",
      "Current state: 0\n",
      "Q-values for current state: [-0.02782158 -0.02362387 -0.0205802  -0.02703295]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 0\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.027032950515403054\n",
      "New Q-value: -0.027284774343510863\n",
      "\n",
      "Step 3\n",
      "Current state: 0\n",
      "Q-values for current state: [-0.02782158 -0.02362387 -0.0205802  -0.02728477]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 8\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.023623867169011156\n",
      "New Q-value: -0.023851885073163433\n",
      "\n",
      "Step 4\n",
      "Current state: 8\n",
      "Q-values for current state: [-0.02212756 -0.0167411  -0.01753062 -0.02459559]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 9\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.01753061543935234\n",
      "New Q-value: -0.017834353935619895\n",
      "\n",
      "Step 5\n",
      "Current state: 9\n",
      "Q-values for current state: [-0.01988948 -0.01112421 -0.01364467 -0.01264948]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 17\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.011124210949503069\n",
      "New Q-value: -0.011644624738877762\n",
      "\n",
      "Step 6\n",
      "Current state: 17\n",
      "Q-values for current state: [-0.01200447 -0.00666142 -0.00667138 -0.01253832]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 25\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.006661419835\n",
      "New Q-value: -0.007192925351499999\n",
      "\n",
      "Step 7\n",
      "Current state: 25\n",
      "Q-values for current state: [-0.0069961  -0.0020805  -0.00643449 -0.00472745]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 24\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.0069960996300000005\n",
      "New Q-value: -0.00785019621225\n",
      "\n",
      "Step 8\n",
      "Current state: 24\n",
      "Q-values for current state: [-0.00797998 -0.00793032 -0.00582849 -0.00731617]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 25\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.005828489950000001\n",
      "New Q-value: -0.0064432884550000005\n",
      "\n",
      "Step 9\n",
      "Current state: 25\n",
      "Q-values for current state: [-0.0078502  -0.0020805  -0.00643449 -0.00472745]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 26\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.00643449345\n",
      "New Q-value: -0.007048494105000001\n",
      "\n",
      "Step 10\n",
      "Current state: 26\n",
      "Q-values for current state: [-0.00363665 -0.00271    -0.0040951  -0.00395937]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 18\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.003959366213835115\n",
      "New Q-value: -0.004921345806286719\n",
      "\n",
      "Step 11\n",
      "Current state: 18\n",
      "Q-values for current state: [-0.00653048 -0.00611523 -0.00468559 -0.00376754]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 10\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.0037675390930012147\n",
      "New Q-value: -0.005136345039264127\n",
      "\n",
      "Step 12\n",
      "Current state: 10\n",
      "Q-values for current state: [-0.00965998 -0.00829685 -0.00928926 -0.007848  ]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 9\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.00965997880733743\n",
      "New Q-value: -0.010800220276797074\n",
      "\n",
      "Step 13\n",
      "Current state: 9\n",
      "Q-values for current state: [-0.01988948 -0.01164462 -0.01364467 -0.01264948]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 8\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.019889478361866504\n",
      "New Q-value: -0.02049093514673325\n",
      "\n",
      "Step 14\n",
      "Current state: 8\n",
      "Q-values for current state: [-0.02212756 -0.0167411  -0.01783435 -0.02459559]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 9\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.017834353935619895\n",
      "New Q-value: -0.018157157892251294\n",
      "\n",
      "Step 15\n",
      "Current state: 9\n",
      "Q-values for current state: [-0.02049094 -0.01164462 -0.01364467 -0.01264948]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 10\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.013644670914037363\n",
      "New Q-value: -0.01402576367819666\n",
      "\n",
      "Step 16\n",
      "Current state: 10\n",
      "Q-values for current state: [-0.01080022 -0.00829685 -0.00928926 -0.007848  ]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 2\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.007847998479610879\n",
      "New Q-value: -0.00871830634226959\n",
      "\n",
      "Step 17\n",
      "Current state: 2\n",
      "Q-values for current state: [-0.01535148 -0.00888119 -0.01397697 -0.00689587]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 10\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.008881194397928104\n",
      "New Q-value: -0.00978127587147171\n",
      "\n",
      "Step 18\n",
      "Current state: 10\n",
      "Q-values for current state: [-0.01080022 -0.00829685 -0.00928926 -0.00871831]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 9\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.010800220276797074\n",
      "New Q-value: -0.011826437599310754\n",
      "\n",
      "Step 19\n",
      "Current state: 9\n",
      "Q-values for current state: [-0.02049094 -0.01164462 -0.01402576 -0.01264948]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 1\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.012649477693510675\n",
      "New Q-value: -0.013621368051559569\n",
      "\n",
      "Step 20\n",
      "Current state: 1\n",
      "Q-values for current state: [-0.02387754 -0.01301935 -0.01344629 -0.01926692]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 9\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.013019348709473286\n",
      "New Q-value: -0.013823653188719344\n",
      "\n",
      "Step 21\n",
      "Current state: 9\n",
      "Q-values for current state: [-0.02049094 -0.01164462 -0.01402576 -0.01362137]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 8\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.02049093514673325\n",
      "New Q-value: -0.02103224625311332\n",
      "\n",
      "Step 22\n",
      "Current state: 8\n",
      "Q-values for current state: [-0.02212756 -0.0167411  -0.01815716 -0.02459559]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 16\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.016741101274246255\n",
      "New Q-value: -0.016918239972320625\n",
      "\n",
      "Step 23\n",
      "Current state: 16\n",
      "Q-values for current state: [-0.01151824 -0.00896051 -0.01079237 -0.02010703]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 8\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.020107026964706846\n",
      "New Q-value: -0.02070355706560662\n",
      "\n",
      "Step 24\n",
      "Current state: 8\n",
      "Q-values for current state: [-0.02212756 -0.01691824 -0.01815716 -0.02459559]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 8\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.02212755837985038\n",
      "New Q-value: -0.022522035339235802\n",
      "\n",
      "Step 25\n",
      "Current state: 8\n",
      "Q-values for current state: [-0.02252204 -0.01691824 -0.01815716 -0.02459559]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 0\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.024595592021615476\n",
      "New Q-value: -0.025091151699102046\n",
      "\n",
      "Step 26\n",
      "Current state: 0\n",
      "Q-values for current state: [-0.02782158 -0.02385189 -0.0205802  -0.02728477]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 0\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.027821579689261706\n",
      "New Q-value: -0.02799454059998365\n",
      "\n",
      "Step 27\n",
      "Current state: 0\n",
      "Q-values for current state: [-0.02799454 -0.02385189 -0.0205802  -0.02728477]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 1\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.020580198733138072\n",
      "New Q-value: -0.020799576265398263\n",
      "\n",
      "Step 28\n",
      "Current state: 1\n",
      "Q-values for current state: [-0.02387754 -0.01382365 -0.01344629 -0.01926692]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 1\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.019266924399903\n",
      "New Q-value: -0.019617629365486697\n",
      "\n",
      "Step 29\n",
      "Current state: 1\n",
      "Q-values for current state: [-0.02387754 -0.01382365 -0.01344629 -0.01961763]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 0\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.023877539984640553\n",
      "New Q-value: -0.02446574573138933\n",
      "\n",
      "Step 30\n",
      "Current state: 0\n",
      "Q-values for current state: [-0.02799454 -0.02385189 -0.02079958 -0.02728477]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 8\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.023851885073163433\n",
      "New Q-value: -0.02407392936321755\n",
      "\n",
      "Step 31\n",
      "Current state: 8\n",
      "Q-values for current state: [-0.02252204 -0.01691824 -0.01815716 -0.02509115]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 9\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.018157157892251294\n",
      "New Q-value: -0.018447681453219553\n",
      "\n",
      "Step 32\n",
      "Current state: 9\n",
      "Q-values for current state: [-0.02103225 -0.01164462 -0.01402576 -0.01362137]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 1\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.013621368051559569\n",
      "New Q-value: -0.01453662865197761\n",
      "\n",
      "Step 33\n",
      "Current state: 1\n",
      "Q-values for current state: [-0.02446575 -0.01382365 -0.01344629 -0.01961763]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 9\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.013823653188719344\n",
      "New Q-value: -0.014547527220040797\n",
      "\n",
      "Step 34\n",
      "Current state: 9\n",
      "Q-values for current state: [-0.02103225 -0.01164462 -0.01402576 -0.01453663]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 8\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.02103224625311332\n",
      "New Q-value: -0.021536254425172447\n",
      "\n",
      "Step 35\n",
      "Current state: 8\n",
      "Q-values for current state: [-0.02252204 -0.01691824 -0.01844768 -0.02509115]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 0\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.025091151699102046\n",
      "New Q-value: -0.025557996274404676\n",
      "\n",
      "Step 36\n",
      "Current state: 0\n",
      "Q-values for current state: [-0.02799454 -0.02407393 -0.02079958 -0.02728477]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 0\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.027284774343510863\n",
      "New Q-value: -0.027532256654372613\n",
      "\n",
      "Step 37\n",
      "Current state: 0\n",
      "Q-values for current state: [-0.02799454 -0.02407393 -0.02079958 -0.02753226]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 1\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.020799576265398263\n",
      "New Q-value: -0.020997016044432434\n",
      "\n",
      "Step 38\n",
      "Current state: 1\n",
      "Q-values for current state: [-0.02446575 -0.01454753 -0.01344629 -0.01961763]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 0\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.02446574573138933\n",
      "New Q-value: -0.02501388768247148\n",
      "\n",
      "Step 39\n",
      "Current state: 0\n",
      "Q-values for current state: [-0.02799454 -0.02407393 -0.02099702 -0.02753226]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 1\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.020997016044432434\n",
      "New Q-value: -0.021174711845563188\n",
      "\n",
      "Step 40\n",
      "Current state: 1\n",
      "Q-values for current state: [-0.02501389 -0.01454753 -0.01344629 -0.01961763]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 9\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.014547527220040797\n",
      "New Q-value: -0.015199013848230105\n",
      "\n",
      "Step 41\n",
      "Current state: 9\n",
      "Q-values for current state: [-0.02153625 -0.01164462 -0.01402576 -0.01453663]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 1\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.01453662865197761\n",
      "New Q-value: -0.015360363192353847\n",
      "\n",
      "Step 42\n",
      "Current state: 1\n",
      "Q-values for current state: [-0.02501389 -0.01519901 -0.01344629 -0.01961763]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 0\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.02501388768247148\n",
      "New Q-value: -0.025524096539552836\n",
      "\n",
      "Step 43\n",
      "Current state: 0\n",
      "Q-values for current state: [-0.02799454 -0.02407393 -0.02117471 -0.02753226]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 0\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.027532256654372613\n",
      "New Q-value: -0.027790628614263853\n",
      "\n",
      "Step 44\n",
      "Current state: 0\n",
      "Q-values for current state: [-0.02799454 -0.02407393 -0.02117471 -0.02779063]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 0\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.02799454059998365\n",
      "New Q-value: -0.02820668416531379\n",
      "\n",
      "Step 45\n",
      "Current state: 0\n",
      "Q-values for current state: [-0.02820668 -0.02407393 -0.02117471 -0.02779063]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 0\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.027790628614263853\n",
      "New Q-value: -0.02802316337816597\n",
      "\n",
      "Step 46\n",
      "Current state: 0\n",
      "Q-values for current state: [-0.02820668 -0.02407393 -0.02117471 -0.02802316]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 1\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.021174711845563188\n",
      "New Q-value: -0.021334638066580865\n",
      "\n",
      "Step 47\n",
      "Current state: 1\n",
      "Q-values for current state: [-0.0255241  -0.01519901 -0.01344629 -0.01961763]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 9\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.015199013848230105\n",
      "New Q-value: -0.01578535181360048\n",
      "\n",
      "Step 48\n",
      "Current state: 9\n",
      "Q-values for current state: [-0.02153625 -0.01164462 -0.01402576 -0.01536036]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 10\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.01402576367819666\n",
      "New Q-value: -0.01441138822371341\n",
      "\n",
      "Step 49\n",
      "Current state: 10\n",
      "Q-values for current state: [-0.01182644 -0.00829685 -0.00928926 -0.00871831]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 9\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.011826437599310754\n",
      "New Q-value: -0.012750033189573066\n",
      "\n",
      "Step 50\n",
      "Current state: 9\n",
      "Q-values for current state: [-0.02153625 -0.01164462 -0.01441139 -0.01536036]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 10\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.01441138822371341\n",
      "New Q-value: -0.014758450314678484\n",
      "\n",
      "Step 51\n",
      "Current state: 10\n",
      "Q-values for current state: [-0.01275003 -0.00829685 -0.00928926 -0.00871831]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 11\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.009289262180492622\n",
      "New Q-value: -0.00990139211294336\n",
      "\n",
      "Step 52\n",
      "Current state: 11\n",
      "Q-values for current state: [-0.00622945 -0.00569533 -0.00797617 -0.00916347]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 19\n",
      "Reward: -0.01\n",
      "Terminated: True\n",
      "Old Q-value: -0.0056953279\n",
      "New Q-value: -0.00612579511\n",
      "\n",
      "Episode 40\n",
      "Starting state: 0\n",
      "\n",
      "Step 0\n",
      "Current state: 0\n",
      "Q-values for current state: [-0.02820668 -0.02407393 -0.02133464 -0.02802316]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 1\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.021334638066580865\n",
      "New Q-value: -0.021478571665496778\n",
      "\n",
      "Step 1\n",
      "Current state: 1\n",
      "Q-values for current state: [-0.0255241  -0.01578535 -0.01344629 -0.01961763]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 2\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.013446288479726289\n",
      "New Q-value: -0.013756767342373459\n",
      "\n",
      "Step 2\n",
      "Current state: 2\n",
      "Q-values for current state: [-0.01535148 -0.00978128 -0.01397697 -0.00689587]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 1\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.015351483873188278\n",
      "New Q-value: -0.01612322838339493\n",
      "\n",
      "Step 3\n",
      "Current state: 1\n",
      "Q-values for current state: [-0.0255241  -0.01578535 -0.01375677 -0.01961763]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 0\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.025524096539552836\n",
      "New Q-value: -0.026012151193819746\n",
      "\n",
      "Step 4\n",
      "Current state: 0\n",
      "Q-values for current state: [-0.02820668 -0.02407393 -0.02147857 -0.02802316]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 0\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.02802316337816597\n",
      "New Q-value: -0.028261311348571568\n",
      "\n",
      "Step 5\n",
      "Current state: 0\n",
      "Q-values for current state: [-0.02820668 -0.02407393 -0.02147857 -0.02826131]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 0\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.028261311348571568\n",
      "New Q-value: -0.028475644521936606\n",
      "\n",
      "Step 6\n",
      "Current state: 0\n",
      "Q-values for current state: [-0.02820668 -0.02407393 -0.02147857 -0.02847564]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 8\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.02407392936321755\n",
      "New Q-value: -0.024273769224266255\n",
      "\n",
      "Step 7\n",
      "Current state: 8\n",
      "Q-values for current state: [-0.02252204 -0.01691824 -0.01844768 -0.025558  ]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 0\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.025557996274404676\n",
      "New Q-value: -0.0260426609551864\n",
      "\n",
      "Step 8\n",
      "Current state: 0\n",
      "Q-values for current state: [-0.02820668 -0.02427377 -0.02147857 -0.02847564]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 0\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.028475644521936606\n",
      "New Q-value: -0.02866854437796514\n",
      "\n",
      "Step 9\n",
      "Current state: 0\n",
      "Q-values for current state: [-0.02820668 -0.02427377 -0.02147857 -0.02866854]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 1\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.021478571665496778\n",
      "New Q-value: -0.02163760739647258\n",
      "\n",
      "Step 10\n",
      "Current state: 1\n",
      "Q-values for current state: [-0.02601215 -0.01578535 -0.01375677 -0.01961763]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 2\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.013756767342373459\n",
      "New Q-value: -0.014036198318755912\n",
      "\n",
      "Step 11\n",
      "Current state: 2\n",
      "Q-values for current state: [-0.01612323 -0.00978128 -0.01397697 -0.00689587]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 1\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.01612322838339493\n",
      "New Q-value: -0.01684434438533725\n",
      "\n",
      "Step 12\n",
      "Current state: 1\n",
      "Q-values for current state: [-0.02601215 -0.01578535 -0.0140362  -0.01961763]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 9\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.01578535181360048\n",
      "New Q-value: -0.01631305598243382\n",
      "\n",
      "Step 13\n",
      "Current state: 9\n",
      "Q-values for current state: [-0.02153625 -0.01164462 -0.01475845 -0.01536036]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 1\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.015360363192353847\n",
      "New Q-value: -0.016157765713400274\n",
      "\n",
      "Step 14\n",
      "Current state: 1\n",
      "Q-values for current state: [-0.02601215 -0.01631306 -0.0140362  -0.01961763]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 2\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.014036198318755912\n",
      "New Q-value: -0.01428768619750012\n",
      "\n",
      "Step 15\n",
      "Current state: 2\n",
      "Q-values for current state: [-0.01684434 -0.00978128 -0.01397697 -0.00689587]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 2\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.0068958706381031425\n",
      "New Q-value: -0.007861391284912627\n",
      "\n",
      "Step 16\n",
      "Current state: 2\n",
      "Q-values for current state: [-0.01684434 -0.00978128 -0.01397697 -0.00786139]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 1\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.01684434438533725\n",
      "New Q-value: -0.017517240135566034\n",
      "\n",
      "Step 17\n",
      "Current state: 1\n",
      "Q-values for current state: [-0.02601215 -0.01631306 -0.01428769 -0.01961763]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 1\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.019617629365486697\n",
      "New Q-value: -0.02001319661770054\n",
      "\n",
      "Step 18\n",
      "Current state: 1\n",
      "Q-values for current state: [-0.02601215 -0.01631306 -0.01428769 -0.0200132 ]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 9\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.01631305598243382\n",
      "New Q-value: -0.016787989734383828\n",
      "\n",
      "Step 19\n",
      "Current state: 9\n",
      "Q-values for current state: [-0.02153625 -0.01164462 -0.01475845 -0.01615777]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 17\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.011644624738877762\n",
      "New Q-value: -0.012113943778731383\n",
      "\n",
      "Step 20\n",
      "Current state: 17\n",
      "Q-values for current state: [-0.01200447 -0.00719293 -0.00667138 -0.01253832]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 16\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.012004474152033715\n",
      "New Q-value: -0.012655275562329338\n",
      "\n",
      "Step 21\n",
      "Current state: 16\n",
      "Q-values for current state: [-0.01151824 -0.00896051 -0.01079237 -0.02070356]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 17\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.010792366513653309\n",
      "New Q-value: -0.011346911376029375\n",
      "\n",
      "Step 22\n",
      "Current state: 17\n",
      "Q-values for current state: [-0.01265528 -0.00719293 -0.00667138 -0.01253832]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 18\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.006671384355172615\n",
      "New Q-value: -0.007449376969655353\n",
      "\n",
      "Step 23\n",
      "Current state: 18\n",
      "Q-values for current state: [-0.00653048 -0.00611523 -0.00468559 -0.00513635]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 19\n",
      "Reward: -0.01\n",
      "Terminated: True\n",
      "Old Q-value: -0.00468559\n",
      "New Q-value: -0.005217031\n",
      "\n",
      "Episode 40 summary:\n",
      "Total reward: -0.24000000000000007\n",
      "Successful episodes so far: []\n",
      "Average reward over last 10 episodes: -0.281\n",
      "\n",
      "Episode 41\n",
      "Starting state: 0\n",
      "\n",
      "Step 0\n",
      "Current state: 0\n",
      "Q-values for current state: [-0.02820668 -0.02427377 -0.02163761 -0.02866854]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 1\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.02163760739647258\n",
      "New Q-value: -0.021831176845587833\n",
      "\n",
      "Step 1\n",
      "Current state: 1\n",
      "Q-values for current state: [-0.02601215 -0.01678799 -0.01428769 -0.0200132 ]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 1\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.02001319661770054\n",
      "New Q-value: -0.020369207144692997\n",
      "\n",
      "Step 2\n",
      "Current state: 1\n",
      "Q-values for current state: [-0.02601215 -0.01678799 -0.01428769 -0.02036921]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 2\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.01428768619750012\n",
      "New Q-value: -0.014605749749816807\n",
      "\n",
      "Step 3\n",
      "Current state: 2\n",
      "Q-values for current state: [-0.01751724 -0.00978128 -0.01397697 -0.00786139]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 3\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.013976972894083126\n",
      "New Q-value: -0.014507470457856601\n",
      "\n",
      "Step 4\n",
      "Current state: 3\n",
      "Q-values for current state: [-0.01054475 -0.01017175 -0.00977047 -0.01082609]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 2\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.010544754470474597\n",
      "New Q-value: -0.011237111195493837\n",
      "\n",
      "Step 5\n",
      "Current state: 2\n",
      "Q-values for current state: [-0.01751724 -0.00978128 -0.01450747 -0.00786139]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 1\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.017517240135566034\n",
      "New Q-value: -0.018153062348242027\n",
      "\n",
      "Step 6\n",
      "Current state: 1\n",
      "Q-values for current state: [-0.02601215 -0.01678799 -0.01460575 -0.02036921]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 2\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.014605749749816807\n",
      "New Q-value: -0.014892006946901825\n",
      "\n",
      "Step 7\n",
      "Current state: 2\n",
      "Q-values for current state: [-0.01815306 -0.00978128 -0.01450747 -0.00786139]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 3\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.014507470457856601\n",
      "New Q-value: -0.01498491826525273\n",
      "\n",
      "Step 8\n",
      "Current state: 3\n",
      "Q-values for current state: [-0.01123711 -0.01017175 -0.00977047 -0.01082609]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 2\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.011237111195493837\n",
      "New Q-value: -0.011860232248011153\n",
      "\n",
      "Step 9\n",
      "Current state: 2\n",
      "Q-values for current state: [-0.01815306 -0.00978128 -0.01498492 -0.00786139]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 2\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.007861391284912627\n",
      "New Q-value: -0.008822084328488064\n",
      "\n",
      "Step 10\n",
      "Current state: 2\n",
      "Q-values for current state: [-0.01815306 -0.00978128 -0.01498492 -0.00882208]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 10\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.00978127587147171\n",
      "New Q-value: -0.010591349197660953\n",
      "\n",
      "Step 11\n",
      "Current state: 10\n",
      "Q-values for current state: [-0.01275003 -0.00829685 -0.00990139 -0.00871831]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 9\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.012750033189573066\n",
      "New Q-value: -0.01362585452959524\n",
      "\n",
      "Step 12\n",
      "Current state: 9\n",
      "Q-values for current state: [-0.02153625 -0.01211394 -0.01475845 -0.01615777]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 1\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.016157765713400274\n",
      "New Q-value: -0.01695672980201592\n",
      "\n",
      "Step 13\n",
      "Current state: 1\n",
      "Q-values for current state: [-0.02601215 -0.01678799 -0.01489201 -0.02036921]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 0\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.026012151193819746\n",
      "New Q-value: -0.026484897874768615\n",
      "\n",
      "Step 14\n",
      "Current state: 0\n",
      "Q-values for current state: [-0.02820668 -0.02427377 -0.02183118 -0.02866854]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 8\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.024273769224266255\n",
      "New Q-value: -0.02445362509921009\n",
      "\n",
      "Step 15\n",
      "Current state: 8\n",
      "Q-values for current state: [-0.02252204 -0.01691824 -0.01844768 -0.02604266]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 0\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.0260426609551864\n",
      "New Q-value: -0.026512356659998605\n",
      "\n",
      "Step 16\n",
      "Current state: 0\n",
      "Q-values for current state: [-0.02820668 -0.02445363 -0.02183118 -0.02866854]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 0\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.02866854437796514\n",
      "New Q-value: -0.02887565174049947\n",
      "\n",
      "Step 17\n",
      "Current state: 0\n",
      "Q-values for current state: [-0.02820668 -0.02445363 -0.02183118 -0.02887565]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 0\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.02887565174049947\n",
      "New Q-value: -0.029062048366780367\n",
      "\n",
      "Step 18\n",
      "Current state: 0\n",
      "Q-values for current state: [-0.02820668 -0.02445363 -0.02183118 -0.02906205]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 0\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.029062048366780367\n",
      "New Q-value: -0.029229805330433175\n",
      "\n",
      "Step 19\n",
      "Current state: 0\n",
      "Q-values for current state: [-0.02820668 -0.02445363 -0.02183118 -0.02922981]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 8\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.02445362509921009\n",
      "New Q-value: -0.024615495386659542\n",
      "\n",
      "Step 20\n",
      "Current state: 8\n",
      "Q-values for current state: [-0.02252204 -0.01691824 -0.01844768 -0.02651236]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 0\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.026512356659998605\n",
      "New Q-value: -0.026935082794329587\n",
      "\n",
      "Step 21\n",
      "Current state: 0\n",
      "Q-values for current state: [-0.02820668 -0.0246155  -0.02183118 -0.02922981]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 1\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.021831176845587833\n",
      "New Q-value: -0.022062799820984722\n",
      "\n",
      "Step 22\n",
      "Current state: 1\n",
      "Q-values for current state: [-0.0264849  -0.01678799 -0.01489201 -0.02036921]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 2\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.014892006946901825\n",
      "New Q-value: -0.01524090426341801\n",
      "\n",
      "Step 23\n",
      "Current state: 2\n",
      "Q-values for current state: [-0.01815306 -0.01059135 -0.01498492 -0.00882208]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 3\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.01498491826525273\n",
      "New Q-value: -0.015414621291909245\n",
      "\n",
      "Step 24\n",
      "Current state: 3\n",
      "Q-values for current state: [-0.01186023 -0.01017175 -0.00977047 -0.01082609]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 3\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.010826085144937162\n",
      "New Q-value: -0.011671671483625233\n",
      "\n",
      "Step 25\n",
      "Current state: 3\n",
      "Q-values for current state: [-0.01186023 -0.01017175 -0.00977047 -0.01167167]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 2\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.011860232248011153\n",
      "New Q-value: -0.012512307034416404\n",
      "\n",
      "Step 26\n",
      "Current state: 2\n",
      "Q-values for current state: [-0.01815306 -0.01059135 -0.01541462 -0.00882208]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 10\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.010591349197660953\n",
      "New Q-value: -0.011320415191231274\n",
      "\n",
      "Step 27\n",
      "Current state: 10\n",
      "Q-values for current state: [-0.01362585 -0.00829685 -0.00990139 -0.00871831]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 9\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.01362585452959524\n",
      "New Q-value: -0.014414093735615198\n",
      "\n",
      "Step 28\n",
      "Current state: 9\n",
      "Q-values for current state: [-0.02153625 -0.01211394 -0.01475845 -0.01695673]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 1\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.01695672980201592\n",
      "New Q-value: -0.01770894272683904\n",
      "\n",
      "Step 29\n",
      "Current state: 1\n",
      "Q-values for current state: [-0.0264849  -0.01678799 -0.0152409  -0.02036921]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 0\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.026484897874768615\n",
      "New Q-value: -0.0269323740702853\n",
      "\n",
      "Step 30\n",
      "Current state: 0\n",
      "Q-values for current state: [-0.02820668 -0.0246155  -0.0220628  -0.02922981]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 0\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.02820668416531379\n",
      "New Q-value: -0.02848198173177596\n",
      "\n",
      "Step 31\n",
      "Current state: 0\n",
      "Q-values for current state: [-0.02848198 -0.0246155  -0.0220628  -0.02922981]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 0\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.02848198173177596\n",
      "New Q-value: -0.028729749541591913\n",
      "\n",
      "Step 32\n",
      "Current state: 0\n",
      "Q-values for current state: [-0.02872975 -0.0246155  -0.0220628  -0.02922981]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 1\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.022062799820984722\n",
      "New Q-value: -0.02230440574391096\n",
      "\n",
      "Step 33\n",
      "Current state: 1\n",
      "Q-values for current state: [-0.02693237 -0.01678799 -0.0152409  -0.02036921]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 9\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.016787989734383828\n",
      "New Q-value: -0.017260015419924925\n",
      "\n",
      "Step 34\n",
      "Current state: 9\n",
      "Q-values for current state: [-0.02153625 -0.01211394 -0.01475845 -0.01770894]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 17\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.012113943778731383\n",
      "New Q-value: -0.012585877309250745\n",
      "\n",
      "Step 35\n",
      "Current state: 17\n",
      "Q-values for current state: [-0.01265528 -0.00719293 -0.00744938 -0.01253832]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 16\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.012655275562329338\n",
      "New Q-value: -0.0132409968315954\n",
      "\n",
      "Step 36\n",
      "Current state: 16\n",
      "Q-values for current state: [-0.01151824 -0.00896051 -0.01134691 -0.02070356]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 8\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.02070355706560662\n",
      "New Q-value: -0.02124043415641642\n",
      "\n",
      "Step 37\n",
      "Current state: 8\n",
      "Q-values for current state: [-0.02252204 -0.01691824 -0.01844768 -0.02693508]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 9\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.018447681453219553\n",
      "New Q-value: -0.018798571652276417\n",
      "\n",
      "Step 38\n",
      "Current state: 9\n",
      "Q-values for current state: [-0.02153625 -0.01258588 -0.01475845 -0.01770894]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 8\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.021536254425172447\n",
      "New Q-value: -0.02198986178002566\n",
      "\n",
      "Step 39\n",
      "Current state: 8\n",
      "Q-values for current state: [-0.02252204 -0.01691824 -0.01879857 -0.02693508]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 16\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.016918239972320625\n",
      "New Q-value: -0.017077664800587557\n",
      "\n",
      "Step 40\n",
      "Current state: 16\n",
      "Q-values for current state: [-0.01151824 -0.00896051 -0.01134691 -0.02124043]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 8\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.02124043415641642\n",
      "New Q-value: -0.021738768896830594\n",
      "\n",
      "Step 41\n",
      "Current state: 8\n",
      "Q-values for current state: [-0.02252204 -0.01707766 -0.01879857 -0.02693508]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 8\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.022522035339235802\n",
      "New Q-value: -0.02289220996136804\n",
      "\n",
      "Step 42\n",
      "Current state: 8\n",
      "Q-values for current state: [-0.02289221 -0.01707766 -0.01879857 -0.02693508]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 8\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.02289220996136804\n",
      "New Q-value: -0.023225367121287054\n",
      "\n",
      "Step 43\n",
      "Current state: 8\n",
      "Q-values for current state: [-0.02322537 -0.01707766 -0.01879857 -0.02693508]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 16\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.017077664800587557\n",
      "New Q-value: -0.017221147146027797\n",
      "\n",
      "Step 44\n",
      "Current state: 16\n",
      "Q-values for current state: [-0.01151824 -0.00896051 -0.01134691 -0.02173877]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 16\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.011518242469081091\n",
      "New Q-value: -0.012217667047671977\n",
      "\n",
      "Step 45\n",
      "Current state: 16\n",
      "Q-values for current state: [-0.01221767 -0.00896051 -0.01134691 -0.02173877]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 8\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.021738768896830594\n",
      "New Q-value: -0.022200900986020174\n",
      "\n",
      "Step 46\n",
      "Current state: 8\n",
      "Q-values for current state: [-0.02322537 -0.01722115 -0.01879857 -0.02693508]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 16\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.017221147146027797\n",
      "New Q-value: -0.017350281256924012\n",
      "\n",
      "Step 47\n",
      "Current state: 16\n",
      "Q-values for current state: [-0.01221767 -0.00896051 -0.01134691 -0.0222009 ]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 17\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.011346911376029375\n",
      "New Q-value: -0.011895548146818938\n",
      "\n",
      "Step 48\n",
      "Current state: 17\n",
      "Q-values for current state: [-0.013241   -0.00719293 -0.00744938 -0.01253832]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 16\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.0132409968315954\n",
      "New Q-value: -0.013768145973934855\n",
      "\n",
      "Step 49\n",
      "Current state: 16\n",
      "Q-values for current state: [-0.01221767 -0.00896051 -0.01189555 -0.0222009 ]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 17\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.011895548146818938\n",
      "New Q-value: -0.012389321240529544\n",
      "\n",
      "Step 50\n",
      "Current state: 17\n",
      "Q-values for current state: [-0.01376815 -0.00719293 -0.00744938 -0.01253832]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 9\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.012538315853802207\n",
      "New Q-value: -0.013480142612800807\n",
      "\n",
      "Step 51\n",
      "Current state: 9\n",
      "Q-values for current state: [-0.02198986 -0.01258588 -0.01475845 -0.01770894]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 17\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.012585877309250745\n",
      "New Q-value: -0.013010617486718171\n",
      "\n",
      "Step 52\n",
      "Current state: 17\n",
      "Q-values for current state: [-0.01376815 -0.00719293 -0.00744938 -0.01348014]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 16\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.013768145973934855\n",
      "New Q-value: -0.014242580202040365\n",
      "\n",
      "Step 53\n",
      "Current state: 16\n",
      "Q-values for current state: [-0.01221767 -0.00896051 -0.01238932 -0.0222009 ]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 17\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.012389321240529544\n",
      "New Q-value: -0.012833717024869089\n",
      "\n",
      "Step 54\n",
      "Current state: 17\n",
      "Q-values for current state: [-0.01424258 -0.00719293 -0.00744938 -0.01348014]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 9\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.013480142612800807\n",
      "New Q-value: -0.014368137012758953\n",
      "\n",
      "Step 55\n",
      "Current state: 9\n",
      "Q-values for current state: [-0.02198986 -0.01301062 -0.01475845 -0.01770894]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 1\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.01770894272683904\n",
      "New Q-value: -0.018385934359179845\n",
      "\n",
      "Step 56\n",
      "Current state: 1\n",
      "Q-values for current state: [-0.02693237 -0.01726002 -0.0152409  -0.02036921]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 1\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.020369207144692997\n",
      "New Q-value: -0.02078017233524841\n",
      "\n",
      "Step 57\n",
      "Current state: 1\n",
      "Q-values for current state: [-0.02693237 -0.01726002 -0.0152409  -0.02078017]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 2\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.01524090426341801\n",
      "New Q-value: -0.015554911848282574\n",
      "\n",
      "Step 58\n",
      "Current state: 2\n",
      "Q-values for current state: [-0.01815306 -0.01132042 -0.01541462 -0.00882208]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 3\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.015414621291909245\n",
      "New Q-value: -0.015801354015900108\n",
      "\n",
      "Step 59\n",
      "Current state: 3\n",
      "Q-values for current state: [-0.01251231 -0.01017175 -0.00977047 -0.01167167]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 4\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.009770472138755662\n",
      "New Q-value: -0.010291026948442595\n",
      "\n",
      "Step 60\n",
      "Current state: 4\n",
      "Q-values for current state: [-0.01031619 -0.00583777 -0.00523792 -0.00840747]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 3\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.010316194917860923\n",
      "New Q-value: -0.011250891337660619\n",
      "\n",
      "Step 61\n",
      "Current state: 3\n",
      "Q-values for current state: [-0.01251231 -0.01017175 -0.01029103 -0.01167167]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 2\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.012512307034416404\n",
      "New Q-value: -0.01309917434218113\n",
      "\n",
      "Step 62\n",
      "Current state: 2\n",
      "Q-values for current state: [-0.01815306 -0.01132042 -0.01580135 -0.00882208]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 3\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.015801354015900108\n",
      "New Q-value: -0.016187534525895886\n",
      "\n",
      "Step 63\n",
      "Current state: 3\n",
      "Q-values for current state: [-0.01309917 -0.01017175 -0.01029103 -0.01167167]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 2\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.01309917434218113\n",
      "New Q-value: -0.013627354919169384\n",
      "\n",
      "Step 64\n",
      "Current state: 2\n",
      "Q-values for current state: [-0.01815306 -0.01132042 -0.01618753 -0.00882208]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 1\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.018153062348242027\n",
      "New Q-value: -0.01881547273900467\n",
      "\n",
      "Step 65\n",
      "Current state: 1\n",
      "Q-values for current state: [-0.02693237 -0.01726002 -0.01555491 -0.02078017]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 1\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.02078017233524841\n",
      "New Q-value: -0.021179871727310413\n",
      "\n",
      "Step 66\n",
      "Current state: 1\n",
      "Q-values for current state: [-0.02693237 -0.01726002 -0.01555491 -0.02117987]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 1\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.021179871727310413\n",
      "New Q-value: -0.021539601180166217\n",
      "\n",
      "Step 67\n",
      "Current state: 1\n",
      "Q-values for current state: [-0.02693237 -0.01726002 -0.01555491 -0.0215396 ]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 0\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.0269323740702853\n",
      "New Q-value: -0.02735805520892831\n",
      "\n",
      "Step 68\n",
      "Current state: 0\n",
      "Q-values for current state: [-0.02872975 -0.0246155  -0.02230441 -0.02922981]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 8\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.024615495386659542\n",
      "New Q-value: -0.024802222567401368\n",
      "\n",
      "Step 69\n",
      "Current state: 8\n",
      "Q-values for current state: [-0.02322537 -0.01735028 -0.01879857 -0.02693508]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 16\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.017350281256924012\n",
      "New Q-value: -0.017466501956730607\n",
      "\n",
      "Step 70\n",
      "Current state: 16\n",
      "Q-values for current state: [-0.01221767 -0.00896051 -0.01283372 -0.0222009 ]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 24\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.008960513952621\n",
      "New Q-value: -0.0096765749605839\n",
      "\n",
      "Step 71\n",
      "Current state: 24\n",
      "Q-values for current state: [-0.00797998 -0.00793032 -0.00644329 -0.00731617]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 32\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.0079303232146\n",
      "New Q-value: -0.00847302089314\n",
      "\n",
      "Step 72\n",
      "Current state: 32\n",
      "Q-values for current state: [-0.00713155 -0.00554374 -0.003534   -0.0038589 ]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 40\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.0055437360000000005\n",
      "New Q-value: -0.0060843624\n",
      "\n",
      "Step 73\n",
      "Current state: 40\n",
      "Q-values for current state: [-0.00369645 -0.003534   -0.001      -0.00323212]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 48\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.0035340000000000002\n",
      "New Q-value: -0.0042756\n",
      "\n",
      "Step 74\n",
      "Current state: 48\n",
      "Q-values for current state: [-0.0019   -0.001    -0.0019   -0.001095]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 56\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.001\n",
      "New Q-value: -0.0019000000000000002\n",
      "\n",
      "Step 75\n",
      "Current state: 56\n",
      "Q-values for current state: [ 0.     0.    -0.001  0.   ]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 57\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.001\n",
      "New Q-value: -0.0019000000000000002\n",
      "\n",
      "Step 76\n",
      "Current state: 57\n",
      "Q-values for current state: [ 0.     -0.0019 -0.001   0.    ]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 58\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.001\n",
      "New Q-value: -0.0019000000000000002\n",
      "\n",
      "Step 77\n",
      "Current state: 58\n",
      "Q-values for current state: [ 0.     0.    -0.001  0.   ]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 58\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: -0.001\n",
      "\n",
      "Step 78\n",
      "Current state: 58\n",
      "Q-values for current state: [ 0.    -0.001 -0.001  0.   ]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 58\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.001\n",
      "New Q-value: -0.0019000000000000002\n",
      "\n",
      "Step 79\n",
      "Current state: 58\n",
      "Q-values for current state: [ 0.     -0.0019 -0.001   0.    ]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 57\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: -0.001\n",
      "\n",
      "Step 80\n",
      "Current state: 57\n",
      "Q-values for current state: [ 0.     -0.0019 -0.0019  0.    ]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 56\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: -0.001\n",
      "\n",
      "Step 81\n",
      "Current state: 56\n",
      "Q-values for current state: [ 0.      0.     -0.0019  0.    ]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 56\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: -0.001\n",
      "\n",
      "Step 82\n",
      "Current state: 56\n",
      "Q-values for current state: [ 0.     -0.001  -0.0019  0.    ]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 56\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: -0.001\n",
      "\n",
      "Step 83\n",
      "Current state: 56\n",
      "Q-values for current state: [-0.001  -0.001  -0.0019  0.    ]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 57\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.0019000000000000002\n",
      "New Q-value: -0.00271\n",
      "\n",
      "Step 84\n",
      "Current state: 57\n",
      "Q-values for current state: [-0.001  -0.0019 -0.0019  0.    ]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 56\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.001\n",
      "New Q-value: -0.0019000000000000002\n",
      "\n",
      "Step 85\n",
      "Current state: 56\n",
      "Q-values for current state: [-0.001   -0.001   -0.00271  0.     ]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 57\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.00271\n",
      "New Q-value: -0.003439\n",
      "\n",
      "Step 86\n",
      "Current state: 57\n",
      "Q-values for current state: [-0.0019 -0.0019 -0.0019  0.    ]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 49\n",
      "Reward: -0.01\n",
      "Terminated: True\n",
      "Old Q-value: 0.0\n",
      "New Q-value: -0.001\n",
      "\n",
      "Episode 42\n",
      "Starting state: 0\n",
      "\n",
      "Step 0\n",
      "Current state: 0\n",
      "Q-values for current state: [-0.02872975 -0.02480222 -0.02230441 -0.02922981]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 1\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.02230440574391096\n",
      "New Q-value: -0.022551681795106707\n",
      "\n",
      "Step 1\n",
      "Current state: 1\n",
      "Q-values for current state: [-0.02735806 -0.01726002 -0.01555491 -0.0215396 ]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 1\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.021539601180166217\n",
      "New Q-value: -0.02186335768773644\n",
      "\n",
      "Step 2\n",
      "Current state: 1\n",
      "Q-values for current state: [-0.02735806 -0.01726002 -0.01555491 -0.02186336]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 1\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.02186335768773644\n",
      "New Q-value: -0.02215473854454964\n",
      "\n",
      "Step 3\n",
      "Current state: 1\n",
      "Q-values for current state: [-0.02735806 -0.01726002 -0.01555491 -0.02215474]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 0\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.02735805520892831\n",
      "New Q-value: -0.027764659458570616\n",
      "\n",
      "Step 4\n",
      "Current state: 0\n",
      "Q-values for current state: [-0.02872975 -0.02480222 -0.02255168 -0.02922981]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 0\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.029229805330433175\n",
      "New Q-value: -0.029449234567924996\n",
      "\n",
      "Step 5\n",
      "Current state: 0\n",
      "Q-values for current state: [-0.02872975 -0.02480222 -0.02255168 -0.02944923]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 0\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.029449234567924996\n",
      "New Q-value: -0.029646720881667633\n",
      "\n",
      "Step 6\n",
      "Current state: 0\n",
      "Q-values for current state: [-0.02872975 -0.02480222 -0.02255168 -0.02964672]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 0\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.028729749541591913\n",
      "New Q-value: -0.028999184357967858\n",
      "\n",
      "Step 7\n",
      "Current state: 0\n",
      "Q-values for current state: [-0.02899918 -0.02480222 -0.02255168 -0.02964672]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 0\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.029646720881667633\n",
      "New Q-value: -0.029824458564036007\n",
      "\n",
      "Step 8\n",
      "Current state: 0\n",
      "Q-values for current state: [-0.02899918 -0.02480222 -0.02255168 -0.02982446]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 0\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.029824458564036007\n",
      "New Q-value: -0.029984422478167544\n",
      "\n",
      "Step 9\n",
      "Current state: 0\n",
      "Q-values for current state: [-0.02899918 -0.02480222 -0.02255168 -0.02998442]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 8\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.024802222567401368\n",
      "New Q-value: -0.02498131799655064\n",
      "\n",
      "Step 10\n",
      "Current state: 8\n",
      "Q-values for current state: [-0.02322537 -0.0174665  -0.01879857 -0.02693508]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 0\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.026935082794329587\n",
      "New Q-value: -0.027383984285431765\n",
      "\n",
      "Step 11\n",
      "Current state: 0\n",
      "Q-values for current state: [-0.02899918 -0.02498132 -0.02255168 -0.02998442]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 0\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.029984422478167544\n",
      "New Q-value: -0.030128390000885928\n",
      "\n",
      "Step 12\n",
      "Current state: 0\n",
      "Q-values for current state: [-0.02899918 -0.02498132 -0.02255168 -0.03012839]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 1\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.022551681795106707\n",
      "New Q-value: -0.02277423024118288\n",
      "\n",
      "Step 13\n",
      "Current state: 1\n",
      "Q-values for current state: [-0.02776466 -0.01726002 -0.01555491 -0.02215474]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 9\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.017260015419924925\n",
      "New Q-value: -0.01777002253917066\n",
      "\n",
      "Step 14\n",
      "Current state: 9\n",
      "Q-values for current state: [-0.02198986 -0.01301062 -0.01475845 -0.01838593]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 17\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.013010617486718171\n",
      "New Q-value: -0.013392883646438853\n",
      "\n",
      "Step 15\n",
      "Current state: 17\n",
      "Q-values for current state: [-0.01424258 -0.00719293 -0.00744938 -0.01436814]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 16\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.014242580202040365\n",
      "New Q-value: -0.014737596803091798\n",
      "\n",
      "Step 16\n",
      "Current state: 16\n",
      "Q-values for current state: [-0.01221767 -0.00967657 -0.01283372 -0.0222009 ]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 24\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.0096765749605839\n",
      "New Q-value: -0.01032102986775051\n",
      "\n",
      "Step 17\n",
      "Current state: 24\n",
      "Q-values for current state: [-0.00797998 -0.00847302 -0.00644329 -0.00731617]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 32\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.00847302089314\n",
      "New Q-value: -0.008961448803826001\n",
      "\n",
      "Step 18\n",
      "Current state: 32\n",
      "Q-values for current state: [-0.00713155 -0.00608436 -0.003534   -0.0038589 ]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 40\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.0060843624\n",
      "New Q-value: -0.006570926160000001\n",
      "\n",
      "Step 19\n",
      "Current state: 40\n",
      "Q-values for current state: [-0.00369645 -0.0042756  -0.001      -0.00323212]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 32\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.00323212\n",
      "New Q-value: -0.004244638\n",
      "\n",
      "Step 20\n",
      "Current state: 32\n",
      "Q-values for current state: [-0.00713155 -0.00657093 -0.003534   -0.0038589 ]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 40\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.006570926160000001\n",
      "New Q-value: -0.007008833544\n",
      "\n",
      "Step 21\n",
      "Current state: 40\n",
      "Q-values for current state: [-0.00369645 -0.0042756  -0.001      -0.00424464]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 41\n",
      "Reward: -0.01\n",
      "Terminated: True\n",
      "Old Q-value: -0.001\n",
      "New Q-value: -0.0019000000000000002\n",
      "\n",
      "Episode 43\n",
      "Starting state: 0\n",
      "\n",
      "Step 0\n",
      "Current state: 0\n",
      "Q-values for current state: [-0.02899918 -0.02498132 -0.02277423 -0.03012839]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 0\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.030128390000885928\n",
      "New Q-value: -0.030279102873709707\n",
      "\n",
      "Step 1\n",
      "Current state: 0\n",
      "Q-values for current state: [-0.02899918 -0.02498132 -0.02277423 -0.0302791 ]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 8\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.02498131799655064\n",
      "New Q-value: -0.025142503882784984\n",
      "\n",
      "Step 2\n",
      "Current state: 8\n",
      "Q-values for current state: [-0.02322537 -0.0174665  -0.01879857 -0.02738398]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 9\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.018798571652276417\n",
      "New Q-value: -0.019191038433460466\n",
      "\n",
      "Step 3\n",
      "Current state: 9\n",
      "Q-values for current state: [-0.02198986 -0.01339288 -0.01475845 -0.01838593]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 1\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.018385934359179845\n",
      "New Q-value: -0.019025057548848704\n",
      "\n",
      "Step 4\n",
      "Current state: 1\n",
      "Q-values for current state: [-0.02776466 -0.01777002 -0.01555491 -0.02215474]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 0\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.027764659458570616\n",
      "New Q-value: -0.02815174538562593\n",
      "\n",
      "Step 5\n",
      "Current state: 0\n",
      "Q-values for current state: [-0.02899918 -0.0251425  -0.02277423 -0.0302791 ]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 0\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.028999184357967858\n",
      "New Q-value: -0.029262817795083447\n",
      "\n",
      "Step 6\n",
      "Current state: 0\n",
      "Q-values for current state: [-0.02926282 -0.0251425  -0.02277423 -0.0302791 ]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 8\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.025142503882784984\n",
      "New Q-value: -0.025287571180395894\n",
      "\n",
      "Step 7\n",
      "Current state: 8\n",
      "Q-values for current state: [-0.02322537 -0.0174665  -0.01919104 -0.02738398]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 0\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.027383984285431765\n",
      "New Q-value: -0.02780913772980096\n",
      "\n",
      "Step 8\n",
      "Current state: 0\n",
      "Q-values for current state: [-0.02926282 -0.02528757 -0.02277423 -0.0302791 ]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 8\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.025287571180395894\n",
      "New Q-value: -0.025418131748245712\n",
      "\n",
      "Step 9\n",
      "Current state: 8\n",
      "Q-values for current state: [-0.02322537 -0.0174665  -0.01919104 -0.02780914]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 16\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.017466501956730607\n",
      "New Q-value: -0.017700349598493845\n",
      "\n",
      "Step 10\n",
      "Current state: 16\n",
      "Q-values for current state: [-0.01221767 -0.01032103 -0.01283372 -0.0222009 ]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 24\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.01032102986775051\n",
      "New Q-value: -0.010901039284200458\n",
      "\n",
      "Step 11\n",
      "Current state: 24\n",
      "Q-values for current state: [-0.00797998 -0.00896145 -0.00644329 -0.00731617]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 16\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.0073161693144895\n",
      "New Q-value: -0.008620151115039594\n",
      "\n",
      "Step 12\n",
      "Current state: 16\n",
      "Q-values for current state: [-0.01221767 -0.01090104 -0.01283372 -0.0222009 ]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 17\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.012833717024869089\n",
      "New Q-value: -0.01323367323077468\n",
      "\n",
      "Step 13\n",
      "Current state: 17\n",
      "Q-values for current state: [-0.0147376  -0.00719293 -0.00744938 -0.01436814]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 16\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.014737596803091798\n",
      "New Q-value: -0.015299435854781662\n",
      "\n",
      "Step 14\n",
      "Current state: 16\n",
      "Q-values for current state: [-0.01221767 -0.01090104 -0.01323367 -0.0222009 ]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 16\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.012217667047671977\n",
      "New Q-value: -0.013031499074903823\n",
      "\n",
      "Step 15\n",
      "Current state: 16\n",
      "Q-values for current state: [-0.0130315  -0.01090104 -0.01323367 -0.0222009 ]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 16\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.013031499074903823\n",
      "New Q-value: -0.013763947899412484\n",
      "\n",
      "Step 16\n",
      "Current state: 16\n",
      "Q-values for current state: [-0.01376395 -0.01090104 -0.01323367 -0.0222009 ]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 24\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.010901039284200458\n",
      "New Q-value: -0.011423047759005413\n",
      "\n",
      "Step 17\n",
      "Current state: 24\n",
      "Q-values for current state: [-0.00797998 -0.00896145 -0.00644329 -0.00862015]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 25\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.0064432884550000005\n",
      "New Q-value: -0.0069966071095\n",
      "\n",
      "Step 18\n",
      "Current state: 25\n",
      "Q-values for current state: [-0.0078502  -0.0020805  -0.00704849 -0.00472745]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 26\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.007048494105000001\n",
      "New Q-value: -0.007601094694500001\n",
      "\n",
      "Step 19\n",
      "Current state: 26\n",
      "Q-values for current state: [-0.00363665 -0.00271    -0.0040951  -0.00492135]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 34\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.00271\n",
      "New Q-value: -0.003439\n",
      "\n",
      "Step 20\n",
      "Current state: 34\n",
      "Q-values for current state: [-0.00289953 -0.0019      0.          0.        ]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 42\n",
      "Reward: -0.01\n",
      "Terminated: True\n",
      "Old Q-value: -0.0019000000000000002\n",
      "New Q-value: -0.00271\n",
      "\n",
      "Episode 44\n",
      "Starting state: 0\n",
      "\n",
      "Step 0\n",
      "Current state: 0\n",
      "Q-values for current state: [-0.02926282 -0.02541813 -0.02277423 -0.0302791 ]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 0\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.029262817795083447\n",
      "New Q-value: -0.029500087888487476\n",
      "\n",
      "Step 1\n",
      "Current state: 0\n",
      "Q-values for current state: [-0.02950009 -0.02541813 -0.02277423 -0.0302791 ]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 8\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.025418131748245712\n",
      "New Q-value: -0.025557851785278057\n",
      "\n",
      "Step 2\n",
      "Current state: 8\n",
      "Q-values for current state: [-0.02322537 -0.01770035 -0.01919104 -0.02780914]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 16\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.017700349598493845\n",
      "New Q-value: -0.018015504175749975\n",
      "\n",
      "Step 3\n",
      "Current state: 16\n",
      "Q-values for current state: [-0.01376395 -0.01142305 -0.01323367 -0.0222009 ]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 16\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.013763947899412484\n",
      "New Q-value: -0.014472742646576749\n",
      "\n",
      "Step 4\n",
      "Current state: 16\n",
      "Q-values for current state: [-0.01447274 -0.01142305 -0.01323367 -0.0222009 ]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 16\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.014472742646576749\n",
      "New Q-value: -0.015110657919024589\n",
      "\n",
      "Step 5\n",
      "Current state: 16\n",
      "Q-values for current state: [-0.01511066 -0.01142305 -0.01323367 -0.0222009 ]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 8\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.022200900986020174\n",
      "New Q-value: -0.022692283784114406\n",
      "\n",
      "Step 6\n",
      "Current state: 8\n",
      "Q-values for current state: [-0.02322537 -0.0180155  -0.01919104 -0.02780914]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 16\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.018015504175749975\n",
      "New Q-value: -0.01829914329528049\n",
      "\n",
      "Step 7\n",
      "Current state: 16\n",
      "Q-values for current state: [-0.01511066 -0.01142305 -0.01323367 -0.02269228]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 8\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.022692283784114406\n",
      "New Q-value: -0.02316147401875461\n",
      "\n",
      "Step 8\n",
      "Current state: 8\n",
      "Q-values for current state: [-0.02322537 -0.01829914 -0.01919104 -0.02780914]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 8\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.023225367121287054\n",
      "New Q-value: -0.023641249022209994\n",
      "\n",
      "Step 9\n",
      "Current state: 8\n",
      "Q-values for current state: [-0.02364125 -0.01829914 -0.01919104 -0.02780914]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 9\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.019191038433460466\n",
      "New Q-value: -0.01954425853652611\n",
      "\n",
      "Step 10\n",
      "Current state: 9\n",
      "Q-values for current state: [-0.02198986 -0.01339288 -0.01475845 -0.01902506]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 10\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.014758450314678484\n",
      "New Q-value: -0.01507080619654705\n",
      "\n",
      "Step 11\n",
      "Current state: 10\n",
      "Q-values for current state: [-0.01441409 -0.00829685 -0.00990139 -0.00871831]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 11\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.00990139211294336\n",
      "New Q-value: -0.010493203437099023\n",
      "\n",
      "Step 12\n",
      "Current state: 11\n",
      "Q-values for current state: [-0.00622945 -0.0061258  -0.00797617 -0.00916347]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 19\n",
      "Reward: -0.01\n",
      "Terminated: True\n",
      "Old Q-value: -0.00612579511\n",
      "New Q-value: -0.006513215599\n",
      "\n",
      "Episode 45\n",
      "Starting state: 0\n",
      "\n",
      "Step 0\n",
      "Current state: 0\n",
      "Q-values for current state: [-0.02950009 -0.02555785 -0.02277423 -0.0302791 ]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 1\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.02277423024118288\n",
      "New Q-value: -0.022974523842651435\n",
      "\n",
      "Step 1\n",
      "Current state: 1\n",
      "Q-values for current state: [-0.02815175 -0.01777002 -0.01555491 -0.02215474]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 2\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.015554911848282574\n",
      "New Q-value: -0.015837518674660683\n",
      "\n",
      "Step 2\n",
      "Current state: 2\n",
      "Q-values for current state: [-0.01881547 -0.01132042 -0.01618753 -0.00882208]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 2\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.008822084328488064\n",
      "New Q-value: -0.009777973906845623\n",
      "\n",
      "Step 3\n",
      "Current state: 2\n",
      "Q-values for current state: [-0.01881547 -0.01132042 -0.01618753 -0.00977797]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 3\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.016187534525895886\n",
      "New Q-value: -0.016535096984892084\n",
      "\n",
      "Step 4\n",
      "Current state: 3\n",
      "Q-values for current state: [-0.01362735 -0.01017175 -0.01029103 -0.01167167]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 2\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.013627354919169384\n",
      "New Q-value: -0.01419352694840278\n",
      "\n",
      "Step 5\n",
      "Current state: 2\n",
      "Q-values for current state: [-0.01881547 -0.01132042 -0.0165351  -0.00977797]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 10\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.011320415191231274\n",
      "New Q-value: -0.011976574585444563\n",
      "\n",
      "Step 6\n",
      "Current state: 10\n",
      "Q-values for current state: [-0.01441409 -0.00829685 -0.0104932  -0.00871831]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 18\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.008296851719330688\n",
      "New Q-value: -0.008955119326127711\n",
      "\n",
      "Step 7\n",
      "Current state: 18\n",
      "Q-values for current state: [-0.00653048 -0.00611523 -0.00521703 -0.00513635]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 26\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.0061152279\n",
      "New Q-value: -0.00683041011\n",
      "\n",
      "Step 8\n",
      "Current state: 26\n",
      "Q-values for current state: [-0.00363665 -0.003439   -0.0040951  -0.00492135]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 34\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.003439\n",
      "New Q-value: -0.0040951\n",
      "\n",
      "Step 9\n",
      "Current state: 34\n",
      "Q-values for current state: [-0.00289953 -0.00271     0.          0.        ]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 26\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: -0.0013454815125\n",
      "\n",
      "Step 10\n",
      "Current state: 26\n",
      "Q-values for current state: [-0.00363665 -0.0040951  -0.0040951  -0.00492135]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 34\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.0040951\n",
      "New Q-value: -0.00468559\n",
      "\n",
      "Step 11\n",
      "Current state: 34\n",
      "Q-values for current state: [-0.00289953 -0.00271     0.         -0.00134548]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 35\n",
      "Reward: -0.01\n",
      "Terminated: True\n",
      "Old Q-value: 0.0\n",
      "New Q-value: -0.001\n",
      "\n",
      "Episode 46\n",
      "Starting state: 0\n",
      "\n",
      "Step 0\n",
      "Current state: 0\n",
      "Q-values for current state: [-0.02950009 -0.02555785 -0.02297452 -0.0302791 ]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 0\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.030279102873709707\n",
      "New Q-value: -0.030433772351390623\n",
      "\n",
      "Step 1\n",
      "Current state: 0\n",
      "Q-values for current state: [-0.02950009 -0.02555785 -0.02297452 -0.03043377]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 1\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.022974523842651435\n",
      "New Q-value: -0.023181635732479056\n",
      "\n",
      "Step 2\n",
      "Current state: 1\n",
      "Q-values for current state: [-0.02815175 -0.01777002 -0.01583752 -0.02215474]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 1\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.02215473854454964\n",
      "New Q-value: -0.022443828964187444\n",
      "\n",
      "Step 3\n",
      "Current state: 1\n",
      "Q-values for current state: [-0.02815175 -0.01777002 -0.01583752 -0.02244383]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 0\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.02815174538562593\n",
      "New Q-value: -0.028538826241648847\n",
      "\n",
      "Step 4\n",
      "Current state: 0\n",
      "Q-values for current state: [-0.02950009 -0.02555785 -0.02318164 -0.03043377]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 0\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.030433772351390623\n",
      "New Q-value: -0.03059265051083707\n",
      "\n",
      "Step 5\n",
      "Current state: 0\n",
      "Q-values for current state: [-0.02950009 -0.02555785 -0.02318164 -0.03059265]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 1\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.023181635732479056\n",
      "New Q-value: -0.023368036433323913\n",
      "\n",
      "Step 6\n",
      "Current state: 1\n",
      "Q-values for current state: [-0.02853883 -0.01777002 -0.01583752 -0.02244383]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 9\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.01777002253917066\n",
      "New Q-value: -0.018265344231665284\n",
      "\n",
      "Step 7\n",
      "Current state: 9\n",
      "Q-values for current state: [-0.02198986 -0.01339288 -0.01507081 -0.01902506]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 10\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.01507080619654705\n",
      "New Q-value: -0.015391964679407956\n",
      "\n",
      "Step 8\n",
      "Current state: 10\n",
      "Q-values for current state: [-0.01441409 -0.00895512 -0.0104932  -0.00871831]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 2\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.00871830634226959\n",
      "New Q-value: -0.009775383229192965\n",
      "\n",
      "Step 9\n",
      "Current state: 2\n",
      "Q-values for current state: [-0.01881547 -0.01197657 -0.0165351  -0.00977797]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 1\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.01881547273900467\n",
      "New Q-value: -0.019438489739196967\n",
      "\n",
      "Step 10\n",
      "Current state: 1\n",
      "Q-values for current state: [-0.02853883 -0.01826534 -0.01583752 -0.02244383]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 9\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.018265344231665284\n",
      "New Q-value: -0.018711133754910445\n",
      "\n",
      "Step 11\n",
      "Current state: 9\n",
      "Q-values for current state: [-0.02198986 -0.01339288 -0.01539196 -0.01902506]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 10\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.015391964679407956\n",
      "New Q-value: -0.015703504547449294\n",
      "\n",
      "Step 12\n",
      "Current state: 10\n",
      "Q-values for current state: [-0.01441409 -0.00895512 -0.0104932  -0.00977538]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 9\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.014414093735615198\n",
      "New Q-value: -0.015245008308465369\n",
      "\n",
      "Step 13\n",
      "Current state: 9\n",
      "Q-values for current state: [-0.02198986 -0.01339288 -0.0157035  -0.01902506]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 1\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.019025057548848704\n",
      "New Q-value: -0.0196271160680566\n",
      "\n",
      "Step 14\n",
      "Current state: 1\n",
      "Q-values for current state: [-0.02853883 -0.01871113 -0.01583752 -0.02244383]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 2\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.015837518674660683\n",
      "New Q-value: -0.01618267432834495\n",
      "\n",
      "Step 15\n",
      "Current state: 2\n",
      "Q-values for current state: [-0.01943849 -0.01197657 -0.0165351  -0.00977797]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 10\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.011976574585444563\n",
      "New Q-value: -0.012629653462882239\n",
      "\n",
      "Step 16\n",
      "Current state: 10\n",
      "Q-values for current state: [-0.01524501 -0.00895512 -0.0104932  -0.00977538]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 11\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.010493203437099023\n",
      "New Q-value: -0.011035681276725428\n",
      "\n",
      "Step 17\n",
      "Current state: 11\n",
      "Q-values for current state: [-0.00622945 -0.00651322 -0.00797617 -0.00916347]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 3\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.009163467692138888\n",
      "New Q-value: -0.010213436834510786\n",
      "\n",
      "Step 18\n",
      "Current state: 3\n",
      "Q-values for current state: [-0.01419353 -0.01017175 -0.01029103 -0.01167167]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 3\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.011671671483625233\n",
      "New Q-value: -0.012470820246848497\n",
      "\n",
      "Step 19\n",
      "Current state: 3\n",
      "Q-values for current state: [-0.01419353 -0.01017175 -0.01029103 -0.01247082]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 3\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.012470820246848497\n",
      "New Q-value: -0.013190054133749435\n",
      "\n",
      "Step 20\n",
      "Current state: 3\n",
      "Q-values for current state: [-0.01419353 -0.01017175 -0.01029103 -0.01319005]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 3\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.013190054133749435\n",
      "New Q-value: -0.013837364631960279\n",
      "\n",
      "Step 21\n",
      "Current state: 3\n",
      "Q-values for current state: [-0.01419353 -0.01017175 -0.01029103 -0.01383736]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 11\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.010171746437745138\n",
      "New Q-value: -0.01074636997730693\n",
      "\n",
      "Step 22\n",
      "Current state: 11\n",
      "Q-values for current state: [-0.00622945 -0.00651322 -0.00797617 -0.01021344]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 12\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.007976168000068187\n",
      "New Q-value: -0.008574149552873868\n",
      "\n",
      "Step 23\n",
      "Current state: 12\n",
      "Q-values for current state: [-0.00450897 -0.00468559 -0.00773667 -0.00416419]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 13\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.0077366682445000005\n",
      "New Q-value: -0.00828970642005\n",
      "\n",
      "Step 24\n",
      "Current state: 13\n",
      "Q-values for current state: [-0.00380893 -0.00547448 -0.003439   -0.00791036]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 14\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.003439\n",
      "New Q-value: -0.0040951\n",
      "\n",
      "Step 25\n",
      "Current state: 14\n",
      "Q-values for current state: [-0.00532997 -0.00435255 -0.003705    0.        ]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 15\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.003705\n",
      "New Q-value: -0.004515\n",
      "\n",
      "Step 26\n",
      "Current state: 15\n",
      "Q-values for current state: [-0.00271    -0.003439   -0.0019     -0.00443895]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 23\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.003439\n",
      "New Q-value: -0.0041901\n",
      "\n",
      "Step 27\n",
      "Current state: 23\n",
      "Q-values for current state: [-0.0011805 -0.00271   -0.001     -0.0028905]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 22\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.0011805000000000001\n",
      "New Q-value: -0.00224295\n",
      "\n",
      "Step 28\n",
      "Current state: 22\n",
      "Q-values for current state: [-0.002805  -0.0040951 -0.0019    -0.0040951]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 23\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.0019000000000000002\n",
      "New Q-value: -0.002805\n",
      "\n",
      "Step 29\n",
      "Current state: 23\n",
      "Q-values for current state: [-0.00224295 -0.00271    -0.001      -0.0028905 ]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 22\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.00224295\n",
      "New Q-value: -0.00328513\n",
      "\n",
      "Step 30\n",
      "Current state: 22\n",
      "Q-values for current state: [-0.002805  -0.0040951 -0.002805  -0.0040951]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 23\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.002805\n",
      "New Q-value: -0.0036195000000000003\n",
      "\n",
      "Step 31\n",
      "Current state: 23\n",
      "Q-values for current state: [-0.00328513 -0.00271    -0.001      -0.0028905 ]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 15\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.0028905000000000003\n",
      "New Q-value: -0.00378195\n",
      "\n",
      "Step 32\n",
      "Current state: 15\n",
      "Q-values for current state: [-0.00271    -0.0041901  -0.0019     -0.00443895]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 23\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.0041901\n",
      "New Q-value: -0.00486609\n",
      "\n",
      "Step 33\n",
      "Current state: 23\n",
      "Q-values for current state: [-0.00328513 -0.00271    -0.001      -0.00378195]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 15\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.00378195\n",
      "New Q-value: -0.004584255000000001\n",
      "\n",
      "Step 34\n",
      "Current state: 15\n",
      "Q-values for current state: [-0.00271    -0.00486609 -0.0019     -0.00443895]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 7\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.0044389525\n",
      "New Q-value: -0.00536165275\n",
      "\n",
      "Step 35\n",
      "Current state: 7\n",
      "Q-values for current state: [-0.00480703 -0.00443805 -0.00627692 -0.0038589 ]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 6\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.00480702525\n",
      "New Q-value: -0.005583772725\n",
      "\n",
      "Step 36\n",
      "Current state: 6\n",
      "Q-values for current state: [-0.00305385 -0.00271    -0.00488612 -0.00458426]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 6\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.004584255000000001\n",
      "New Q-value: -0.005383279500000001\n",
      "\n",
      "Step 37\n",
      "Current state: 6\n",
      "Q-values for current state: [-0.00305385 -0.00271    -0.00488612 -0.00538328]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 6\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.005383279500000001\n",
      "New Q-value: -0.006102401550000001\n",
      "\n",
      "Step 38\n",
      "Current state: 6\n",
      "Q-values for current state: [-0.00305385 -0.00271    -0.00488612 -0.0061024 ]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 5\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.0030538525\n",
      "New Q-value: -0.0041717110125\n",
      "\n",
      "Step 39\n",
      "Current state: 5\n",
      "Q-values for current state: [-0.00490646 -0.00606345 -0.00494304 -0.0044552 ]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 5\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.0044551975\n",
      "New Q-value: -0.0054329215125\n",
      "\n",
      "Step 40\n",
      "Current state: 5\n",
      "Q-values for current state: [-0.00490646 -0.00606345 -0.00494304 -0.00543292]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 13\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.006063452500000001\n",
      "New Q-value: -0.0068189556439968765\n",
      "\n",
      "Step 41\n",
      "Current state: 13\n",
      "Q-values for current state: [-0.00380893 -0.00547448 -0.0040951  -0.00791036]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 5\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.0079103605490125\n",
      "New Q-value: -0.00858543802192375\n",
      "\n",
      "Step 42\n",
      "Current state: 5\n",
      "Q-values for current state: [-0.00490646 -0.00681896 -0.00494304 -0.00543292]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 13\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.0068189556439968765\n",
      "New Q-value: -0.007498908473594064\n",
      "\n",
      "Step 43\n",
      "Current state: 13\n",
      "Q-values for current state: [-0.00380893 -0.00547448 -0.0040951  -0.00858544]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 12\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.0038089304631250005\n",
      "New Q-value: -0.004823635769625001\n",
      "\n",
      "Step 44\n",
      "Current state: 12\n",
      "Q-values for current state: [-0.00450897 -0.00468559 -0.00828971 -0.00416419]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 13\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.00828970642005\n",
      "New Q-value: -0.008849770278045\n",
      "\n",
      "Step 45\n",
      "Current state: 13\n",
      "Q-values for current state: [-0.00482364 -0.00547448 -0.0040951  -0.00858544]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 12\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.004823635769625001\n",
      "New Q-value: -0.005736870545475001\n",
      "\n",
      "Step 46\n",
      "Current state: 12\n",
      "Q-values for current state: [-0.00450897 -0.00468559 -0.00884977 -0.00416419]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 20\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.00468559\n",
      "New Q-value: -0.005217031\n",
      "\n",
      "Step 47\n",
      "Current state: 20\n",
      "Q-values for current state: [-0.00271     0.         -0.0020805  -0.00403209]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 28\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: -0.001\n",
      "\n",
      "Step 48\n",
      "Current state: 28\n",
      "Q-values for current state: [-0.001 -0.001 -0.001  0.   ]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 36\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.001\n",
      "New Q-value: -0.0019000000000000002\n",
      "\n",
      "Step 49\n",
      "Current state: 36\n",
      "Q-values for current state: [ 0.     0.    -0.001  0.   ]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 44\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: -0.001\n",
      "\n",
      "Step 50\n",
      "Current state: 44\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 52\n",
      "Reward: -0.01\n",
      "Terminated: True\n",
      "Old Q-value: 0.0\n",
      "New Q-value: -0.001\n",
      "\n",
      "Episode 47\n",
      "Starting state: 0\n",
      "\n",
      "Step 0\n",
      "Current state: 0\n",
      "Q-values for current state: [-0.02950009 -0.02555785 -0.02336804 -0.03059265]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 0\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.029500087888487476\n",
      "New Q-value: -0.0297700425608045\n",
      "\n",
      "Step 1\n",
      "Current state: 0\n",
      "Q-values for current state: [-0.02977004 -0.02555785 -0.02336804 -0.03059265]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 1\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.023368036433323913\n",
      "New Q-value: -0.023568586851184292\n",
      "\n",
      "Step 2\n",
      "Current state: 1\n",
      "Q-values for current state: [-0.02853883 -0.01871113 -0.01618267 -0.02244383]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 1\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.022443828964187444\n",
      "New Q-value: -0.02273680012896147\n",
      "\n",
      "Step 3\n",
      "Current state: 1\n",
      "Q-values for current state: [-0.02853883 -0.01871113 -0.01618267 -0.0227368 ]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 2\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.01618267432834495\n",
      "New Q-value: -0.016493314416660788\n",
      "\n",
      "Step 4\n",
      "Current state: 2\n",
      "Q-values for current state: [-0.01943849 -0.01262965 -0.0165351  -0.00977797]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 2\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.009777973906845623\n",
      "New Q-value: -0.010729084037311395\n",
      "\n",
      "Step 5\n",
      "Current state: 2\n",
      "Q-values for current state: [-0.01943849 -0.01262965 -0.0165351  -0.01072908]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 1\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.019438489739196967\n",
      "New Q-value: -0.020061505634860047\n",
      "\n",
      "Step 6\n",
      "Current state: 1\n",
      "Q-values for current state: [-0.02853883 -0.01871113 -0.01649331 -0.0227368 ]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 0\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.028538826241648847\n",
      "New Q-value: -0.02892395936834647\n",
      "\n",
      "Step 7\n",
      "Current state: 0\n",
      "Q-values for current state: [-0.02977004 -0.02555785 -0.02356859 -0.03059265]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 1\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.023568586851184292\n",
      "New Q-value: -0.023778593035648637\n",
      "\n",
      "Step 8\n",
      "Current state: 1\n",
      "Q-values for current state: [-0.02892396 -0.01871113 -0.01649331 -0.0227368 ]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 0\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.02892395936834647\n",
      "New Q-value: -0.029290529769898443\n",
      "\n",
      "Step 9\n",
      "Current state: 0\n",
      "Q-values for current state: [-0.02977004 -0.02555785 -0.02377859 -0.03059265]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 0\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.0297700425608045\n",
      "New Q-value: -0.03005200464311067\n",
      "\n",
      "Step 10\n",
      "Current state: 0\n",
      "Q-values for current state: [-0.030052   -0.02555785 -0.02377859 -0.03059265]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 8\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.025557851785278057\n",
      "New Q-value: -0.0257404852198019\n",
      "\n",
      "Step 11\n",
      "Current state: 8\n",
      "Q-values for current state: [-0.02364125 -0.01829914 -0.01954426 -0.02780914]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 8\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.023641249022209994\n",
      "New Q-value: -0.02401554273304064\n",
      "\n",
      "Step 12\n",
      "Current state: 8\n",
      "Q-values for current state: [-0.02401554 -0.01829914 -0.01954426 -0.02780914]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 8\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.02401554273304064\n",
      "New Q-value: -0.024352407072788223\n",
      "\n",
      "Step 13\n",
      "Current state: 8\n",
      "Q-values for current state: [-0.02435241 -0.01829914 -0.01954426 -0.02780914]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 9\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.01954425853652611\n",
      "New Q-value: -0.01986215662928519\n",
      "\n",
      "Step 14\n",
      "Current state: 9\n",
      "Q-values for current state: [-0.02198986 -0.01339288 -0.0157035  -0.01962712]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 10\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.015703504547449294\n",
      "New Q-value: -0.015983890428686497\n",
      "\n",
      "Step 15\n",
      "Current state: 10\n",
      "Q-values for current state: [-0.01524501 -0.00895512 -0.01103568 -0.00977538]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 18\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.008955119326127711\n",
      "New Q-value: -0.009547560172245033\n",
      "\n",
      "Step 16\n",
      "Current state: 18\n",
      "Q-values for current state: [-0.00653048 -0.00683041 -0.00521703 -0.00513635]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 26\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.00683041011\n",
      "New Q-value: -0.0074928506115\n",
      "\n",
      "Step 17\n",
      "Current state: 26\n",
      "Q-values for current state: [-0.00363665 -0.00468559 -0.0040951  -0.00492135]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 34\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.00468559\n",
      "New Q-value: -0.005312031\n",
      "\n",
      "Step 18\n",
      "Current state: 34\n",
      "Q-values for current state: [-0.00289953 -0.00271    -0.001      -0.00134548]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 26\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.0013454815125\n",
      "New Q-value: -0.00255641487375\n",
      "\n",
      "Step 19\n",
      "Current state: 26\n",
      "Q-values for current state: [-0.00363665 -0.00531203 -0.0040951  -0.00492135]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 27\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.0040951\n",
      "New Q-value: -0.00468559\n",
      "\n",
      "Step 20\n",
      "Current state: 27\n",
      "Q-values for current state: [ 0.      -0.001   -0.00271 -0.0019 ]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 28\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.00271\n",
      "New Q-value: -0.003439\n",
      "\n",
      "Step 21\n",
      "Current state: 28\n",
      "Q-values for current state: [-0.001  -0.0019 -0.001   0.    ]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 29\n",
      "Reward: -0.01\n",
      "Terminated: True\n",
      "Old Q-value: -0.001\n",
      "New Q-value: -0.0019000000000000002\n",
      "\n",
      "Episode 48\n",
      "Starting state: 0\n",
      "\n",
      "Step 0\n",
      "Current state: 0\n",
      "Q-values for current state: [-0.030052   -0.02574049 -0.02377859 -0.03059265]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 1\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.023778593035648637\n",
      "New Q-value: -0.02396759860166655\n",
      "\n",
      "Step 1\n",
      "Current state: 1\n",
      "Q-values for current state: [-0.02929053 -0.01871113 -0.01649331 -0.0227368 ]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 2\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.016493314416660788\n",
      "New Q-value: -0.016863245958539292\n",
      "\n",
      "Step 2\n",
      "Current state: 2\n",
      "Q-values for current state: [-0.02006151 -0.01262965 -0.0165351  -0.01072908]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 1\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.020061505634860047\n",
      "New Q-value: -0.020657363437435274\n",
      "\n",
      "Step 3\n",
      "Current state: 1\n",
      "Q-values for current state: [-0.02929053 -0.01871113 -0.01686325 -0.0227368 ]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 9\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.018711133754910445\n",
      "New Q-value: -0.01911234432583109\n",
      "\n",
      "Step 4\n",
      "Current state: 9\n",
      "Q-values for current state: [-0.02198986 -0.01339288 -0.01598389 -0.01962712]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 10\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.015983890428686497\n",
      "New Q-value: -0.016292519602181126\n",
      "\n",
      "Step 5\n",
      "Current state: 10\n",
      "Q-values for current state: [-0.01524501 -0.00954756 -0.01103568 -0.00977538]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 2\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.009775383229192965\n",
      "New Q-value: -0.010817107889818251\n",
      "\n",
      "Step 6\n",
      "Current state: 2\n",
      "Q-values for current state: [-0.02065736 -0.01262965 -0.0165351  -0.01072908]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 3\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.016535096984892084\n",
      "New Q-value: -0.016859234846504922\n",
      "\n",
      "Step 7\n",
      "Current state: 3\n",
      "Q-values for current state: [-0.01419353 -0.01074637 -0.01029103 -0.01383736]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 3\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.013837364631960279\n",
      "New Q-value: -0.014431275728866298\n",
      "\n",
      "Step 8\n",
      "Current state: 3\n",
      "Q-values for current state: [-0.01419353 -0.01074637 -0.01029103 -0.01443128]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 11\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.01074636997730693\n",
      "New Q-value: -0.011263531162912545\n",
      "\n",
      "Step 9\n",
      "Current state: 11\n",
      "Q-values for current state: [-0.00622945 -0.00651322 -0.00857415 -0.01021344]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 19\n",
      "Reward: -0.01\n",
      "Terminated: True\n",
      "Old Q-value: -0.006513215599\n",
      "New Q-value: -0.0068618940391\n",
      "\n",
      "Episode 49\n",
      "Starting state: 0\n",
      "\n",
      "Step 0\n",
      "Current state: 0\n",
      "Q-values for current state: [-0.030052   -0.02574049 -0.0239676  -0.03059265]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 0\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.03059265051083707\n",
      "New Q-value: -0.030810307326911684\n",
      "\n",
      "Step 1\n",
      "Current state: 0\n",
      "Q-values for current state: [-0.030052   -0.02574049 -0.0239676  -0.03081031]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 0\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.030810307326911684\n",
      "New Q-value: -0.031006198461378837\n",
      "\n",
      "Step 2\n",
      "Current state: 0\n",
      "Q-values for current state: [-0.030052   -0.02574049 -0.0239676  -0.0310062 ]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 0\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.03005200464311067\n",
      "New Q-value: -0.030323726045957924\n",
      "\n",
      "Step 3\n",
      "Current state: 0\n",
      "Q-values for current state: [-0.03032373 -0.02574049 -0.0239676  -0.0310062 ]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 1\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.02396759860166655\n",
      "New Q-value: -0.024172847107561127\n",
      "\n",
      "Step 4\n",
      "Current state: 1\n",
      "Q-values for current state: [-0.02929053 -0.01911234 -0.01686325 -0.0227368 ]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 1\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.02273680012896147\n",
      "New Q-value: -0.023065128482126558\n",
      "\n",
      "Step 5\n",
      "Current state: 1\n",
      "Q-values for current state: [-0.02929053 -0.01911234 -0.01686325 -0.02306513]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 1\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.023065128482126558\n",
      "New Q-value: -0.023360623999975134\n",
      "\n",
      "Step 6\n",
      "Current state: 1\n",
      "Q-values for current state: [-0.02929053 -0.01911234 -0.01686325 -0.02336062]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 2\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.016863245958539292\n",
      "New Q-value: -0.017196184346229944\n",
      "\n",
      "Step 7\n",
      "Current state: 2\n",
      "Q-values for current state: [-0.02065736 -0.01262965 -0.01685923 -0.01072908]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 2\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.010729084037311395\n",
      "New Q-value: -0.011675438617124839\n",
      "\n",
      "Step 8\n",
      "Current state: 2\n",
      "Q-values for current state: [-0.02065736 -0.01262965 -0.01685923 -0.01167544]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 2\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.011675438617124839\n",
      "New Q-value: -0.012617061424039214\n",
      "\n",
      "Step 9\n",
      "Current state: 2\n",
      "Q-values for current state: [-0.02065736 -0.01262965 -0.01685923 -0.01261706]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 1\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.020657363437435274\n",
      "New Q-value: -0.02122526460658359\n",
      "\n",
      "Step 10\n",
      "Current state: 1\n",
      "Q-values for current state: [-0.02929053 -0.01911234 -0.01719618 -0.02336062]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 0\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.029290529769898443\n",
      "New Q-value: -0.029657897268126905\n",
      "\n",
      "Step 11\n",
      "Current state: 0\n",
      "Q-values for current state: [-0.03032373 -0.02574049 -0.02417285 -0.0310062 ]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 0\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.031006198461378837\n",
      "New Q-value: -0.03120199909045926\n",
      "\n",
      "Step 12\n",
      "Current state: 0\n",
      "Q-values for current state: [-0.03032373 -0.02574049 -0.02417285 -0.031202  ]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 1\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.024172847107561127\n",
      "New Q-value: -0.02438919990969686\n",
      "\n",
      "Step 13\n",
      "Current state: 1\n",
      "Q-values for current state: [-0.0296579  -0.01911234 -0.01719618 -0.02336062]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 9\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.01911234432583109\n",
      "New Q-value: -0.019473433839659673\n",
      "\n",
      "Step 14\n",
      "Current state: 9\n",
      "Q-values for current state: [-0.02198986 -0.01339288 -0.01629252 -0.01962712]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 1\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.0196271160680566\n",
      "New Q-value: -0.020298041974142783\n",
      "\n",
      "Step 15\n",
      "Current state: 1\n",
      "Q-values for current state: [-0.0296579  -0.01947343 -0.01719618 -0.02336062]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 2\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.017196184346229944\n",
      "New Q-value: -0.017675186746890673\n",
      "\n",
      "Step 16\n",
      "Current state: 2\n",
      "Q-values for current state: [-0.02122526 -0.01262965 -0.01685923 -0.01261706]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 10\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.012629653462882239\n",
      "New Q-value: -0.013273706332957293\n",
      "\n",
      "Step 17\n",
      "Current state: 10\n",
      "Q-values for current state: [-0.01524501 -0.00954756 -0.01103568 -0.01081711]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 11\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.011035681276725428\n",
      "New Q-value: -0.011523911332389192\n",
      "\n",
      "Step 18\n",
      "Current state: 11\n",
      "Q-values for current state: [-0.00622945 -0.00686189 -0.00857415 -0.01021344]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 12\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.008574149552873868\n",
      "New Q-value: -0.00911233295039898\n",
      "\n",
      "Step 19\n",
      "Current state: 12\n",
      "Q-values for current state: [-0.00450897 -0.00521703 -0.00884977 -0.00416419]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 11\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.004508973188125\n",
      "New Q-value: -0.005649874052648807\n",
      "\n",
      "Step 20\n",
      "Current state: 11\n",
      "Q-values for current state: [-0.00622945 -0.00686189 -0.00911233 -0.01021344]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 3\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.010213436834510786\n",
      "New Q-value: -0.011169740711161754\n",
      "\n",
      "Step 21\n",
      "Current state: 3\n",
      "Q-values for current state: [-0.01419353 -0.01126353 -0.01029103 -0.01443128]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 11\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.011263531162912545\n",
      "New Q-value: -0.011728976229957597\n",
      "\n",
      "Step 22\n",
      "Current state: 11\n",
      "Q-values for current state: [-0.00622945 -0.00686189 -0.00911233 -0.01116974]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 12\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.00911233295039898\n",
      "New Q-value: -0.009596698008171582\n",
      "\n",
      "Step 23\n",
      "Current state: 12\n",
      "Q-values for current state: [-0.00564987 -0.00521703 -0.00884977 -0.00416419]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 13\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.008849770278045\n",
      "New Q-value: -0.0093538277502405\n",
      "\n",
      "Step 24\n",
      "Current state: 13\n",
      "Q-values for current state: [-0.00573687 -0.00547448 -0.0040951  -0.00858544]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 12\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.005736870545475001\n",
      "New Q-value: -0.006558781843740001\n",
      "\n",
      "Step 25\n",
      "Current state: 12\n",
      "Q-values for current state: [-0.00564987 -0.00521703 -0.00935383 -0.00416419]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 20\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.005217031\n",
      "New Q-value: -0.0057903278999999995\n",
      "\n",
      "Step 26\n",
      "Current state: 20\n",
      "Q-values for current state: [-0.00271    -0.001      -0.0020805  -0.00403209]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 28\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.001\n",
      "New Q-value: -0.0019000000000000002\n",
      "\n",
      "Step 27\n",
      "Current state: 28\n",
      "Q-values for current state: [-0.001  -0.0019 -0.0019  0.    ]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 29\n",
      "Reward: -0.01\n",
      "Terminated: True\n",
      "Old Q-value: -0.0019000000000000002\n",
      "New Q-value: -0.00271\n",
      "\n",
      "Episode 50\n",
      "Starting state: 0\n",
      "\n",
      "Step 0\n",
      "Current state: 0\n",
      "Q-values for current state: [-0.03032373 -0.02574049 -0.0243892  -0.031202  ]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 0\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.03120199909045926\n",
      "New Q-value: -0.031398773172834536\n",
      "\n",
      "Step 1\n",
      "Current state: 0\n",
      "Q-values for current state: [-0.03032373 -0.02574049 -0.0243892  -0.03139877]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 0\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.030323726045957924\n",
      "New Q-value: -0.030608327432783335\n",
      "\n",
      "Step 2\n",
      "Current state: 0\n",
      "Q-values for current state: [-0.03060833 -0.02574049 -0.0243892  -0.03139877]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 0\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.030608327432783335\n",
      "New Q-value: -0.030864468680926203\n",
      "\n",
      "Step 3\n",
      "Current state: 0\n",
      "Q-values for current state: [-0.03086447 -0.02574049 -0.0243892  -0.03139877]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 0\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.030864468680926203\n",
      "New Q-value: -0.031094995804254785\n",
      "\n",
      "Step 4\n",
      "Current state: 0\n",
      "Q-values for current state: [-0.031095   -0.02574049 -0.0243892  -0.03139877]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 1\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.02438919990969686\n",
      "New Q-value: -0.024629422659681786\n",
      "\n",
      "Step 5\n",
      "Current state: 1\n",
      "Q-values for current state: [-0.0296579  -0.01947343 -0.01767519 -0.02336062]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 1\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.023360623999975134\n",
      "New Q-value: -0.023703704340932236\n",
      "\n",
      "Step 6\n",
      "Current state: 1\n",
      "Q-values for current state: [-0.0296579  -0.01947343 -0.01767519 -0.0237037 ]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 0\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.029657897268126905\n",
      "New Q-value: -0.030031902693983984\n",
      "\n",
      "Step 7\n",
      "Current state: 0\n",
      "Q-values for current state: [-0.031095   -0.02574049 -0.02462942 -0.03139877]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 0\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.031094995804254785\n",
      "New Q-value: -0.03132529137649907\n",
      "\n",
      "Step 8\n",
      "Current state: 0\n",
      "Q-values for current state: [-0.03132529 -0.02574049 -0.02462942 -0.03139877]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 1\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.024629422659681786\n",
      "New Q-value: -0.02484562313466822\n",
      "\n",
      "Step 9\n",
      "Current state: 1\n",
      "Q-values for current state: [-0.0300319  -0.01947343 -0.01767519 -0.0237037 ]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 0\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.030031902693983984\n",
      "New Q-value: -0.030389046622379068\n",
      "\n",
      "Step 10\n",
      "Current state: 0\n",
      "Q-values for current state: [-0.03132529 -0.02574049 -0.02484562 -0.03139877]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 0\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.031398773172834536\n",
      "New Q-value: -0.03161923005334456\n",
      "\n",
      "Step 11\n",
      "Current state: 0\n",
      "Q-values for current state: [-0.03132529 -0.02574049 -0.02484562 -0.03161923]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 0\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.03132529137649907\n",
      "New Q-value: -0.03155309643664265\n",
      "\n",
      "Step 12\n",
      "Current state: 0\n",
      "Q-values for current state: [-0.0315531  -0.02574049 -0.02484562 -0.03161923]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 8\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.0257404852198019\n",
      "New Q-value: -0.025904855310873356\n",
      "\n",
      "Step 13\n",
      "Current state: 8\n",
      "Q-values for current state: [-0.02435241 -0.01829914 -0.01986216 -0.02780914]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 8\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.024352407072788223\n",
      "New Q-value: -0.024655584978561046\n",
      "\n",
      "Step 14\n",
      "Current state: 8\n",
      "Q-values for current state: [-0.02465558 -0.01829914 -0.01986216 -0.02780914]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 8\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.024655584978561046\n",
      "New Q-value: -0.024928445093756586\n",
      "\n",
      "Step 15\n",
      "Current state: 8\n",
      "Q-values for current state: [-0.02492845 -0.01829914 -0.01986216 -0.02780914]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 8\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.024928445093756586\n",
      "New Q-value: -0.025174019197432573\n",
      "\n",
      "Step 16\n",
      "Current state: 8\n",
      "Q-values for current state: [-0.02517402 -0.01829914 -0.01986216 -0.02780914]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 0\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.02780913772980096\n",
      "New Q-value: -0.028388558154614347\n",
      "\n",
      "Step 17\n",
      "Current state: 0\n",
      "Q-values for current state: [-0.0315531  -0.02590486 -0.02484562 -0.03161923]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 0\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.03155309643664265\n",
      "New Q-value: -0.03175812099077187\n",
      "\n",
      "Step 18\n",
      "Current state: 0\n",
      "Q-values for current state: [-0.03175812 -0.02590486 -0.02484562 -0.03161923]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 0\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.03161923005334456\n",
      "New Q-value: -0.031817641245803586\n",
      "\n",
      "Step 19\n",
      "Current state: 0\n",
      "Q-values for current state: [-0.03175812 -0.02590486 -0.02484562 -0.03181764]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 0\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.031817641245803586\n",
      "New Q-value: -0.03199621131901671\n",
      "\n",
      "Step 20\n",
      "Current state: 0\n",
      "Q-values for current state: [-0.03175812 -0.02590486 -0.02484562 -0.03199621]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 0\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.03175812099077187\n",
      "New Q-value: -0.03194264308948816\n",
      "\n",
      "Step 21\n",
      "Current state: 0\n",
      "Q-values for current state: [-0.03194264 -0.02590486 -0.02484562 -0.03199621]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 0\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.03194264308948816\n",
      "New Q-value: -0.03210871297833282\n",
      "\n",
      "Step 22\n",
      "Current state: 0\n",
      "Q-values for current state: [-0.03210871 -0.02590486 -0.02484562 -0.03199621]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 0\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.03199621131901671\n",
      "New Q-value: -0.032156924384908525\n",
      "\n",
      "Step 23\n",
      "Current state: 0\n",
      "Q-values for current state: [-0.03210871 -0.02590486 -0.02484562 -0.03215692]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 8\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.025904855310873356\n",
      "New Q-value: -0.026052788392837666\n",
      "\n",
      "Step 24\n",
      "Current state: 8\n",
      "Q-values for current state: [-0.02517402 -0.01829914 -0.01986216 -0.02838856]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 9\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.01986215662928519\n",
      "New Q-value: -0.02014826491276836\n",
      "\n",
      "Step 25\n",
      "Current state: 9\n",
      "Q-values for current state: [-0.02198986 -0.01339288 -0.01629252 -0.02029804]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 8\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.02198986178002566\n",
      "New Q-value: -0.02252929421507474\n",
      "\n",
      "Step 26\n",
      "Current state: 8\n",
      "Q-values for current state: [-0.02517402 -0.01829914 -0.02014826 -0.02838856]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 9\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.02014826491276836\n",
      "New Q-value: -0.020405762367903216\n",
      "\n",
      "Step 27\n",
      "Current state: 9\n",
      "Q-values for current state: [-0.02252929 -0.01339288 -0.01629252 -0.02029804]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 8\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.02252929421507474\n",
      "New Q-value: -0.023014783406618913\n",
      "\n",
      "Step 28\n",
      "Current state: 8\n",
      "Q-values for current state: [-0.02517402 -0.01829914 -0.02040576 -0.02838856]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 9\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.020405762367903216\n",
      "New Q-value: -0.020637510077524585\n",
      "\n",
      "Step 29\n",
      "Current state: 9\n",
      "Q-values for current state: [-0.02301478 -0.01339288 -0.01629252 -0.02029804]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 10\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.016292519602181126\n",
      "New Q-value: -0.016570285858326293\n",
      "\n",
      "Step 30\n",
      "Current state: 10\n",
      "Q-values for current state: [-0.01524501 -0.00954756 -0.01152391 -0.01081711]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 2\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.010817107889818251\n",
      "New Q-value: -0.011934017936120152\n",
      "\n",
      "Step 31\n",
      "Current state: 2\n",
      "Q-values for current state: [-0.02122526 -0.01327371 -0.01685923 -0.01261706]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 3\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.016859234846504922\n",
      "New Q-value: -0.017150958921956477\n",
      "\n",
      "Step 32\n",
      "Current state: 3\n",
      "Q-values for current state: [-0.01419353 -0.01172898 -0.01029103 -0.01443128]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 3\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.014431275728866298\n",
      "New Q-value: -0.014965795716081714\n",
      "\n",
      "Step 33\n",
      "Current state: 3\n",
      "Q-values for current state: [-0.01419353 -0.01172898 -0.01029103 -0.0149658 ]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 3\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.014965795716081714\n",
      "New Q-value: -0.015446863704575589\n",
      "\n",
      "Step 34\n",
      "Current state: 3\n",
      "Q-values for current state: [-0.01419353 -0.01172898 -0.01029103 -0.01544686]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 2\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.01419352694840278\n",
      "New Q-value: -0.014972795088846226\n",
      "\n",
      "Step 35\n",
      "Current state: 2\n",
      "Q-values for current state: [-0.02122526 -0.01327371 -0.01715096 -0.01261706]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 3\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.017150958921956477\n",
      "New Q-value: -0.017413510589862877\n",
      "\n",
      "Step 36\n",
      "Current state: 3\n",
      "Q-values for current state: [-0.0149728  -0.01172898 -0.01029103 -0.01544686]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 3\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.015446863704575589\n",
      "New Q-value: -0.015879824894220077\n",
      "\n",
      "Step 37\n",
      "Current state: 3\n",
      "Q-values for current state: [-0.0149728  -0.01172898 -0.01029103 -0.01587982]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 4\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.010291026948442595\n",
      "New Q-value: -0.010759526277160836\n",
      "\n",
      "Step 38\n",
      "Current state: 4\n",
      "Q-values for current state: [-0.01125089 -0.00583777 -0.00523792 -0.00840747]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 5\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.0052379160375\n",
      "New Q-value: -0.0061802379615625005\n",
      "\n",
      "Step 39\n",
      "Current state: 5\n",
      "Q-values for current state: [-0.00490646 -0.00749891 -0.00494304 -0.00543292]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 4\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.004906458187500001\n",
      "New Q-value: -0.005970400566250001\n",
      "\n",
      "Step 40\n",
      "Current state: 4\n",
      "Q-values for current state: [-0.01125089 -0.00583777 -0.00618024 -0.00840747]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 5\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.0061802379615625005\n",
      "New Q-value: -0.007031802965406251\n",
      "\n",
      "Step 41\n",
      "Current state: 5\n",
      "Q-values for current state: [-0.0059704  -0.00749891 -0.00494304 -0.00543292]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 5\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.0054329215125\n",
      "New Q-value: -0.00635921816125\n",
      "\n",
      "Step 42\n",
      "Current state: 5\n",
      "Q-values for current state: [-0.0059704  -0.00749891 -0.00494304 -0.00635922]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 13\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.007498908473594064\n",
      "New Q-value: -0.008138052126234658\n",
      "\n",
      "Step 43\n",
      "Current state: 13\n",
      "Q-values for current state: [-0.00655878 -0.00547448 -0.0040951  -0.00858544]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 14\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.0040951\n",
      "New Q-value: -0.00468559\n",
      "\n",
      "Step 44\n",
      "Current state: 14\n",
      "Q-values for current state: [-0.00532997 -0.00435255 -0.004515    0.        ]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 22\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.00435255\n",
      "New Q-value: -0.005183770000000001\n",
      "\n",
      "Step 45\n",
      "Current state: 22\n",
      "Q-values for current state: [-0.002805  -0.0040951 -0.0036195 -0.0040951]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 21\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.002805\n",
      "New Q-value: -0.003730315125\n",
      "\n",
      "Step 46\n",
      "Current state: 21\n",
      "Q-values for current state: [-0.00271    -0.00271    -0.00378195 -0.00216648]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 20\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.00271\n",
      "New Q-value: -0.0036195000000000003\n",
      "\n",
      "Step 47\n",
      "Current state: 20\n",
      "Q-values for current state: [-0.00271    -0.0019     -0.0020805  -0.00403209]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 28\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.0019000000000000002\n",
      "New Q-value: -0.00271\n",
      "\n",
      "Step 48\n",
      "Current state: 28\n",
      "Q-values for current state: [-0.001   -0.0019  -0.00271  0.     ]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 20\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: -0.0011976475000000002\n",
      "\n",
      "Step 49\n",
      "Current state: 20\n",
      "Q-values for current state: [-0.00271    -0.00271    -0.0020805  -0.00403209]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 19\n",
      "Reward: -0.01\n",
      "Terminated: True\n",
      "Old Q-value: -0.00271\n",
      "New Q-value: -0.003439\n",
      "\n",
      "Episode 50 summary:\n",
      "Total reward: -0.5000000000000002\n",
      "Successful episodes so far: []\n",
      "Average reward over last 10 episodes: -0.316\n",
      "\n",
      "Episode 51\n",
      "Starting state: 0\n",
      "\n",
      "Step 0\n",
      "Current state: 0\n",
      "Q-values for current state: [-0.03210871 -0.02605279 -0.02484562 -0.03215692]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 1\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.02484562313466822\n",
      "New Q-value: -0.025040203562156013\n",
      "\n",
      "Step 1\n",
      "Current state: 1\n",
      "Q-values for current state: [-0.03038905 -0.01947343 -0.01767519 -0.0237037 ]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 9\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.019473433839659673\n",
      "New Q-value: -0.019798414402105398\n",
      "\n",
      "Step 2\n",
      "Current state: 9\n",
      "Q-values for current state: [-0.02301478 -0.01339288 -0.01657029 -0.02029804]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 10\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.016570285858326293\n",
      "New Q-value: -0.01682027548885694\n",
      "\n",
      "Step 3\n",
      "Current state: 10\n",
      "Q-values for current state: [-0.01524501 -0.00954756 -0.01152391 -0.01193402]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 9\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.015245008308465369\n",
      "New Q-value: -0.01599283142403052\n",
      "\n",
      "Step 4\n",
      "Current state: 9\n",
      "Q-values for current state: [-0.02301478 -0.01339288 -0.01682028 -0.02029804]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 1\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.020298041974142783\n",
      "New Q-value: -0.02094738051768312\n",
      "\n",
      "Step 5\n",
      "Current state: 1\n",
      "Q-values for current state: [-0.03038905 -0.01979841 -0.01767519 -0.0237037 ]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 1\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.023703704340932236\n",
      "New Q-value: -0.024012476647793626\n",
      "\n",
      "Step 6\n",
      "Current state: 1\n",
      "Q-values for current state: [-0.03038905 -0.01979841 -0.01767519 -0.02401248]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 9\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.019798414402105398\n",
      "New Q-value: -0.02009089690830655\n",
      "\n",
      "Step 7\n",
      "Current state: 9\n",
      "Q-values for current state: [-0.02301478 -0.01339288 -0.01682028 -0.02094738]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 10\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.01682027548885694\n",
      "New Q-value: -0.017045266156334525\n",
      "\n",
      "Step 8\n",
      "Current state: 10\n",
      "Q-values for current state: [-0.01599283 -0.00954756 -0.01152391 -0.01193402]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 9\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.01599283142403052\n",
      "New Q-value: -0.016665872228039162\n",
      "\n",
      "Step 9\n",
      "Current state: 9\n",
      "Q-values for current state: [-0.02301478 -0.01339288 -0.01704527 -0.02094738]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 10\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.017045266156334525\n",
      "New Q-value: -0.01724775775706435\n",
      "\n",
      "Step 10\n",
      "Current state: 10\n",
      "Q-values for current state: [-0.01666587 -0.00954756 -0.01152391 -0.01193402]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 2\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.011934017936120152\n",
      "New Q-value: -0.012939236977791861\n",
      "\n",
      "Step 11\n",
      "Current state: 2\n",
      "Q-values for current state: [-0.02122526 -0.01327371 -0.01741351 -0.01261706]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 2\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.012617061424039214\n",
      "New Q-value: -0.013553976116919018\n",
      "\n",
      "Step 12\n",
      "Current state: 2\n",
      "Q-values for current state: [-0.02122526 -0.01327371 -0.01741351 -0.01355398]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 3\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.017413510589862877\n",
      "New Q-value: -0.017694314527206868\n",
      "\n",
      "Step 13\n",
      "Current state: 3\n",
      "Q-values for current state: [-0.0149728  -0.01172898 -0.01075953 -0.01587982]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 2\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.014972795088846226\n",
      "New Q-value: -0.015736517681592545\n",
      "\n",
      "Step 14\n",
      "Current state: 2\n",
      "Q-values for current state: [-0.02122526 -0.01327371 -0.01769431 -0.01355398]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 1\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.02122526460658359\n",
      "New Q-value: -0.021781880886879844\n",
      "\n",
      "Step 15\n",
      "Current state: 1\n",
      "Q-values for current state: [-0.03038905 -0.0200909  -0.01767519 -0.02401248]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 9\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.02009089690830655\n",
      "New Q-value: -0.020354131163887585\n",
      "\n",
      "Step 16\n",
      "Current state: 9\n",
      "Q-values for current state: [-0.02301478 -0.01339288 -0.01724776 -0.02094738]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 1\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.02094738051768312\n",
      "New Q-value: -0.021531785206869422\n",
      "\n",
      "Step 17\n",
      "Current state: 1\n",
      "Q-values for current state: [-0.03038905 -0.02035413 -0.01767519 -0.02401248]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 1\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.024012476647793626\n",
      "New Q-value: -0.024290371723968876\n",
      "\n",
      "Step 18\n",
      "Current state: 1\n",
      "Q-values for current state: [-0.03038905 -0.02035413 -0.01767519 -0.02429037]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 0\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.030389046622379068\n",
      "New Q-value: -0.030728961298545982\n",
      "\n",
      "Step 19\n",
      "Current state: 0\n",
      "Q-values for current state: [-0.03210871 -0.02605279 -0.0250402  -0.03215692]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 8\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.026052788392837666\n",
      "New Q-value: -0.026185928166605546\n",
      "\n",
      "Step 20\n",
      "Current state: 8\n",
      "Q-values for current state: [-0.02517402 -0.01829914 -0.02063751 -0.02838856]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 0\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.028388558154614347\n",
      "New Q-value: -0.028928521677557734\n",
      "\n",
      "Step 21\n",
      "Current state: 0\n",
      "Q-values for current state: [-0.03210871 -0.02618593 -0.0250402  -0.03215692]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 8\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.026185928166605546\n",
      "New Q-value: -0.02630575396299664\n",
      "\n",
      "Step 22\n",
      "Current state: 8\n",
      "Q-values for current state: [-0.02517402 -0.01829914 -0.02063751 -0.02892852]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 0\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.028928521677557734\n",
      "New Q-value: -0.02941448884820678\n",
      "\n",
      "Step 23\n",
      "Current state: 0\n",
      "Q-values for current state: [-0.03210871 -0.02630575 -0.0250402  -0.03215692]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 0\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.032156924384908525\n",
      "New Q-value: -0.032320051284822494\n",
      "\n",
      "Step 24\n",
      "Current state: 0\n",
      "Q-values for current state: [-0.03210871 -0.02630575 -0.0250402  -0.03232005]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 1\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.025040203562156013\n",
      "New Q-value: -0.025215325946895024\n",
      "\n",
      "Step 25\n",
      "Current state: 1\n",
      "Q-values for current state: [-0.03072896 -0.02035413 -0.01767519 -0.02429037]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 0\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.030728961298545982\n",
      "New Q-value: -0.031051521133646412\n",
      "\n",
      "Step 26\n",
      "Current state: 0\n",
      "Q-values for current state: [-0.03210871 -0.02630575 -0.02521533 -0.03232005]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 8\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.02630575396299664\n",
      "New Q-value: -0.026413597179748623\n",
      "\n",
      "Step 27\n",
      "Current state: 8\n",
      "Q-values for current state: [-0.02517402 -0.01829914 -0.02063751 -0.02941449]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 16\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.01829914329528049\n",
      "New Q-value: -0.018554418502857956\n",
      "\n",
      "Step 28\n",
      "Current state: 16\n",
      "Q-values for current state: [-0.01511066 -0.01142305 -0.01323367 -0.02316147]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 16\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.015110657919024589\n",
      "New Q-value: -0.015684781664227643\n",
      "\n",
      "Step 29\n",
      "Current state: 16\n",
      "Q-values for current state: [-0.01568478 -0.01142305 -0.01323367 -0.02316147]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 24\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.011423047759005413\n",
      "New Q-value: -0.011945420658507372\n",
      "\n",
      "Step 30\n",
      "Current state: 24\n",
      "Q-values for current state: [-0.00797998 -0.00896145 -0.00699661 -0.00862015]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 32\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.008961448803826001\n",
      "New Q-value: -0.009401033923443401\n",
      "\n",
      "Step 31\n",
      "Current state: 32\n",
      "Q-values for current state: [-0.00713155 -0.00700883 -0.003534   -0.0038589 ]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 32\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.0071315485105000005\n",
      "New Q-value: -0.007754123659450001\n",
      "\n",
      "Step 32\n",
      "Current state: 32\n",
      "Q-values for current state: [-0.00775412 -0.00700883 -0.003534   -0.0038589 ]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 33\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.0035340000000000002\n",
      "New Q-value: -0.004284625\n",
      "\n",
      "Step 33\n",
      "Current state: 33\n",
      "Q-values for current state: [-0.001095   -0.003439   -0.0019     -0.00209765]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 41\n",
      "Reward: -0.01\n",
      "Terminated: True\n",
      "Old Q-value: -0.003439\n",
      "New Q-value: -0.0040951\n",
      "\n",
      "Episode 52\n",
      "Starting state: 0\n",
      "\n",
      "Step 0\n",
      "Current state: 0\n",
      "Q-values for current state: [-0.03210871 -0.0264136  -0.02521533 -0.03232005]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 8\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.026413597179748623\n",
      "New Q-value: -0.026534907219545267\n",
      "\n",
      "Step 1\n",
      "Current state: 8\n",
      "Q-values for current state: [-0.02517402 -0.01855442 -0.02063751 -0.02941449]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 16\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.018554418502857956\n",
      "New Q-value: -0.018833791615130362\n",
      "\n",
      "Step 2\n",
      "Current state: 16\n",
      "Q-values for current state: [-0.01568478 -0.01194542 -0.01323367 -0.02316147]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 24\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.011945420658507372\n",
      "New Q-value: -0.012415556268059135\n",
      "\n",
      "Step 3\n",
      "Current state: 24\n",
      "Q-values for current state: [-0.00797998 -0.00940103 -0.00699661 -0.00862015]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 25\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.0069966071095\n",
      "New Q-value: -0.007494593898550001\n",
      "\n",
      "Step 4\n",
      "Current state: 25\n",
      "Q-values for current state: [-0.0078502  -0.0020805  -0.00760109 -0.00472745]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 26\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.007601094694500001\n",
      "New Q-value: -0.00818646673755\n",
      "\n",
      "Step 5\n",
      "Current state: 26\n",
      "Q-values for current state: [-0.00363665 -0.00531203 -0.00468559 -0.00492135]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 25\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.0036366475000000004\n",
      "New Q-value: -0.0044706302500000005\n",
      "\n",
      "Step 6\n",
      "Current state: 25\n",
      "Q-values for current state: [-0.0078502  -0.0020805  -0.00818647 -0.00472745]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 24\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.00785019621225\n",
      "New Q-value: -0.00877716301138725\n",
      "\n",
      "Step 7\n",
      "Current state: 24\n",
      "Q-values for current state: [-0.00797998 -0.00940103 -0.00749459 -0.00862015]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 16\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.008620151115039594\n",
      "New Q-value: -0.009937613849001253\n",
      "\n",
      "Step 8\n",
      "Current state: 16\n",
      "Q-values for current state: [-0.01568478 -0.01241556 -0.01323367 -0.02316147]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 17\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.01323367323077468\n",
      "New Q-value: -0.013593633816089712\n",
      "\n",
      "Step 9\n",
      "Current state: 17\n",
      "Q-values for current state: [-0.01529944 -0.00719293 -0.00744938 -0.01436814]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 9\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.014368137012758953\n",
      "New Q-value: -0.01520364725789475\n",
      "\n",
      "Step 10\n",
      "Current state: 9\n",
      "Q-values for current state: [-0.02301478 -0.01339288 -0.01724776 -0.02153179]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 1\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.021531785206869422\n",
      "New Q-value: -0.022057749427137095\n",
      "\n",
      "Step 11\n",
      "Current state: 1\n",
      "Q-values for current state: [-0.03105152 -0.02035413 -0.01767519 -0.02429037]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 2\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.017675186746890673\n",
      "New Q-value: -0.01816867017383255\n",
      "\n",
      "Step 12\n",
      "Current state: 2\n",
      "Q-values for current state: [-0.02178188 -0.01327371 -0.01769431 -0.01355398]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 2\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.013553976116919018\n",
      "New Q-value: -0.01445958060685806\n",
      "\n",
      "Step 13\n",
      "Current state: 2\n",
      "Q-values for current state: [-0.02178188 -0.01327371 -0.01769431 -0.01445958]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 1\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.021781880886879844\n",
      "New Q-value: -0.02232971646470595\n",
      "\n",
      "Step 14\n",
      "Current state: 1\n",
      "Q-values for current state: [-0.03105152 -0.02035413 -0.01816867 -0.02429037]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 2\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.01816867017383255\n",
      "New Q-value: -0.018612805258080237\n",
      "\n",
      "Step 15\n",
      "Current state: 2\n",
      "Q-values for current state: [-0.02232972 -0.01327371 -0.01769431 -0.01445958]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 1\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.02232971646470595\n",
      "New Q-value: -0.02286496131775298\n",
      "\n",
      "Step 16\n",
      "Current state: 1\n",
      "Q-values for current state: [-0.03105152 -0.02035413 -0.01861281 -0.02429037]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 0\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.031051521133646412\n",
      "New Q-value: -0.0313418249852368\n",
      "\n",
      "Step 17\n",
      "Current state: 0\n",
      "Q-values for current state: [-0.03210871 -0.02653491 -0.02521533 -0.03232005]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 1\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.025215325946895024\n",
      "New Q-value: -0.025462009851723144\n",
      "\n",
      "Step 18\n",
      "Current state: 1\n",
      "Q-values for current state: [-0.03134182 -0.02035413 -0.01861281 -0.02429037]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 1\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.024290371723968876\n",
      "New Q-value: -0.02462955105108961\n",
      "\n",
      "Step 19\n",
      "Current state: 1\n",
      "Q-values for current state: [-0.03134182 -0.02035413 -0.01861281 -0.02462955]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 0\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.0313418249852368\n",
      "New Q-value: -0.03162653342262682\n",
      "\n",
      "Step 20\n",
      "Current state: 0\n",
      "Q-values for current state: [-0.03210871 -0.02653491 -0.02546201 -0.03232005]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 1\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.025462009851723144\n",
      "New Q-value: -0.025684025366068453\n",
      "\n",
      "Step 21\n",
      "Current state: 1\n",
      "Q-values for current state: [-0.03162653 -0.02035413 -0.01861281 -0.02462955]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 2\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.018612805258080237\n",
      "New Q-value: -0.019012526833903157\n",
      "\n",
      "Step 22\n",
      "Current state: 2\n",
      "Q-values for current state: [-0.02286496 -0.01327371 -0.01769431 -0.01445958]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 2\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.01445958060685806\n",
      "New Q-value: -0.015274624647803197\n",
      "\n",
      "Step 23\n",
      "Current state: 2\n",
      "Q-values for current state: [-0.02286496 -0.01327371 -0.01769431 -0.01527462]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 1\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.02286496131775298\n",
      "New Q-value: -0.023384655235198483\n",
      "\n",
      "Step 24\n",
      "Current state: 1\n",
      "Q-values for current state: [-0.03162653 -0.02035413 -0.01901253 -0.02462955]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 0\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.03162653342262682\n",
      "New Q-value: -0.03190386249014064\n",
      "\n",
      "Step 25\n",
      "Current state: 0\n",
      "Q-values for current state: [-0.03210871 -0.02653491 -0.02568403 -0.03232005]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 8\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.026534907219545267\n",
      "New Q-value: -0.026670626701028124\n",
      "\n",
      "Step 26\n",
      "Current state: 8\n",
      "Q-values for current state: [-0.02517402 -0.01883379 -0.02063751 -0.02941449]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 8\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.025174019197432573\n",
      "New Q-value: -0.025445827481126698\n",
      "\n",
      "Step 27\n",
      "Current state: 8\n",
      "Q-values for current state: [-0.02544583 -0.01883379 -0.02063751 -0.02941449]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 8\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.025445827481126698\n",
      "New Q-value: -0.025690454936451414\n",
      "\n",
      "Step 28\n",
      "Current state: 8\n",
      "Q-values for current state: [-0.02569045 -0.01883379 -0.02063751 -0.02941449]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 16\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.018833791615130362\n",
      "New Q-value: -0.019129890299082943\n",
      "\n",
      "Step 29\n",
      "Current state: 16\n",
      "Q-values for current state: [-0.01568478 -0.01241556 -0.01359363 -0.02316147]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 16\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.015684781664227643\n",
      "New Q-value: -0.016295781343270498\n",
      "\n",
      "Step 30\n",
      "Current state: 16\n",
      "Q-values for current state: [-0.01629578 -0.01241556 -0.01359363 -0.02316147]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 8\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.02316147401875461\n",
      "New Q-value: -0.023662666195292028\n",
      "\n",
      "Step 31\n",
      "Current state: 8\n",
      "Q-values for current state: [-0.02569045 -0.01912989 -0.02063751 -0.02941449]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 8\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.025690454936451414\n",
      "New Q-value: -0.025938749021219152\n",
      "\n",
      "Step 32\n",
      "Current state: 8\n",
      "Q-values for current state: [-0.02593875 -0.01912989 -0.02063751 -0.02941449]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 0\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.02941448884820678\n",
      "New Q-value: -0.029913022373162606\n",
      "\n",
      "Step 33\n",
      "Current state: 0\n",
      "Q-values for current state: [-0.03210871 -0.02667063 -0.02568403 -0.03232005]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 8\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.026670626701028124\n",
      "New Q-value: -0.02682090360933819\n",
      "\n",
      "Step 34\n",
      "Current state: 8\n",
      "Q-values for current state: [-0.02593875 -0.01912989 -0.02063751 -0.02991302]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 0\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.029913022373162606\n",
      "New Q-value: -0.03036170254562285\n",
      "\n",
      "Step 35\n",
      "Current state: 0\n",
      "Q-values for current state: [-0.03210871 -0.0268209  -0.02568403 -0.03232005]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 1\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.025684025366068453\n",
      "New Q-value: -0.025921812878682407\n",
      "\n",
      "Step 36\n",
      "Current state: 1\n",
      "Q-values for current state: [-0.03190386 -0.02035413 -0.01901253 -0.02462955]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 2\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.019012526833903157\n",
      "New Q-value: -0.019372276252143782\n",
      "\n",
      "Step 37\n",
      "Current state: 2\n",
      "Q-values for current state: [-0.02338466 -0.01327371 -0.01769431 -0.01527462]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 10\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.013273706332957293\n",
      "New Q-value: -0.013853353916024842\n",
      "\n",
      "Step 38\n",
      "Current state: 10\n",
      "Q-values for current state: [-0.01666587 -0.00954756 -0.01152391 -0.01293924]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 9\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.016665872228039162\n",
      "New Q-value: -0.017271608951646936\n",
      "\n",
      "Step 39\n",
      "Current state: 9\n",
      "Q-values for current state: [-0.02301478 -0.01339288 -0.01724776 -0.02205775]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 10\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.01724775775706435\n",
      "New Q-value: -0.017430000197721195\n",
      "\n",
      "Step 40\n",
      "Current state: 10\n",
      "Q-values for current state: [-0.01727161 -0.00954756 -0.01152391 -0.01293924]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 18\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.009547560172245033\n",
      "New Q-value: -0.010080756933750622\n",
      "\n",
      "Step 41\n",
      "Current state: 18\n",
      "Q-values for current state: [-0.00653048 -0.00749285 -0.00521703 -0.00513635]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 17\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.006530476631585625\n",
      "New Q-value: -0.007560756876819563\n",
      "\n",
      "Step 42\n",
      "Current state: 17\n",
      "Q-values for current state: [-0.01529944 -0.00719293 -0.00744938 -0.01520365]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 16\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.015299435854781662\n",
      "New Q-value: -0.015948970114769114\n",
      "\n",
      "Step 43\n",
      "Current state: 16\n",
      "Q-values for current state: [-0.01629578 -0.01241556 -0.01359363 -0.02366267]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 8\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.023662666195292028\n",
      "New Q-value: -0.024113739154175706\n",
      "\n",
      "Step 44\n",
      "Current state: 8\n",
      "Q-values for current state: [-0.02593875 -0.01912989 -0.02063751 -0.0303617 ]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 16\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.019129890299082943\n",
      "New Q-value: -0.019396379114640268\n",
      "\n",
      "Step 45\n",
      "Current state: 16\n",
      "Q-values for current state: [-0.01629578 -0.01241556 -0.01359363 -0.02411374]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 8\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.024113739154175706\n",
      "New Q-value: -0.024545021254648962\n",
      "\n",
      "Step 46\n",
      "Current state: 8\n",
      "Q-values for current state: [-0.02593875 -0.01939638 -0.02063751 -0.0303617 ]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 16\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.019396379114640268\n",
      "New Q-value: -0.019636219048641858\n",
      "\n",
      "Step 47\n",
      "Current state: 16\n",
      "Q-values for current state: [-0.01629578 -0.01241556 -0.01359363 -0.02454502]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 16\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.016295781343270498\n",
      "New Q-value: -0.016845681054409065\n",
      "\n",
      "Step 48\n",
      "Current state: 16\n",
      "Q-values for current state: [-0.01684568 -0.01241556 -0.01359363 -0.02454502]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 8\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.024545021254648962\n",
      "New Q-value: -0.02495595993880504\n",
      "\n",
      "Step 49\n",
      "Current state: 8\n",
      "Q-values for current state: [-0.02593875 -0.01963622 -0.02063751 -0.0303617 ]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 16\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.019636219048641858\n",
      "New Q-value: -0.01985207498924329\n",
      "\n",
      "Step 50\n",
      "Current state: 16\n",
      "Q-values for current state: [-0.01684568 -0.01241556 -0.01359363 -0.02495596]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 8\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.02495595993880504\n",
      "New Q-value: -0.025346311068902647\n",
      "\n",
      "Step 51\n",
      "Current state: 8\n",
      "Q-values for current state: [-0.02593875 -0.01985207 -0.02063751 -0.0303617 ]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 8\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.025938749021219152\n",
      "New Q-value: -0.02623082124307535\n",
      "\n",
      "Step 52\n",
      "Current state: 8\n",
      "Q-values for current state: [-0.02623082 -0.01985207 -0.02063751 -0.0303617 ]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 0\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.03036170254562285\n",
      "New Q-value: -0.030788104514535394\n",
      "\n",
      "Step 53\n",
      "Current state: 0\n",
      "Q-values for current state: [-0.03210871 -0.0268209  -0.02592181 -0.03232005]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 1\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.025921812878682407\n",
      "New Q-value: -0.026169997834767825\n",
      "\n",
      "Step 54\n",
      "Current state: 1\n",
      "Q-values for current state: [-0.03190386 -0.02035413 -0.01937228 -0.02462955]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 9\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.020354131163887585\n",
      "New Q-value: -0.020591041993910517\n",
      "\n",
      "Step 55\n",
      "Current state: 9\n",
      "Q-values for current state: [-0.02301478 -0.01339288 -0.01743    -0.02205775]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 17\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.013392883646438853\n",
      "New Q-value: -0.013736923190187468\n",
      "\n",
      "Step 56\n",
      "Current state: 17\n",
      "Q-values for current state: [-0.01594897 -0.00719293 -0.00744938 -0.01520365]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 9\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.01520364725789475\n",
      "New Q-value: -0.015988290235173085\n",
      "\n",
      "Step 57\n",
      "Current state: 9\n",
      "Q-values for current state: [-0.02301478 -0.01373692 -0.01743    -0.02205775]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 17\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.013736923190187468\n",
      "New Q-value: -0.014046558779561222\n",
      "\n",
      "Step 58\n",
      "Current state: 17\n",
      "Q-values for current state: [-0.01594897 -0.00719293 -0.00744938 -0.01598829]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 18\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.007449376969655353\n",
      "New Q-value: -0.00819239205141991\n",
      "\n",
      "Step 59\n",
      "Current state: 18\n",
      "Q-values for current state: [-0.00756076 -0.00749285 -0.00521703 -0.00513635]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 10\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.005136345039264127\n",
      "New Q-value: -0.006580382444044024\n",
      "\n",
      "Step 60\n",
      "Current state: 10\n",
      "Q-values for current state: [-0.01727161 -0.01008076 -0.01152391 -0.01293924]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 2\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.012939236977791861\n",
      "New Q-value: -0.013961381902035035\n",
      "\n",
      "Step 61\n",
      "Current state: 2\n",
      "Q-values for current state: [-0.02338466 -0.01385335 -0.01769431 -0.01527462]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 1\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.023384655235198483\n",
      "New Q-value: -0.023886555955632292\n",
      "\n",
      "Step 62\n",
      "Current state: 1\n",
      "Q-values for current state: [-0.03190386 -0.02059104 -0.01937228 -0.02462955]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 0\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.03190386249014064\n",
      "New Q-value: -0.03219962603542952\n",
      "\n",
      "Step 63\n",
      "Current state: 0\n",
      "Q-values for current state: [-0.03210871 -0.0268209  -0.02617    -0.03232005]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 8\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.02682090360933819\n",
      "New Q-value: -0.027024760372382484\n",
      "\n",
      "Step 64\n",
      "Current state: 8\n",
      "Q-values for current state: [-0.02623082 -0.01985207 -0.02063751 -0.0307881 ]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 0\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.030788104514535394\n",
      "New Q-value: -0.0311954438573848\n",
      "\n",
      "Step 65\n",
      "Current state: 0\n",
      "Q-values for current state: [-0.03210871 -0.02702476 -0.02617    -0.03232005]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 1\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.026169997834767825\n",
      "New Q-value: -0.026393364295244703\n",
      "\n",
      "Step 66\n",
      "Current state: 1\n",
      "Q-values for current state: [-0.03219963 -0.02059104 -0.01937228 -0.02462955]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 2\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.019372276252143782\n",
      "New Q-value: -0.019751117248951764\n",
      "\n",
      "Step 67\n",
      "Current state: 2\n",
      "Q-values for current state: [-0.02388656 -0.01385335 -0.01769431 -0.01527462]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 2\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.015274624647803197\n",
      "New Q-value: -0.016063230805045237\n",
      "\n",
      "Step 68\n",
      "Current state: 2\n",
      "Q-values for current state: [-0.02388656 -0.01385335 -0.01769431 -0.01606323]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 2\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.016063230805045237\n",
      "New Q-value: -0.016772976346563072\n",
      "\n",
      "Step 69\n",
      "Current state: 2\n",
      "Q-values for current state: [-0.02388656 -0.01385335 -0.01769431 -0.01677298]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 2\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.016772976346563072\n",
      "New Q-value: -0.017411747333929126\n",
      "\n",
      "Step 70\n",
      "Current state: 2\n",
      "Q-values for current state: [-0.02388656 -0.01385335 -0.01769431 -0.01741175]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 1\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.023886555955632292\n",
      "New Q-value: -0.02437425649871948\n",
      "\n",
      "Step 71\n",
      "Current state: 1\n",
      "Q-values for current state: [-0.03219963 -0.02059104 -0.01975112 -0.02462955]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 2\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.019751117248951764\n",
      "New Q-value: -0.02009207414607895\n",
      "\n",
      "Step 72\n",
      "Current state: 2\n",
      "Q-values for current state: [-0.02437426 -0.01385335 -0.01769431 -0.01741175]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 1\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.02437425649871948\n",
      "New Q-value: -0.02484557789272503\n",
      "\n",
      "Step 73\n",
      "Current state: 1\n",
      "Q-values for current state: [-0.03219963 -0.02059104 -0.02009207 -0.02462955]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 2\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.02009207414607895\n",
      "New Q-value: -0.020398935353493415\n",
      "\n",
      "Step 74\n",
      "Current state: 2\n",
      "Q-values for current state: [-0.02484558 -0.01385335 -0.01769431 -0.01741175]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 3\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.017694314527206868\n",
      "New Q-value: -0.01794703807081646\n",
      "\n",
      "Step 75\n",
      "Current state: 3\n",
      "Q-values for current state: [-0.01573652 -0.01172898 -0.01075953 -0.01587982]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 4\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.010759526277160836\n",
      "New Q-value: -0.011238161846944752\n",
      "\n",
      "Step 76\n",
      "Current state: 4\n",
      "Q-values for current state: [-0.01125089 -0.00583777 -0.0070318  -0.00840747]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 3\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.011250891337660619\n",
      "New Q-value: -0.012193427579354309\n",
      "\n",
      "Step 77\n",
      "Current state: 3\n",
      "Q-values for current state: [-0.01573652 -0.01172898 -0.01123816 -0.01587982]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 4\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.011238161846944752\n",
      "New Q-value: -0.011668933859750277\n",
      "\n",
      "Step 78\n",
      "Current state: 4\n",
      "Q-values for current state: [-0.01219343 -0.00583777 -0.0070318  -0.00840747]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 4\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.008407474717486126\n",
      "New Q-value: -0.009121315443237514\n",
      "\n",
      "Step 79\n",
      "Current state: 4\n",
      "Q-values for current state: [-0.01219343 -0.00583777 -0.0070318  -0.00912132]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 4\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.009121315443237514\n",
      "New Q-value: -0.009763772096413763\n",
      "\n",
      "Step 80\n",
      "Current state: 4\n",
      "Q-values for current state: [-0.01219343 -0.00583777 -0.0070318  -0.00976377]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 5\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.007031802965406251\n",
      "New Q-value: -0.007798211468865626\n",
      "\n",
      "Step 81\n",
      "Current state: 5\n",
      "Q-values for current state: [-0.0059704  -0.00813805 -0.00494304 -0.00635922]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 6\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.00494304\n",
      "New Q-value: -0.005706186\n",
      "\n",
      "Step 82\n",
      "Current state: 6\n",
      "Q-values for current state: [-0.00417171 -0.00271    -0.00488612 -0.0061024 ]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 5\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.0041717110125\n",
      "New Q-value: -0.00529662758125\n",
      "\n",
      "Step 83\n",
      "Current state: 5\n",
      "Q-values for current state: [-0.0059704  -0.00813805 -0.00570619 -0.00635922]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 6\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.005706186\n",
      "New Q-value: -0.0063930174\n",
      "\n",
      "Step 84\n",
      "Current state: 6\n",
      "Q-values for current state: [-0.00529663 -0.00271    -0.00488612 -0.0061024 ]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 6\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.006102401550000001\n",
      "New Q-value: -0.006749611395000001\n",
      "\n",
      "Step 85\n",
      "Current state: 6\n",
      "Q-values for current state: [-0.00529663 -0.00271    -0.00488612 -0.00674961]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 6\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.006749611395000001\n",
      "New Q-value: -0.007332100255500001\n",
      "\n",
      "Step 86\n",
      "Current state: 6\n",
      "Q-values for current state: [-0.00529663 -0.00271    -0.00488612 -0.0073321 ]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 7\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.0048861175\n",
      "New Q-value: -0.0057641012500000005\n",
      "\n",
      "Step 87\n",
      "Current state: 7\n",
      "Q-values for current state: [-0.00558377 -0.00443805 -0.00627692 -0.0038589 ]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 15\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.004438050000000001\n",
      "New Q-value: -0.0051747450000000006\n",
      "\n",
      "Step 88\n",
      "Current state: 15\n",
      "Q-values for current state: [-0.00271    -0.00486609 -0.0019     -0.00536165]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 7\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.00536165275\n",
      "New Q-value: -0.006192082975000001\n",
      "\n",
      "Step 89\n",
      "Current state: 7\n",
      "Q-values for current state: [-0.00558377 -0.00517475 -0.00627692 -0.0038589 ]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 7\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.0038589\n",
      "New Q-value: -0.0048396055\n",
      "\n",
      "Step 90\n",
      "Current state: 7\n",
      "Q-values for current state: [-0.00558377 -0.00517475 -0.00627692 -0.00483961]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 7\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.0048396055\n",
      "New Q-value: -0.0058154074725\n",
      "\n",
      "Step 91\n",
      "Current state: 7\n",
      "Q-values for current state: [-0.00558377 -0.00517475 -0.00627692 -0.00581541]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 7\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.0058154074725\n",
      "New Q-value: -0.00672546750025\n",
      "\n",
      "Step 92\n",
      "Current state: 7\n",
      "Q-values for current state: [-0.00558377 -0.00517475 -0.00627692 -0.00672547]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 15\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.0051747450000000006\n",
      "New Q-value: -0.0058377705000000005\n",
      "\n",
      "Step 93\n",
      "Current state: 15\n",
      "Q-values for current state: [-0.00271    -0.00486609 -0.0019     -0.00619208]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 23\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.00486609\n",
      "New Q-value: -0.005474481\n",
      "\n",
      "Step 94\n",
      "Current state: 23\n",
      "Q-values for current state: [-0.00328513 -0.00271    -0.001      -0.00458426]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 22\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.00328513\n",
      "New Q-value: -0.0043004695\n",
      "\n",
      "Step 95\n",
      "Current state: 22\n",
      "Q-values for current state: [-0.00373032 -0.0040951  -0.0036195  -0.0040951 ]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 21\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.003730315125\n",
      "New Q-value: -0.0045630987375\n",
      "\n",
      "Step 96\n",
      "Current state: 21\n",
      "Q-values for current state: [-0.0036195  -0.00271    -0.00378195 -0.00216648]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 22\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.00378195\n",
      "New Q-value: -0.004747607500000001\n",
      "\n",
      "Step 97\n",
      "Current state: 22\n",
      "Q-values for current state: [-0.0045631 -0.0040951 -0.0036195 -0.0040951]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 30\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.0040951\n",
      "New Q-value: -0.00468559\n",
      "\n",
      "Step 98\n",
      "Current state: 30\n",
      "Q-values for current state: [ 0.         -0.00271    -0.001      -0.00435255]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 29\n",
      "Reward: -0.01\n",
      "Terminated: True\n",
      "Old Q-value: 0.0\n",
      "New Q-value: -0.001\n",
      "\n",
      "Episode 53\n",
      "Starting state: 0\n",
      "\n",
      "Step 0\n",
      "Current state: 0\n",
      "Q-values for current state: [-0.03210871 -0.02702476 -0.02639336 -0.03232005]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 0\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.032320051284822494\n",
      "New Q-value: -0.03259541576438849\n",
      "\n",
      "Step 1\n",
      "Current state: 0\n",
      "Q-values for current state: [-0.03210871 -0.02702476 -0.02639336 -0.03259542]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 1\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.026393364295244703\n",
      "New Q-value: -0.026691926724302106\n",
      "\n",
      "Step 2\n",
      "Current state: 1\n",
      "Q-values for current state: [-0.03219963 -0.02059104 -0.02039894 -0.02462955]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 1\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.02462955105108961\n",
      "New Q-value: -0.025104494804562526\n",
      "\n",
      "Step 3\n",
      "Current state: 1\n",
      "Q-values for current state: [-0.03219963 -0.02059104 -0.02039894 -0.02510449]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 0\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.03219962603542952\n",
      "New Q-value: -0.03251539647069527\n",
      "\n",
      "Step 4\n",
      "Current state: 0\n",
      "Q-values for current state: [-0.03210871 -0.02702476 -0.02669193 -0.03259542]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 1\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.026691926724302106\n",
      "New Q-value: -0.02696063291045377\n",
      "\n",
      "Step 5\n",
      "Current state: 1\n",
      "Q-values for current state: [-0.0325154  -0.02059104 -0.02039894 -0.02510449]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 0\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.03251539647069527\n",
      "New Q-value: -0.03282511695011885\n",
      "\n",
      "Step 6\n",
      "Current state: 0\n",
      "Q-values for current state: [-0.03210871 -0.02702476 -0.02696063 -0.03259542]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 0\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.03259541576438849\n",
      "New Q-value: -0.03289713431444275\n",
      "\n",
      "Step 7\n",
      "Current state: 0\n",
      "Q-values for current state: [-0.03210871 -0.02702476 -0.02696063 -0.03289713]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 0\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.03210871297833282\n",
      "New Q-value: -0.03245910180699265\n",
      "\n",
      "Step 8\n",
      "Current state: 0\n",
      "Q-values for current state: [-0.0324591  -0.02702476 -0.02696063 -0.03289713]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 0\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.03289713431444275\n",
      "New Q-value: -0.03316868100949159\n",
      "\n",
      "Step 9\n",
      "Current state: 0\n",
      "Q-values for current state: [-0.0324591  -0.02702476 -0.02696063 -0.03316868]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 0\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.03245910180699265\n",
      "New Q-value: -0.032774451752786496\n",
      "\n",
      "Step 10\n",
      "Current state: 0\n",
      "Q-values for current state: [-0.03277445 -0.02702476 -0.02696063 -0.03316868]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 0\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.032774451752786496\n",
      "New Q-value: -0.03305826670400096\n",
      "\n",
      "Step 11\n",
      "Current state: 0\n",
      "Q-values for current state: [-0.03305827 -0.02702476 -0.02696063 -0.03316868]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 8\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.027024760372382484\n",
      "New Q-value: -0.027208231459122347\n",
      "\n",
      "Step 12\n",
      "Current state: 8\n",
      "Q-values for current state: [-0.02623082 -0.01985207 -0.02063751 -0.03119544]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 9\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.020637510077524585\n",
      "New Q-value: -0.020908182153830442\n",
      "\n",
      "Step 13\n",
      "Current state: 9\n",
      "Q-values for current state: [-0.02301478 -0.01404656 -0.01743    -0.02205775]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 17\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.014046558779561222\n",
      "New Q-value: -0.0143252308099976\n",
      "\n",
      "Step 14\n",
      "Current state: 17\n",
      "Q-values for current state: [-0.01594897 -0.00719293 -0.00819239 -0.01598829]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 25\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.007192925351499999\n",
      "New Q-value: -0.007671280316349999\n",
      "\n",
      "Step 15\n",
      "Current state: 25\n",
      "Q-values for current state: [-0.00877716 -0.0020805  -0.00818647 -0.00472745]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 33\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.0020805\n",
      "New Q-value: -0.0029764749999999997\n",
      "\n",
      "Step 16\n",
      "Current state: 33\n",
      "Q-values for current state: [-0.001095   -0.0040951  -0.0019     -0.00209765]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 34\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.0019000000000000002\n",
      "New Q-value: -0.002805\n",
      "\n",
      "Step 17\n",
      "Current state: 34\n",
      "Q-values for current state: [-0.00289953 -0.00271    -0.001      -0.00255641]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 26\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.00255641487375\n",
      "New Q-value: -0.0037254832601250005\n",
      "\n",
      "Step 18\n",
      "Current state: 26\n",
      "Q-values for current state: [-0.00447063 -0.00531203 -0.00468559 -0.00492135]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 34\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.005312031\n",
      "New Q-value: -0.0058758279\n",
      "\n",
      "Step 19\n",
      "Current state: 34\n",
      "Q-values for current state: [-0.00289953 -0.00271    -0.001      -0.00372548]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 42\n",
      "Reward: -0.01\n",
      "Terminated: True\n",
      "Old Q-value: -0.00271\n",
      "New Q-value: -0.003439\n",
      "\n",
      "Episode 54\n",
      "Starting state: 0\n",
      "\n",
      "Step 0\n",
      "Current state: 0\n",
      "Q-values for current state: [-0.03305827 -0.02720823 -0.02696063 -0.03316868]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 0\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.03305826670400096\n",
      "New Q-value: -0.03331370016009397\n",
      "\n",
      "Step 1\n",
      "Current state: 0\n",
      "Q-values for current state: [-0.0333137  -0.02720823 -0.02696063 -0.03316868]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 0\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.03331370016009397\n",
      "New Q-value: -0.033543590270577676\n",
      "\n",
      "Step 2\n",
      "Current state: 0\n",
      "Q-values for current state: [-0.03354359 -0.02720823 -0.02696063 -0.03316868]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 8\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.027208231459122347\n",
      "New Q-value: -0.027373355437188225\n",
      "\n",
      "Step 3\n",
      "Current state: 8\n",
      "Q-values for current state: [-0.02623082 -0.01985207 -0.02090818 -0.03119544]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 9\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.020908182153830442\n",
      "New Q-value: -0.02117826086539717\n",
      "\n",
      "Step 4\n",
      "Current state: 9\n",
      "Q-values for current state: [-0.02301478 -0.01432523 -0.01743    -0.02205775]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 10\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.017430000197721195\n",
      "New Q-value: -0.017644672086655384\n",
      "\n",
      "Step 5\n",
      "Current state: 10\n",
      "Q-values for current state: [-0.01727161 -0.01008076 -0.01152391 -0.01396138]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 2\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.013961381902035035\n",
      "New Q-value: -0.014881312333853891\n",
      "\n",
      "Step 6\n",
      "Current state: 2\n",
      "Q-values for current state: [-0.02484558 -0.01385335 -0.01794704 -0.01741175]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 3\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.01794703807081646\n",
      "New Q-value: -0.018260882980411088\n",
      "\n",
      "Step 7\n",
      "Current state: 3\n",
      "Q-values for current state: [-0.01573652 -0.01172898 -0.01166893 -0.01587982]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 4\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.011668933859750277\n",
      "New Q-value: -0.01205662867127525\n",
      "\n",
      "Step 8\n",
      "Current state: 4\n",
      "Q-values for current state: [-0.01219343 -0.00583777 -0.00779821 -0.00976377]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 5\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.007798211468865626\n",
      "New Q-value: -0.008585578375772814\n",
      "\n",
      "Step 9\n",
      "Current state: 5\n",
      "Q-values for current state: [-0.0059704  -0.00813805 -0.00639302 -0.00635922]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 6\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.0063930174\n",
      "New Q-value: -0.007011165660000001\n",
      "\n",
      "Step 10\n",
      "Current state: 6\n",
      "Q-values for current state: [-0.00529663 -0.00271    -0.0057641  -0.0073321 ]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 5\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.00529662758125\n",
      "New Q-value: -0.00633415287691875\n",
      "\n",
      "Step 11\n",
      "Current state: 5\n",
      "Q-values for current state: [-0.0059704  -0.00813805 -0.00701117 -0.00635922]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 6\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.007011165660000001\n",
      "New Q-value: -0.007567499094000001\n",
      "\n",
      "Step 12\n",
      "Current state: 6\n",
      "Q-values for current state: [-0.00633415 -0.00271    -0.0057641  -0.0073321 ]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 5\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.00633415287691875\n",
      "New Q-value: -0.007267925643020625\n",
      "\n",
      "Step 13\n",
      "Current state: 5\n",
      "Q-values for current state: [-0.0059704  -0.00813805 -0.0075675  -0.00635922]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 13\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.008138052126234658\n",
      "New Q-value: -0.008769377963611193\n",
      "\n",
      "Step 14\n",
      "Current state: 13\n",
      "Q-values for current state: [-0.00655878 -0.00547448 -0.00468559 -0.00858544]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 12\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.006558781843740001\n",
      "New Q-value: -0.007298502012178501\n",
      "\n",
      "Step 15\n",
      "Current state: 12\n",
      "Q-values for current state: [-0.00564987 -0.00579033 -0.00935383 -0.00416419]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 11\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.005649874052648807\n",
      "New Q-value: -0.0066766848307202335\n",
      "\n",
      "Step 16\n",
      "Current state: 11\n",
      "Q-values for current state: [-0.00622945 -0.00686189 -0.0095967  -0.01116974]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 12\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.009596698008171582\n",
      "New Q-value: -0.010032626560166924\n",
      "\n",
      "Step 17\n",
      "Current state: 12\n",
      "Q-values for current state: [-0.00667668 -0.00579033 -0.00935383 -0.00416419]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 11\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.0066766848307202335\n",
      "New Q-value: -0.007600814530984517\n",
      "\n",
      "Step 18\n",
      "Current state: 11\n",
      "Q-values for current state: [-0.00622945 -0.00686189 -0.01003263 -0.01116974]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 3\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.011169740711161754\n",
      "New Q-value: -0.01216701938189155\n",
      "\n",
      "Step 19\n",
      "Current state: 3\n",
      "Q-values for current state: [-0.01573652 -0.01172898 -0.01205663 -0.01587982]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 11\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.011728976229957597\n",
      "New Q-value: -0.012147876790298143\n",
      "\n",
      "Step 20\n",
      "Current state: 11\n",
      "Q-values for current state: [-0.00622945 -0.00686189 -0.01003263 -0.01216702]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 3\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.01216701938189155\n",
      "New Q-value: -0.013095697167473544\n",
      "\n",
      "Step 21\n",
      "Current state: 3\n",
      "Q-values for current state: [-0.01573652 -0.01214788 -0.01205663 -0.01587982]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 2\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.015736517681592545\n",
      "New Q-value: -0.01647893453545565\n",
      "\n",
      "Step 22\n",
      "Current state: 2\n",
      "Q-values for current state: [-0.02484558 -0.01385335 -0.01826088 -0.01741175]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 2\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.017411747333929126\n",
      "New Q-value: -0.017986641222558573\n",
      "\n",
      "Step 23\n",
      "Current state: 2\n",
      "Q-values for current state: [-0.02484558 -0.01385335 -0.01826088 -0.01798664]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 10\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.013853353916024842\n",
      "New Q-value: -0.014425690433128666\n",
      "\n",
      "Step 24\n",
      "Current state: 10\n",
      "Q-values for current state: [-0.01727161 -0.01008076 -0.01152391 -0.01488131]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 2\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.014881312333853891\n",
      "New Q-value: -0.015763621691615725\n",
      "\n",
      "Step 25\n",
      "Current state: 2\n",
      "Q-values for current state: [-0.02484558 -0.01442569 -0.01826088 -0.01798664]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 3\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.018260882980411088\n",
      "New Q-value: -0.01858017440614113\n",
      "\n",
      "Step 26\n",
      "Current state: 3\n",
      "Q-values for current state: [-0.01647893 -0.01214788 -0.01205663 -0.01587982]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 2\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.01647893453545565\n",
      "New Q-value: -0.01720148167305731\n",
      "\n",
      "Step 27\n",
      "Current state: 2\n",
      "Q-values for current state: [-0.02484558 -0.01442569 -0.01858017 -0.01798664]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 10\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.014425690433128666\n",
      "New Q-value: -0.014940793298522109\n",
      "\n",
      "Step 28\n",
      "Current state: 10\n",
      "Q-values for current state: [-0.01727161 -0.01008076 -0.01152391 -0.01576362]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 11\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.011523911332389192\n",
      "New Q-value: -0.01196331838248658\n",
      "\n",
      "Step 29\n",
      "Current state: 11\n",
      "Q-values for current state: [-0.00622945 -0.00686189 -0.01003263 -0.0130957 ]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 10\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.006229454561434807\n",
      "New Q-value: -0.007564181013997636\n",
      "\n",
      "Step 30\n",
      "Current state: 10\n",
      "Q-values for current state: [-0.01727161 -0.01008076 -0.01196332 -0.01576362]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 2\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.015763621691615725\n",
      "New Q-value: -0.01660663488581375\n",
      "\n",
      "Step 31\n",
      "Current state: 2\n",
      "Q-values for current state: [-0.02484558 -0.01494079 -0.01858017 -0.01798664]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 3\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.01858017440614113\n",
      "New Q-value: -0.018867536689298164\n",
      "\n",
      "Step 32\n",
      "Current state: 3\n",
      "Q-values for current state: [-0.01720148 -0.01214788 -0.01205663 -0.01587982]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 3\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.015879824894220077\n",
      "New Q-value: -0.01643722212856922\n",
      "\n",
      "Step 33\n",
      "Current state: 3\n",
      "Q-values for current state: [-0.01720148 -0.01214788 -0.01205663 -0.01643722]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 2\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.01720148167305731\n",
      "New Q-value: -0.017900708869111177\n",
      "\n",
      "Step 34\n",
      "Current state: 2\n",
      "Q-values for current state: [-0.02484558 -0.01494079 -0.01886754 -0.01798664]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 3\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.018867536689298164\n",
      "New Q-value: -0.019126162744139495\n",
      "\n",
      "Step 35\n",
      "Current state: 3\n",
      "Q-values for current state: [-0.01790071 -0.01214788 -0.01205663 -0.01643722]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 3\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.01643722212856922\n",
      "New Q-value: -0.016938879639483446\n",
      "\n",
      "Step 36\n",
      "Current state: 3\n",
      "Q-values for current state: [-0.01790071 -0.01214788 -0.01205663 -0.01693888]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 3\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.016938879639483446\n",
      "New Q-value: -0.01739037139930625\n",
      "\n",
      "Step 37\n",
      "Current state: 3\n",
      "Q-values for current state: [-0.01790071 -0.01214788 -0.01205663 -0.01739037]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 2\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.017900708869111177\n",
      "New Q-value: -0.01853001334555966\n",
      "\n",
      "Step 38\n",
      "Current state: 2\n",
      "Q-values for current state: [-0.02484558 -0.01494079 -0.01912616 -0.01798664]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 3\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.019126162744139495\n",
      "New Q-value: -0.019358926193496693\n",
      "\n",
      "Step 39\n",
      "Current state: 3\n",
      "Q-values for current state: [-0.01853001 -0.01214788 -0.01205663 -0.01739037]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 3\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.01739037139930625\n",
      "New Q-value: -0.017796713983146775\n",
      "\n",
      "Step 40\n",
      "Current state: 3\n",
      "Q-values for current state: [-0.01853001 -0.01214788 -0.01205663 -0.01779671]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 4\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.01205662867127525\n",
      "New Q-value: -0.012405554001647726\n",
      "\n",
      "Step 41\n",
      "Current state: 4\n",
      "Q-values for current state: [-0.01219343 -0.00583777 -0.00858558 -0.00976377]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 4\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.009763772096413763\n",
      "New Q-value: -0.010341983084272387\n",
      "\n",
      "Step 42\n",
      "Current state: 4\n",
      "Q-values for current state: [-0.01219343 -0.00583777 -0.00858558 -0.01034198]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 3\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.012193427579354309\n",
      "New Q-value: -0.013128133116497201\n",
      "\n",
      "Step 43\n",
      "Current state: 3\n",
      "Q-values for current state: [-0.01853001 -0.01214788 -0.01240555 -0.01779671]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 2\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.01853001334555966\n",
      "New Q-value: -0.019096387374363293\n",
      "\n",
      "Step 44\n",
      "Current state: 2\n",
      "Q-values for current state: [-0.02484558 -0.01494079 -0.01935893 -0.01798664]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 1\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.02484557789272503\n",
      "New Q-value: -0.0252989189620344\n",
      "\n",
      "Step 45\n",
      "Current state: 1\n",
      "Q-values for current state: [-0.03282512 -0.02059104 -0.02039894 -0.02510449]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 0\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.03282511695011885\n",
      "New Q-value: -0.033103865381600074\n",
      "\n",
      "Step 46\n",
      "Current state: 0\n",
      "Q-values for current state: [-0.03354359 -0.02737336 -0.02696063 -0.03316868]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 0\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.03316868100949159\n",
      "New Q-value: -0.03341307303503554\n",
      "\n",
      "Step 47\n",
      "Current state: 0\n",
      "Q-values for current state: [-0.03354359 -0.02737336 -0.02696063 -0.03341307]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 1\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.02696063291045377\n",
      "New Q-value: -0.027202468477990266\n",
      "\n",
      "Step 48\n",
      "Current state: 1\n",
      "Q-values for current state: [-0.03310387 -0.02059104 -0.02039894 -0.02510449]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 2\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.020398935353493415\n",
      "New Q-value: -0.020778417181503674\n",
      "\n",
      "Step 49\n",
      "Current state: 2\n",
      "Q-values for current state: [-0.02529892 -0.01494079 -0.01935893 -0.01798664]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 3\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.019358926193496693\n",
      "New Q-value: -0.019577081869225348\n",
      "\n",
      "Step 50\n",
      "Current state: 3\n",
      "Q-values for current state: [-0.01909639 -0.01214788 -0.01240555 -0.01779671]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 4\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.012405554001647726\n",
      "New Q-value: -0.012719586798982954\n",
      "\n",
      "Step 51\n",
      "Current state: 4\n",
      "Q-values for current state: [-0.01312813 -0.00583777 -0.00858558 -0.01034198]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 12\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.0058377705000000005\n",
      "New Q-value: -0.0066495918028125\n",
      "\n",
      "Step 52\n",
      "Current state: 12\n",
      "Q-values for current state: [-0.00760081 -0.00579033 -0.00935383 -0.00416419]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 4\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.0041641931875\n",
      "New Q-value: -0.005379485090017188\n",
      "\n",
      "Step 53\n",
      "Current state: 4\n",
      "Q-values for current state: [-0.01312813 -0.00664959 -0.00858558 -0.01034198]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 4\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.010341983084272387\n",
      "New Q-value: -0.010939495997112336\n",
      "\n",
      "Step 54\n",
      "Current state: 4\n",
      "Q-values for current state: [-0.01312813 -0.00664959 -0.00858558 -0.0109395 ]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 3\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.013128133116497201\n",
      "New Q-value: -0.013969368099925805\n",
      "\n",
      "Step 55\n",
      "Current state: 3\n",
      "Q-values for current state: [-0.01909639 -0.01214788 -0.01271959 -0.01779671]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 2\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.019096387374363293\n",
      "New Q-value: -0.019606124000286565\n",
      "\n",
      "Step 56\n",
      "Current state: 2\n",
      "Q-values for current state: [-0.02529892 -0.01494079 -0.01957708 -0.01798664]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 3\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.019577081869225348\n",
      "New Q-value: -0.019773421977381137\n",
      "\n",
      "Step 57\n",
      "Current state: 3\n",
      "Q-values for current state: [-0.01960612 -0.01214788 -0.01271959 -0.01779671]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 4\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.012719586798982954\n",
      "New Q-value: -0.013079339340351846\n",
      "\n",
      "Step 58\n",
      "Current state: 4\n",
      "Q-values for current state: [-0.01396937 -0.00664959 -0.00858558 -0.0109395 ]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 4\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.010939495997112336\n",
      "New Q-value: -0.01147725761866829\n",
      "\n",
      "Step 59\n",
      "Current state: 4\n",
      "Q-values for current state: [-0.01396937 -0.00664959 -0.00858558 -0.01147726]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 3\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.013969368099925805\n",
      "New Q-value: -0.014726479585011549\n",
      "\n",
      "Step 60\n",
      "Current state: 3\n",
      "Q-values for current state: [-0.01960612 -0.01214788 -0.01307934 -0.01779671]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 4\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.013079339340351846\n",
      "New Q-value: -0.013403116627583849\n",
      "\n",
      "Step 61\n",
      "Current state: 4\n",
      "Q-values for current state: [-0.01472648 -0.00664959 -0.00858558 -0.01147726]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 4\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.01147725761866829\n",
      "New Q-value: -0.011961243078068649\n",
      "\n",
      "Step 62\n",
      "Current state: 4\n",
      "Q-values for current state: [-0.01472648 -0.00664959 -0.00858558 -0.01196124]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 4\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.011961243078068649\n",
      "New Q-value: -0.012396829991528972\n",
      "\n",
      "Step 63\n",
      "Current state: 4\n",
      "Q-values for current state: [-0.01472648 -0.00664959 -0.00858558 -0.01239683]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 5\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.008585578375772814\n",
      "New Q-value: -0.009294208591989282\n",
      "\n",
      "Step 64\n",
      "Current state: 5\n",
      "Q-values for current state: [-0.0059704  -0.00876938 -0.0075675  -0.00635922]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 13\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.008769377963611193\n",
      "New Q-value: -0.009337571217250073\n",
      "\n",
      "Step 65\n",
      "Current state: 13\n",
      "Q-values for current state: [-0.0072985  -0.00547448 -0.00468559 -0.00858544]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 12\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.007298502012178501\n",
      "New Q-value: -0.008079702894512285\n",
      "\n",
      "Step 66\n",
      "Current state: 12\n",
      "Q-values for current state: [-0.00760081 -0.00579033 -0.00935383 -0.00537949]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 13\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.0093538277502405\n",
      "New Q-value: -0.00986357602521645\n",
      "\n",
      "Step 67\n",
      "Current state: 13\n",
      "Q-values for current state: [-0.0080797  -0.00547448 -0.00468559 -0.00858544]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 12\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.008079702894512285\n",
      "New Q-value: -0.00878278368861269\n",
      "\n",
      "Step 68\n",
      "Current state: 12\n",
      "Q-values for current state: [-0.00760081 -0.00579033 -0.00986358 -0.00537949]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 13\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.00986357602521645\n",
      "New Q-value: -0.010322349472694805\n",
      "\n",
      "Step 69\n",
      "Current state: 13\n",
      "Q-values for current state: [-0.00878278 -0.00547448 -0.00468559 -0.00858544]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 12\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.00878278368861269\n",
      "New Q-value: -0.009415556403303054\n",
      "\n",
      "Step 70\n",
      "Current state: 12\n",
      "Q-values for current state: [-0.00760081 -0.00579033 -0.01032235 -0.00537949]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 4\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.005379485090017188\n",
      "New Q-value: -0.006473247802282657\n",
      "\n",
      "Step 71\n",
      "Current state: 4\n",
      "Q-values for current state: [-0.01472648 -0.00664959 -0.00929421 -0.01239683]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 12\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.0066495918028125\n",
      "New Q-value: -0.00753471377303125\n",
      "\n",
      "Step 72\n",
      "Current state: 12\n",
      "Q-values for current state: [-0.00760081 -0.00579033 -0.01032235 -0.00647325]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 11\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.007600814530984517\n",
      "New Q-value: -0.008492613011600566\n",
      "\n",
      "Step 73\n",
      "Current state: 11\n",
      "Q-values for current state: [-0.00756418 -0.00686189 -0.01003263 -0.0130957 ]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 19\n",
      "Reward: -0.01\n",
      "Terminated: True\n",
      "Old Q-value: -0.0068618940391\n",
      "New Q-value: -0.007175704635189999\n",
      "\n",
      "Episode 55\n",
      "Starting state: 0\n",
      "\n",
      "Step 0\n",
      "Current state: 0\n",
      "Q-values for current state: [-0.03354359 -0.02737336 -0.02720247 -0.03341307]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 1\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.027202468477990266\n",
      "New Q-value: -0.02743837061961274\n",
      "\n",
      "Step 1\n",
      "Current state: 1\n",
      "Q-values for current state: [-0.03310387 -0.02059104 -0.02077842 -0.02510449]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 9\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.020591041993910517\n",
      "New Q-value: -0.020892834721469238\n",
      "\n",
      "Step 2\n",
      "Current state: 9\n",
      "Q-values for current state: [-0.02301478 -0.01432523 -0.01764467 -0.02205775]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 8\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.023014783406618913\n",
      "New Q-value: -0.023599252189935135\n",
      "\n",
      "Step 3\n",
      "Current state: 8\n",
      "Q-values for current state: [-0.02623082 -0.01985207 -0.02117826 -0.03119544]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 0\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.0311954438573848\n",
      "New Q-value: -0.0316763682381792\n",
      "\n",
      "Step 4\n",
      "Current state: 0\n",
      "Q-values for current state: [-0.03354359 -0.02737336 -0.02743837 -0.03341307]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 1\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.02743837061961274\n",
      "New Q-value: -0.027668483189894316\n",
      "\n",
      "Step 5\n",
      "Current state: 1\n",
      "Q-values for current state: [-0.03310387 -0.02089283 -0.02077842 -0.02510449]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 9\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.020892834721469238\n",
      "New Q-value: -0.021164448176272086\n",
      "\n",
      "Step 6\n",
      "Current state: 9\n",
      "Q-values for current state: [-0.02359925 -0.01432523 -0.01764467 -0.02205775]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 17\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.0143252308099976\n",
      "New Q-value: -0.01462147935905109\n",
      "\n",
      "Step 7\n",
      "Current state: 17\n",
      "Q-values for current state: [-0.01594897 -0.00767128 -0.00819239 -0.01598829]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 25\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.007671280316349999\n",
      "New Q-value: -0.008186917409715\n",
      "\n",
      "Step 8\n",
      "Current state: 25\n",
      "Q-values for current state: [-0.00877716 -0.00297647 -0.00818647 -0.00472745]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 24\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.00877716301138725\n",
      "New Q-value: -0.009611433130610775\n",
      "\n",
      "Step 9\n",
      "Current state: 24\n",
      "Q-values for current state: [-0.00797998 -0.00940103 -0.00749459 -0.00993761]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 16\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.009937613849001253\n",
      "New Q-value: -0.011123330309566744\n",
      "\n",
      "Step 10\n",
      "Current state: 16\n",
      "Q-values for current state: [-0.01684568 -0.01241556 -0.01359363 -0.02534631]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 16\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.016845681054409065\n",
      "New Q-value: -0.017340590794433775\n",
      "\n",
      "Step 11\n",
      "Current state: 16\n",
      "Q-values for current state: [-0.01734059 -0.01241556 -0.01359363 -0.02534631]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 8\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.025346311068902647\n",
      "New Q-value: -0.025697627085990493\n",
      "\n",
      "Step 12\n",
      "Current state: 8\n",
      "Q-values for current state: [-0.02623082 -0.01985207 -0.02117826 -0.03167637]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 16\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.01985207498924329\n",
      "New Q-value: -0.020046345335784577\n",
      "\n",
      "Step 13\n",
      "Current state: 16\n",
      "Q-values for current state: [-0.01734059 -0.01241556 -0.01359363 -0.02569763]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 8\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.025697627085990493\n",
      "New Q-value: -0.02603226718429098\n",
      "\n",
      "Step 14\n",
      "Current state: 8\n",
      "Q-values for current state: [-0.02623082 -0.02004635 -0.02117826 -0.03167637]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 8\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.02623082124307535\n",
      "New Q-value: -0.02651214192566735\n",
      "\n",
      "Step 15\n",
      "Current state: 8\n",
      "Q-values for current state: [-0.02651214 -0.02004635 -0.02117826 -0.03167637]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 8\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.02651214192566735\n",
      "New Q-value: -0.026765330540000153\n",
      "\n",
      "Step 16\n",
      "Current state: 8\n",
      "Q-values for current state: [-0.02676533 -0.02004635 -0.02117826 -0.03167637]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 8\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.026765330540000153\n",
      "New Q-value: -0.02699320029289967\n",
      "\n",
      "Step 17\n",
      "Current state: 8\n",
      "Q-values for current state: [-0.0269932  -0.02004635 -0.02117826 -0.03167637]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 16\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.020046345335784577\n",
      "New Q-value: -0.020221188647671737\n",
      "\n",
      "Step 18\n",
      "Current state: 16\n",
      "Q-values for current state: [-0.01734059 -0.01241556 -0.01359363 -0.02603227]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 8\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.02603226718429098\n",
      "New Q-value: -0.026350053387390698\n",
      "\n",
      "Step 19\n",
      "Current state: 8\n",
      "Q-values for current state: [-0.0269932  -0.02022119 -0.02117826 -0.03167637]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 9\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.02117826086539717\n",
      "New Q-value: -0.021449475317967308\n",
      "\n",
      "Step 20\n",
      "Current state: 9\n",
      "Q-values for current state: [-0.02359925 -0.01462148 -0.01764467 -0.02205775]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 1\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.022057749427137095\n",
      "New Q-value: -0.022825924116666233\n",
      "\n",
      "Step 21\n",
      "Current state: 1\n",
      "Q-values for current state: [-0.03310387 -0.02116445 -0.02077842 -0.02510449]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 9\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.021164448176272086\n",
      "New Q-value: -0.02143704389775473\n",
      "\n",
      "Step 22\n",
      "Current state: 9\n",
      "Q-values for current state: [-0.02359925 -0.01462148 -0.01764467 -0.02282592]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 10\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.017644672086655384\n",
      "New Q-value: -0.017837876786696155\n",
      "\n",
      "Step 23\n",
      "Current state: 10\n",
      "Q-values for current state: [-0.01727161 -0.01008076 -0.01196332 -0.01660663]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 18\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.010080756933750622\n",
      "New Q-value: -0.01056829918537556\n",
      "\n",
      "Step 24\n",
      "Current state: 18\n",
      "Q-values for current state: [-0.00756076 -0.00749285 -0.00521703 -0.00658038]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 17\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.007560756876819563\n",
      "New Q-value: -0.008582438343060533\n",
      "\n",
      "Step 25\n",
      "Current state: 17\n",
      "Q-values for current state: [-0.01594897 -0.00818692 -0.00819239 -0.01598829]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 16\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.015948970114769114\n",
      "New Q-value: -0.01653355094875782\n",
      "\n",
      "Step 26\n",
      "Current state: 16\n",
      "Q-values for current state: [-0.01734059 -0.01241556 -0.01359363 -0.02635005]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 8\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.026350053387390698\n",
      "New Q-value: -0.026636060970180443\n",
      "\n",
      "Step 27\n",
      "Current state: 8\n",
      "Q-values for current state: [-0.0269932  -0.02022119 -0.02144948 -0.03167637]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 8\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.02699320029289967\n",
      "New Q-value: -0.02721489318513852\n",
      "\n",
      "Step 28\n",
      "Current state: 8\n",
      "Q-values for current state: [-0.02721489 -0.02022119 -0.02144948 -0.03167637]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 9\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.021449475317967308\n",
      "New Q-value: -0.02169356832528043\n",
      "\n",
      "Step 29\n",
      "Current state: 9\n",
      "Q-values for current state: [-0.02359925 -0.01462148 -0.01783788 -0.02282592]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 10\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.017837876786696155\n",
      "New Q-value: -0.018058077530637217\n",
      "\n",
      "Step 30\n",
      "Current state: 10\n",
      "Q-values for current state: [-0.01727161 -0.0105683  -0.01196332 -0.01660663]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 11\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.01196331838248658\n",
      "New Q-value: -0.012448678484580972\n",
      "\n",
      "Step 31\n",
      "Current state: 11\n",
      "Q-values for current state: [-0.00756418 -0.0071757  -0.01003263 -0.0130957 ]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 10\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.007564181013997636\n",
      "New Q-value: -0.008811751335208551\n",
      "\n",
      "Step 32\n",
      "Current state: 10\n",
      "Q-values for current state: [-0.01727161 -0.0105683  -0.01244868 -0.01660663]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 2\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.01660663488581375\n",
      "New Q-value: -0.017365346760591975\n",
      "\n",
      "Step 33\n",
      "Current state: 2\n",
      "Q-values for current state: [-0.02529892 -0.01494079 -0.01977342 -0.01798664]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 3\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.019773421977381137\n",
      "New Q-value: -0.019950128074721348\n",
      "\n",
      "Step 34\n",
      "Current state: 3\n",
      "Q-values for current state: [-0.01960612 -0.01214788 -0.01340312 -0.01779671]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 11\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.012147876790298143\n",
      "New Q-value: -0.012614781051611379\n",
      "\n",
      "Step 35\n",
      "Current state: 11\n",
      "Q-values for current state: [-0.00881175 -0.0071757  -0.01003263 -0.0130957 ]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 10\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.008811751335208551\n",
      "New Q-value: -0.009934564624298375\n",
      "\n",
      "Step 36\n",
      "Current state: 10\n",
      "Q-values for current state: [-0.01727161 -0.0105683  -0.01244868 -0.01736535]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 2\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.017365346760591975\n",
      "New Q-value: -0.018048187447892376\n",
      "\n",
      "Step 37\n",
      "Current state: 2\n",
      "Q-values for current state: [-0.02529892 -0.01494079 -0.01995013 -0.01798664]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 2\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.017986641222558573\n",
      "New Q-value: -0.018607352463662317\n",
      "\n",
      "Step 38\n",
      "Current state: 2\n",
      "Q-values for current state: [-0.02529892 -0.01494079 -0.01995013 -0.01860735]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 10\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.014940793298522109\n",
      "New Q-value: -0.015450702391280576\n",
      "\n",
      "Step 39\n",
      "Current state: 10\n",
      "Q-values for current state: [-0.01727161 -0.0105683  -0.01244868 -0.01804819]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 18\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.01056829918537556\n",
      "New Q-value: -0.011007087211838004\n",
      "\n",
      "Step 40\n",
      "Current state: 18\n",
      "Q-values for current state: [-0.00858244 -0.00749285 -0.00521703 -0.00658038]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 10\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.006580382444044024\n",
      "New Q-value: -0.007968017484764231\n",
      "\n",
      "Step 41\n",
      "Current state: 10\n",
      "Q-values for current state: [-0.01727161 -0.01100709 -0.01244868 -0.01804819]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 18\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.011007087211838004\n",
      "New Q-value: -0.011401996435654203\n",
      "\n",
      "Step 42\n",
      "Current state: 18\n",
      "Q-values for current state: [-0.00858244 -0.00749285 -0.00521703 -0.00796802]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 17\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.008582438343060533\n",
      "New Q-value: -0.009501951662677404\n",
      "\n",
      "Step 43\n",
      "Current state: 17\n",
      "Q-values for current state: [-0.01653355 -0.00818692 -0.00819239 -0.01598829]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 18\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.00819239205141991\n",
      "New Q-value: -0.008868770791277918\n",
      "\n",
      "Step 44\n",
      "Current state: 18\n",
      "Q-values for current state: [-0.00950195 -0.00749285 -0.00521703 -0.00796802]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 26\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.0074928506115\n",
      "New Q-value: -0.0081682754241\n",
      "\n",
      "Step 45\n",
      "Current state: 26\n",
      "Q-values for current state: [-0.00447063 -0.00587583 -0.00468559 -0.00492135]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 34\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.0058758279\n",
      "New Q-value: -0.00638324511\n",
      "\n",
      "Step 46\n",
      "Current state: 34\n",
      "Q-values for current state: [-0.00289953 -0.003439   -0.001      -0.00372548]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 33\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.002899525\n",
      "New Q-value: -0.0037135975\n",
      "\n",
      "Step 47\n",
      "Current state: 33\n",
      "Q-values for current state: [-0.001095   -0.0040951  -0.002805   -0.00209765]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 25\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.0020976475\n",
      "New Q-value: -0.003170647875\n",
      "\n",
      "Step 48\n",
      "Current state: 25\n",
      "Q-values for current state: [-0.00961143 -0.00297647 -0.00818647 -0.00472745]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 33\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.0029764749999999997\n",
      "New Q-value: -0.0037828525\n",
      "\n",
      "Step 49\n",
      "Current state: 33\n",
      "Q-values for current state: [-0.001095   -0.0040951  -0.002805   -0.00317065]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 25\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.003170647875\n",
      "New Q-value: -0.004212954075\n",
      "\n",
      "Step 50\n",
      "Current state: 25\n",
      "Q-values for current state: [-0.00961143 -0.00378285 -0.00818647 -0.00472745]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 33\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.0037828525\n",
      "New Q-value: -0.00450859225\n",
      "\n",
      "Step 51\n",
      "Current state: 33\n",
      "Q-values for current state: [-0.001095   -0.0040951  -0.002805   -0.00421295]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 41\n",
      "Reward: -0.01\n",
      "Terminated: True\n",
      "Old Q-value: -0.0040951\n",
      "New Q-value: -0.00468559\n",
      "\n",
      "Episode 56\n",
      "Starting state: 0\n",
      "\n",
      "Step 0\n",
      "Current state: 0\n",
      "Q-values for current state: [-0.03354359 -0.02737336 -0.02766848 -0.03341307]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 8\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.027373355437188225\n",
      "New Q-value: -0.027557032814998217\n",
      "\n",
      "Step 1\n",
      "Current state: 8\n",
      "Q-values for current state: [-0.02721489 -0.02022119 -0.02169357 -0.03167637]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 16\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.020221188647671737\n",
      "New Q-value: -0.02037854762837018\n",
      "\n",
      "Step 2\n",
      "Current state: 16\n",
      "Q-values for current state: [-0.01734059 -0.01241556 -0.01359363 -0.02663606]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 8\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.026636060970180443\n",
      "New Q-value: -0.026908416897857564\n",
      "\n",
      "Step 3\n",
      "Current state: 8\n",
      "Q-values for current state: [-0.02721489 -0.02037855 -0.02169357 -0.03167637]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 16\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.02037854762837018\n",
      "New Q-value: -0.020520170710998778\n",
      "\n",
      "Step 4\n",
      "Current state: 16\n",
      "Q-values for current state: [-0.01734059 -0.01241556 -0.01359363 -0.02690842]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 17\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.013593633816089712\n",
      "New Q-value: -0.014012027588403666\n",
      "\n",
      "Step 5\n",
      "Current state: 17\n",
      "Q-values for current state: [-0.01653355 -0.00818692 -0.00886877 -0.01598829]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 16\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.01653355094875782\n",
      "New Q-value: -0.017059673699347657\n",
      "\n",
      "Step 6\n",
      "Current state: 16\n",
      "Q-values for current state: [-0.01734059 -0.01241556 -0.01401203 -0.02690842]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 16\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.017340590794433775\n",
      "New Q-value: -0.017786009560456017\n",
      "\n",
      "Step 7\n",
      "Current state: 16\n",
      "Q-values for current state: [-0.01778601 -0.01241556 -0.01401203 -0.02690842]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 17\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.014012027588403666\n",
      "New Q-value: -0.014388581983486224\n",
      "\n",
      "Step 8\n",
      "Current state: 17\n",
      "Q-values for current state: [-0.01705967 -0.00818692 -0.00886877 -0.01598829]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 25\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.008186917409715\n",
      "New Q-value: -0.0087965419324935\n",
      "\n",
      "Step 9\n",
      "Current state: 25\n",
      "Q-values for current state: [-0.00961143 -0.00450859 -0.00818647 -0.00472745]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 17\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.004727452143335625\n",
      "New Q-value: -0.006090378412588944\n",
      "\n",
      "Step 10\n",
      "Current state: 17\n",
      "Q-values for current state: [-0.01705967 -0.00879654 -0.00886877 -0.01598829]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 18\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.008868770791277918\n",
      "New Q-value: -0.009477511657150127\n",
      "\n",
      "Step 11\n",
      "Current state: 18\n",
      "Q-values for current state: [-0.00950195 -0.00816828 -0.00521703 -0.00796802]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 10\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.007968017484764231\n",
      "New Q-value: -0.009254405397674957\n",
      "\n",
      "Step 12\n",
      "Current state: 10\n",
      "Q-values for current state: [-0.01727161 -0.011402   -0.01244868 -0.01804819]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 18\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.011401996435654203\n",
      "New Q-value: -0.011757414737088783\n",
      "\n",
      "Step 13\n",
      "Current state: 18\n",
      "Q-values for current state: [-0.00950195 -0.00816828 -0.00521703 -0.00925441]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 17\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.009501951662677404\n",
      "New Q-value: -0.010387427979996547\n",
      "\n",
      "Step 14\n",
      "Current state: 17\n",
      "Q-values for current state: [-0.01705967 -0.00879654 -0.00947751 -0.01598829]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 16\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.017059673699347657\n",
      "New Q-value: -0.017533184174878508\n",
      "\n",
      "Step 15\n",
      "Current state: 16\n",
      "Q-values for current state: [-0.01778601 -0.01241556 -0.01438858 -0.02690842]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 24\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.012415556268059135\n",
      "New Q-value: -0.012885987061615471\n",
      "\n",
      "Step 16\n",
      "Current state: 24\n",
      "Q-values for current state: [-0.00797998 -0.00940103 -0.00749459 -0.01112333]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 32\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.009401033923443401\n",
      "New Q-value: -0.00982752603109906\n",
      "\n",
      "Step 17\n",
      "Current state: 32\n",
      "Q-values for current state: [-0.00775412 -0.00700883 -0.00428463 -0.0038589 ]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 32\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.007754123659450001\n",
      "New Q-value: -0.008345306793505002\n",
      "\n",
      "Step 18\n",
      "Current state: 32\n",
      "Q-values for current state: [-0.00834531 -0.00700883 -0.00428463 -0.0038589 ]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 24\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.0038589\n",
      "New Q-value: -0.00518499642036225\n",
      "\n",
      "Step 19\n",
      "Current state: 24\n",
      "Q-values for current state: [-0.00797998 -0.00982753 -0.00749459 -0.01112333]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 16\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.011123330309566744\n",
      "New Q-value: -0.01223516604946354\n",
      "\n",
      "Step 20\n",
      "Current state: 16\n",
      "Q-values for current state: [-0.01778601 -0.01288599 -0.01438858 -0.02690842]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 24\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.012885987061615471\n",
      "New Q-value: -0.013309374775816174\n",
      "\n",
      "Step 21\n",
      "Current state: 24\n",
      "Q-values for current state: [-0.00797998 -0.00982753 -0.00749459 -0.01223517]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 16\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.01223516604946354\n",
      "New Q-value: -0.013276040048219723\n",
      "\n",
      "Step 22\n",
      "Current state: 16\n",
      "Q-values for current state: [-0.01778601 -0.01330937 -0.01438858 -0.02690842]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 16\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.017786009560456017\n",
      "New Q-value: -0.018271799208112952\n",
      "\n",
      "Step 23\n",
      "Current state: 16\n",
      "Q-values for current state: [-0.0182718  -0.01330937 -0.01438858 -0.02690842]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 17\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.014388581983486224\n",
      "New Q-value: -0.014785395268724483\n",
      "\n",
      "Step 24\n",
      "Current state: 17\n",
      "Q-values for current state: [-0.01753318 -0.00879654 -0.00947751 -0.01598829]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 18\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.009477511657150127\n",
      "New Q-value: -0.010025378436435113\n",
      "\n",
      "Step 25\n",
      "Current state: 18\n",
      "Q-values for current state: [-0.01038743 -0.00816828 -0.00521703 -0.00925441]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 19\n",
      "Reward: -0.01\n",
      "Terminated: True\n",
      "Old Q-value: -0.005217031\n",
      "New Q-value: -0.0056953279\n",
      "\n",
      "Episode 57\n",
      "Starting state: 0\n",
      "\n",
      "Step 0\n",
      "Current state: 0\n",
      "Q-values for current state: [-0.03354359 -0.02755703 -0.02766848 -0.03341307]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 0\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.03341307303503554\n",
      "New Q-value: -0.03368968384895681\n",
      "\n",
      "Step 1\n",
      "Current state: 0\n",
      "Q-values for current state: [-0.03354359 -0.02755703 -0.02766848 -0.03368968]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 1\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.027668483189894316\n",
      "New Q-value: -0.027875584503147734\n",
      "\n",
      "Step 2\n",
      "Current state: 1\n",
      "Q-values for current state: [-0.03310387 -0.02143704 -0.02077842 -0.02510449]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 9\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.02143704389775473\n",
      "New Q-value: -0.02168238004708911\n",
      "\n",
      "Step 3\n",
      "Current state: 9\n",
      "Q-values for current state: [-0.02359925 -0.01462148 -0.01805808 -0.02282592]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 10\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.018058077530637217\n",
      "New Q-value: -0.01836922417759693\n",
      "\n",
      "Step 4\n",
      "Current state: 10\n",
      "Q-values for current state: [-0.01727161 -0.01175741 -0.01244868 -0.01804819]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 18\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.011757414737088783\n",
      "New Q-value: -0.012122729413879905\n",
      "\n",
      "Step 5\n",
      "Current state: 18\n",
      "Q-values for current state: [-0.01038743 -0.00816828 -0.00569533 -0.00925441]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 17\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.010387427979996547\n",
      "New Q-value: -0.011184356665583775\n",
      "\n",
      "Step 6\n",
      "Current state: 17\n",
      "Q-values for current state: [-0.01753318 -0.00879654 -0.01002538 -0.01598829]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 16\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.017533184174878508\n",
      "New Q-value: -0.018044256361093195\n",
      "\n",
      "Step 7\n",
      "Current state: 16\n",
      "Q-values for current state: [-0.0182718  -0.01330937 -0.0147854  -0.02690842]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 8\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.026908416897857564\n",
      "New Q-value: -0.027166991425616693\n",
      "\n",
      "Step 8\n",
      "Current state: 8\n",
      "Q-values for current state: [-0.02721489 -0.02052017 -0.02169357 -0.03167637]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 16\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.020520170710998778\n",
      "New Q-value: -0.020732544243601436\n",
      "\n",
      "Step 9\n",
      "Current state: 16\n",
      "Q-values for current state: [-0.0182718  -0.01330937 -0.0147854  -0.02716699]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 17\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.014785395268724483\n",
      "New Q-value: -0.015142527225438917\n",
      "\n",
      "Step 10\n",
      "Current state: 17\n",
      "Q-values for current state: [-0.01804426 -0.00879654 -0.01002538 -0.01598829]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 16\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.018044256361093195\n",
      "New Q-value: -0.018504221328686413\n",
      "\n",
      "Step 11\n",
      "Current state: 16\n",
      "Q-values for current state: [-0.0182718  -0.01330937 -0.01514253 -0.02716699]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 8\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.027166991425616693\n",
      "New Q-value: -0.027419883986197158\n",
      "\n",
      "Step 12\n",
      "Current state: 8\n",
      "Q-values for current state: [-0.02721489 -0.02073254 -0.02169357 -0.03167637]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 16\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.020732544243601436\n",
      "New Q-value: -0.02092368042294383\n",
      "\n",
      "Step 13\n",
      "Current state: 16\n",
      "Q-values for current state: [-0.0182718  -0.01330937 -0.01514253 -0.02741988]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 8\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.027419883986197158\n",
      "New Q-value: -0.027665645227757104\n",
      "\n",
      "Step 14\n",
      "Current state: 8\n",
      "Q-values for current state: [-0.02721489 -0.02092368 -0.02169357 -0.03167637]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 0\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.0316763682381792\n",
      "New Q-value: -0.032126649531786115\n",
      "\n",
      "Step 15\n",
      "Current state: 0\n",
      "Q-values for current state: [-0.03354359 -0.02755703 -0.02787558 -0.03368968]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 0\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.03368968384895681\n",
      "New Q-value: -0.03393863358148596\n",
      "\n",
      "Step 16\n",
      "Current state: 0\n",
      "Q-values for current state: [-0.03354359 -0.02755703 -0.02787558 -0.03393863]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 8\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.027557032814998217\n",
      "New Q-value: -0.02778907917367806\n",
      "\n",
      "Step 17\n",
      "Current state: 8\n",
      "Q-values for current state: [-0.02721489 -0.02092368 -0.02169357 -0.03212665]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 16\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.02092368042294383\n",
      "New Q-value: -0.021095702984351983\n",
      "\n",
      "Step 18\n",
      "Current state: 16\n",
      "Q-values for current state: [-0.0182718  -0.01330937 -0.01514253 -0.02766565]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 16\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.018271799208112952\n",
      "New Q-value: -0.018709009891004193\n",
      "\n",
      "Step 19\n",
      "Current state: 16\n",
      "Q-values for current state: [-0.01870901 -0.01330937 -0.01514253 -0.02766565]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 16\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.018709009891004193\n",
      "New Q-value: -0.01910249950560631\n",
      "\n",
      "Step 20\n",
      "Current state: 16\n",
      "Q-values for current state: [-0.0191025  -0.01330937 -0.01514253 -0.02766565]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 17\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.015142527225438917\n",
      "New Q-value: -0.015463945986481908\n",
      "\n",
      "Step 21\n",
      "Current state: 17\n",
      "Q-values for current state: [-0.01850422 -0.00879654 -0.01002538 -0.01598829]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 25\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.0087965419324935\n",
      "New Q-value: -0.009345204002994151\n",
      "\n",
      "Step 22\n",
      "Current state: 25\n",
      "Q-values for current state: [-0.00961143 -0.00450859 -0.00818647 -0.00609038]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 33\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.00450859225\n",
      "New Q-value: -0.005161758025\n",
      "\n",
      "Step 23\n",
      "Current state: 33\n",
      "Q-values for current state: [-0.001095   -0.00468559 -0.002805   -0.00421295]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 32\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.001095\n",
      "New Q-value: -0.002392539375\n",
      "\n",
      "Step 24\n",
      "Current state: 32\n",
      "Q-values for current state: [-0.00834531 -0.00700883 -0.00428463 -0.005185  ]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 32\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.008345306793505002\n",
      "New Q-value: -0.0089178154891545\n",
      "\n",
      "Step 25\n",
      "Current state: 32\n",
      "Q-values for current state: [-0.00891782 -0.00700883 -0.00428463 -0.005185  ]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 24\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.00518499642036225\n",
      "New Q-value: -0.006378483198688275\n",
      "\n",
      "Step 26\n",
      "Current state: 24\n",
      "Q-values for current state: [-0.00797998 -0.00982753 -0.00749459 -0.01327604]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 16\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.013276040048219723\n",
      "New Q-value: -0.014212826647100288\n",
      "\n",
      "Step 27\n",
      "Current state: 16\n",
      "Q-values for current state: [-0.0191025  -0.01330937 -0.01546395 -0.02766565]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 16\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.01910249950560631\n",
      "New Q-value: -0.019456640158748213\n",
      "\n",
      "Step 28\n",
      "Current state: 16\n",
      "Q-values for current state: [-0.01945664 -0.01330937 -0.01546395 -0.02766565]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 17\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.015463945986481908\n",
      "New Q-value: -0.01580534576811816\n",
      "\n",
      "Step 29\n",
      "Current state: 17\n",
      "Q-values for current state: [-0.01850422 -0.0093452  -0.01002538 -0.01598829]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 18\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.010025378436435113\n",
      "New Q-value: -0.010563896743291602\n",
      "\n",
      "Step 30\n",
      "Current state: 18\n",
      "Q-values for current state: [-0.01118436 -0.00816828 -0.00569533 -0.00925441]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 10\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.009254405397674957\n",
      "New Q-value: -0.010480624152226052\n",
      "\n",
      "Step 31\n",
      "Current state: 10\n",
      "Q-values for current state: [-0.01727161 -0.01212273 -0.01244868 -0.01804819]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 9\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.017271608951646936\n",
      "New Q-value: -0.017933488595592094\n",
      "\n",
      "Step 32\n",
      "Current state: 9\n",
      "Q-values for current state: [-0.02359925 -0.01462148 -0.01836922 -0.02282592]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 8\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.023599252189935135\n",
      "New Q-value: -0.02424341875445506\n",
      "\n",
      "Step 33\n",
      "Current state: 8\n",
      "Q-values for current state: [-0.02721489 -0.0210957  -0.02169357 -0.03212665]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 8\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.02721489318513852\n",
      "New Q-value: -0.027497495650138107\n",
      "\n",
      "Step 34\n",
      "Current state: 8\n",
      "Q-values for current state: [-0.0274975  -0.0210957  -0.02169357 -0.03212665]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 8\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.027497495650138107\n",
      "New Q-value: -0.027751837868637736\n",
      "\n",
      "Step 35\n",
      "Current state: 8\n",
      "Q-values for current state: [-0.02775184 -0.0210957  -0.02169357 -0.03212665]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 0\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.032126649531786115\n",
      "New Q-value: -0.03255394710010692\n",
      "\n",
      "Step 36\n",
      "Current state: 0\n",
      "Q-values for current state: [-0.03354359 -0.02778908 -0.02787558 -0.03393863]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 0\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.033543590270577676\n",
      "New Q-value: -0.033829193765019326\n",
      "\n",
      "Step 37\n",
      "Current state: 0\n",
      "Q-values for current state: [-0.03382919 -0.02778908 -0.02787558 -0.03393863]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 0\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.03393863358148596\n",
      "New Q-value: -0.03418473274483678\n",
      "\n",
      "Step 38\n",
      "Current state: 0\n",
      "Q-values for current state: [-0.03382919 -0.02778908 -0.02787558 -0.03418473]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 0\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.03418473274483678\n",
      "New Q-value: -0.03440622199185252\n",
      "\n",
      "Step 39\n",
      "Current state: 0\n",
      "Q-values for current state: [-0.03382919 -0.02778908 -0.02787558 -0.03440622]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 1\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.027875584503147734\n",
      "New Q-value: -0.028061975685075808\n",
      "\n",
      "Step 40\n",
      "Current state: 1\n",
      "Q-values for current state: [-0.03310387 -0.02168238 -0.02077842 -0.02510449]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 1\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.025104494804562526\n",
      "New Q-value: -0.02556799495634912\n",
      "\n",
      "Step 41\n",
      "Current state: 1\n",
      "Q-values for current state: [-0.03310387 -0.02168238 -0.02077842 -0.02556799]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 0\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.033103865381600074\n",
      "New Q-value: -0.03343344136493948\n",
      "\n",
      "Step 42\n",
      "Current state: 0\n",
      "Q-values for current state: [-0.03382919 -0.02778908 -0.02806198 -0.03440622]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 1\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.028061975685075808\n",
      "New Q-value: -0.028229727748811075\n",
      "\n",
      "Step 43\n",
      "Current state: 1\n",
      "Q-values for current state: [-0.03343344 -0.02168238 -0.02077842 -0.02556799]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 0\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.03343344136493948\n",
      "New Q-value: -0.033730059749944946\n",
      "\n",
      "Step 44\n",
      "Current state: 0\n",
      "Q-values for current state: [-0.03382919 -0.02778908 -0.02822973 -0.03440622]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 1\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.028229727748811075\n",
      "New Q-value: -0.028380704606172816\n",
      "\n",
      "Step 45\n",
      "Current state: 1\n",
      "Q-values for current state: [-0.03373006 -0.02168238 -0.02077842 -0.02556799]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 2\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.020778417181503674\n",
      "New Q-value: -0.02116839219052496\n",
      "\n",
      "Step 46\n",
      "Current state: 2\n",
      "Q-values for current state: [-0.02529892 -0.0154507  -0.01995013 -0.01860735]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 1\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.0252989189620344\n",
      "New Q-value: -0.025780024323930832\n",
      "\n",
      "Step 47\n",
      "Current state: 1\n",
      "Q-values for current state: [-0.03373006 -0.02168238 -0.02116839 -0.02556799]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 2\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.02116839219052496\n",
      "New Q-value: -0.021519369698644118\n",
      "\n",
      "Step 48\n",
      "Current state: 2\n",
      "Q-values for current state: [-0.02578002 -0.0154507  -0.01995013 -0.01860735]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 2\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.018607352463662317\n",
      "New Q-value: -0.01921443394446774\n",
      "\n",
      "Step 49\n",
      "Current state: 2\n",
      "Q-values for current state: [-0.02578002 -0.0154507  -0.01995013 -0.01921443]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 10\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.015450702391280576\n",
      "New Q-value: -0.016057291446471108\n",
      "\n",
      "Step 50\n",
      "Current state: 10\n",
      "Q-values for current state: [-0.01793349 -0.01212273 -0.01244868 -0.01804819]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 18\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.012122729413879905\n",
      "New Q-value: -0.012451512622991915\n",
      "\n",
      "Step 51\n",
      "Current state: 18\n",
      "Q-values for current state: [-0.01118436 -0.00816828 -0.00569533 -0.01048062]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 19\n",
      "Reward: -0.01\n",
      "Terminated: True\n",
      "Old Q-value: -0.0056953279\n",
      "New Q-value: -0.00612579511\n",
      "\n",
      "Episode 58\n",
      "Starting state: 0\n",
      "\n",
      "Step 0\n",
      "Current state: 0\n",
      "Q-values for current state: [-0.03382919 -0.02778908 -0.0283807  -0.03440622]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 1\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.028380704606172816\n",
      "New Q-value: -0.028586974266926725\n",
      "\n",
      "Step 1\n",
      "Current state: 1\n",
      "Q-values for current state: [-0.03373006 -0.02168238 -0.02151937 -0.02556799]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 1\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.02556799495634912\n",
      "New Q-value: -0.026055535582085402\n",
      "\n",
      "Step 2\n",
      "Current state: 1\n",
      "Q-values for current state: [-0.03373006 -0.02168238 -0.02151937 -0.02605554]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 2\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.021519369698644118\n",
      "New Q-value: -0.02189287541619446\n",
      "\n",
      "Step 3\n",
      "Current state: 2\n",
      "Q-values for current state: [-0.02578002 -0.01605729 -0.01995013 -0.01921443]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 3\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.019950128074721348\n",
      "New Q-value: -0.020153519467152294\n",
      "\n",
      "Step 4\n",
      "Current state: 3\n",
      "Q-values for current state: [-0.01960612 -0.01261478 -0.01340312 -0.01779671]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 2\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.019606124000286565\n",
      "New Q-value: -0.020170954287672665\n",
      "\n",
      "Step 5\n",
      "Current state: 2\n",
      "Q-values for current state: [-0.02578002 -0.01605729 -0.02015352 -0.01921443]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 3\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.020153519467152294\n",
      "New Q-value: -0.020336571720340146\n",
      "\n",
      "Step 6\n",
      "Current state: 3\n",
      "Q-values for current state: [-0.02017095 -0.01261478 -0.01340312 -0.01779671]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 11\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.012614781051611379\n",
      "New Q-value: -0.013034994886793291\n",
      "\n",
      "Step 7\n",
      "Current state: 11\n",
      "Q-values for current state: [-0.00993456 -0.0071757  -0.01003263 -0.0130957 ]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 19\n",
      "Reward: -0.01\n",
      "Terminated: True\n",
      "Old Q-value: -0.007175704635189999\n",
      "New Q-value: -0.007458134171670999\n",
      "\n",
      "Episode 59\n",
      "Starting state: 0\n",
      "\n",
      "Step 0\n",
      "Current state: 0\n",
      "Q-values for current state: [-0.03382919 -0.02778908 -0.02858697 -0.03440622]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 0\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.03440622199185252\n",
      "New Q-value: -0.03460556231416668\n",
      "\n",
      "Step 1\n",
      "Current state: 0\n",
      "Q-values for current state: [-0.03382919 -0.02778908 -0.02858697 -0.03460556]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 1\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.028586974266926725\n",
      "New Q-value: -0.028788102944707517\n",
      "\n",
      "Step 2\n",
      "Current state: 1\n",
      "Q-values for current state: [-0.03373006 -0.02168238 -0.02189288 -0.02605554]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 2\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.02189287541619446\n",
      "New Q-value: -0.022229030561989768\n",
      "\n",
      "Step 3\n",
      "Current state: 2\n",
      "Q-values for current state: [-0.02578002 -0.01605729 -0.02033657 -0.01921443]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 3\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.020336571720340146\n",
      "New Q-value: -0.020541239062551495\n",
      "\n",
      "Step 4\n",
      "Current state: 3\n",
      "Q-values for current state: [-0.02017095 -0.01303499 -0.01340312 -0.01779671]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 11\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.013034994886793291\n",
      "New Q-value: -0.013440018144422706\n",
      "\n",
      "Step 5\n",
      "Current state: 11\n",
      "Q-values for current state: [-0.00993456 -0.00745813 -0.01003263 -0.0130957 ]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 3\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.013095697167473544\n",
      "New Q-value: -0.014059423530346655\n",
      "\n",
      "Step 6\n",
      "Current state: 3\n",
      "Q-values for current state: [-0.02017095 -0.01344002 -0.01340312 -0.01779671]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 4\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.013403116627583849\n",
      "New Q-value: -0.013778602773263433\n",
      "\n",
      "Step 7\n",
      "Current state: 4\n",
      "Q-values for current state: [-0.01472648 -0.00753471 -0.00929421 -0.01239683]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 4\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.012396829991528972\n",
      "New Q-value: -0.012872944800814043\n",
      "\n",
      "Step 8\n",
      "Current state: 4\n",
      "Q-values for current state: [-0.01472648 -0.00753471 -0.00929421 -0.01287294]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 12\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.00753471377303125\n",
      "New Q-value: -0.008331323546228125\n",
      "\n",
      "Step 9\n",
      "Current state: 12\n",
      "Q-values for current state: [-0.00849261 -0.00579033 -0.01032235 -0.00647325]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 13\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.010322349472694805\n",
      "New Q-value: -0.010735245575425325\n",
      "\n",
      "Step 10\n",
      "Current state: 13\n",
      "Q-values for current state: [-0.00941556 -0.00547448 -0.00468559 -0.00858544]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 21\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.005474481\n",
      "New Q-value: -0.006132848025\n",
      "\n",
      "Step 11\n",
      "Current state: 21\n",
      "Q-values for current state: [-0.0036195  -0.00271    -0.00474761 -0.00216648]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 29\n",
      "Reward: -0.01\n",
      "Terminated: True\n",
      "Old Q-value: -0.00271\n",
      "New Q-value: -0.003439\n",
      "\n",
      "Episode 60\n",
      "Starting state: 0\n",
      "\n",
      "Step 0\n",
      "Current state: 0\n",
      "Q-values for current state: [-0.03382919 -0.02778908 -0.0287881  -0.03460556]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 0\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.033829193765019326\n",
      "New Q-value: -0.03408623691001681\n",
      "\n",
      "Step 1\n",
      "Current state: 0\n",
      "Q-values for current state: [-0.03408624 -0.02778908 -0.0287881  -0.03460556]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 8\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.02778907917367806\n",
      "New Q-value: -0.02801426303982369\n",
      "\n",
      "Step 2\n",
      "Current state: 8\n",
      "Q-values for current state: [-0.02775184 -0.0210957  -0.02169357 -0.03255395]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 8\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.027751837868637736\n",
      "New Q-value: -0.027980745865287402\n",
      "\n",
      "Step 3\n",
      "Current state: 8\n",
      "Q-values for current state: [-0.02798075 -0.0210957  -0.02169357 -0.03255395]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 0\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.03255394710010692\n",
      "New Q-value: -0.03295990737887948\n",
      "\n",
      "Step 4\n",
      "Current state: 0\n",
      "Q-values for current state: [-0.03408624 -0.02801426 -0.0287881  -0.03460556]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 1\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.028788102944707517\n",
      "New Q-value: -0.02896911875471023\n",
      "\n",
      "Step 5\n",
      "Current state: 1\n",
      "Q-values for current state: [-0.03373006 -0.02168238 -0.02222903 -0.02605554]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 0\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.033730059749944946\n",
      "New Q-value: -0.0340184087637337\n",
      "\n",
      "Step 6\n",
      "Current state: 0\n",
      "Q-values for current state: [-0.03408624 -0.02801426 -0.02896912 -0.03460556]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 0\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.03460556231416668\n",
      "New Q-value: -0.03480636107153326\n",
      "\n",
      "Step 7\n",
      "Current state: 0\n",
      "Q-values for current state: [-0.03408624 -0.02801426 -0.02896912 -0.03480636]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 0\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.03480636107153326\n",
      "New Q-value: -0.034987079953163185\n",
      "\n",
      "Step 8\n",
      "Current state: 0\n",
      "Q-values for current state: [-0.03408624 -0.02801426 -0.02896912 -0.03498708]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 0\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.03408623691001681\n",
      "New Q-value: -0.03433896820779838\n",
      "\n",
      "Step 9\n",
      "Current state: 0\n",
      "Q-values for current state: [-0.03433897 -0.02801426 -0.02896912 -0.03498708]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 8\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.02801426303982369\n",
      "New Q-value: -0.02821692851935476\n",
      "\n",
      "Step 10\n",
      "Current state: 8\n",
      "Q-values for current state: [-0.02798075 -0.0210957  -0.02169357 -0.03295991]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 8\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.027980745865287402\n",
      "New Q-value: -0.0281867630622721\n",
      "\n",
      "Step 11\n",
      "Current state: 8\n",
      "Q-values for current state: [-0.02818676 -0.0210957  -0.02169357 -0.03295991]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 0\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.03295990737887948\n",
      "New Q-value: -0.03334452485033024\n",
      "\n",
      "Step 12\n",
      "Current state: 0\n",
      "Q-values for current state: [-0.03433897 -0.02821693 -0.02896912 -0.03498708]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 0\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.03433896820779838\n",
      "New Q-value: -0.03458567959635724\n",
      "\n",
      "Step 13\n",
      "Current state: 0\n",
      "Q-values for current state: [-0.03458568 -0.02821693 -0.02896912 -0.03498708]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 8\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.02821692851935476\n",
      "New Q-value: -0.028399327450932724\n",
      "\n",
      "Step 14\n",
      "Current state: 8\n",
      "Q-values for current state: [-0.02818676 -0.0210957  -0.02169357 -0.03334452]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 8\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.0281867630622721\n",
      "New Q-value: -0.02837217853955833\n",
      "\n",
      "Step 15\n",
      "Current state: 8\n",
      "Q-values for current state: [-0.02837218 -0.0210957  -0.02169357 -0.03334452]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 9\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.02169356832528043\n",
      "New Q-value: -0.021913252031862243\n",
      "\n",
      "Step 16\n",
      "Current state: 9\n",
      "Q-values for current state: [-0.02424342 -0.01462148 -0.01836922 -0.02282592]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 8\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.02424341875445506\n",
      "New Q-value: -0.024823168662522994\n",
      "\n",
      "Step 17\n",
      "Current state: 8\n",
      "Q-values for current state: [-0.02837218 -0.0210957  -0.02191325 -0.03334452]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 9\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.021913252031862243\n",
      "New Q-value: -0.022110967367785873\n",
      "\n",
      "Step 18\n",
      "Current state: 9\n",
      "Q-values for current state: [-0.02482317 -0.01462148 -0.01836922 -0.02282592]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 8\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.024823168662522994\n",
      "New Q-value: -0.025344943579784132\n",
      "\n",
      "Step 19\n",
      "Current state: 8\n",
      "Q-values for current state: [-0.02837218 -0.0210957  -0.02211097 -0.03334452]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 0\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.03334452485033024\n",
      "New Q-value: -0.03370800847313582\n",
      "\n",
      "Step 20\n",
      "Current state: 0\n",
      "Q-values for current state: [-0.03458568 -0.02839933 -0.02896912 -0.03498708]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 0\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.034987079953163185\n",
      "New Q-value: -0.03518630806568548\n",
      "\n",
      "Step 21\n",
      "Current state: 0\n",
      "Q-values for current state: [-0.03458568 -0.02839933 -0.02896912 -0.03518631]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 1\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.02896911875471023\n",
      "New Q-value: -0.029132032983712672\n",
      "\n",
      "Step 22\n",
      "Current state: 1\n",
      "Q-values for current state: [-0.03401841 -0.02168238 -0.02222903 -0.02605554]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 0\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.0340184087637337\n",
      "New Q-value: -0.03431450399519894\n",
      "\n",
      "Step 23\n",
      "Current state: 0\n",
      "Q-values for current state: [-0.03458568 -0.02839933 -0.02913203 -0.03518631]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 8\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.028399327450932724\n",
      "New Q-value: -0.02856348648935289\n",
      "\n",
      "Step 24\n",
      "Current state: 8\n",
      "Q-values for current state: [-0.02837218 -0.0210957  -0.02211097 -0.03370801]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 16\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.021095702984351983\n",
      "New Q-value: -0.021250523289619322\n",
      "\n",
      "Step 25\n",
      "Current state: 16\n",
      "Q-values for current state: [-0.01945664 -0.01330937 -0.01580535 -0.02766565]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 16\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.019456640158748213\n",
      "New Q-value: -0.01977536674657593\n",
      "\n",
      "Step 26\n",
      "Current state: 16\n",
      "Q-values for current state: [-0.01977537 -0.01330937 -0.01580535 -0.02766565]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 16\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.01977536674657593\n",
      "New Q-value: -0.020062220675620874\n",
      "\n",
      "Step 27\n",
      "Current state: 16\n",
      "Q-values for current state: [-0.02006222 -0.01330937 -0.01580535 -0.02766565]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 8\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.027665645227757104\n",
      "New Q-value: -0.02791788041749523\n",
      "\n",
      "Step 28\n",
      "Current state: 8\n",
      "Q-values for current state: [-0.02837218 -0.02125052 -0.02211097 -0.03370801]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 9\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.022110967367785873\n",
      "New Q-value: -0.02228891117011714\n",
      "\n",
      "Step 29\n",
      "Current state: 9\n",
      "Q-values for current state: [-0.02534494 -0.01462148 -0.01836922 -0.02282592]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 1\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.022825924116666233\n",
      "New Q-value: -0.023603157809473075\n",
      "\n",
      "Step 30\n",
      "Current state: 1\n",
      "Q-values for current state: [-0.0343145  -0.02168238 -0.02222903 -0.02605554]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 0\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.03431450399519894\n",
      "New Q-value: -0.03459658481216757\n",
      "\n",
      "Step 31\n",
      "Current state: 0\n",
      "Q-values for current state: [-0.03458568 -0.02856349 -0.02913203 -0.03518631]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 8\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.02856348648935289\n",
      "New Q-value: -0.028725937552931437\n",
      "\n",
      "Step 32\n",
      "Current state: 8\n",
      "Q-values for current state: [-0.02837218 -0.02125052 -0.02228891 -0.03370801]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 9\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.02228891117011714\n",
      "New Q-value: -0.02244906059221528\n",
      "\n",
      "Step 33\n",
      "Current state: 9\n",
      "Q-values for current state: [-0.02534494 -0.01462148 -0.01836922 -0.02360316]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 10\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.01836922417759693\n",
      "New Q-value: -0.01871492621587243\n",
      "\n",
      "Step 34\n",
      "Current state: 10\n",
      "Q-values for current state: [-0.01793349 -0.01245151 -0.01244868 -0.01804819]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 2\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.018048187447892376\n",
      "New Q-value: -0.018768811390517892\n",
      "\n",
      "Step 35\n",
      "Current state: 2\n",
      "Q-values for current state: [-0.02578002 -0.01605729 -0.02054124 -0.01921443]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 2\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.01921443394446774\n",
      "New Q-value: -0.01981843323743572\n",
      "\n",
      "Step 36\n",
      "Current state: 2\n",
      "Q-values for current state: [-0.02578002 -0.01605729 -0.02054124 -0.01981843]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 3\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.020541239062551495\n",
      "New Q-value: -0.0207639168800165\n",
      "\n",
      "Step 37\n",
      "Current state: 3\n",
      "Q-values for current state: [-0.02017095 -0.01344002 -0.0137786  -0.01779671]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 11\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.013440018144422706\n",
      "New Q-value: -0.013804539076289181\n",
      "\n",
      "Step 38\n",
      "Current state: 11\n",
      "Q-values for current state: [-0.00993456 -0.00745813 -0.01003263 -0.01405942]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 3\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.014059423530346655\n",
      "New Q-value: -0.014962448440772016\n",
      "\n",
      "Step 39\n",
      "Current state: 3\n",
      "Q-values for current state: [-0.02017095 -0.01380454 -0.0137786  -0.01779671]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 2\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.020170954287672665\n",
      "New Q-value: -0.020679301546320153\n",
      "\n",
      "Step 40\n",
      "Current state: 2\n",
      "Q-values for current state: [-0.02578002 -0.01605729 -0.02076392 -0.01981843]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 2\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.01981843323743572\n",
      "New Q-value: -0.020362032601106902\n",
      "\n",
      "Step 41\n",
      "Current state: 2\n",
      "Q-values for current state: [-0.02578002 -0.01605729 -0.02076392 -0.02036203]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 2\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.020362032601106902\n",
      "New Q-value: -0.02085127202841097\n",
      "\n",
      "Step 42\n",
      "Current state: 2\n",
      "Q-values for current state: [-0.02578002 -0.01605729 -0.02076392 -0.02085127]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 10\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.016057291446471108\n",
      "New Q-value: -0.01663418675785919\n",
      "\n",
      "Step 43\n",
      "Current state: 10\n",
      "Q-values for current state: [-0.01793349 -0.01245151 -0.01244868 -0.01876881]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 2\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.018768811390517892\n",
      "New Q-value: -0.019472177993462726\n",
      "\n",
      "Step 44\n",
      "Current state: 2\n",
      "Q-values for current state: [-0.02578002 -0.01663419 -0.02076392 -0.02085127]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 10\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.01663418675785919\n",
      "New Q-value: -0.017153392538108464\n",
      "\n",
      "Step 45\n",
      "Current state: 10\n",
      "Q-values for current state: [-0.01793349 -0.01245151 -0.01244868 -0.01947218]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 18\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.012451512622991915\n",
      "New Q-value: -0.012788311896142723\n",
      "\n",
      "Step 46\n",
      "Current state: 18\n",
      "Q-values for current state: [-0.01118436 -0.00816828 -0.0061258  -0.01048062]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 10\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.010480624152226052\n",
      "New Q-value: -0.011615186193038638\n",
      "\n",
      "Step 47\n",
      "Current state: 10\n",
      "Q-values for current state: [-0.01793349 -0.01278831 -0.01244868 -0.01947218]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 11\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.012448678484580972\n",
      "New Q-value: -0.01291233338243162\n",
      "\n",
      "Step 48\n",
      "Current state: 11\n",
      "Q-values for current state: [-0.00993456 -0.00745813 -0.01003263 -0.01496245]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 12\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.010032626560166924\n",
      "New Q-value: -0.010579445054650232\n",
      "\n",
      "Step 49\n",
      "Current state: 12\n",
      "Q-values for current state: [-0.00849261 -0.00579033 -0.01073525 -0.00647325]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 13\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.010735245575425325\n",
      "New Q-value: -0.011106852067882792\n",
      "\n",
      "Step 50\n",
      "Current state: 13\n",
      "Q-values for current state: [-0.00941556 -0.00613285 -0.00468559 -0.00858544]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 21\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.006132848025\n",
      "New Q-value: -0.0067253783475\n",
      "\n",
      "Step 51\n",
      "Current state: 21\n",
      "Q-values for current state: [-0.0036195  -0.003439   -0.00474761 -0.00216648]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 29\n",
      "Reward: -0.01\n",
      "Terminated: True\n",
      "Old Q-value: -0.003439\n",
      "New Q-value: -0.0040951\n",
      "\n",
      "Episode 60 summary:\n",
      "Total reward: -0.5200000000000002\n",
      "Successful episodes so far: []\n",
      "Average reward over last 10 episodes: -0.429\n",
      "\n",
      "Episode 61\n",
      "Starting state: 0\n",
      "\n",
      "Step 0\n",
      "Current state: 0\n",
      "Q-values for current state: [-0.03458568 -0.02872594 -0.02913203 -0.03518631]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 0\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.03458567959635724\n",
      "New Q-value: -0.034856075704250006\n",
      "\n",
      "Step 1\n",
      "Current state: 0\n",
      "Q-values for current state: [-0.03485608 -0.02872594 -0.02913203 -0.03518631]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 0\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.03518630806568548\n",
      "New Q-value: -0.03539664132664542\n",
      "\n",
      "Step 2\n",
      "Current state: 0\n",
      "Q-values for current state: [-0.03485608 -0.02872594 -0.02913203 -0.03539664]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 0\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.034856075704250006\n",
      "New Q-value: -0.03509943220135349\n",
      "\n",
      "Step 3\n",
      "Current state: 0\n",
      "Q-values for current state: [-0.03509943 -0.02872594 -0.02913203 -0.03539664]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 0\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.03539664132664542\n",
      "New Q-value: -0.035585941261509366\n",
      "\n",
      "Step 4\n",
      "Current state: 0\n",
      "Q-values for current state: [-0.03509943 -0.02872594 -0.02913203 -0.03558594]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 1\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.029132032983712672\n",
      "New Q-value: -0.02927865578981487\n",
      "\n",
      "Step 5\n",
      "Current state: 1\n",
      "Q-values for current state: [-0.03459658 -0.02168238 -0.02222903 -0.02605554]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 2\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.022229030561989768\n",
      "New Q-value: -0.022635699796911095\n",
      "\n",
      "Step 6\n",
      "Current state: 2\n",
      "Q-values for current state: [-0.02578002 -0.01715339 -0.02076392 -0.02085127]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 3\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.0207639168800165\n",
      "New Q-value: -0.02099649245547488\n",
      "\n",
      "Step 7\n",
      "Current state: 3\n",
      "Q-values for current state: [-0.0206793  -0.01380454 -0.0137786  -0.01779671]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 3\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.017796713983146775\n",
      "New Q-value: -0.01832600984829212\n",
      "\n",
      "Step 8\n",
      "Current state: 3\n",
      "Q-values for current state: [-0.0206793  -0.01380454 -0.0137786  -0.01832601]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 4\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.013778602773263433\n",
      "New Q-value: -0.014192218232828763\n",
      "\n",
      "Step 9\n",
      "Current state: 4\n",
      "Q-values for current state: [-0.01472648 -0.00833132 -0.00929421 -0.01287294]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 5\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.009294208591989282\n",
      "New Q-value: -0.009931975786584104\n",
      "\n",
      "Step 10\n",
      "Current state: 5\n",
      "Q-values for current state: [-0.0059704  -0.00933757 -0.0075675  -0.00635922]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 5\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.00635921816125\n",
      "New Q-value: -0.00729048439891875\n",
      "\n",
      "Step 11\n",
      "Current state: 5\n",
      "Q-values for current state: [-0.0059704  -0.00933757 -0.0075675  -0.00729048]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 6\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.007567499094000001\n",
      "New Q-value: -0.0080681991846\n",
      "\n",
      "Step 12\n",
      "Current state: 6\n",
      "Q-values for current state: [-0.00726793 -0.00271    -0.0057641  -0.0073321 ]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 5\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.007267925643020625\n",
      "New Q-value: -0.008108321132512312\n",
      "\n",
      "Step 13\n",
      "Current state: 5\n",
      "Q-values for current state: [-0.0059704  -0.00933757 -0.0080682  -0.00729048]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 6\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.0080681991846\n",
      "New Q-value: -0.008518829266140001\n",
      "\n",
      "Step 14\n",
      "Current state: 6\n",
      "Q-values for current state: [-0.00810832 -0.00271    -0.0057641  -0.0073321 ]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 5\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.008108321132512312\n",
      "New Q-value: -0.008864677073054831\n",
      "\n",
      "Step 15\n",
      "Current state: 5\n",
      "Q-values for current state: [-0.0059704  -0.00933757 -0.00851883 -0.00729048]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 6\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.008518829266140001\n",
      "New Q-value: -0.008924396339526\n",
      "\n",
      "Step 16\n",
      "Current state: 6\n",
      "Q-values for current state: [-0.00886468 -0.00271    -0.0057641  -0.0073321 ]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 14\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.00271\n",
      "New Q-value: -0.003439\n",
      "\n",
      "Step 17\n",
      "Current state: 14\n",
      "Q-values for current state: [-0.00532997 -0.00518377 -0.004515    0.        ]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 15\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.004515\n",
      "New Q-value: -0.005244\n",
      "\n",
      "Step 18\n",
      "Current state: 15\n",
      "Q-values for current state: [-0.00271    -0.00547448 -0.0019     -0.00619208]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 7\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.006192082975000001\n",
      "New Q-value: -0.007103333086375001\n",
      "\n",
      "Step 19\n",
      "Current state: 7\n",
      "Q-values for current state: [-0.00558377 -0.00583777 -0.00627692 -0.00672547]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 7\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.006276924150000001\n",
      "New Q-value: -0.007179690143875001\n",
      "\n",
      "Step 20\n",
      "Current state: 7\n",
      "Q-values for current state: [-0.00558377 -0.00583777 -0.00717969 -0.00672547]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 7\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.007179690143875001\n",
      "New Q-value: -0.0079921795383625\n",
      "\n",
      "Step 21\n",
      "Current state: 7\n",
      "Q-values for current state: [-0.00558377 -0.00583777 -0.00799218 -0.00672547]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 6\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.005583772725\n",
      "New Q-value: -0.0063521004525\n",
      "\n",
      "Step 22\n",
      "Current state: 6\n",
      "Q-values for current state: [-0.00886468 -0.003439   -0.0057641  -0.0073321 ]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 5\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.008864677073054831\n",
      "New Q-value: -0.009545397419543097\n",
      "\n",
      "Step 23\n",
      "Current state: 5\n",
      "Q-values for current state: [-0.0059704  -0.00933757 -0.0089244  -0.00729048]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 5\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.00729048439891875\n",
      "New Q-value: -0.008128624012820626\n",
      "\n",
      "Step 24\n",
      "Current state: 5\n",
      "Q-values for current state: [-0.0059704  -0.00933757 -0.0089244  -0.00812862]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 13\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.009337571217250073\n",
      "New Q-value: -0.009848945145525066\n",
      "\n",
      "Step 25\n",
      "Current state: 13\n",
      "Q-values for current state: [-0.00941556 -0.00672538 -0.00468559 -0.00858544]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 14\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.00468559\n",
      "New Q-value: -0.005217031\n",
      "\n",
      "Step 26\n",
      "Current state: 14\n",
      "Q-values for current state: [-0.00532997 -0.00518377 -0.005244    0.        ]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 15\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.005244\n",
      "New Q-value: -0.0059001\n",
      "\n",
      "Step 27\n",
      "Current state: 15\n",
      "Q-values for current state: [-0.00271    -0.00547448 -0.0019     -0.00710333]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 15\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.0019000000000000002\n",
      "New Q-value: -0.0028905\n",
      "\n",
      "Step 28\n",
      "Current state: 15\n",
      "Q-values for current state: [-0.00271    -0.00547448 -0.0028905  -0.00710333]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 23\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.005474481\n",
      "New Q-value: -0.0060220329\n",
      "\n",
      "Step 29\n",
      "Current state: 23\n",
      "Q-values for current state: [-0.00430047 -0.00271    -0.001      -0.00458426]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 15\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.004584255000000001\n",
      "New Q-value: -0.005383279500000001\n",
      "\n",
      "Step 30\n",
      "Current state: 15\n",
      "Q-values for current state: [-0.00271    -0.00602203 -0.0028905  -0.00710333]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 23\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.0060220329\n",
      "New Q-value: -0.00651482961\n",
      "\n",
      "Step 31\n",
      "Current state: 23\n",
      "Q-values for current state: [-0.00430047 -0.00271    -0.001      -0.00538328]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 31\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.00271\n",
      "New Q-value: -0.003439\n",
      "\n",
      "Step 32\n",
      "Current state: 31\n",
      "Q-values for current state: [-0.00271 -0.001    0.      -0.001  ]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 31\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: -0.001\n",
      "\n",
      "Step 33\n",
      "Current state: 31\n",
      "Q-values for current state: [-0.00271 -0.001   -0.001   -0.001  ]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 39\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.001\n",
      "New Q-value: -0.0019950000000000002\n",
      "\n",
      "Step 34\n",
      "Current state: 39\n",
      "Q-values for current state: [-0.001   -0.0019  -0.00271 -0.001  ]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 39\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.00271\n",
      "New Q-value: -0.0035340000000000002\n",
      "\n",
      "Step 35\n",
      "Current state: 39\n",
      "Q-values for current state: [-0.001    -0.0019   -0.003534 -0.001   ]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 39\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.0035340000000000002\n",
      "New Q-value: -0.0042756\n",
      "\n",
      "Step 36\n",
      "Current state: 39\n",
      "Q-values for current state: [-0.001     -0.0019    -0.0042756 -0.001    ]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 47\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.0019000000000000002\n",
      "New Q-value: -0.002805\n",
      "\n",
      "Step 37\n",
      "Current state: 47\n",
      "Q-values for current state: [-0.001   -0.0019  -0.00271 -0.001  ]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 55\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.0019000000000000002\n",
      "New Q-value: -0.00271\n",
      "\n",
      "Step 38\n",
      "Current state: 55\n",
      "Q-values for current state: [ 0.      0.      0.     -0.0019]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 54\n",
      "Reward: -0.01\n",
      "Terminated: True\n",
      "Old Q-value: 0.0\n",
      "New Q-value: -0.001\n",
      "\n",
      "Episode 62\n",
      "Starting state: 0\n",
      "\n",
      "Step 0\n",
      "Current state: 0\n",
      "Q-values for current state: [-0.03509943 -0.02872594 -0.02927866 -0.03558594]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 8\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.028725937552931437\n",
      "New Q-value: -0.02887214351015213\n",
      "\n",
      "Step 1\n",
      "Current state: 8\n",
      "Q-values for current state: [-0.02837218 -0.02125052 -0.02244906 -0.03370801]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 9\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.02244906059221528\n",
      "New Q-value: -0.022593195072103604\n",
      "\n",
      "Step 2\n",
      "Current state: 9\n",
      "Q-values for current state: [-0.02534494 -0.01462148 -0.01871493 -0.02360316]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 17\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.01462147935905109\n",
      "New Q-value: -0.015047125803430426\n",
      "\n",
      "Step 3\n",
      "Current state: 17\n",
      "Q-values for current state: [-0.01850422 -0.0093452  -0.0105639  -0.01598829]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 16\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.018504221328686413\n",
      "New Q-value: -0.01891818979952031\n",
      "\n",
      "Step 4\n",
      "Current state: 16\n",
      "Q-values for current state: [-0.02006222 -0.01330937 -0.01580535 -0.02791788]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 16\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.020062220675620874\n",
      "New Q-value: -0.020320389211761325\n",
      "\n",
      "Step 5\n",
      "Current state: 16\n",
      "Q-values for current state: [-0.02032039 -0.01330937 -0.01580535 -0.02791788]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 24\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.013309374775816174\n",
      "New Q-value: -0.013690423718596807\n",
      "\n",
      "Step 6\n",
      "Current state: 24\n",
      "Q-values for current state: [-0.00797998 -0.00982753 -0.00749459 -0.01421283]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 16\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.014212826647100288\n",
      "New Q-value: -0.015092134235656956\n",
      "\n",
      "Step 7\n",
      "Current state: 16\n",
      "Q-values for current state: [-0.02032039 -0.01369042 -0.01580535 -0.02791788]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 8\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.02791788041749523\n",
      "New Q-value: -0.02814489208825954\n",
      "\n",
      "Step 8\n",
      "Current state: 8\n",
      "Q-values for current state: [-0.02837218 -0.02125052 -0.0225932  -0.03370801]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 0\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.03370800847313582\n",
      "New Q-value: -0.03408006125928669\n",
      "\n",
      "Step 9\n",
      "Current state: 0\n",
      "Q-values for current state: [-0.03509943 -0.02887214 -0.02927866 -0.03558594]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 8\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.02887214351015213\n",
      "New Q-value: -0.02900372887165075\n",
      "\n",
      "Step 10\n",
      "Current state: 8\n",
      "Q-values for current state: [-0.02837218 -0.02125052 -0.0225932  -0.03408006]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 9\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.022593195072103604\n",
      "New Q-value: -0.022763352516219134\n",
      "\n",
      "Step 11\n",
      "Current state: 9\n",
      "Q-values for current state: [-0.02534494 -0.01504713 -0.01871493 -0.02360316]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 1\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.023603157809473075\n",
      "New Q-value: -0.024302668132999233\n",
      "\n",
      "Step 12\n",
      "Current state: 1\n",
      "Q-values for current state: [-0.03459658 -0.02168238 -0.0226357  -0.02605554]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 1\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.026055535582085402\n",
      "New Q-value: -0.026509808128350326\n",
      "\n",
      "Step 13\n",
      "Current state: 1\n",
      "Q-values for current state: [-0.03459658 -0.02168238 -0.0226357  -0.02650981]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 0\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.03459658481216757\n",
      "New Q-value: -0.034892280573757636\n",
      "\n",
      "Step 14\n",
      "Current state: 0\n",
      "Q-values for current state: [-0.03509943 -0.02900373 -0.02927866 -0.03558594]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 0\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.035585941261509366\n",
      "New Q-value: -0.03578270137816525\n",
      "\n",
      "Step 15\n",
      "Current state: 0\n",
      "Q-values for current state: [-0.03509943 -0.02900373 -0.02927866 -0.0357827 ]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 0\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.03509943220135349\n",
      "New Q-value: -0.03534484322402496\n",
      "\n",
      "Step 16\n",
      "Current state: 0\n",
      "Q-values for current state: [-0.03534484 -0.02900373 -0.02927866 -0.0357827 ]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 1\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.02927865578981487\n",
      "New Q-value: -0.02941061631530685\n",
      "\n",
      "Step 17\n",
      "Current state: 1\n",
      "Q-values for current state: [-0.03489228 -0.02168238 -0.0226357  -0.02650981]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 9\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.02168238004708911\n",
      "New Q-value: -0.02194361899370609\n",
      "\n",
      "Step 18\n",
      "Current state: 9\n",
      "Q-values for current state: [-0.02534494 -0.01504713 -0.01871493 -0.02430267]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 10\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.01871492621587243\n",
      "New Q-value: -0.019058323224418746\n",
      "\n",
      "Step 19\n",
      "Current state: 10\n",
      "Q-values for current state: [-0.01793349 -0.01278831 -0.01291233 -0.01947218]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 18\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.012788311896142723\n",
      "New Q-value: -0.01309143124197845\n",
      "\n",
      "Step 20\n",
      "Current state: 18\n",
      "Q-values for current state: [-0.01118436 -0.00816828 -0.0061258  -0.01161519]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 17\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.011184356665583775\n",
      "New Q-value: -0.011953715379309842\n",
      "\n",
      "Step 21\n",
      "Current state: 17\n",
      "Q-values for current state: [-0.01891819 -0.0093452  -0.0105639  -0.01598829]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 25\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.009345204002994151\n",
      "New Q-value: -0.009901050615069736\n",
      "\n",
      "Step 22\n",
      "Current state: 25\n",
      "Q-values for current state: [-0.00961143 -0.00516176 -0.00818647 -0.00609038]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 26\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.00818646673755\n",
      "New Q-value: -0.008792529937545\n",
      "\n",
      "Step 23\n",
      "Current state: 26\n",
      "Q-values for current state: [-0.00447063 -0.00638325 -0.00468559 -0.00492135]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 25\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.0044706302500000005\n",
      "New Q-value: -0.005513934237375001\n",
      "\n",
      "Step 24\n",
      "Current state: 25\n",
      "Q-values for current state: [-0.00961143 -0.00516176 -0.00879253 -0.00609038]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 17\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.006090378412588944\n",
      "New Q-value: -0.007421940379761675\n",
      "\n",
      "Step 25\n",
      "Current state: 17\n",
      "Q-values for current state: [-0.01891819 -0.00990105 -0.0105639  -0.01598829]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 25\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.009901050615069736\n",
      "New Q-value: -0.010401312565937763\n",
      "\n",
      "Step 26\n",
      "Current state: 25\n",
      "Q-values for current state: [-0.00961143 -0.00516176 -0.00879253 -0.00742194]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 17\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.007421940379761675\n",
      "New Q-value: -0.008667871035549595\n",
      "\n",
      "Step 27\n",
      "Current state: 17\n",
      "Q-values for current state: [-0.01891819 -0.01040131 -0.0105639  -0.01598829]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 25\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.010401312565937763\n",
      "New Q-value: -0.010851548321718987\n",
      "\n",
      "Step 28\n",
      "Current state: 25\n",
      "Q-values for current state: [-0.00961143 -0.00516176 -0.00879253 -0.00866787]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 26\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.008792529937545\n",
      "New Q-value: -0.0093584079937905\n",
      "\n",
      "Step 29\n",
      "Current state: 26\n",
      "Q-values for current state: [-0.00551393 -0.00638325 -0.00468559 -0.00492135]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 18\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.004921345806286719\n",
      "New Q-value: -0.006011161761108047\n",
      "\n",
      "Step 30\n",
      "Current state: 18\n",
      "Q-values for current state: [-0.01195372 -0.00816828 -0.0061258  -0.01161519]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 10\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.011615186193038638\n",
      "New Q-value: -0.012680339245065779\n",
      "\n",
      "Step 31\n",
      "Current state: 10\n",
      "Q-values for current state: [-0.01793349 -0.01309143 -0.01291233 -0.01947218]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 2\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.019472177993462726\n",
      "New Q-value: -0.020154532485236756\n",
      "\n",
      "Step 32\n",
      "Current state: 2\n",
      "Q-values for current state: [-0.02578002 -0.01715339 -0.02099649 -0.02085127]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 2\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.02085127202841097\n",
      "New Q-value: -0.021395717116690176\n",
      "\n",
      "Step 33\n",
      "Current state: 2\n",
      "Q-values for current state: [-0.02578002 -0.01715339 -0.02099649 -0.02139572]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 1\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.025780024323930832\n",
      "New Q-value: -0.026286665695939827\n",
      "\n",
      "Step 34\n",
      "Current state: 1\n",
      "Q-values for current state: [-0.03489228 -0.02194362 -0.0226357  -0.02650981]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 9\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.02194361899370609\n",
      "New Q-value: -0.022178734045661373\n",
      "\n",
      "Step 35\n",
      "Current state: 9\n",
      "Q-values for current state: [-0.02534494 -0.01504713 -0.01905832 -0.02430267]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 10\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.019058323224418746\n",
      "New Q-value: -0.019379162573307875\n",
      "\n",
      "Step 36\n",
      "Current state: 10\n",
      "Q-values for current state: [-0.01793349 -0.01309143 -0.01291233 -0.02015453]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 11\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.01291233338243162\n",
      "New Q-value: -0.013329622790497202\n",
      "\n",
      "Step 37\n",
      "Current state: 11\n",
      "Q-values for current state: [-0.00993456 -0.00745813 -0.01057945 -0.01496245]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 10\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.009934564624298375\n",
      "New Q-value: -0.01118479412985649\n",
      "\n",
      "Step 38\n",
      "Current state: 10\n",
      "Q-values for current state: [-0.01793349 -0.01309143 -0.01332962 -0.02015453]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 9\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.017933488595592094\n",
      "New Q-value: -0.018569616687358775\n",
      "\n",
      "Step 39\n",
      "Current state: 9\n",
      "Q-values for current state: [-0.02534494 -0.01504713 -0.01937916 -0.02430267]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 8\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.025344943579784132\n",
      "New Q-value: -0.025829248934319555\n",
      "\n",
      "Step 40\n",
      "Current state: 8\n",
      "Q-values for current state: [-0.02837218 -0.02125052 -0.02276335 -0.03408006]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 9\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.022763352516219134\n",
      "New Q-value: -0.022916494215923112\n",
      "\n",
      "Step 41\n",
      "Current state: 9\n",
      "Q-values for current state: [-0.02582925 -0.01504713 -0.01937916 -0.02430267]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 17\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.015047125803430426\n",
      "New Q-value: -0.015545983413700086\n",
      "\n",
      "Step 42\n",
      "Current state: 17\n",
      "Q-values for current state: [-0.01891819 -0.01085155 -0.0105639  -0.01598829]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 16\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.01891818979952031\n",
      "New Q-value: -0.019326961072834974\n",
      "\n",
      "Step 43\n",
      "Current state: 16\n",
      "Q-values for current state: [-0.02032039 -0.01369042 -0.01580535 -0.02814489]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 17\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.01580534576811816\n",
      "New Q-value: -0.01622838138191905\n",
      "\n",
      "Step 44\n",
      "Current state: 17\n",
      "Q-values for current state: [-0.01932696 -0.01085155 -0.0105639  -0.01598829]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 25\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.010851548321718987\n",
      "New Q-value: -0.011256760501922088\n",
      "\n",
      "Step 45\n",
      "Current state: 25\n",
      "Q-values for current state: [-0.00961143 -0.00516176 -0.00935841 -0.00866787]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 24\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.009611433130610775\n",
      "New Q-value: -0.010362276237911947\n",
      "\n",
      "Step 46\n",
      "Current state: 24\n",
      "Q-values for current state: [-0.00797998 -0.00982753 -0.00749459 -0.01509213]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 16\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.015092134235656956\n",
      "New Q-value: -0.015883511065357958\n",
      "\n",
      "Step 47\n",
      "Current state: 16\n",
      "Q-values for current state: [-0.02032039 -0.01369042 -0.01622838 -0.02814489]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 24\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.013690423718596807\n",
      "New Q-value: -0.014033367767099377\n",
      "\n",
      "Step 48\n",
      "Current state: 24\n",
      "Q-values for current state: [-0.00797998 -0.00982753 -0.00749459 -0.01588351]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 24\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.00797997758019\n",
      "New Q-value: -0.00889396624253325\n",
      "\n",
      "Step 49\n",
      "Current state: 24\n",
      "Q-values for current state: [-0.00889397 -0.00982753 -0.00749459 -0.01588351]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 24\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.00889396624253325\n",
      "New Q-value: -0.009716556038642176\n",
      "\n",
      "Step 50\n",
      "Current state: 24\n",
      "Q-values for current state: [-0.00971656 -0.00982753 -0.00749459 -0.01588351]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 24\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.009716556038642176\n",
      "New Q-value: -0.010456886855140207\n",
      "\n",
      "Step 51\n",
      "Current state: 24\n",
      "Q-values for current state: [-0.01045689 -0.00982753 -0.00749459 -0.01588351]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 24\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.010456886855140207\n",
      "New Q-value: -0.011123184589988436\n",
      "\n",
      "Step 52\n",
      "Current state: 24\n",
      "Q-values for current state: [-0.01112318 -0.00982753 -0.00749459 -0.01588351]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 32\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.00982752603109906\n",
      "New Q-value: -0.010251812802989155\n",
      "\n",
      "Step 53\n",
      "Current state: 32\n",
      "Q-values for current state: [-0.00891782 -0.00700883 -0.00428463 -0.00637848]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 32\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.0089178154891545\n",
      "New Q-value: -0.00943307331523905\n",
      "\n",
      "Step 54\n",
      "Current state: 32\n",
      "Q-values for current state: [-0.00943307 -0.00700883 -0.00428463 -0.00637848]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 24\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.006378483198688275\n",
      "New Q-value: -0.007452621299181697\n",
      "\n",
      "Step 55\n",
      "Current state: 24\n",
      "Q-values for current state: [-0.01112318 -0.01025181 -0.00749459 -0.01588351]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 32\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.010251812802989155\n",
      "New Q-value: -0.01063367089769024\n",
      "\n",
      "Step 56\n",
      "Current state: 32\n",
      "Q-values for current state: [-0.00943307 -0.00700883 -0.00428463 -0.00745262]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 40\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.007008833544\n",
      "New Q-value: -0.0074884501896\n",
      "\n",
      "Step 57\n",
      "Current state: 40\n",
      "Q-values for current state: [-0.00369645 -0.0042756  -0.0019     -0.00424464]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 32\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.004244638\n",
      "New Q-value: -0.005227213575\n",
      "\n",
      "Step 58\n",
      "Current state: 32\n",
      "Q-values for current state: [-0.00943307 -0.00748845 -0.00428463 -0.00745262]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 40\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.0074884501896\n",
      "New Q-value: -0.00792010517064\n",
      "\n",
      "Step 59\n",
      "Current state: 40\n",
      "Q-values for current state: [-0.00369645 -0.0042756  -0.0019     -0.00522721]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 41\n",
      "Reward: -0.01\n",
      "Terminated: True\n",
      "Old Q-value: -0.0019000000000000002\n",
      "New Q-value: -0.00271\n",
      "\n",
      "Episode 63\n",
      "Starting state: 0\n",
      "\n",
      "Step 0\n",
      "Current state: 0\n",
      "Q-values for current state: [-0.03534484 -0.02900373 -0.02941062 -0.0357827 ]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 1\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.02941061631530685\n",
      "New Q-value: -0.029576534418113996\n",
      "\n",
      "Step 1\n",
      "Current state: 1\n",
      "Q-values for current state: [-0.03489228 -0.02217873 -0.0226357  -0.02650981]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 0\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.034892280573757636\n",
      "New Q-value: -0.035158406759188694\n",
      "\n",
      "Step 2\n",
      "Current state: 0\n",
      "Q-values for current state: [-0.03534484 -0.02900373 -0.02957653 -0.0357827 ]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 0\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.03578270137816525\n",
      "New Q-value: -0.035959785483155544\n",
      "\n",
      "Step 3\n",
      "Current state: 0\n",
      "Q-values for current state: [-0.03534484 -0.02900373 -0.02957653 -0.03595979]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 0\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.03534484322402496\n",
      "New Q-value: -0.03556571314442929\n",
      "\n",
      "Step 4\n",
      "Current state: 0\n",
      "Q-values for current state: [-0.03556571 -0.02900373 -0.02957653 -0.03595979]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 0\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.035959785483155544\n",
      "New Q-value: -0.03611916117764681\n",
      "\n",
      "Step 5\n",
      "Current state: 0\n",
      "Q-values for current state: [-0.03556571 -0.02900373 -0.02957653 -0.03611916]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 0\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.03611916117764681\n",
      "New Q-value: -0.03626259930268895\n",
      "\n",
      "Step 6\n",
      "Current state: 0\n",
      "Q-values for current state: [-0.03556571 -0.02900373 -0.02957653 -0.0362626 ]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 8\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.02900372887165075\n",
      "New Q-value: -0.02912215569699951\n",
      "\n",
      "Step 7\n",
      "Current state: 8\n",
      "Q-values for current state: [-0.02837218 -0.02125052 -0.02291649 -0.03408006]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 0\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.03408006125928669\n",
      "New Q-value: -0.03443865992457298\n",
      "\n",
      "Step 8\n",
      "Current state: 0\n",
      "Q-values for current state: [-0.03556571 -0.02912216 -0.02957653 -0.0362626 ]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 0\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.03556571314442929\n",
      "New Q-value: -0.03577574662120131\n",
      "\n",
      "Step 9\n",
      "Current state: 0\n",
      "Q-values for current state: [-0.03577575 -0.02912216 -0.02957653 -0.0362626 ]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 0\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.03626259930268895\n",
      "New Q-value: -0.03640294416363501\n",
      "\n",
      "Step 10\n",
      "Current state: 0\n",
      "Q-values for current state: [-0.03577575 -0.02912216 -0.02957653 -0.03640294]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 0\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.03640294416363501\n",
      "New Q-value: -0.03652925453848646\n",
      "\n",
      "Step 11\n",
      "Current state: 0\n",
      "Q-values for current state: [-0.03577575 -0.02912216 -0.02957653 -0.03652925]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 0\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.03652925453848646\n",
      "New Q-value: -0.03664293387585277\n",
      "\n",
      "Step 12\n",
      "Current state: 0\n",
      "Q-values for current state: [-0.03577575 -0.02912216 -0.02957653 -0.03664293]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 1\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.029576534418113996\n",
      "New Q-value: -0.029725860710640428\n",
      "\n",
      "Step 13\n",
      "Current state: 1\n",
      "Q-values for current state: [-0.03515841 -0.02217873 -0.0226357  -0.02650981]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 1\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.026509808128350326\n",
      "New Q-value: -0.026965807049853123\n",
      "\n",
      "Step 14\n",
      "Current state: 1\n",
      "Q-values for current state: [-0.03515841 -0.02217873 -0.0226357  -0.02696581]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 2\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.022635699796911095\n",
      "New Q-value: -0.02300170210834029\n",
      "\n",
      "Step 15\n",
      "Current state: 2\n",
      "Q-values for current state: [-0.02628667 -0.01715339 -0.02099649 -0.02139572]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 2\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.021395717116690176\n",
      "New Q-value: -0.021885717696141463\n",
      "\n",
      "Step 16\n",
      "Current state: 2\n",
      "Q-values for current state: [-0.02628667 -0.01715339 -0.02099649 -0.02188572]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 10\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.017153392538108464\n",
      "New Q-value: -0.01768173925228557\n",
      "\n",
      "Step 17\n",
      "Current state: 10\n",
      "Q-values for current state: [-0.01856962 -0.01309143 -0.01332962 -0.02015453]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 2\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.020154532485236756\n",
      "New Q-value: -0.02081884446568021\n",
      "\n",
      "Step 18\n",
      "Current state: 2\n",
      "Q-values for current state: [-0.02628667 -0.01768174 -0.02099649 -0.02188572]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 3\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.02099649245547488\n",
      "New Q-value: -0.021208274422174864\n",
      "\n",
      "Step 19\n",
      "Current state: 3\n",
      "Q-values for current state: [-0.0206793  -0.01380454 -0.01419222 -0.01832601]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 3\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.01832600984829212\n",
      "New Q-value: -0.018804840075710382\n",
      "\n",
      "Step 20\n",
      "Current state: 3\n",
      "Q-values for current state: [-0.0206793  -0.01380454 -0.01419222 -0.01880484]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 3\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.018804840075710382\n",
      "New Q-value: -0.019235787280386815\n",
      "\n",
      "Step 21\n",
      "Current state: 3\n",
      "Q-values for current state: [-0.0206793  -0.01380454 -0.01419222 -0.01923579]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 2\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.020679301546320153\n",
      "New Q-value: -0.021291136620655265\n",
      "\n",
      "Step 22\n",
      "Current state: 2\n",
      "Q-values for current state: [-0.02628667 -0.01768174 -0.02120827 -0.02188572]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 10\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.01768173925228557\n",
      "New Q-value: -0.018157251295044967\n",
      "\n",
      "Step 23\n",
      "Current state: 10\n",
      "Q-values for current state: [-0.01856962 -0.01309143 -0.01332962 -0.02081884]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 9\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.018569616687358775\n",
      "New Q-value: -0.019189523442924405\n",
      "\n",
      "Step 24\n",
      "Current state: 9\n",
      "Q-values for current state: [-0.02582925 -0.01554598 -0.01937916 -0.02430267]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 1\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.024302668132999233\n",
      "New Q-value: -0.024979381054037138\n",
      "\n",
      "Step 25\n",
      "Current state: 1\n",
      "Q-values for current state: [-0.03515841 -0.02217873 -0.0230017  -0.02696581]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 2\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.02300170210834029\n",
      "New Q-value: -0.023426470770535533\n",
      "\n",
      "Step 26\n",
      "Current state: 2\n",
      "Q-values for current state: [-0.02628667 -0.01815725 -0.02120827 -0.02188572]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 1\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.026286665695939827\n",
      "New Q-value: -0.026764978860683675\n",
      "\n",
      "Step 27\n",
      "Current state: 1\n",
      "Q-values for current state: [-0.03515841 -0.02217873 -0.02342647 -0.02696581]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 0\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.035158406759188694\n",
      "New Q-value: -0.03540917087448478\n",
      "\n",
      "Step 28\n",
      "Current state: 0\n",
      "Q-values for current state: [-0.03577575 -0.02912216 -0.02972586 -0.03664293]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 0\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.03664293387585277\n",
      "New Q-value: -0.03674524527948245\n",
      "\n",
      "Step 29\n",
      "Current state: 0\n",
      "Q-values for current state: [-0.03577575 -0.02912216 -0.02972586 -0.03674525]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 0\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.03674524527948245\n",
      "New Q-value: -0.036837325542749155\n",
      "\n",
      "Step 30\n",
      "Current state: 0\n",
      "Q-values for current state: [-0.03577575 -0.02912216 -0.02972586 -0.03683733]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 1\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.029725860710640428\n",
      "New Q-value: -0.029860254373914215\n",
      "\n",
      "Step 31\n",
      "Current state: 1\n",
      "Q-values for current state: [-0.03540917 -0.02217873 -0.02342647 -0.02696581]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 1\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.026965807049853123\n",
      "New Q-value: -0.027376206079205642\n",
      "\n",
      "Step 32\n",
      "Current state: 1\n",
      "Q-values for current state: [-0.03540917 -0.02217873 -0.02342647 -0.02737621]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 1\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.027376206079205642\n",
      "New Q-value: -0.027745565205622907\n",
      "\n",
      "Step 33\n",
      "Current state: 1\n",
      "Q-values for current state: [-0.03540917 -0.02217873 -0.02342647 -0.02774557]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 2\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.023426470770535533\n",
      "New Q-value: -0.023808762566511252\n",
      "\n",
      "Step 34\n",
      "Current state: 2\n",
      "Q-values for current state: [-0.02676498 -0.01815725 -0.02120827 -0.02188572]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 1\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.026764978860683675\n",
      "New Q-value: -0.027195460708953137\n",
      "\n",
      "Step 35\n",
      "Current state: 1\n",
      "Q-values for current state: [-0.03540917 -0.02217873 -0.02380876 -0.02774557]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 2\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.023808762566511252\n",
      "New Q-value: -0.024152825182889397\n",
      "\n",
      "Step 36\n",
      "Current state: 2\n",
      "Q-values for current state: [-0.02719546 -0.01815725 -0.02120827 -0.02188572]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 1\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.027195460708953137\n",
      "New Q-value: -0.027582894372395653\n",
      "\n",
      "Step 37\n",
      "Current state: 1\n",
      "Q-values for current state: [-0.03540917 -0.02217873 -0.02415283 -0.02774557]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 9\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.022178734045661373\n",
      "New Q-value: -0.022437729065396744\n",
      "\n",
      "Step 38\n",
      "Current state: 9\n",
      "Q-values for current state: [-0.02582925 -0.01554598 -0.01937916 -0.02497938]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 10\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.019379162573307875\n",
      "New Q-value: -0.01968493228396504\n",
      "\n",
      "Step 39\n",
      "Current state: 10\n",
      "Q-values for current state: [-0.01918952 -0.01309143 -0.01332962 -0.02081884]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 11\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.013329622790497202\n",
      "New Q-value: -0.013705183257756227\n",
      "\n",
      "Step 40\n",
      "Current state: 11\n",
      "Q-values for current state: [-0.01118479 -0.00745813 -0.01057945 -0.01496245]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 12\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.010579445054650232\n",
      "New Q-value: -0.01107158169968521\n",
      "\n",
      "Step 41\n",
      "Current state: 12\n",
      "Q-values for current state: [-0.00849261 -0.00579033 -0.01110685 -0.00647325]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 20\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.0057903278999999995\n",
      "New Q-value: -0.006408942609999999\n",
      "\n",
      "Step 42\n",
      "Current state: 20\n",
      "Q-values for current state: [-0.003439   -0.00271    -0.0020805  -0.00403209]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 19\n",
      "Reward: -0.01\n",
      "Terminated: True\n",
      "Old Q-value: -0.003439\n",
      "New Q-value: -0.0040951\n",
      "\n",
      "Episode 64\n",
      "Starting state: 0\n",
      "\n",
      "Step 0\n",
      "Current state: 0\n",
      "Q-values for current state: [-0.03577575 -0.02912216 -0.02986025 -0.03683733]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 0\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.03577574662120131\n",
      "New Q-value: -0.035964776750296135\n",
      "\n",
      "Step 1\n",
      "Current state: 0\n",
      "Q-values for current state: [-0.03596478 -0.02912216 -0.02986025 -0.03683733]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 0\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.035964776750296135\n",
      "New Q-value: -0.03613490386648147\n",
      "\n",
      "Step 2\n",
      "Current state: 0\n",
      "Q-values for current state: [-0.0361349  -0.02912216 -0.02986025 -0.03683733]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 8\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.02912215569699951\n",
      "New Q-value: -0.029228739839813396\n",
      "\n",
      "Step 3\n",
      "Current state: 8\n",
      "Q-values for current state: [-0.02837218 -0.02125052 -0.02291649 -0.03443866]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 0\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.03443865992457298\n",
      "New Q-value: -0.03477152421689795\n",
      "\n",
      "Step 4\n",
      "Current state: 0\n",
      "Q-values for current state: [-0.0361349  -0.02922874 -0.02986025 -0.03683733]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 8\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.029228739839813396\n",
      "New Q-value: -0.02932466556834589\n",
      "\n",
      "Step 5\n",
      "Current state: 8\n",
      "Q-values for current state: [-0.02837218 -0.02125052 -0.02291649 -0.03477152]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 16\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.021250523289619322\n",
      "New Q-value: -0.02145864089853183\n",
      "\n",
      "Step 6\n",
      "Current state: 16\n",
      "Q-values for current state: [-0.02032039 -0.01403337 -0.01622838 -0.02814489]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 8\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.02814489208825954\n",
      "New Q-value: -0.02836897376479411\n",
      "\n",
      "Step 7\n",
      "Current state: 8\n",
      "Q-values for current state: [-0.02837218 -0.02145864 -0.02291649 -0.03477152]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 8\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.02837217853955833\n",
      "New Q-value: -0.02857353157096302\n",
      "\n",
      "Step 8\n",
      "Current state: 8\n",
      "Q-values for current state: [-0.02857353 -0.02145864 -0.02291649 -0.03477152]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 0\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.03477152421689795\n",
      "New Q-value: -0.035080215024201014\n",
      "\n",
      "Step 9\n",
      "Current state: 0\n",
      "Q-values for current state: [-0.0361349  -0.02932467 -0.02986025 -0.03683733]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 0\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.036837325542749155\n",
      "New Q-value: -0.0369394362174671\n",
      "\n",
      "Step 10\n",
      "Current state: 0\n",
      "Q-values for current state: [-0.0361349  -0.02932467 -0.02986025 -0.03693944]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 0\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.0369394362174671\n",
      "New Q-value: -0.03703133582471325\n",
      "\n",
      "Step 11\n",
      "Current state: 0\n",
      "Q-values for current state: [-0.0361349  -0.02932467 -0.02986025 -0.03703134]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 0\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.03703133582471325\n",
      "New Q-value: -0.037114045471234786\n",
      "\n",
      "Step 12\n",
      "Current state: 0\n",
      "Q-values for current state: [-0.0361349  -0.02932467 -0.02986025 -0.03711405]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 1\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.029860254373914215\n",
      "New Q-value: -0.030005813197735483\n",
      "\n",
      "Step 13\n",
      "Current state: 1\n",
      "Q-values for current state: [-0.03540917 -0.02243773 -0.02415283 -0.02774557]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 9\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.022437729065396744\n",
      "New Q-value: -0.022670824583158576\n",
      "\n",
      "Step 14\n",
      "Current state: 9\n",
      "Q-values for current state: [-0.02582925 -0.01554598 -0.01968493 -0.02497938]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 10\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.01968493228396504\n",
      "New Q-value: -0.01996012502355649\n",
      "\n",
      "Step 15\n",
      "Current state: 10\n",
      "Q-values for current state: [-0.01918952 -0.01309143 -0.01370518 -0.02081884]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 11\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.013705183257756227\n",
      "New Q-value: -0.014043187678289349\n",
      "\n",
      "Step 16\n",
      "Current state: 11\n",
      "Q-values for current state: [-0.01118479 -0.00745813 -0.01107158 -0.01496245]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 19\n",
      "Reward: -0.01\n",
      "Terminated: True\n",
      "Old Q-value: -0.007458134171670999\n",
      "New Q-value: -0.007712320754503899\n",
      "\n",
      "Episode 65\n",
      "Starting state: 0\n",
      "\n",
      "Step 0\n",
      "Current state: 0\n",
      "Q-values for current state: [-0.0361349  -0.02932467 -0.03000581 -0.03711405]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 0\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.037114045471234786\n",
      "New Q-value: -0.03718848415310417\n",
      "\n",
      "Step 1\n",
      "Current state: 0\n",
      "Q-values for current state: [-0.0361349  -0.02932467 -0.03000581 -0.03718848]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 0\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.03613490386648147\n",
      "New Q-value: -0.03630725670882619\n",
      "\n",
      "Step 2\n",
      "Current state: 0\n",
      "Q-values for current state: [-0.03630726 -0.02932467 -0.03000581 -0.03718848]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 8\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.02932466556834589\n",
      "New Q-value: -0.029430769896871826\n",
      "\n",
      "Step 3\n",
      "Current state: 8\n",
      "Q-values for current state: [-0.02857353 -0.02145864 -0.02291649 -0.03508022]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 8\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.02857353157096302\n",
      "New Q-value: -0.02875474929922724\n",
      "\n",
      "Step 4\n",
      "Current state: 8\n",
      "Q-values for current state: [-0.02875475 -0.02145864 -0.02291649 -0.03508022]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 8\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.02875474929922724\n",
      "New Q-value: -0.02891784525466504\n",
      "\n",
      "Step 5\n",
      "Current state: 8\n",
      "Q-values for current state: [-0.02891785 -0.02145864 -0.02291649 -0.03508022]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 16\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.02145864089853183\n",
      "New Q-value: -0.02164594674655309\n",
      "\n",
      "Step 6\n",
      "Current state: 16\n",
      "Q-values for current state: [-0.02032039 -0.01403337 -0.01622838 -0.02836897]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 24\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.014033367767099377\n",
      "New Q-value: -0.014342017410751688\n",
      "\n",
      "Step 7\n",
      "Current state: 24\n",
      "Q-values for current state: [-0.01112318 -0.01063367 -0.00749459 -0.01588351]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 24\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.011123184589988436\n",
      "New Q-value: -0.011722852551351842\n",
      "\n",
      "Step 8\n",
      "Current state: 24\n",
      "Q-values for current state: [-0.01172285 -0.01063367 -0.00749459 -0.01588351]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 32\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.01063367089769024\n",
      "New Q-value: -0.010977343182921215\n",
      "\n",
      "Step 9\n",
      "Current state: 32\n",
      "Q-values for current state: [-0.00943307 -0.00792011 -0.00428463 -0.00745262]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 33\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.004284625\n",
      "New Q-value: -0.0050834537406250005\n",
      "\n",
      "Step 10\n",
      "Current state: 33\n",
      "Q-values for current state: [-0.00239254 -0.00468559 -0.002805   -0.00421295]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 41\n",
      "Reward: -0.01\n",
      "Terminated: True\n",
      "Old Q-value: -0.00468559\n",
      "New Q-value: -0.005217031\n",
      "\n",
      "Episode 66\n",
      "Starting state: 0\n",
      "\n",
      "Step 0\n",
      "Current state: 0\n",
      "Q-values for current state: [-0.03630726 -0.02943077 -0.03000581 -0.03718848]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 0\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.03718848415310417\n",
      "New Q-value: -0.037265558877996575\n",
      "\n",
      "Step 1\n",
      "Current state: 0\n",
      "Q-values for current state: [-0.03630726 -0.02943077 -0.03000581 -0.03726556]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 1\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.030005813197735483\n",
      "New Q-value: -0.030158960213362\n",
      "\n",
      "Step 2\n",
      "Current state: 1\n",
      "Q-values for current state: [-0.03540917 -0.02267082 -0.02415283 -0.02774557]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 1\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.027745565205622907\n",
      "New Q-value: -0.028124737020460682\n",
      "\n",
      "Step 3\n",
      "Current state: 1\n",
      "Q-values for current state: [-0.03540917 -0.02267082 -0.02415283 -0.02812474]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 1\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.028124737020460682\n",
      "New Q-value: -0.02846599165381468\n",
      "\n",
      "Step 4\n",
      "Current state: 1\n",
      "Q-values for current state: [-0.03540917 -0.02267082 -0.02415283 -0.02846599]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 1\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.02846599165381468\n",
      "New Q-value: -0.028773120823833275\n",
      "\n",
      "Step 5\n",
      "Current state: 1\n",
      "Q-values for current state: [-0.03540917 -0.02267082 -0.02415283 -0.02877312]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 0\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.03540917087448478\n",
      "New Q-value: -0.035664176927239125\n",
      "\n",
      "Step 6\n",
      "Current state: 0\n",
      "Q-values for current state: [-0.03630726 -0.02943077 -0.03015896 -0.03726556]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 8\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.029430769896871826\n",
      "New Q-value: -0.029544057848107187\n",
      "\n",
      "Step 7\n",
      "Current state: 8\n",
      "Q-values for current state: [-0.02891785 -0.02164595 -0.02291649 -0.03508022]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 8\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.02891784525466504\n",
      "New Q-value: -0.02908242567012108\n",
      "\n",
      "Step 8\n",
      "Current state: 8\n",
      "Q-values for current state: [-0.02908243 -0.02164595 -0.02291649 -0.03508022]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 0\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.035080215024201014\n",
      "New Q-value: -0.035378879017351095\n",
      "\n",
      "Step 9\n",
      "Current state: 0\n",
      "Q-values for current state: [-0.03630726 -0.02954406 -0.03015896 -0.03726556]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 8\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.029544057848107187\n",
      "New Q-value: -0.02964601700421901\n",
      "\n",
      "Step 10\n",
      "Current state: 8\n",
      "Q-values for current state: [-0.02908243 -0.02164595 -0.02291649 -0.03537888]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 0\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.035378879017351095\n",
      "New Q-value: -0.035657362731016794\n",
      "\n",
      "Step 11\n",
      "Current state: 0\n",
      "Q-values for current state: [-0.03630726 -0.02964602 -0.03015896 -0.03726556]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 1\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.030158960213362\n",
      "New Q-value: -0.030296792527425866\n",
      "\n",
      "Step 12\n",
      "Current state: 1\n",
      "Q-values for current state: [-0.03566418 -0.02267082 -0.02415283 -0.02877312]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 1\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.028773120823833275\n",
      "New Q-value: -0.029049537076850013\n",
      "\n",
      "Step 13\n",
      "Current state: 1\n",
      "Q-values for current state: [-0.03566418 -0.02267082 -0.02415283 -0.02904954]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 9\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.022670824583158576\n",
      "New Q-value: -0.022880610549144226\n",
      "\n",
      "Step 14\n",
      "Current state: 9\n",
      "Q-values for current state: [-0.02582925 -0.01554598 -0.01996013 -0.02497938]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 8\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.025829248934319555\n",
      "New Q-value: -0.026302688981810145\n",
      "\n",
      "Step 15\n",
      "Current state: 8\n",
      "Q-values for current state: [-0.02908243 -0.02164595 -0.02291649 -0.03565736]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 16\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.02164594674655309\n",
      "New Q-value: -0.02184384372591919\n",
      "\n",
      "Step 16\n",
      "Current state: 16\n",
      "Q-values for current state: [-0.02032039 -0.01434202 -0.01622838 -0.02836897]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 8\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.02836897376479411\n",
      "New Q-value: -0.02860724154227702\n",
      "\n",
      "Step 17\n",
      "Current state: 8\n",
      "Q-values for current state: [-0.02908243 -0.02184384 -0.02291649 -0.03565736]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 16\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.02184384372591919\n",
      "New Q-value: -0.022021951007348682\n",
      "\n",
      "Step 18\n",
      "Current state: 16\n",
      "Q-values for current state: [-0.02032039 -0.01434202 -0.01622838 -0.02860724]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 16\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.020320389211761325\n",
      "New Q-value: -0.020650841944606604\n",
      "\n",
      "Step 19\n",
      "Current state: 16\n",
      "Q-values for current state: [-0.02065084 -0.01434202 -0.01622838 -0.02860724]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 16\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.020650841944606604\n",
      "New Q-value: -0.020948249404167354\n",
      "\n",
      "Step 20\n",
      "Current state: 16\n",
      "Q-values for current state: [-0.02094825 -0.01434202 -0.01622838 -0.02860724]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 17\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.01622838138191905\n",
      "New Q-value: -0.016609113434339847\n",
      "\n",
      "Step 21\n",
      "Current state: 17\n",
      "Q-values for current state: [-0.01932696 -0.01125676 -0.0105639  -0.01598829]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 9\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.015988290235173085\n",
      "New Q-value: -0.016866329635957283\n",
      "\n",
      "Step 22\n",
      "Current state: 9\n",
      "Q-values for current state: [-0.02630269 -0.01554598 -0.01996013 -0.02497938]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 17\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.015545983413700086\n",
      "New Q-value: -0.01599495526294278\n",
      "\n",
      "Step 23\n",
      "Current state: 17\n",
      "Q-values for current state: [-0.01932696 -0.01125676 -0.0105639  -0.01686633]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 16\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.019326961072834974\n",
      "New Q-value: -0.019756756619572886\n",
      "\n",
      "Step 24\n",
      "Current state: 16\n",
      "Q-values for current state: [-0.02094825 -0.01434202 -0.01660911 -0.02860724]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 8\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.02860724154227702\n",
      "New Q-value: -0.028838602733747443\n",
      "\n",
      "Step 25\n",
      "Current state: 8\n",
      "Q-values for current state: [-0.02908243 -0.02202195 -0.02291649 -0.03565736]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 9\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.022916494215923112\n",
      "New Q-value: -0.023144365544310364\n",
      "\n",
      "Step 26\n",
      "Current state: 9\n",
      "Q-values for current state: [-0.02630269 -0.01599496 -0.01996013 -0.02497938]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 1\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.024979381054037138\n",
      "New Q-value: -0.025655100950802125\n",
      "\n",
      "Step 27\n",
      "Current state: 1\n",
      "Q-values for current state: [-0.03566418 -0.02288061 -0.02415283 -0.02904954]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 9\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.022880610549144226\n",
      "New Q-value: -0.023112070244209366\n",
      "\n",
      "Step 28\n",
      "Current state: 9\n",
      "Q-values for current state: [-0.02630269 -0.01599496 -0.01996013 -0.0256551 ]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 17\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.01599495526294278\n",
      "New Q-value: -0.016399029927261204\n",
      "\n",
      "Step 29\n",
      "Current state: 17\n",
      "Q-values for current state: [-0.01975676 -0.01125676 -0.0105639  -0.01686633]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 16\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.019756756619572886\n",
      "New Q-value: -0.020143572611637007\n",
      "\n",
      "Step 30\n",
      "Current state: 16\n",
      "Q-values for current state: [-0.02094825 -0.01434202 -0.01660911 -0.0288386 ]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 16\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.020948249404167354\n",
      "New Q-value: -0.02121591611777203\n",
      "\n",
      "Step 31\n",
      "Current state: 16\n",
      "Q-values for current state: [-0.02121592 -0.01434202 -0.01660911 -0.0288386 ]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 24\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.014342017410751688\n",
      "New Q-value: -0.01461980209003877\n",
      "\n",
      "Step 32\n",
      "Current state: 24\n",
      "Q-values for current state: [-0.01172285 -0.01097734 -0.00749459 -0.01588351]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 25\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.007494593898550001\n",
      "New Q-value: -0.00823550152107\n",
      "\n",
      "Step 33\n",
      "Current state: 25\n",
      "Q-values for current state: [-0.01036228 -0.00516176 -0.00935841 -0.00866787]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 26\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.0093584079937905\n",
      "New Q-value: -0.00986769824441145\n",
      "\n",
      "Step 34\n",
      "Current state: 26\n",
      "Q-values for current state: [-0.00551393 -0.00638325 -0.00468559 -0.00601116]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 18\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.006011161761108047\n",
      "New Q-value: -0.006991996120447243\n",
      "\n",
      "Step 35\n",
      "Current state: 18\n",
      "Q-values for current state: [-0.01195372 -0.00816828 -0.0061258  -0.01268034]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 26\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.0081682754241\n",
      "New Q-value: -0.00879657893169\n",
      "\n",
      "Step 36\n",
      "Current state: 26\n",
      "Q-values for current state: [-0.00551393 -0.00638325 -0.00468559 -0.006992  ]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 25\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.005513934237375001\n",
      "New Q-value: -0.0064529078260125005\n",
      "\n",
      "Step 37\n",
      "Current state: 25\n",
      "Q-values for current state: [-0.01036228 -0.00516176 -0.0098677  -0.00866787]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 33\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.005161758025\n",
      "New Q-value: -0.005872873463125\n",
      "\n",
      "Step 38\n",
      "Current state: 33\n",
      "Q-values for current state: [-0.00239254 -0.00521703 -0.002805   -0.00421295]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 41\n",
      "Reward: -0.01\n",
      "Terminated: True\n",
      "Old Q-value: -0.005217031\n",
      "New Q-value: -0.0056953279\n",
      "\n",
      "Episode 67\n",
      "Starting state: 0\n",
      "\n",
      "Step 0\n",
      "Current state: 0\n",
      "Q-values for current state: [-0.03630726 -0.02964602 -0.03029679 -0.03726556]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 1\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.030296792527425866\n",
      "New Q-value: -0.03046275994788317\n",
      "\n",
      "Step 1\n",
      "Current state: 1\n",
      "Q-values for current state: [-0.03566418 -0.02311207 -0.02415283 -0.02904954]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 0\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.035664176927239125\n",
      "New Q-value: -0.035914130849916015\n",
      "\n",
      "Step 2\n",
      "Current state: 0\n",
      "Q-values for current state: [-0.03630726 -0.02964602 -0.03046276 -0.03726556]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 8\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.02964601700421901\n",
      "New Q-value: -0.029773500649495233\n",
      "\n",
      "Step 3\n",
      "Current state: 8\n",
      "Q-values for current state: [-0.02908243 -0.02202195 -0.02314437 -0.03565736]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 9\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.023144365544310364\n",
      "New Q-value: -0.02338783683296914\n",
      "\n",
      "Step 4\n",
      "Current state: 9\n",
      "Q-values for current state: [-0.02630269 -0.01639903 -0.01996013 -0.0256551 ]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 1\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.025655100950802125\n",
      "New Q-value: -0.0262852375289218\n",
      "\n",
      "Step 5\n",
      "Current state: 1\n",
      "Q-values for current state: [-0.03591413 -0.02311207 -0.02415283 -0.02904954]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 0\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.035914130849916015\n",
      "New Q-value: -0.03615120032662646\n",
      "\n",
      "Step 6\n",
      "Current state: 0\n",
      "Q-values for current state: [-0.03630726 -0.0297735  -0.03046276 -0.03726556]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 0\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.03630725670882619\n",
      "New Q-value: -0.036505013599645614\n",
      "\n",
      "Step 7\n",
      "Current state: 0\n",
      "Q-values for current state: [-0.03650501 -0.0297735  -0.03046276 -0.03726556]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 0\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.037265558877996575\n",
      "New Q-value: -0.03736748555189896\n",
      "\n",
      "Step 8\n",
      "Current state: 0\n",
      "Q-values for current state: [-0.03650501 -0.0297735  -0.03046276 -0.03736749]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 0\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.03736748555189896\n",
      "New Q-value: -0.03745921955841111\n",
      "\n",
      "Step 9\n",
      "Current state: 0\n",
      "Q-values for current state: [-0.03650501 -0.0297735  -0.03046276 -0.03745922]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 8\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.029773500649495233\n",
      "New Q-value: -0.029888235930243836\n",
      "\n",
      "Step 10\n",
      "Current state: 8\n",
      "Q-values for current state: [-0.02908243 -0.02202195 -0.02338784 -0.03565736]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 0\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.035657362731016794\n",
      "New Q-value: -0.03593100887128828\n",
      "\n",
      "Step 11\n",
      "Current state: 0\n",
      "Q-values for current state: [-0.03650501 -0.02988824 -0.03046276 -0.03745922]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 8\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.029888235930243836\n",
      "New Q-value: -0.029991497682917576\n",
      "\n",
      "Step 12\n",
      "Current state: 8\n",
      "Q-values for current state: [-0.02908243 -0.02202195 -0.02338784 -0.03593101]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 16\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.022021951007348682\n",
      "New Q-value: -0.022208637105167496\n",
      "\n",
      "Step 13\n",
      "Current state: 16\n",
      "Q-values for current state: [-0.02121592 -0.0146198  -0.01660911 -0.0288386 ]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 16\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.02121591611777203\n",
      "New Q-value: -0.021483205704548508\n",
      "\n",
      "Step 14\n",
      "Current state: 16\n",
      "Q-values for current state: [-0.02148321 -0.0146198  -0.01660911 -0.0288386 ]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 16\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.021483205704548508\n",
      "New Q-value: -0.02172376633264734\n",
      "\n",
      "Step 15\n",
      "Current state: 16\n",
      "Q-values for current state: [-0.02172377 -0.0146198  -0.01660911 -0.0288386 ]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 8\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.028838602733747443\n",
      "New Q-value: -0.02906456298536361\n",
      "\n",
      "Step 16\n",
      "Current state: 8\n",
      "Q-values for current state: [-0.02908243 -0.02220864 -0.02338784 -0.03593101]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 8\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.02908242567012108\n",
      "New Q-value: -0.029284003628099884\n",
      "\n",
      "Step 17\n",
      "Current state: 8\n",
      "Q-values for current state: [-0.029284   -0.02220864 -0.02338784 -0.03593101]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 16\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.022208637105167496\n",
      "New Q-value: -0.02237665459320443\n",
      "\n",
      "Step 18\n",
      "Current state: 16\n",
      "Q-values for current state: [-0.02172377 -0.0146198  -0.01660911 -0.02906456]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 17\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.016609113434339847\n",
      "New Q-value: -0.016951772281518566\n",
      "\n",
      "Step 19\n",
      "Current state: 17\n",
      "Q-values for current state: [-0.02014357 -0.01125676 -0.0105639  -0.01686633]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 18\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.010563896743291602\n",
      "New Q-value: -0.01108945760441244\n",
      "\n",
      "Step 20\n",
      "Current state: 18\n",
      "Q-values for current state: [-0.01195372 -0.00879658 -0.0061258  -0.01268034]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 17\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.011953715379309842\n",
      "New Q-value: -0.01281184231379804\n",
      "\n",
      "Step 21\n",
      "Current state: 17\n",
      "Q-values for current state: [-0.02014357 -0.01125676 -0.01108946 -0.01686633]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 9\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.016866329635957283\n",
      "New Q-value: -0.01773760451545137\n",
      "\n",
      "Step 22\n",
      "Current state: 9\n",
      "Q-values for current state: [-0.02630269 -0.01639903 -0.01996013 -0.02628524]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 1\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.0262852375289218\n",
      "New Q-value: -0.02685236044922951\n",
      "\n",
      "Step 23\n",
      "Current state: 1\n",
      "Q-values for current state: [-0.0361512  -0.02311207 -0.02415283 -0.02904954]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 1\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.029049537076850013\n",
      "New Q-value: -0.0293402300423649\n",
      "\n",
      "Step 24\n",
      "Current state: 1\n",
      "Q-values for current state: [-0.0361512  -0.02311207 -0.02415283 -0.02934023]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 9\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.023112070244209366\n",
      "New Q-value: -0.023358771062878245\n",
      "\n",
      "Step 25\n",
      "Current state: 9\n",
      "Q-values for current state: [-0.02630269 -0.01639903 -0.01996013 -0.02685236]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 17\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.016399029927261204\n",
      "New Q-value: -0.016812625406954267\n",
      "\n",
      "Step 26\n",
      "Current state: 17\n",
      "Q-values for current state: [-0.02014357 -0.01125676 -0.01108946 -0.0177376 ]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 25\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.011256760501922088\n",
      "New Q-value: -0.011689007430726754\n",
      "\n",
      "Step 27\n",
      "Current state: 25\n",
      "Q-values for current state: [-0.01036228 -0.00587287 -0.0098677  -0.00866787]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 26\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.00986769824441145\n",
      "New Q-value: -0.010326059469970306\n",
      "\n",
      "Step 28\n",
      "Current state: 26\n",
      "Q-values for current state: [-0.00645291 -0.00638325 -0.00468559 -0.006992  ]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 18\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.006991996120447243\n",
      "New Q-value: -0.007874747043852518\n",
      "\n",
      "Step 29\n",
      "Current state: 18\n",
      "Q-values for current state: [-0.01281184 -0.00879658 -0.0061258  -0.01268034]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 26\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.00879657893169\n",
      "New Q-value: -0.009362052088521\n",
      "\n",
      "Step 30\n",
      "Current state: 26\n",
      "Q-values for current state: [-0.00645291 -0.00638325 -0.00468559 -0.00787475]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 27\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.00468559\n",
      "New Q-value: -0.005217031\n",
      "\n",
      "Step 31\n",
      "Current state: 27\n",
      "Q-values for current state: [ 0.       -0.001    -0.003439 -0.0019  ]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 19\n",
      "Reward: -0.01\n",
      "Terminated: True\n",
      "Old Q-value: -0.0019000000000000002\n",
      "New Q-value: -0.00271\n",
      "\n",
      "Episode 68\n",
      "Starting state: 0\n",
      "\n",
      "Step 0\n",
      "Current state: 0\n",
      "Q-values for current state: [-0.03650501 -0.0299915  -0.03046276 -0.03745922]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 0\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.036505013599645614\n",
      "New Q-value: -0.03670370451955822\n",
      "\n",
      "Step 1\n",
      "Current state: 0\n",
      "Q-values for current state: [-0.0367037  -0.0299915  -0.03046276 -0.03745922]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 0\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.03670370451955822\n",
      "New Q-value: -0.036882526347479565\n",
      "\n",
      "Step 2\n",
      "Current state: 0\n",
      "Q-values for current state: [-0.03688253 -0.0299915  -0.03046276 -0.03745922]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 0\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.03745921955841111\n",
      "New Q-value: -0.03756248988244717\n",
      "\n",
      "Step 3\n",
      "Current state: 0\n",
      "Q-values for current state: [-0.03688253 -0.0299915  -0.03046276 -0.03756249]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 0\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.03756248988244717\n",
      "New Q-value: -0.03765543317407962\n",
      "\n",
      "Step 4\n",
      "Current state: 0\n",
      "Q-values for current state: [-0.03688253 -0.0299915  -0.03046276 -0.03765543]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 0\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.036882526347479565\n",
      "New Q-value: -0.03704346599260878\n",
      "\n",
      "Step 5\n",
      "Current state: 0\n",
      "Q-values for current state: [-0.03704347 -0.0299915  -0.03046276 -0.03765543]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 8\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.029991497682917576\n",
      "New Q-value: -0.030118130100980238\n",
      "\n",
      "Step 6\n",
      "Current state: 8\n",
      "Q-values for current state: [-0.029284   -0.02237665 -0.02338784 -0.03593101]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 9\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.02338783683296914\n",
      "New Q-value: -0.02364625256333288\n",
      "\n",
      "Step 7\n",
      "Current state: 9\n",
      "Q-values for current state: [-0.02630269 -0.01681263 -0.01996013 -0.02685236]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 17\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.016812625406954267\n",
      "New Q-value: -0.017184861338678022\n",
      "\n",
      "Step 8\n",
      "Current state: 17\n",
      "Q-values for current state: [-0.02014357 -0.01168901 -0.01108946 -0.0177376 ]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 18\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.01108945760441244\n",
      "New Q-value: -0.011562462379421197\n",
      "\n",
      "Step 9\n",
      "Current state: 18\n",
      "Q-values for current state: [-0.01281184 -0.00936205 -0.0061258  -0.01268034]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 17\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.01281184231379804\n",
      "New Q-value: -0.01362909200846325\n",
      "\n",
      "Step 10\n",
      "Current state: 17\n",
      "Q-values for current state: [-0.02014357 -0.01168901 -0.01156246 -0.0177376 ]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 18\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.011562462379421197\n",
      "New Q-value: -0.011988166676929077\n",
      "\n",
      "Step 11\n",
      "Current state: 18\n",
      "Q-values for current state: [-0.01362909 -0.00936205 -0.0061258  -0.01268034]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 26\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.009362052088521\n",
      "New Q-value: -0.0099214648246689\n",
      "\n",
      "Step 12\n",
      "Current state: 26\n",
      "Q-values for current state: [-0.00645291 -0.00638325 -0.00521703 -0.00787475]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 18\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.007874747043852518\n",
      "New Q-value: -0.008669222874917266\n",
      "\n",
      "Step 13\n",
      "Current state: 18\n",
      "Q-values for current state: [-0.01362909 -0.00992146 -0.0061258  -0.01268034]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 26\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.0099214648246689\n",
      "New Q-value: -0.01042493628720201\n",
      "\n",
      "Step 14\n",
      "Current state: 26\n",
      "Q-values for current state: [-0.00645291 -0.00638325 -0.00521703 -0.00866922]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 34\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.00638324511\n",
      "New Q-value: -0.006839920599\n",
      "\n",
      "Step 15\n",
      "Current state: 34\n",
      "Q-values for current state: [-0.0037136  -0.003439   -0.001      -0.00372548]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 42\n",
      "Reward: -0.01\n",
      "Terminated: True\n",
      "Old Q-value: -0.003439\n",
      "New Q-value: -0.0040951\n",
      "\n",
      "Episode 69\n",
      "Starting state: 0\n",
      "\n",
      "Step 0\n",
      "Current state: 0\n",
      "Q-values for current state: [-0.03704347 -0.03011813 -0.03046276 -0.03765543]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 0\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.03765543317407962\n",
      "New Q-value: -0.03775111221626478\n",
      "\n",
      "Step 1\n",
      "Current state: 0\n",
      "Q-values for current state: [-0.03704347 -0.03011813 -0.03046276 -0.03775111]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 0\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.03704346599260878\n",
      "New Q-value: -0.03720034175294102\n",
      "\n",
      "Step 2\n",
      "Current state: 0\n",
      "Q-values for current state: [-0.03720034 -0.03011813 -0.03046276 -0.03775111]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 0\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.03720034175294102\n",
      "New Q-value: -0.03734152993724004\n",
      "\n",
      "Step 3\n",
      "Current state: 0\n",
      "Q-values for current state: [-0.03734153 -0.03011813 -0.03046276 -0.03775111]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 1\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.03046275994788317\n",
      "New Q-value: -0.030635567204068286\n",
      "\n",
      "Step 4\n",
      "Current state: 1\n",
      "Q-values for current state: [-0.0361512  -0.02335877 -0.02415283 -0.02934023]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 9\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.023358771062878245\n",
      "New Q-value: -0.023655455783764833\n",
      "\n",
      "Step 5\n",
      "Current state: 9\n",
      "Q-values for current state: [-0.02630269 -0.01718486 -0.01996013 -0.02685236]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 8\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.026302688981810145\n",
      "New Q-value: -0.02679820226998355\n",
      "\n",
      "Step 6\n",
      "Current state: 8\n",
      "Q-values for current state: [-0.029284   -0.02237665 -0.02364625 -0.03593101]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 9\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.02364625256333288\n",
      "New Q-value: -0.023914189134174004\n",
      "\n",
      "Step 7\n",
      "Current state: 9\n",
      "Q-values for current state: [-0.0267982  -0.01718486 -0.01996013 -0.02685236]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 1\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.02685236044922951\n",
      "New Q-value: -0.027414392703764216\n",
      "\n",
      "Step 8\n",
      "Current state: 1\n",
      "Q-values for current state: [-0.0361512  -0.02365546 -0.02415283 -0.02934023]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 2\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.024152825182889397\n",
      "New Q-value: -0.02446248153762973\n",
      "\n",
      "Step 9\n",
      "Current state: 2\n",
      "Q-values for current state: [-0.02758289 -0.01815725 -0.02120827 -0.02188572]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 10\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.018157251295044967\n",
      "New Q-value: -0.01858521213352842\n",
      "\n",
      "Step 10\n",
      "Current state: 10\n",
      "Q-values for current state: [-0.01918952 -0.01309143 -0.01404319 -0.02081884]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 9\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.019189523442924405\n",
      "New Q-value: -0.019903132925806376\n",
      "\n",
      "Step 11\n",
      "Current state: 9\n",
      "Q-values for current state: [-0.0267982  -0.01718486 -0.01996013 -0.02741439]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 10\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.01996012502355649\n",
      "New Q-value: -0.020207798489188793\n",
      "\n",
      "Step 12\n",
      "Current state: 10\n",
      "Q-values for current state: [-0.01990313 -0.01309143 -0.01404319 -0.02081884]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 9\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.019903132925806376\n",
      "New Q-value: -0.02054538146040015\n",
      "\n",
      "Step 13\n",
      "Current state: 9\n",
      "Q-values for current state: [-0.0267982  -0.01718486 -0.0202078  -0.02741439]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 10\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.020207798489188793\n",
      "New Q-value: -0.020430704608257865\n",
      "\n",
      "Step 14\n",
      "Current state: 10\n",
      "Q-values for current state: [-0.02054538 -0.01309143 -0.01404319 -0.02081884]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 2\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.02081884446568021\n",
      "New Q-value: -0.02150255517179739\n",
      "\n",
      "Step 15\n",
      "Current state: 2\n",
      "Q-values for current state: [-0.02758289 -0.01858521 -0.02120827 -0.02188572]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 2\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.021885717696141463\n",
      "New Q-value: -0.022462741079212516\n",
      "\n",
      "Step 16\n",
      "Current state: 2\n",
      "Q-values for current state: [-0.02758289 -0.01858521 -0.02120827 -0.02246274]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 2\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.022462741079212516\n",
      "New Q-value: -0.022982062123976466\n",
      "\n",
      "Step 17\n",
      "Current state: 2\n",
      "Q-values for current state: [-0.02758289 -0.01858521 -0.02120827 -0.02298206]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 2\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.022982062123976466\n",
      "New Q-value: -0.023449451064264017\n",
      "\n",
      "Step 18\n",
      "Current state: 2\n",
      "Q-values for current state: [-0.02758289 -0.01858521 -0.02120827 -0.02344945]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 2\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.023449451064264017\n",
      "New Q-value: -0.023870101110522814\n",
      "\n",
      "Step 19\n",
      "Current state: 2\n",
      "Q-values for current state: [-0.02758289 -0.01858521 -0.02120827 -0.0238701 ]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 1\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.027582894372395653\n",
      "New Q-value: -0.028071873234613746\n",
      "\n",
      "Step 20\n",
      "Current state: 1\n",
      "Q-values for current state: [-0.0361512  -0.02365546 -0.02446248 -0.02934023]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 0\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.03615120032662646\n",
      "New Q-value: -0.03639730265355694\n",
      "\n",
      "Step 21\n",
      "Current state: 0\n",
      "Q-values for current state: [-0.03734153 -0.03011813 -0.03063557 -0.03775111]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 1\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.030635567204068286\n",
      "New Q-value: -0.030819278783119117\n",
      "\n",
      "Step 22\n",
      "Current state: 1\n",
      "Q-values for current state: [-0.0363973  -0.02365546 -0.02446248 -0.02934023]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 0\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.03639730265355694\n",
      "New Q-value: -0.036618794747794364\n",
      "\n",
      "Step 23\n",
      "Current state: 0\n",
      "Q-values for current state: [-0.03734153 -0.03011813 -0.03081928 -0.03775111]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 1\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.030819278783119117\n",
      "New Q-value: -0.030984619204264863\n",
      "\n",
      "Step 24\n",
      "Current state: 1\n",
      "Q-values for current state: [-0.03661879 -0.02365546 -0.02446248 -0.02934023]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 2\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.02446248153762973\n",
      "New Q-value: -0.024781828536551958\n",
      "\n",
      "Step 25\n",
      "Current state: 2\n",
      "Q-values for current state: [-0.02807187 -0.01858521 -0.02120827 -0.0238701 ]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 10\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.01858521213352842\n",
      "New Q-value: -0.018970376888163533\n",
      "\n",
      "Step 26\n",
      "Current state: 10\n",
      "Q-values for current state: [-0.02054538 -0.01309143 -0.01404319 -0.02150256]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 9\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.02054538146040015\n",
      "New Q-value: -0.021123405141534546\n",
      "\n",
      "Step 27\n",
      "Current state: 9\n",
      "Q-values for current state: [-0.0267982  -0.01718486 -0.0204307  -0.02741439]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 10\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.020430704608257865\n",
      "New Q-value: -0.020631320115420033\n",
      "\n",
      "Step 28\n",
      "Current state: 10\n",
      "Q-values for current state: [-0.02112341 -0.01309143 -0.01404319 -0.02150256]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 2\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.02150255517179739\n",
      "New Q-value: -0.022154485458993185\n",
      "\n",
      "Step 29\n",
      "Current state: 2\n",
      "Q-values for current state: [-0.02807187 -0.01897038 -0.02120827 -0.0238701 ]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 10\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.018970376888163533\n",
      "New Q-value: -0.019317025167335133\n",
      "\n",
      "Step 30\n",
      "Current state: 10\n",
      "Q-values for current state: [-0.02112341 -0.01309143 -0.01404319 -0.02215449]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 9\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.021123405141534546\n",
      "New Q-value: -0.021643626454555503\n",
      "\n",
      "Step 31\n",
      "Current state: 9\n",
      "Q-values for current state: [-0.0267982  -0.01718486 -0.02063132 -0.02741439]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 1\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.027414392703764216\n",
      "New Q-value: -0.027920221732845453\n",
      "\n",
      "Step 32\n",
      "Current state: 1\n",
      "Q-values for current state: [-0.03661879 -0.02365546 -0.02478183 -0.02934023]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 1\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.0293402300423649\n",
      "New Q-value: -0.02965347533758607\n",
      "\n",
      "Step 33\n",
      "Current state: 1\n",
      "Q-values for current state: [-0.03661879 -0.02365546 -0.02478183 -0.02965348]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 1\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.02965347533758607\n",
      "New Q-value: -0.02993539610328512\n",
      "\n",
      "Step 34\n",
      "Current state: 1\n",
      "Q-values for current state: [-0.03661879 -0.02365546 -0.02478183 -0.0299354 ]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 2\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.024781828536551958\n",
      "New Q-value: -0.025138763073793598\n",
      "\n",
      "Step 35\n",
      "Current state: 2\n",
      "Q-values for current state: [-0.02807187 -0.01931703 -0.02120827 -0.0238701 ]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 2\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.023870101110522814\n",
      "New Q-value: -0.02431820839036737\n",
      "\n",
      "Step 36\n",
      "Current state: 2\n",
      "Q-values for current state: [-0.02807187 -0.01931703 -0.02120827 -0.02431821]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 10\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.019317025167335133\n",
      "New Q-value: -0.019629008618589572\n",
      "\n",
      "Step 37\n",
      "Current state: 10\n",
      "Q-values for current state: [-0.02164363 -0.01309143 -0.01404319 -0.02215449]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 9\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.021643626454555503\n",
      "New Q-value: -0.022111825636274365\n",
      "\n",
      "Step 38\n",
      "Current state: 9\n",
      "Q-values for current state: [-0.0267982  -0.01718486 -0.02063132 -0.02792022]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 1\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.027920221732845453\n",
      "New Q-value: -0.028375467859018566\n",
      "\n",
      "Step 39\n",
      "Current state: 1\n",
      "Q-values for current state: [-0.03661879 -0.02365546 -0.02513876 -0.0299354 ]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 1\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.02993539610328512\n",
      "New Q-value: -0.030189124792414267\n",
      "\n",
      "Step 40\n",
      "Current state: 1\n",
      "Q-values for current state: [-0.03661879 -0.02365546 -0.02513876 -0.03018912]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 0\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.036618794747794364\n",
      "New Q-value: -0.03681813763260805\n",
      "\n",
      "Step 41\n",
      "Current state: 0\n",
      "Q-values for current state: [-0.03734153 -0.03011813 -0.03098462 -0.03775111]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 0\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.03734152993724004\n",
      "New Q-value: -0.03746859930310916\n",
      "\n",
      "Step 42\n",
      "Current state: 0\n",
      "Q-values for current state: [-0.0374686  -0.03011813 -0.03098462 -0.03775111]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 1\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.030984619204264863\n",
      "New Q-value: -0.031133425583296036\n",
      "\n",
      "Step 43\n",
      "Current state: 1\n",
      "Q-values for current state: [-0.03681814 -0.02365546 -0.02513876 -0.03018912]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 0\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.03681813763260805\n",
      "New Q-value: -0.036997546228940366\n",
      "\n",
      "Step 44\n",
      "Current state: 0\n",
      "Q-values for current state: [-0.0374686  -0.03011813 -0.03113343 -0.03775111]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 0\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.03775111221626478\n",
      "New Q-value: -0.037837223354231425\n",
      "\n",
      "Step 45\n",
      "Current state: 0\n",
      "Q-values for current state: [-0.0374686  -0.03011813 -0.03113343 -0.03783722]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 0\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.037837223354231425\n",
      "New Q-value: -0.037914723378401405\n",
      "\n",
      "Step 46\n",
      "Current state: 0\n",
      "Q-values for current state: [-0.0374686  -0.03011813 -0.03113343 -0.03791472]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 0\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.037914723378401405\n",
      "New Q-value: -0.03798447340015439\n",
      "\n",
      "Step 47\n",
      "Current state: 0\n",
      "Q-values for current state: [-0.0374686  -0.03011813 -0.03113343 -0.03798447]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 0\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.03746859930310916\n",
      "New Q-value: -0.03758296173239137\n",
      "\n",
      "Step 48\n",
      "Current state: 0\n",
      "Q-values for current state: [-0.03758296 -0.03011813 -0.03113343 -0.03798447]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 0\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.03798447340015439\n",
      "New Q-value: -0.03804724841973207\n",
      "\n",
      "Step 49\n",
      "Current state: 0\n",
      "Q-values for current state: [-0.03758296 -0.03011813 -0.03113343 -0.03804725]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 8\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.030118130100980238\n",
      "New Q-value: -0.030232099277236635\n",
      "\n",
      "Step 50\n",
      "Current state: 8\n",
      "Q-values for current state: [-0.029284   -0.02237665 -0.02391419 -0.03593101]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 8\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.029284003628099884\n",
      "New Q-value: -0.029481385451644318\n",
      "\n",
      "Step 51\n",
      "Current state: 8\n",
      "Q-values for current state: [-0.02948139 -0.02237665 -0.02391419 -0.03593101]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 16\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.02237665459320443\n",
      "New Q-value: -0.02252787033243767\n",
      "\n",
      "Step 52\n",
      "Current state: 16\n",
      "Q-values for current state: [-0.02172377 -0.0146198  -0.01695177 -0.02906456]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 17\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.016951772281518566\n",
      "New Q-value: -0.01736705075928575\n",
      "\n",
      "Step 53\n",
      "Current state: 17\n",
      "Q-values for current state: [-0.02014357 -0.01168901 -0.01198817 -0.0177376 ]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 9\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.01773760451545137\n",
      "New Q-value: -0.018596405891080643\n",
      "\n",
      "Step 54\n",
      "Current state: 9\n",
      "Q-values for current state: [-0.0267982  -0.01718486 -0.02063132 -0.02837547]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 10\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.020631320115420033\n",
      "New Q-value: -0.020811874071865983\n",
      "\n",
      "Step 55\n",
      "Current state: 10\n",
      "Q-values for current state: [-0.02211183 -0.01309143 -0.01404319 -0.02215449]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 2\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.022154485458993185\n",
      "New Q-value: -0.022803792731859875\n",
      "\n",
      "Step 56\n",
      "Current state: 2\n",
      "Q-values for current state: [-0.02807187 -0.01962901 -0.02120827 -0.02431821]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 3\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.021208274422174864\n",
      "New Q-value: -0.02139887819220485\n",
      "\n",
      "Step 57\n",
      "Current state: 3\n",
      "Q-values for current state: [-0.02129114 -0.01380454 -0.01419222 -0.01923579]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 2\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.021291136620655265\n",
      "New Q-value: -0.02202677877735575\n",
      "\n",
      "Step 58\n",
      "Current state: 2\n",
      "Q-values for current state: [-0.02807187 -0.01962901 -0.02139888 -0.02431821]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 3\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.02139887819220485\n",
      "New Q-value: -0.021570421585231835\n",
      "\n",
      "Step 59\n",
      "Current state: 3\n",
      "Q-values for current state: [-0.02202678 -0.01380454 -0.01419222 -0.01923579]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 4\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.014192218232828763\n",
      "New Q-value: -0.014564472146437558\n",
      "\n",
      "Step 60\n",
      "Current state: 4\n",
      "Q-values for current state: [-0.01472648 -0.00833132 -0.00993198 -0.01287294]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 4\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.012872944800814043\n",
      "New Q-value: -0.013377126057624311\n",
      "\n",
      "Step 61\n",
      "Current state: 4\n",
      "Q-values for current state: [-0.01472648 -0.00833132 -0.00993198 -0.01337713]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 5\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.009931975786584104\n",
      "New Q-value: -0.010505966261719443\n",
      "\n",
      "Step 62\n",
      "Current state: 5\n",
      "Q-values for current state: [-0.0059704  -0.00984895 -0.0089244  -0.00812862]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 6\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.008924396339526\n",
      "New Q-value: -0.0093586617055734\n",
      "\n",
      "Step 63\n",
      "Current state: 6\n",
      "Q-values for current state: [-0.0095454 -0.003439  -0.0057641 -0.0073321]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 5\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.009545397419543097\n",
      "New Q-value: -0.010158045731382537\n",
      "\n",
      "Step 64\n",
      "Current state: 5\n",
      "Q-values for current state: [-0.0059704  -0.00984895 -0.00935866 -0.00812862]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 5\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.008128624012820626\n",
      "New Q-value: -0.008882949665332313\n",
      "\n",
      "Step 65\n",
      "Current state: 5\n",
      "Q-values for current state: [-0.0059704  -0.00984895 -0.00935866 -0.00888295]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 5\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.008882949665332313\n",
      "New Q-value: -0.009561842752592831\n",
      "\n",
      "Step 66\n",
      "Current state: 5\n",
      "Q-values for current state: [-0.0059704  -0.00984895 -0.00935866 -0.00956184]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 4\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.005970400566250001\n",
      "New Q-value: -0.0071648362465166725\n",
      "\n",
      "Step 67\n",
      "Current state: 4\n",
      "Q-values for current state: [-0.01472648 -0.00833132 -0.01050597 -0.01337713]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 3\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.014726479585011549\n",
      "New Q-value: -0.015565262838757865\n",
      "\n",
      "Step 68\n",
      "Current state: 3\n",
      "Q-values for current state: [-0.02202678 -0.01380454 -0.01456447 -0.01923579]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 4\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.014564472146437558\n",
      "New Q-value: -0.014899500668685475\n",
      "\n",
      "Step 69\n",
      "Current state: 4\n",
      "Q-values for current state: [-0.01556526 -0.00833132 -0.01050597 -0.01337713]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 12\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.008331323546228125\n",
      "New Q-value: -0.009107040739555313\n",
      "\n",
      "Step 70\n",
      "Current state: 12\n",
      "Q-values for current state: [-0.00849261 -0.00640894 -0.01110685 -0.00647325]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 4\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.006473247802282657\n",
      "New Q-value: -0.007691091892312146\n",
      "\n",
      "Step 71\n",
      "Current state: 4\n",
      "Q-values for current state: [-0.01556526 -0.00910704 -0.01050597 -0.01337713]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 12\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.009107040739555313\n",
      "New Q-value: -0.009805186213549782\n",
      "\n",
      "Step 72\n",
      "Current state: 12\n",
      "Q-values for current state: [-0.00849261 -0.00640894 -0.01110685 -0.00769109]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 13\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.011106852067882792\n",
      "New Q-value: -0.011491784806094512\n",
      "\n",
      "Step 73\n",
      "Current state: 13\n",
      "Q-values for current state: [-0.00941556 -0.00672538 -0.00521703 -0.00858544]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 21\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.0067253783475\n",
      "New Q-value: -0.00725865563775\n",
      "\n",
      "Step 74\n",
      "Current state: 21\n",
      "Q-values for current state: [-0.0036195  -0.0040951  -0.00474761 -0.00216648]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 13\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.002166475\n",
      "New Q-value: -0.003445445445\n",
      "\n",
      "Step 75\n",
      "Current state: 13\n",
      "Q-values for current state: [-0.00941556 -0.00725866 -0.00521703 -0.00858544]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 5\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.00858543802192375\n",
      "New Q-value: -0.009407553663150459\n",
      "\n",
      "Step 76\n",
      "Current state: 5\n",
      "Q-values for current state: [-0.00716484 -0.00984895 -0.00935866 -0.00956184]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 6\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.0093586617055734\n",
      "New Q-value: -0.00974950053501606\n",
      "\n",
      "Step 77\n",
      "Current state: 6\n",
      "Q-values for current state: [-0.01015805 -0.003439   -0.0057641  -0.0073321 ]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 6\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.007332100255500001\n",
      "New Q-value: -0.00792559522995\n",
      "\n",
      "Step 78\n",
      "Current state: 6\n",
      "Q-values for current state: [-0.01015805 -0.003439   -0.0057641  -0.0079256 ]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 5\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.010158045731382537\n",
      "New Q-value: -0.010822900601663367\n",
      "\n",
      "Step 79\n",
      "Current state: 5\n",
      "Q-values for current state: [-0.00716484 -0.00984895 -0.0097495  -0.00956184]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 4\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.0071648362465166725\n",
      "New Q-value: -0.008379845312152235\n",
      "\n",
      "Step 80\n",
      "Current state: 4\n",
      "Q-values for current state: [-0.01556526 -0.00980519 -0.01050597 -0.01337713]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 5\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.010505966261719443\n",
      "New Q-value: -0.01125145494020196\n",
      "\n",
      "Step 81\n",
      "Current state: 5\n",
      "Q-values for current state: [-0.00837985 -0.00984895 -0.0097495  -0.00956184]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 13\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.009848945145525066\n",
      "New Q-value: -0.010359668575972559\n",
      "\n",
      "Step 82\n",
      "Current state: 13\n",
      "Q-values for current state: [-0.00941556 -0.00725866 -0.00521703 -0.00940755]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 12\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.009415556403303054\n",
      "New Q-value: -0.010082850310922748\n",
      "\n",
      "Step 83\n",
      "Current state: 12\n",
      "Q-values for current state: [-0.00849261 -0.00640894 -0.01149178 -0.00769109]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 13\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.011491784806094512\n",
      "New Q-value: -0.011838224270485061\n",
      "\n",
      "Step 84\n",
      "Current state: 13\n",
      "Q-values for current state: [-0.01008285 -0.00725866 -0.00521703 -0.00940755]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 21\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.00725865563775\n",
      "New Q-value: -0.00786010739125\n",
      "\n",
      "Step 85\n",
      "Current state: 21\n",
      "Q-values for current state: [-0.0036195  -0.0040951  -0.00474761 -0.00344545]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 20\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.0036195000000000003\n",
      "New Q-value: -0.004455197500000001\n",
      "\n",
      "Step 86\n",
      "Current state: 20\n",
      "Q-values for current state: [-0.0040951  -0.00271    -0.0020805  -0.00403209]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 19\n",
      "Reward: -0.01\n",
      "Terminated: True\n",
      "Old Q-value: -0.0040951\n",
      "New Q-value: -0.00468559\n",
      "\n",
      "Episode 70\n",
      "Starting state: 0\n",
      "\n",
      "Step 0\n",
      "Current state: 0\n",
      "Q-values for current state: [-0.03758296 -0.0302321  -0.03113343 -0.03804725]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 0\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.03804724841973207\n",
      "New Q-value: -0.03811457300909634\n",
      "\n",
      "Step 1\n",
      "Current state: 0\n",
      "Q-values for current state: [-0.03758296 -0.0302321  -0.03113343 -0.03811457]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 1\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.031133425583296036\n",
      "New Q-value: -0.03126735132442409\n",
      "\n",
      "Step 2\n",
      "Current state: 1\n",
      "Q-values for current state: [-0.03699755 -0.02365546 -0.02513876 -0.03018912]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 1\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.030189124792414267\n",
      "New Q-value: -0.0304174806126305\n",
      "\n",
      "Step 3\n",
      "Current state: 1\n",
      "Q-values for current state: [-0.03699755 -0.02365546 -0.02513876 -0.03041748]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 1\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.0304174806126305\n",
      "New Q-value: -0.03062300085082511\n",
      "\n",
      "Step 4\n",
      "Current state: 1\n",
      "Q-values for current state: [-0.03699755 -0.02365546 -0.02513876 -0.030623  ]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 2\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.025138763073793598\n",
      "New Q-value: -0.025489642585180248\n",
      "\n",
      "Step 5\n",
      "Current state: 2\n",
      "Q-values for current state: [-0.02807187 -0.01962901 -0.02157042 -0.02431821]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 3\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.021570421585231835\n",
      "New Q-value: -0.021724810638956125\n",
      "\n",
      "Step 6\n",
      "Current state: 3\n",
      "Q-values for current state: [-0.02202678 -0.01380454 -0.0148995  -0.01923579]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 4\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.014899500668685475\n",
      "New Q-value: -0.015341043292104157\n",
      "\n",
      "Step 7\n",
      "Current state: 4\n",
      "Q-values for current state: [-0.01556526 -0.00980519 -0.01125145 -0.01337713]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 12\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.009805186213549782\n",
      "New Q-value: -0.010433517140144804\n",
      "\n",
      "Step 8\n",
      "Current state: 12\n",
      "Q-values for current state: [-0.00849261 -0.00640894 -0.01183822 -0.00769109]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 20\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.006408942609999999\n",
      "New Q-value: -0.006965695849\n",
      "\n",
      "Step 9\n",
      "Current state: 20\n",
      "Q-values for current state: [-0.00468559 -0.00271    -0.0020805  -0.00403209]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 12\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.004032085463125\n",
      "New Q-value: -0.0052906180224675\n",
      "\n",
      "Step 10\n",
      "Current state: 12\n",
      "Q-values for current state: [-0.00849261 -0.0069657  -0.01183822 -0.00769109]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 11\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.008492613011600566\n",
      "New Q-value: -0.009376022182118379\n",
      "\n",
      "Step 11\n",
      "Current state: 11\n",
      "Q-values for current state: [-0.01118479 -0.00771232 -0.01107158 -0.01496245]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 12\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.01107158169968521\n",
      "New Q-value: -0.011626164635371689\n",
      "\n",
      "Step 12\n",
      "Current state: 12\n",
      "Q-values for current state: [-0.00937602 -0.0069657  -0.01183822 -0.00769109]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 20\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.006965695849\n",
      "New Q-value: -0.0074667737641\n",
      "\n",
      "Step 13\n",
      "Current state: 20\n",
      "Q-values for current state: [-0.00468559 -0.00271    -0.0020805  -0.00529062]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 21\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.0020805\n",
      "New Q-value: -0.003199767317275\n",
      "\n",
      "Step 14\n",
      "Current state: 21\n",
      "Q-values for current state: [-0.0044552  -0.0040951  -0.00474761 -0.00344545]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 20\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.004455197500000001\n",
      "New Q-value: -0.005267127750000001\n",
      "\n",
      "Step 15\n",
      "Current state: 20\n",
      "Q-values for current state: [-0.00468559 -0.00271    -0.00319977 -0.00529062]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 21\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.003199767317275\n",
      "New Q-value: -0.004207107902822501\n",
      "\n",
      "Step 16\n",
      "Current state: 21\n",
      "Q-values for current state: [-0.00526713 -0.0040951  -0.00474761 -0.00344545]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 29\n",
      "Reward: -0.01\n",
      "Terminated: True\n",
      "Old Q-value: -0.0040951\n",
      "New Q-value: -0.00468559\n",
      "\n",
      "Episode 70 summary:\n",
      "Total reward: -0.17\n",
      "Successful episodes so far: []\n",
      "Average reward over last 10 episodes: -0.361\n",
      "\n",
      "Episode 71\n",
      "Starting state: 0\n",
      "\n",
      "Step 0\n",
      "Current state: 0\n",
      "Q-values for current state: [-0.03758296 -0.0302321  -0.03126735 -0.03811457]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 8\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.030232099277236635\n",
      "New Q-value: -0.03034903703109455\n",
      "\n",
      "Step 1\n",
      "Current state: 8\n",
      "Q-values for current state: [-0.02948139 -0.02252787 -0.02391419 -0.03593101]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 9\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.023914189134174004\n",
      "New Q-value: -0.024155332047931015\n",
      "\n",
      "Step 2\n",
      "Current state: 9\n",
      "Q-values for current state: [-0.0267982  -0.01718486 -0.02081187 -0.02837547]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 1\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.028375467859018566\n",
      "New Q-value: -0.02878518937257437\n",
      "\n",
      "Step 3\n",
      "Current state: 1\n",
      "Q-values for current state: [-0.03699755 -0.02365546 -0.02548964 -0.030623  ]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 0\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.036997546228940366\n",
      "New Q-value: -0.037180950124000314\n",
      "\n",
      "Step 4\n",
      "Current state: 0\n",
      "Q-values for current state: [-0.03758296 -0.03034904 -0.03126735 -0.03811457]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 0\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.03811457300909634\n",
      "New Q-value: -0.03818627422614069\n",
      "\n",
      "Step 5\n",
      "Current state: 0\n",
      "Q-values for current state: [-0.03758296 -0.03034904 -0.03126735 -0.03818627]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 0\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.03758296173239137\n",
      "New Q-value: -0.03770782407710621\n",
      "\n",
      "Step 6\n",
      "Current state: 0\n",
      "Q-values for current state: [-0.03770782 -0.03034904 -0.03126735 -0.03818627]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 8\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.03034903703109455\n",
      "New Q-value: -0.030454281009566674\n",
      "\n",
      "Step 7\n",
      "Current state: 8\n",
      "Q-values for current state: [-0.02948139 -0.02252787 -0.02415533 -0.03593101]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 8\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.029481385451644318\n",
      "New Q-value: -0.029673394588061466\n",
      "\n",
      "Step 8\n",
      "Current state: 8\n",
      "Q-values for current state: [-0.02967339 -0.02252787 -0.02415533 -0.03593101]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 9\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.024155332047931015\n",
      "New Q-value: -0.024372360670312326\n",
      "\n",
      "Step 9\n",
      "Current state: 9\n",
      "Q-values for current state: [-0.0267982  -0.01718486 -0.02081187 -0.02878519]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 17\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.017184861338678022\n",
      "New Q-value: -0.01757683091072926\n",
      "\n",
      "Step 10\n",
      "Current state: 17\n",
      "Q-values for current state: [-0.02014357 -0.01168901 -0.01198817 -0.01859641]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 18\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.011988166676929077\n",
      "New Q-value: -0.01237130054468617\n",
      "\n",
      "Step 11\n",
      "Current state: 18\n",
      "Q-values for current state: [-0.01362909 -0.01042494 -0.0061258  -0.01268034]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 17\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.01362909200846325\n",
      "New Q-value: -0.014376638513535966\n",
      "\n",
      "Step 12\n",
      "Current state: 17\n",
      "Q-values for current state: [-0.02014357 -0.01168901 -0.0123713  -0.01859641]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 9\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.018596405891080643\n",
      "New Q-value: -0.01940656423849186\n",
      "\n",
      "Step 13\n",
      "Current state: 9\n",
      "Q-values for current state: [-0.0267982  -0.01757683 -0.02081187 -0.02878519]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 17\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.01757683091072926\n",
      "New Q-value: -0.017929603525575377\n",
      "\n",
      "Step 14\n",
      "Current state: 17\n",
      "Q-values for current state: [-0.02014357 -0.01168901 -0.0123713  -0.01940656]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 18\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.01237130054468617\n",
      "New Q-value: -0.012716121025667552\n",
      "\n",
      "Step 15\n",
      "Current state: 18\n",
      "Q-values for current state: [-0.01437664 -0.01042494 -0.0061258  -0.01268034]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 17\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.014376638513535966\n",
      "New Q-value: -0.015049430368101411\n",
      "\n",
      "Step 16\n",
      "Current state: 17\n",
      "Q-values for current state: [-0.02014357 -0.01168901 -0.01271612 -0.01940656]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 16\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.020143572611637007\n",
      "New Q-value: -0.02051809654902699\n",
      "\n",
      "Step 17\n",
      "Current state: 16\n",
      "Q-values for current state: [-0.02172377 -0.0146198  -0.01736705 -0.02906456]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 24\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.01461980209003877\n",
      "New Q-value: -0.014940194525536542\n",
      "\n",
      "Step 18\n",
      "Current state: 24\n",
      "Q-values for current state: [-0.01172285 -0.01097734 -0.0082355  -0.01588351]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 16\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.015883511065357958\n",
      "New Q-value: -0.016714478438748133\n",
      "\n",
      "Step 19\n",
      "Current state: 16\n",
      "Q-values for current state: [-0.02172377 -0.01494019 -0.01736705 -0.02906456]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 8\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.02906456298536361\n",
      "New Q-value: -0.029298254368408827\n",
      "\n",
      "Step 20\n",
      "Current state: 8\n",
      "Q-values for current state: [-0.02967339 -0.02252787 -0.02437236 -0.03593101]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 0\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.03593100887128828\n",
      "New Q-value: -0.03623106468006829\n",
      "\n",
      "Step 21\n",
      "Current state: 0\n",
      "Q-values for current state: [-0.03770782 -0.03045428 -0.03126735 -0.03818627]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 8\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.030454281009566674\n",
      "New Q-value: -0.030549000590191586\n",
      "\n",
      "Step 22\n",
      "Current state: 8\n",
      "Q-values for current state: [-0.02967339 -0.02252787 -0.02437236 -0.03623106]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 9\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.024372360670312326\n",
      "New Q-value: -0.024638436938210752\n",
      "\n",
      "Step 23\n",
      "Current state: 9\n",
      "Q-values for current state: [-0.0267982  -0.0179296  -0.02081187 -0.02878519]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 8\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.02679820226998355\n",
      "New Q-value: -0.027258529724566775\n",
      "\n",
      "Step 24\n",
      "Current state: 8\n",
      "Q-values for current state: [-0.02967339 -0.02252787 -0.02463844 -0.03623106]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 8\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.029673394588061466\n",
      "New Q-value: -0.0298462028108369\n",
      "\n",
      "Step 25\n",
      "Current state: 8\n",
      "Q-values for current state: [-0.0298462  -0.02252787 -0.02463844 -0.03623106]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 16\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.02252787033243767\n",
      "New Q-value: -0.022694401779119876\n",
      "\n",
      "Step 26\n",
      "Current state: 16\n",
      "Q-values for current state: [-0.02172377 -0.01494019 -0.01736705 -0.02929825]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 16\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.02172376633264734\n",
      "New Q-value: -0.021970708179308577\n",
      "\n",
      "Step 27\n",
      "Current state: 16\n",
      "Q-values for current state: [-0.02197071 -0.01494019 -0.01736705 -0.02929825]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 8\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.029298254368408827\n",
      "New Q-value: -0.029524397100584333\n",
      "\n",
      "Step 28\n",
      "Current state: 8\n",
      "Q-values for current state: [-0.0298462  -0.0226944  -0.02463844 -0.03623106]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 0\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.03623106468006829\n",
      "New Q-value: -0.03651011326812966\n",
      "\n",
      "Step 29\n",
      "Current state: 0\n",
      "Q-values for current state: [-0.03770782 -0.030549   -0.03126735 -0.03818627]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 0\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.03770782407710621\n",
      "New Q-value: -0.03783919672546379\n",
      "\n",
      "Step 30\n",
      "Current state: 0\n",
      "Q-values for current state: [-0.0378392  -0.030549   -0.03126735 -0.03818627]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 1\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.03126735132442409\n",
      "New Q-value: -0.03138788449143934\n",
      "\n",
      "Step 31\n",
      "Current state: 1\n",
      "Q-values for current state: [-0.03718095 -0.02365546 -0.02548964 -0.030623  ]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 2\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.025489642585180248\n",
      "New Q-value: -0.025805434145428234\n",
      "\n",
      "Step 32\n",
      "Current state: 2\n",
      "Q-values for current state: [-0.02807187 -0.01962901 -0.02172481 -0.02431821]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 1\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.028071873234613746\n",
      "New Q-value: -0.02851195421061003\n",
      "\n",
      "Step 33\n",
      "Current state: 1\n",
      "Q-values for current state: [-0.03718095 -0.02365546 -0.02580543 -0.030623  ]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 0\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.037180950124000314\n",
      "New Q-value: -0.03736501016766848\n",
      "\n",
      "Step 34\n",
      "Current state: 0\n",
      "Q-values for current state: [-0.0378392  -0.030549   -0.03138788 -0.03818627]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 0\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.03783919672546379\n",
      "New Q-value: -0.03795743210898561\n",
      "\n",
      "Step 35\n",
      "Current state: 0\n",
      "Q-values for current state: [-0.03795743 -0.030549   -0.03138788 -0.03818627]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 0\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.03795743210898561\n",
      "New Q-value: -0.03806384395415525\n",
      "\n",
      "Step 36\n",
      "Current state: 0\n",
      "Q-values for current state: [-0.03806384 -0.030549   -0.03138788 -0.03818627]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 0\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.03818627422614069\n",
      "New Q-value: -0.03826980185959482\n",
      "\n",
      "Step 37\n",
      "Current state: 0\n",
      "Q-values for current state: [-0.03806384 -0.030549   -0.03138788 -0.0382698 ]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 0\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.03806384395415525\n",
      "New Q-value: -0.03815961461480792\n",
      "\n",
      "Step 38\n",
      "Current state: 0\n",
      "Q-values for current state: [-0.03815961 -0.030549   -0.03138788 -0.0382698 ]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 0\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.03826980185959482\n",
      "New Q-value: -0.03834497672970354\n",
      "\n",
      "Step 39\n",
      "Current state: 0\n",
      "Q-values for current state: [-0.03815961 -0.030549   -0.03138788 -0.03834498]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 8\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.030549000590191586\n",
      "New Q-value: -0.030650068700188816\n",
      "\n",
      "Step 40\n",
      "Current state: 8\n",
      "Q-values for current state: [-0.0298462  -0.0226944  -0.02463844 -0.03651011]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 16\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.022694401779119876\n",
      "New Q-value: -0.022844280081133858\n",
      "\n",
      "Step 41\n",
      "Current state: 16\n",
      "Q-values for current state: [-0.02197071 -0.01494019 -0.01736705 -0.0295244 ]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 24\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.014940194525536542\n",
      "New Q-value: -0.015228547717484537\n",
      "\n",
      "Step 42\n",
      "Current state: 24\n",
      "Q-values for current state: [-0.01172285 -0.01097734 -0.0082355  -0.01671448]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 25\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.00823550152107\n",
      "New Q-value: -0.008969874347959875\n",
      "\n",
      "Step 43\n",
      "Current state: 25\n",
      "Q-values for current state: [-0.01036228 -0.00587287 -0.01032606 -0.00866787]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 17\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.008667871035549595\n",
      "New Q-value: -0.009911539637913677\n",
      "\n",
      "Step 44\n",
      "Current state: 17\n",
      "Q-values for current state: [-0.0205181  -0.01168901 -0.01271612 -0.01940656]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 9\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.01940656423849186\n",
      "New Q-value: -0.020169220149572335\n",
      "\n",
      "Step 45\n",
      "Current state: 9\n",
      "Q-values for current state: [-0.02725853 -0.0179296  -0.02081187 -0.02878519]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 17\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.017929603525575377\n",
      "New Q-value: -0.01824709887893688\n",
      "\n",
      "Step 46\n",
      "Current state: 17\n",
      "Q-values for current state: [-0.0205181  -0.01168901 -0.01271612 -0.02016922]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 9\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.020169220149572335\n",
      "New Q-value: -0.020885772528114105\n",
      "\n",
      "Step 47\n",
      "Current state: 9\n",
      "Q-values for current state: [-0.02725853 -0.0182471  -0.02081187 -0.02878519]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 10\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.020811874071865983\n",
      "New Q-value: -0.020974372632667337\n",
      "\n",
      "Step 48\n",
      "Current state: 10\n",
      "Q-values for current state: [-0.02211183 -0.01309143 -0.01404319 -0.02280379]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 18\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.01309143124197845\n",
      "New Q-value: -0.013364238653230606\n",
      "\n",
      "Step 49\n",
      "Current state: 18\n",
      "Q-values for current state: [-0.01504943 -0.01042494 -0.0061258  -0.01268034]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 10\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.012680339245065779\n",
      "New Q-value: -0.01368190799261611\n",
      "\n",
      "Step 50\n",
      "Current state: 10\n",
      "Q-values for current state: [-0.02211183 -0.01336424 -0.01404319 -0.02280379]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 2\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.022803792731859875\n",
      "New Q-value: -0.023388169277439898\n",
      "\n",
      "Step 51\n",
      "Current state: 2\n",
      "Q-values for current state: [-0.02851195 -0.01962901 -0.02172481 -0.02431821]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 3\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.021724810638956125\n",
      "New Q-value: -0.021863760787307986\n",
      "\n",
      "Step 52\n",
      "Current state: 3\n",
      "Q-values for current state: [-0.02202678 -0.01380454 -0.01534104 -0.01923579]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 2\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.02202677877735575\n",
      "New Q-value: -0.022688856718386184\n",
      "\n",
      "Step 53\n",
      "Current state: 2\n",
      "Q-values for current state: [-0.02851195 -0.01962901 -0.02186376 -0.02431821]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 10\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.019629008618589572\n",
      "New Q-value: -0.01993571042878752\n",
      "\n",
      "Step 54\n",
      "Current state: 10\n",
      "Q-values for current state: [-0.02211183 -0.01336424 -0.01404319 -0.02338817]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 18\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.013364238653230606\n",
      "New Q-value: -0.013609765323357545\n",
      "\n",
      "Step 55\n",
      "Current state: 18\n",
      "Q-values for current state: [-0.01504943 -0.01042494 -0.0061258  -0.01368191]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 26\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.01042493628720201\n",
      "New Q-value: -0.010878060603481808\n",
      "\n",
      "Step 56\n",
      "Current state: 26\n",
      "Q-values for current state: [-0.00645291 -0.00683992 -0.00521703 -0.00866922]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 18\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.008669222874917266\n",
      "New Q-value: -0.00938425112287554\n",
      "\n",
      "Step 57\n",
      "Current state: 18\n",
      "Q-values for current state: [-0.01504943 -0.01087806 -0.0061258  -0.01368191]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 17\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.015049430368101411\n",
      "New Q-value: -0.015654943037210312\n",
      "\n",
      "Step 58\n",
      "Current state: 17\n",
      "Q-values for current state: [-0.0205181  -0.01168901 -0.01271612 -0.02088577]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 25\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.011689007430726754\n",
      "New Q-value: -0.012078029666650954\n",
      "\n",
      "Step 59\n",
      "Current state: 25\n",
      "Q-values for current state: [-0.01036228 -0.00587287 -0.01032606 -0.00991154]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 33\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.005872873463125\n",
      "New Q-value: -0.0065128773574375\n",
      "\n",
      "Step 60\n",
      "Current state: 33\n",
      "Q-values for current state: [-0.00239254 -0.00569533 -0.002805   -0.00421295]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 34\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.002805\n",
      "New Q-value: -0.0036195000000000003\n",
      "\n",
      "Step 61\n",
      "Current state: 34\n",
      "Q-values for current state: [-0.0037136  -0.0040951  -0.001      -0.00372548]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 35\n",
      "Reward: -0.01\n",
      "Terminated: True\n",
      "Old Q-value: -0.001\n",
      "New Q-value: -0.0019000000000000002\n",
      "\n",
      "Episode 72\n",
      "Starting state: 0\n",
      "\n",
      "Step 0\n",
      "Current state: 0\n",
      "Q-values for current state: [-0.03815961 -0.03065007 -0.03138788 -0.03834498]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 1\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.03138788449143934\n",
      "New Q-value: -0.03149636434175306\n",
      "\n",
      "Step 1\n",
      "Current state: 1\n",
      "Q-values for current state: [-0.03736501 -0.02365546 -0.02580543 -0.030623  ]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 0\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.03736501016766848\n",
      "New Q-value: -0.03754026567741957\n",
      "\n",
      "Step 2\n",
      "Current state: 0\n",
      "Q-values for current state: [-0.03815961 -0.03065007 -0.03149636 -0.03834498]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 0\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.03834497672970354\n",
      "New Q-value: -0.03842223558325112\n",
      "\n",
      "Step 3\n",
      "Current state: 0\n",
      "Q-values for current state: [-0.03815961 -0.03065007 -0.03149636 -0.03842224]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 8\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.030650068700188816\n",
      "New Q-value: -0.03075526843787765\n",
      "\n",
      "Step 4\n",
      "Current state: 8\n",
      "Q-values for current state: [-0.0298462  -0.02284428 -0.02463844 -0.03651011]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 9\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.024638436938210752\n",
      "New Q-value: -0.024908067637888682\n",
      "\n",
      "Step 5\n",
      "Current state: 9\n",
      "Q-values for current state: [-0.02725853 -0.0182471  -0.02097437 -0.02878519]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 8\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.027258529724566775\n",
      "New Q-value: -0.027702883359817812\n",
      "\n",
      "Step 6\n",
      "Current state: 8\n",
      "Q-values for current state: [-0.0298462  -0.02284428 -0.02490807 -0.03651011]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 16\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.022844280081133858\n",
      "New Q-value: -0.023006564106181505\n",
      "\n",
      "Step 7\n",
      "Current state: 16\n",
      "Q-values for current state: [-0.02197071 -0.01522855 -0.01736705 -0.0295244 ]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 16\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.021970708179308577\n",
      "New Q-value: -0.02222034939453875\n",
      "\n",
      "Step 8\n",
      "Current state: 16\n",
      "Q-values for current state: [-0.02222035 -0.01522855 -0.01736705 -0.0295244 ]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 17\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.01736705075928575\n",
      "New Q-value: -0.017777758501689014\n",
      "\n",
      "Step 9\n",
      "Current state: 17\n",
      "Q-values for current state: [-0.0205181  -0.01207803 -0.01271612 -0.02088577]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 16\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.02051809654902699\n",
      "New Q-value: -0.020912998927285324\n",
      "\n",
      "Step 10\n",
      "Current state: 16\n",
      "Q-values for current state: [-0.02222035 -0.01522855 -0.01777776 -0.0295244 ]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 8\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.029524397100584333\n",
      "New Q-value: -0.02975758098061314\n",
      "\n",
      "Step 11\n",
      "Current state: 8\n",
      "Q-values for current state: [-0.0298462  -0.02300656 -0.02490807 -0.03651011]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 8\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.0298462028108369\n",
      "New Q-value: -0.030047206119840453\n",
      "\n",
      "Step 12\n",
      "Current state: 8\n",
      "Q-values for current state: [-0.03004721 -0.02300656 -0.02490807 -0.03651011]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 8\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.030047206119840453\n",
      "New Q-value: -0.03022810909794365\n",
      "\n",
      "Step 13\n",
      "Current state: 8\n",
      "Q-values for current state: [-0.03022811 -0.02300656 -0.02490807 -0.03651011]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 8\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.03022810909794365\n",
      "New Q-value: -0.03039092177823653\n",
      "\n",
      "Step 14\n",
      "Current state: 8\n",
      "Q-values for current state: [-0.03039092 -0.02300656 -0.02490807 -0.03651011]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 0\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.03651011326812966\n",
      "New Q-value: -0.03678085244291507\n",
      "\n",
      "Step 15\n",
      "Current state: 0\n",
      "Q-values for current state: [-0.03815961 -0.03075527 -0.03149636 -0.03842224]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 0\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.03842223558325112\n",
      "New Q-value: -0.03850176252652438\n",
      "\n",
      "Step 16\n",
      "Current state: 0\n",
      "Q-values for current state: [-0.03815961 -0.03075527 -0.03149636 -0.03850176]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 0\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.03815961461480792\n",
      "New Q-value: -0.038265403654925506\n",
      "\n",
      "Step 17\n",
      "Current state: 0\n",
      "Q-values for current state: [-0.0382654  -0.03075527 -0.03149636 -0.03850176]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 1\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.03149636434175306\n",
      "New Q-value: -0.031593996207035416\n",
      "\n",
      "Step 18\n",
      "Current state: 1\n",
      "Q-values for current state: [-0.03754027 -0.02365546 -0.02580543 -0.030623  ]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 0\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.03754026567741957\n",
      "New Q-value: -0.037707989611275985\n",
      "\n",
      "Step 19\n",
      "Current state: 0\n",
      "Q-values for current state: [-0.0382654  -0.03075527 -0.031594   -0.03850176]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 1\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.031593996207035416\n",
      "New Q-value: -0.031681864885789536\n",
      "\n",
      "Step 20\n",
      "Current state: 1\n",
      "Q-values for current state: [-0.03770799 -0.02365546 -0.02580543 -0.030623  ]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 2\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.025805434145428234\n",
      "New Q-value: -0.026118783221620225\n",
      "\n",
      "Step 21\n",
      "Current state: 2\n",
      "Q-values for current state: [-0.02851195 -0.01993571 -0.02186376 -0.02431821]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 1\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.02851195421061003\n",
      "New Q-value: -0.028908027089006683\n",
      "\n",
      "Step 22\n",
      "Current state: 1\n",
      "Q-values for current state: [-0.03770799 -0.02365546 -0.02611878 -0.030623  ]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 0\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.037707989611275985\n",
      "New Q-value: -0.03785894115174676\n",
      "\n",
      "Step 23\n",
      "Current state: 0\n",
      "Q-values for current state: [-0.0382654  -0.03075527 -0.03168186 -0.03850176]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 8\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.03075526843787765\n",
      "New Q-value: -0.030865365184177127\n",
      "\n",
      "Step 24\n",
      "Current state: 8\n",
      "Q-values for current state: [-0.03039092 -0.02300656 -0.02490807 -0.03678085]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 0\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.03678085244291507\n",
      "New Q-value: -0.03703497689112039\n",
      "\n",
      "Step 25\n",
      "Current state: 0\n",
      "Q-values for current state: [-0.0382654  -0.03086537 -0.03168186 -0.03850176]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 8\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.030865365184177127\n",
      "New Q-value: -0.030964452255846656\n",
      "\n",
      "Step 26\n",
      "Current state: 8\n",
      "Q-values for current state: [-0.03039092 -0.02300656 -0.02490807 -0.03703498]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 0\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.03703497689112039\n",
      "New Q-value: -0.037273102166313785\n",
      "\n",
      "Step 27\n",
      "Current state: 0\n",
      "Q-values for current state: [-0.0382654  -0.03096445 -0.03168186 -0.03850176]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 8\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.030964452255846656\n",
      "New Q-value: -0.031053630620349233\n",
      "\n",
      "Step 28\n",
      "Current state: 8\n",
      "Q-values for current state: [-0.03039092 -0.02300656 -0.02490807 -0.0372731 ]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 0\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.037273102166313785\n",
      "New Q-value: -0.03749588685861558\n",
      "\n",
      "Step 29\n",
      "Current state: 0\n",
      "Q-values for current state: [-0.0382654  -0.03105363 -0.03168186 -0.03850176]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 8\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.031053630620349233\n",
      "New Q-value: -0.031133891148401552\n",
      "\n",
      "Step 30\n",
      "Current state: 8\n",
      "Q-values for current state: [-0.03039092 -0.02300656 -0.02490807 -0.03749589]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 0\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.03749588685861558\n",
      "New Q-value: -0.03770401783185217\n",
      "\n",
      "Step 31\n",
      "Current state: 0\n",
      "Q-values for current state: [-0.0382654  -0.03113389 -0.03168186 -0.03850176]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 0\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.038265403654925506\n",
      "New Q-value: -0.0383965829485311\n",
      "\n",
      "Step 32\n",
      "Current state: 0\n",
      "Q-values for current state: [-0.03839658 -0.03113389 -0.03168186 -0.03850176]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 0\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.0383965829485311\n",
      "New Q-value: -0.03851464431277614\n",
      "\n",
      "Step 33\n",
      "Current state: 0\n",
      "Q-values for current state: [-0.03851464 -0.03113389 -0.03168186 -0.03850176]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 0\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.03850176252652438\n",
      "New Q-value: -0.038609305932970094\n",
      "\n",
      "Step 34\n",
      "Current state: 0\n",
      "Q-values for current state: [-0.03851464 -0.03113389 -0.03168186 -0.03860931]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 1\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.031681864885789536\n",
      "New Q-value: -0.03176094669666824\n",
      "\n",
      "Step 35\n",
      "Current state: 1\n",
      "Q-values for current state: [-0.03785894 -0.02365546 -0.02611878 -0.030623  ]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 9\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.023655455783764833\n",
      "New Q-value: -0.024023384598887355\n",
      "\n",
      "Step 36\n",
      "Current state: 9\n",
      "Q-values for current state: [-0.02770288 -0.0182471  -0.02097437 -0.02878519]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 17\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.01824709887893688\n",
      "New Q-value: -0.018569801809375033\n",
      "\n",
      "Step 37\n",
      "Current state: 17\n",
      "Q-values for current state: [-0.020913   -0.01207803 -0.01271612 -0.02088577]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 16\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.020912998927285324\n",
      "New Q-value: -0.021268411067717822\n",
      "\n",
      "Step 38\n",
      "Current state: 16\n",
      "Q-values for current state: [-0.02222035 -0.01522855 -0.01777776 -0.02975758]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 16\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.02222034939453875\n",
      "New Q-value: -0.022445026488245905\n",
      "\n",
      "Step 39\n",
      "Current state: 16\n",
      "Q-values for current state: [-0.02244503 -0.01522855 -0.01777776 -0.02975758]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 24\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.015228547717484537\n",
      "New Q-value: -0.015557831008792272\n",
      "\n",
      "Step 40\n",
      "Current state: 24\n",
      "Q-values for current state: [-0.01172285 -0.01097734 -0.00896987 -0.01671448]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 25\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.008969874347959875\n",
      "New Q-value: -0.00969161026212045\n",
      "\n",
      "Step 41\n",
      "Current state: 25\n",
      "Q-values for current state: [-0.01036228 -0.00651288 -0.01032606 -0.00991154]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 17\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.009911539637913677\n",
      "New Q-value: -0.01106779849245415\n",
      "\n",
      "Step 42\n",
      "Current state: 17\n",
      "Q-values for current state: [-0.02126841 -0.01207803 -0.01271612 -0.02088577]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 16\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.021268411067717822\n",
      "New Q-value: -0.021619563906781304\n",
      "\n",
      "Step 43\n",
      "Current state: 16\n",
      "Q-values for current state: [-0.02244503 -0.01555783 -0.01777776 -0.02975758]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 8\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.02975758098061314\n",
      "New Q-value: -0.02996744647263907\n",
      "\n",
      "Step 44\n",
      "Current state: 8\n",
      "Q-values for current state: [-0.03039092 -0.02300656 -0.02490807 -0.03770402]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 8\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.03039092177823653\n",
      "New Q-value: -0.03053745319050012\n",
      "\n",
      "Step 45\n",
      "Current state: 8\n",
      "Q-values for current state: [-0.03053745 -0.02300656 -0.02490807 -0.03770402]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 8\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.03053745319050012\n",
      "New Q-value: -0.03066933146153735\n",
      "\n",
      "Step 46\n",
      "Current state: 8\n",
      "Q-values for current state: [-0.03066933 -0.02300656 -0.02490807 -0.03770402]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 0\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.03770401783185217\n",
      "New Q-value: -0.0378913357077651\n",
      "\n",
      "Step 47\n",
      "Current state: 0\n",
      "Q-values for current state: [-0.03851464 -0.03113389 -0.03176095 -0.03860931]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 0\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.038609305932970094\n",
      "New Q-value: -0.03870609499877123\n",
      "\n",
      "Step 48\n",
      "Current state: 0\n",
      "Q-values for current state: [-0.03851464 -0.03113389 -0.03176095 -0.03870609]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 0\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.03870609499877123\n",
      "New Q-value: -0.038793205157992255\n",
      "\n",
      "Step 49\n",
      "Current state: 0\n",
      "Q-values for current state: [-0.03851464 -0.03113389 -0.03176095 -0.03879321]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 0\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.038793205157992255\n",
      "New Q-value: -0.03887160430129118\n",
      "\n",
      "Step 50\n",
      "Current state: 0\n",
      "Q-values for current state: [-0.03851464 -0.03113389 -0.03176095 -0.0388716 ]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 0\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.03887160430129118\n",
      "New Q-value: -0.038942163530260206\n",
      "\n",
      "Step 51\n",
      "Current state: 0\n",
      "Q-values for current state: [-0.03851464 -0.03113389 -0.03176095 -0.03894216]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 8\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.031133891148401552\n",
      "New Q-value: -0.03120612562364864\n",
      "\n",
      "Step 52\n",
      "Current state: 8\n",
      "Q-values for current state: [-0.03066933 -0.02300656 -0.02490807 -0.03789134]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 16\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.023006564106181505\n",
      "New Q-value: -0.02318390164139862\n",
      "\n",
      "Step 53\n",
      "Current state: 16\n",
      "Q-values for current state: [-0.02244503 -0.01555783 -0.01777776 -0.02996745]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 16\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.022445026488245905\n",
      "New Q-value: -0.02267851778525658\n",
      "\n",
      "Step 54\n",
      "Current state: 16\n",
      "Q-values for current state: [-0.02267852 -0.01555783 -0.01777776 -0.02996745]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 24\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.015557831008792272\n",
      "New Q-value: -0.015922750882814488\n",
      "\n",
      "Step 55\n",
      "Current state: 24\n",
      "Q-values for current state: [-0.01172285 -0.01097734 -0.00969161 -0.01671448]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 16\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.016714478438748133\n",
      "New Q-value: -0.017555691928740697\n",
      "\n",
      "Step 56\n",
      "Current state: 16\n",
      "Q-values for current state: [-0.02267852 -0.01592275 -0.01777776 -0.02996745]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 24\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.015922750882814488\n",
      "New Q-value: -0.016251178769434483\n",
      "\n",
      "Step 57\n",
      "Current state: 24\n",
      "Q-values for current state: [-0.01172285 -0.01097734 -0.00969161 -0.01755569]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 32\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.010977343182921215\n",
      "New Q-value: -0.01136253696998847\n",
      "\n",
      "Step 58\n",
      "Current state: 32\n",
      "Q-values for current state: [-0.00943307 -0.00792011 -0.00508345 -0.00745262]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 33\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.0050834537406250005\n",
      "New Q-value: -0.0058023996071875006\n",
      "\n",
      "Step 59\n",
      "Current state: 33\n",
      "Q-values for current state: [-0.00239254 -0.00569533 -0.0036195  -0.00421295]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 34\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.0036195000000000003\n",
      "New Q-value: -0.004438050000000001\n",
      "\n",
      "Step 60\n",
      "Current state: 34\n",
      "Q-values for current state: [-0.0037136  -0.0040951  -0.0019     -0.00372548]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 35\n",
      "Reward: -0.01\n",
      "Terminated: True\n",
      "Old Q-value: -0.0019000000000000002\n",
      "New Q-value: -0.00271\n",
      "\n",
      "Episode 73\n",
      "Starting state: 0\n",
      "\n",
      "Step 0\n",
      "Current state: 0\n",
      "Q-values for current state: [-0.03851464 -0.03120613 -0.03176095 -0.03894216]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 8\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.03120612562364864\n",
      "New Q-value: -0.031287983717216646\n",
      "\n",
      "Step 1\n",
      "Current state: 8\n",
      "Q-values for current state: [-0.03066933 -0.0231839  -0.02490807 -0.03789134]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 0\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.0378913357077651\n",
      "New Q-value: -0.03807456059012417\n",
      "\n",
      "Step 2\n",
      "Current state: 0\n",
      "Q-values for current state: [-0.03851464 -0.03128798 -0.03176095 -0.03894216]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 0\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.038942163530260206\n",
      "New Q-value: -0.03902030563036977\n",
      "\n",
      "Step 3\n",
      "Current state: 0\n",
      "Q-values for current state: [-0.03851464 -0.03128798 -0.03176095 -0.03902031]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 8\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.031287983717216646\n",
      "New Q-value: -0.03136165600142785\n",
      "\n",
      "Step 4\n",
      "Current state: 8\n",
      "Q-values for current state: [-0.03066933 -0.0231839  -0.02490807 -0.03807456]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 16\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.02318390164139862\n",
      "New Q-value: -0.023409373460355036\n",
      "\n",
      "Step 5\n",
      "Current state: 16\n",
      "Q-values for current state: [-0.02267852 -0.01625118 -0.01777776 -0.02996745]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 16\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.02267851778525658\n",
      "New Q-value: -0.022954527989827198\n",
      "\n",
      "Step 6\n",
      "Current state: 16\n",
      "Q-values for current state: [-0.02295453 -0.01625118 -0.01777776 -0.02996745]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 16\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.022954527989827198\n",
      "New Q-value: -0.023202937173940754\n",
      "\n",
      "Step 7\n",
      "Current state: 16\n",
      "Q-values for current state: [-0.02320294 -0.01625118 -0.01777776 -0.02996745]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 24\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.016251178769434483\n",
      "New Q-value: -0.016546763867392476\n",
      "\n",
      "Step 8\n",
      "Current state: 24\n",
      "Q-values for current state: [-0.01172285 -0.01136254 -0.00969161 -0.01755569]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 25\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.00969161026212045\n",
      "New Q-value: -0.010341172584864968\n",
      "\n",
      "Step 9\n",
      "Current state: 25\n",
      "Q-values for current state: [-0.01036228 -0.00651288 -0.01032606 -0.0110678 ]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 33\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.0065128773574375\n",
      "New Q-value: -0.00708888086231875\n",
      "\n",
      "Step 10\n",
      "Current state: 33\n",
      "Q-values for current state: [-0.00239254 -0.00569533 -0.00443805 -0.00421295]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 25\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.004212954075\n",
      "New Q-value: -0.005465102349420281\n",
      "\n",
      "Step 11\n",
      "Current state: 25\n",
      "Q-values for current state: [-0.01036228 -0.00708888 -0.01032606 -0.0110678 ]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 26\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.010326059469970306\n",
      "New Q-value: -0.010789071467973275\n",
      "\n",
      "Step 12\n",
      "Current state: 26\n",
      "Q-values for current state: [-0.00645291 -0.00683992 -0.00521703 -0.00938425]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 34\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.006839920599\n",
      "New Q-value: -0.0074133785391\n",
      "\n",
      "Step 13\n",
      "Current state: 34\n",
      "Q-values for current state: [-0.0037136  -0.0040951  -0.00271    -0.00372548]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 33\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.0037135975\n",
      "New Q-value: -0.004569528990625\n",
      "\n",
      "Step 14\n",
      "Current state: 33\n",
      "Q-values for current state: [-0.00239254 -0.00569533 -0.00443805 -0.0054651 ]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 25\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.005465102349420281\n",
      "New Q-value: -0.006592035796398534\n",
      "\n",
      "Step 15\n",
      "Current state: 25\n",
      "Q-values for current state: [-0.01036228 -0.00708888 -0.01078907 -0.0110678 ]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 26\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.010789071467973275\n",
      "New Q-value: -0.011205782266175949\n",
      "\n",
      "Step 16\n",
      "Current state: 26\n",
      "Q-values for current state: [-0.00645291 -0.00741338 -0.00521703 -0.00938425]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 25\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.0064529078260125005\n",
      "New Q-value: -0.007481060725331532\n",
      "\n",
      "Step 17\n",
      "Current state: 25\n",
      "Q-values for current state: [-0.01036228 -0.00708888 -0.01120578 -0.0110678 ]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 17\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.01106779849245415\n",
      "New Q-value: -0.012108431461540576\n",
      "\n",
      "Step 18\n",
      "Current state: 17\n",
      "Q-values for current state: [-0.02161956 -0.01207803 -0.01271612 -0.02088577]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 16\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.021619563906781304\n",
      "New Q-value: -0.022029550083505458\n",
      "\n",
      "Step 19\n",
      "Current state: 16\n",
      "Q-values for current state: [-0.02320294 -0.01654676 -0.01777776 -0.02996745]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 8\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.02996744647263907\n",
      "New Q-value: -0.03019459230410889\n",
      "\n",
      "Step 20\n",
      "Current state: 8\n",
      "Q-values for current state: [-0.03066933 -0.02340937 -0.02490807 -0.03807456]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 9\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.024908067637888682\n",
      "New Q-value: -0.02518139204599044\n",
      "\n",
      "Step 21\n",
      "Current state: 9\n",
      "Q-values for current state: [-0.02770288 -0.0185698  -0.02097437 -0.02878519]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 10\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.020974372632667337\n",
      "New Q-value: -0.02116986307511957\n",
      "\n",
      "Step 22\n",
      "Current state: 10\n",
      "Q-values for current state: [-0.02211183 -0.01360977 -0.01404319 -0.02338817]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 18\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.013609765323357545\n",
      "New Q-value: -0.01383073932647179\n",
      "\n",
      "Step 23\n",
      "Current state: 18\n",
      "Q-values for current state: [-0.01565494 -0.01087806 -0.0061258  -0.01368191]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 10\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.01368190799261611\n",
      "New Q-value: -0.014627637429369319\n",
      "\n",
      "Step 24\n",
      "Current state: 10\n",
      "Q-values for current state: [-0.02211183 -0.01383074 -0.01404319 -0.02338817]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 18\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.01383073932647179\n",
      "New Q-value: -0.014029615929274611\n",
      "\n",
      "Step 25\n",
      "Current state: 18\n",
      "Q-values for current state: [-0.01565494 -0.01087806 -0.0061258  -0.01462764]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 26\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.010878060603481808\n",
      "New Q-value: -0.011285872488133628\n",
      "\n",
      "Step 26\n",
      "Current state: 26\n",
      "Q-values for current state: [-0.00748106 -0.00741338 -0.00521703 -0.00938425]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 25\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.007481060725331532\n",
      "New Q-value: -0.00840639833471866\n",
      "\n",
      "Step 27\n",
      "Current state: 25\n",
      "Q-values for current state: [-0.01036228 -0.00708888 -0.01120578 -0.01210843]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 24\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.010362276237911947\n",
      "New Q-value: -0.011308460009682924\n",
      "\n",
      "Step 28\n",
      "Current state: 24\n",
      "Q-values for current state: [-0.01172285 -0.01136254 -0.01034117 -0.01755569]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 24\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.011722852551351842\n",
      "New Q-value: -0.012532978691778829\n",
      "\n",
      "Step 29\n",
      "Current state: 24\n",
      "Q-values for current state: [-0.01253298 -0.01136254 -0.01034117 -0.01755569]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 25\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.010341172584864968\n",
      "New Q-value: -0.010980499008298752\n",
      "\n",
      "Step 30\n",
      "Current state: 25\n",
      "Q-values for current state: [-0.01130846 -0.00708888 -0.01120578 -0.01210843]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 33\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.00708888086231875\n",
      "New Q-value: -0.007607284016711875\n",
      "\n",
      "Step 31\n",
      "Current state: 33\n",
      "Q-values for current state: [-0.00239254 -0.00569533 -0.00443805 -0.00659204]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 25\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.006592035796398534\n",
      "New Q-value: -0.007655524198346309\n",
      "\n",
      "Step 32\n",
      "Current state: 25\n",
      "Q-values for current state: [-0.01130846 -0.00760728 -0.01120578 -0.01210843]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 17\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.012108431461540576\n",
      "New Q-value: -0.013045001133718358\n",
      "\n",
      "Step 33\n",
      "Current state: 17\n",
      "Q-values for current state: [-0.02202955 -0.01207803 -0.01271612 -0.02088577]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 9\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.020885772528114105\n",
      "New Q-value: -0.021561326447193323\n",
      "\n",
      "Step 34\n",
      "Current state: 9\n",
      "Q-values for current state: [-0.02770288 -0.0185698  -0.02116986 -0.02878519]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 17\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.018569801809375033\n",
      "New Q-value: -0.018860234446769368\n",
      "\n",
      "Step 35\n",
      "Current state: 17\n",
      "Q-values for current state: [-0.02202955 -0.01207803 -0.01271612 -0.02156133]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 9\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.021561326447193323\n",
      "New Q-value: -0.02219691607491708\n",
      "\n",
      "Step 36\n",
      "Current state: 9\n",
      "Q-values for current state: [-0.02770288 -0.01886023 -0.02116986 -0.02878519]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 10\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.02116986307511957\n",
      "New Q-value: -0.0213856902808887\n",
      "\n",
      "Step 37\n",
      "Current state: 10\n",
      "Q-values for current state: [-0.02211183 -0.01402962 -0.01404319 -0.02338817]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 9\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.022111825636274365\n",
      "New Q-value: -0.022692365345090018\n",
      "\n",
      "Step 38\n",
      "Current state: 9\n",
      "Q-values for current state: [-0.02770288 -0.01886023 -0.02138569 -0.02878519]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 1\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.02878518937257437\n",
      "New Q-value: -0.029188891972211233\n",
      "\n",
      "Step 39\n",
      "Current state: 1\n",
      "Q-values for current state: [-0.03785894 -0.02402338 -0.02611878 -0.030623  ]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 0\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.03785894115174676\n",
      "New Q-value: -0.03805240435670773\n",
      "\n",
      "Step 40\n",
      "Current state: 0\n",
      "Q-values for current state: [-0.03851464 -0.03136166 -0.03176095 -0.03902031]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 8\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.03136165600142785\n",
      "New Q-value: -0.03144938088001879\n",
      "\n",
      "Step 41\n",
      "Current state: 8\n",
      "Q-values for current state: [-0.03066933 -0.02340937 -0.02518139 -0.03807456]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 8\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.03066933146153735\n",
      "New Q-value: -0.030826288794117345\n",
      "\n",
      "Step 42\n",
      "Current state: 8\n",
      "Q-values for current state: [-0.03082629 -0.02340937 -0.02518139 -0.03807456]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 9\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.02518139204599044\n",
      "New Q-value: -0.025454975113834488\n",
      "\n",
      "Step 43\n",
      "Current state: 9\n",
      "Q-values for current state: [-0.02770288 -0.01886023 -0.02138569 -0.02918889]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 17\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.018860234446769368\n",
      "New Q-value: -0.01912162382042427\n",
      "\n",
      "Step 44\n",
      "Current state: 17\n",
      "Q-values for current state: [-0.02202955 -0.01207803 -0.01271612 -0.02219692]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 9\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.02219691607491708\n",
      "New Q-value: -0.02279377873036568\n",
      "\n",
      "Step 45\n",
      "Current state: 9\n",
      "Q-values for current state: [-0.02770288 -0.01912162 -0.02138569 -0.02918889]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 10\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.0213856902808887\n",
      "New Q-value: -0.02157993476608092\n",
      "\n",
      "Step 46\n",
      "Current state: 10\n",
      "Q-values for current state: [-0.02269237 -0.01402962 -0.01404319 -0.02338817]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 2\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.023388169277439898\n",
      "New Q-value: -0.023943244840430724\n",
      "\n",
      "Step 47\n",
      "Current state: 2\n",
      "Q-values for current state: [-0.02890803 -0.01993571 -0.02186376 -0.02431821]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 2\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.02431820839036737\n",
      "New Q-value: -0.02478028004206545\n",
      "\n",
      "Step 48\n",
      "Current state: 2\n",
      "Q-values for current state: [-0.02890803 -0.01993571 -0.02186376 -0.02478028]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 2\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.02478028004206545\n",
      "New Q-value: -0.02519614452859372\n",
      "\n",
      "Step 49\n",
      "Current state: 2\n",
      "Q-values for current state: [-0.02890803 -0.01993571 -0.02186376 -0.02519614]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 1\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.028908027089006683\n",
      "New Q-value: -0.029299445917000313\n",
      "\n",
      "Step 50\n",
      "Current state: 1\n",
      "Q-values for current state: [-0.0380524  -0.02402338 -0.02611878 -0.030623  ]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 1\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.03062300085082511\n",
      "New Q-value: -0.030842922302636896\n",
      "\n",
      "Step 51\n",
      "Current state: 1\n",
      "Q-values for current state: [-0.0380524  -0.02402338 -0.02611878 -0.03084292]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 9\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.024023384598887355\n",
      "New Q-value: -0.024437600401938923\n",
      "\n",
      "Step 52\n",
      "Current state: 9\n",
      "Q-values for current state: [-0.02770288 -0.01912162 -0.02157993 -0.02918889]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 8\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.027702883359817812\n",
      "New Q-value: -0.02815648550256976\n",
      "\n",
      "Step 53\n",
      "Current state: 8\n",
      "Q-values for current state: [-0.03082629 -0.02340937 -0.02545498 -0.03807456]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 9\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.025454975113834488\n",
      "New Q-value: -0.025726031865391345\n",
      "\n",
      "Step 54\n",
      "Current state: 9\n",
      "Q-values for current state: [-0.02815649 -0.01912162 -0.02157993 -0.02918889]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 17\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.01912162382042427\n",
      "New Q-value: -0.019356874256713685\n",
      "\n",
      "Step 55\n",
      "Current state: 17\n",
      "Q-values for current state: [-0.02202955 -0.01207803 -0.01271612 -0.02279378]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 9\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.02279377873036568\n",
      "New Q-value: -0.02335330391171691\n",
      "\n",
      "Step 56\n",
      "Current state: 9\n",
      "Q-values for current state: [-0.02815649 -0.01935687 -0.02157993 -0.02918889]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 8\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.02815648550256976\n",
      "New Q-value: -0.028564727431046513\n",
      "\n",
      "Step 57\n",
      "Current state: 8\n",
      "Q-values for current state: [-0.03082629 -0.02340937 -0.02572603 -0.03807456]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 8\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.030826288794117345\n",
      "New Q-value: -0.03096755039343934\n",
      "\n",
      "Step 58\n",
      "Current state: 8\n",
      "Q-values for current state: [-0.03096755 -0.02340937 -0.02572603 -0.03807456]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 8\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.03096755039343934\n",
      "New Q-value: -0.031094685832829136\n",
      "\n",
      "Step 59\n",
      "Current state: 8\n",
      "Q-values for current state: [-0.03109469 -0.02340937 -0.02572603 -0.03807456]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 16\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.023409373460355036\n",
      "New Q-value: -0.023640378681721818\n",
      "\n",
      "Step 60\n",
      "Current state: 16\n",
      "Q-values for current state: [-0.02320294 -0.01654676 -0.01777776 -0.03019459]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 16\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.023202937173940754\n",
      "New Q-value: -0.023454586023948964\n",
      "\n",
      "Step 61\n",
      "Current state: 16\n",
      "Q-values for current state: [-0.02345459 -0.01654676 -0.01777776 -0.03019459]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 16\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.023454586023948964\n",
      "New Q-value: -0.023681069988956353\n",
      "\n",
      "Step 62\n",
      "Current state: 16\n",
      "Q-values for current state: [-0.02368107 -0.01654676 -0.01777776 -0.03019459]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 16\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.023681069988956353\n",
      "New Q-value: -0.023884905557463004\n",
      "\n",
      "Step 63\n",
      "Current state: 16\n",
      "Q-values for current state: [-0.02388491 -0.01654676 -0.01777776 -0.03019459]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 16\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.023884905557463004\n",
      "New Q-value: -0.02406835756911899\n",
      "\n",
      "Step 64\n",
      "Current state: 16\n",
      "Q-values for current state: [-0.02406836 -0.01654676 -0.01777776 -0.03019459]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 16\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.02406835756911899\n",
      "New Q-value: -0.024233464379609375\n",
      "\n",
      "Step 65\n",
      "Current state: 16\n",
      "Q-values for current state: [-0.02423346 -0.01654676 -0.01777776 -0.03019459]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 16\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.024233464379609375\n",
      "New Q-value: -0.024382060509050722\n",
      "\n",
      "Step 66\n",
      "Current state: 16\n",
      "Q-values for current state: [-0.02438206 -0.01654676 -0.01777776 -0.03019459]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 8\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.03019459230410889\n",
      "New Q-value: -0.030420969048461574\n",
      "\n",
      "Step 67\n",
      "Current state: 8\n",
      "Q-values for current state: [-0.03109469 -0.02364038 -0.02572603 -0.03807456]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 16\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.023640378681721818\n",
      "New Q-value: -0.02384828338095192\n",
      "\n",
      "Step 68\n",
      "Current state: 16\n",
      "Q-values for current state: [-0.02438206 -0.01654676 -0.01777776 -0.03042097]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 24\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.016546763867392476\n",
      "New Q-value: -0.01693523488644161\n",
      "\n",
      "Step 69\n",
      "Current state: 24\n",
      "Q-values for current state: [-0.01253298 -0.01136254 -0.0109805  -0.01755569]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 32\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.01136253696998847\n",
      "New Q-value: -0.011777511235672434\n",
      "\n",
      "Step 70\n",
      "Current state: 32\n",
      "Q-values for current state: [-0.00943307 -0.00792011 -0.0058024  -0.00745262]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 33\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.0058023996071875006\n",
      "New Q-value: -0.006449450887093751\n",
      "\n",
      "Step 71\n",
      "Current state: 33\n",
      "Q-values for current state: [-0.00239254 -0.00569533 -0.00443805 -0.00765552]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 34\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.004438050000000001\n",
      "New Q-value: -0.005251695000000001\n",
      "\n",
      "Step 72\n",
      "Current state: 34\n",
      "Q-values for current state: [-0.00456953 -0.0040951  -0.00271    -0.00372548]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 42\n",
      "Reward: -0.01\n",
      "Terminated: True\n",
      "Old Q-value: -0.0040951\n",
      "New Q-value: -0.00468559\n",
      "\n",
      "Episode 74\n",
      "Starting state: 0\n",
      "\n",
      "Step 0\n",
      "Current state: 0\n",
      "Q-values for current state: [-0.03851464 -0.03144938 -0.03176095 -0.03902031]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 1\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.03176094669666824\n",
      "New Q-value: -0.031906424065185617\n",
      "\n",
      "Step 1\n",
      "Current state: 1\n",
      "Q-values for current state: [-0.0380524  -0.0244376  -0.02611878 -0.03084292]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 9\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.024437600401938923\n",
      "New Q-value: -0.02483274341613283\n",
      "\n",
      "Step 2\n",
      "Current state: 9\n",
      "Q-values for current state: [-0.02856473 -0.01935687 -0.02157993 -0.02918889]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 1\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.029188891972211233\n",
      "New Q-value: -0.02962911339952273\n",
      "\n",
      "Step 3\n",
      "Current state: 1\n",
      "Q-values for current state: [-0.0380524  -0.02483274 -0.02611878 -0.03084292]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 0\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.03805240435670773\n",
      "New Q-value: -0.03823485510463874\n",
      "\n",
      "Step 4\n",
      "Current state: 0\n",
      "Q-values for current state: [-0.03851464 -0.03144938 -0.03190642 -0.03902031]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 0\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.03851464431277614\n",
      "New Q-value: -0.03865087106510031\n",
      "\n",
      "Step 5\n",
      "Current state: 0\n",
      "Q-values for current state: [-0.03865087 -0.03144938 -0.03190642 -0.03902031]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 8\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.03144938088001879\n",
      "New Q-value: -0.03157002971320735\n",
      "\n",
      "Step 6\n",
      "Current state: 8\n",
      "Q-values for current state: [-0.03109469 -0.02384828 -0.02572603 -0.03807456]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 9\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.025726031865391345\n",
      "New Q-value: -0.02599233173324001\n",
      "\n",
      "Step 7\n",
      "Current state: 9\n",
      "Q-values for current state: [-0.02856473 -0.01935687 -0.02157993 -0.02962911]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 8\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.028564727431046513\n",
      "New Q-value: -0.028973841609132293\n",
      "\n",
      "Step 8\n",
      "Current state: 8\n",
      "Q-values for current state: [-0.03109469 -0.02384828 -0.02599233 -0.03807456]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 16\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.02384828338095192\n",
      "New Q-value: -0.02407230235706868\n",
      "\n",
      "Step 9\n",
      "Current state: 16\n",
      "Q-values for current state: [-0.02438206 -0.01693523 -0.01777776 -0.03042097]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 16\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.024382060509050722\n",
      "New Q-value: -0.024552701772357605\n",
      "\n",
      "Step 10\n",
      "Current state: 16\n",
      "Q-values for current state: [-0.0245527  -0.01693523 -0.01777776 -0.03042097]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 16\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.024552701772357605\n",
      "New Q-value: -0.024706278909333797\n",
      "\n",
      "Step 11\n",
      "Current state: 16\n",
      "Q-values for current state: [-0.02470628 -0.01693523 -0.01777776 -0.03042097]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 16\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.024706278909333797\n",
      "New Q-value: -0.02484449833261237\n",
      "\n",
      "Step 12\n",
      "Current state: 16\n",
      "Q-values for current state: [-0.0248445  -0.01693523 -0.01777776 -0.03042097]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 24\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.01693523488644161\n",
      "New Q-value: -0.01728485880358583\n",
      "\n",
      "Step 13\n",
      "Current state: 24\n",
      "Q-values for current state: [-0.01253298 -0.01177751 -0.0109805  -0.01755569]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 24\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.012532978691778829\n",
      "New Q-value: -0.013322828228389327\n",
      "\n",
      "Step 14\n",
      "Current state: 24\n",
      "Q-values for current state: [-0.01332283 -0.01177751 -0.0109805  -0.01755569]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 16\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.017555691928740697\n",
      "New Q-value: -0.018442184322207282\n",
      "\n",
      "Step 15\n",
      "Current state: 16\n",
      "Q-values for current state: [-0.0248445  -0.01728486 -0.01777776 -0.03042097]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 17\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.017777758501689014\n",
      "New Q-value: -0.018147395469851953\n",
      "\n",
      "Step 16\n",
      "Current state: 17\n",
      "Q-values for current state: [-0.02202955 -0.01207803 -0.01271612 -0.0233533 ]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 9\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.02335330391171691\n",
      "New Q-value: -0.023856876574933017\n",
      "\n",
      "Step 17\n",
      "Current state: 9\n",
      "Q-values for current state: [-0.02897384 -0.01935687 -0.02157993 -0.02962911]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 1\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.02962911339952273\n",
      "New Q-value: -0.030025312684103075\n",
      "\n",
      "Step 18\n",
      "Current state: 1\n",
      "Q-values for current state: [-0.03823486 -0.02483274 -0.02611878 -0.03084292]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 0\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.03823485510463874\n",
      "New Q-value: -0.03841052241692956\n",
      "\n",
      "Step 19\n",
      "Current state: 0\n",
      "Q-values for current state: [-0.03865087 -0.03157003 -0.03190642 -0.03902031]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 1\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.031906424065185617\n",
      "New Q-value: -0.03207489228319967\n",
      "\n",
      "Step 20\n",
      "Current state: 1\n",
      "Q-values for current state: [-0.03841052 -0.02483274 -0.02611878 -0.03084292]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 0\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.03841052241692956\n",
      "New Q-value: -0.038568622997991305\n",
      "\n",
      "Step 21\n",
      "Current state: 0\n",
      "Q-values for current state: [-0.03865087 -0.03157003 -0.03207489 -0.03902031]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 1\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.03207489228319967\n",
      "New Q-value: -0.03222651367941232\n",
      "\n",
      "Step 22\n",
      "Current state: 1\n",
      "Q-values for current state: [-0.03856862 -0.02483274 -0.02611878 -0.03084292]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 0\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.038568622997991305\n",
      "New Q-value: -0.03871091352094687\n",
      "\n",
      "Step 23\n",
      "Current state: 0\n",
      "Q-values for current state: [-0.03865087 -0.03157003 -0.03222651 -0.03902031]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 8\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.03157002971320735\n",
      "New Q-value: -0.031699895465808134\n",
      "\n",
      "Step 24\n",
      "Current state: 8\n",
      "Q-values for current state: [-0.03109469 -0.0240723  -0.02599233 -0.03807456]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 9\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.02599233173324001\n",
      "New Q-value: -0.02623200161430381\n",
      "\n",
      "Step 25\n",
      "Current state: 9\n",
      "Q-values for current state: [-0.02897384 -0.01935687 -0.02157993 -0.03002531]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 17\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.019356874256713685\n",
      "New Q-value: -0.019568599649374158\n",
      "\n",
      "Step 26\n",
      "Current state: 17\n",
      "Q-values for current state: [-0.02202955 -0.01207803 -0.01271612 -0.02385688]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 25\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.012078029666650954\n",
      "New Q-value: -0.012592918681573487\n",
      "\n",
      "Step 27\n",
      "Current state: 25\n",
      "Q-values for current state: [-0.01130846 -0.00760728 -0.01120578 -0.013045  ]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 26\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.011205782266175949\n",
      "New Q-value: -0.011580821984558353\n",
      "\n",
      "Step 28\n",
      "Current state: 26\n",
      "Q-values for current state: [-0.0084064  -0.00741338 -0.00521703 -0.00938425]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 34\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.0074133785391\n",
      "New Q-value: -0.00792949068519\n",
      "\n",
      "Step 29\n",
      "Current state: 34\n",
      "Q-values for current state: [-0.00456953 -0.00468559 -0.00271    -0.00372548]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 35\n",
      "Reward: -0.01\n",
      "Terminated: True\n",
      "Old Q-value: -0.00271\n",
      "New Q-value: -0.003439\n",
      "\n",
      "Episode 75\n",
      "Starting state: 0\n",
      "\n",
      "Step 0\n",
      "Current state: 0\n",
      "Q-values for current state: [-0.03865087 -0.0316999  -0.03222651 -0.03902031]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 0\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.03865087106510031\n",
      "New Q-value: -0.038797274027842055\n",
      "\n",
      "Step 1\n",
      "Current state: 0\n",
      "Q-values for current state: [-0.03879727 -0.0316999  -0.03222651 -0.03902031]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 0\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.038797274027842055\n",
      "New Q-value: -0.038929036694309624\n",
      "\n",
      "Step 2\n",
      "Current state: 0\n",
      "Q-values for current state: [-0.03892904 -0.0316999  -0.03222651 -0.03902031]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 0\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.038929036694309624\n",
      "New Q-value: -0.039047623094130436\n",
      "\n",
      "Step 3\n",
      "Current state: 0\n",
      "Q-values for current state: [-0.03904762 -0.0316999  -0.03222651 -0.03902031]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 0\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.03902030563036977\n",
      "New Q-value: -0.03912976513658457\n",
      "\n",
      "Step 4\n",
      "Current state: 0\n",
      "Q-values for current state: [-0.03904762 -0.0316999  -0.03222651 -0.03912977]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 8\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.031699895465808134\n",
      "New Q-value: -0.031816774643148846\n",
      "\n",
      "Step 5\n",
      "Current state: 8\n",
      "Q-values for current state: [-0.03109469 -0.0240723  -0.026232   -0.03807456]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 0\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.03807456059012417\n",
      "New Q-value: -0.0382896981222109\n",
      "\n",
      "Step 6\n",
      "Current state: 0\n",
      "Q-values for current state: [-0.03904762 -0.03181677 -0.03222651 -0.03912977]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 8\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.031816774643148846\n",
      "New Q-value: -0.031921965902755485\n",
      "\n",
      "Step 7\n",
      "Current state: 8\n",
      "Q-values for current state: [-0.03109469 -0.0240723  -0.026232   -0.0382897 ]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 8\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.031094685832829136\n",
      "New Q-value: -0.03127208597346775\n",
      "\n",
      "Step 8\n",
      "Current state: 8\n",
      "Q-values for current state: [-0.03127209 -0.0240723  -0.026232   -0.0382897 ]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 8\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.03127208597346775\n",
      "New Q-value: -0.0314317461000425\n",
      "\n",
      "Step 9\n",
      "Current state: 8\n",
      "Q-values for current state: [-0.03143175 -0.0240723  -0.026232   -0.0382897 ]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 9\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.02623200161430381\n",
      "New Q-value: -0.026467818419563974\n",
      "\n",
      "Step 10\n",
      "Current state: 9\n",
      "Q-values for current state: [-0.02897384 -0.0195686  -0.02157993 -0.03002531]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 17\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.019568599649374158\n",
      "New Q-value: -0.019808066959186223\n",
      "\n",
      "Step 11\n",
      "Current state: 17\n",
      "Q-values for current state: [-0.02202955 -0.01259292 -0.01271612 -0.02385688]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 18\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.012716121025667552\n",
      "New Q-value: -0.013026459458550797\n",
      "\n",
      "Step 12\n",
      "Current state: 18\n",
      "Q-values for current state: [-0.01565494 -0.01128587 -0.0061258  -0.01462764]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 17\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.015654943037210312\n",
      "New Q-value: -0.016285776008238762\n",
      "\n",
      "Step 13\n",
      "Current state: 17\n",
      "Q-values for current state: [-0.02202955 -0.01259292 -0.01302646 -0.02385688]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 25\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.012592918681573487\n",
      "New Q-value: -0.013056318795003767\n",
      "\n",
      "Step 14\n",
      "Current state: 25\n",
      "Q-values for current state: [-0.01130846 -0.00760728 -0.01158082 -0.013045  ]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 17\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.013045001133718358\n",
      "New Q-value: -0.013978014668908849\n",
      "\n",
      "Step 15\n",
      "Current state: 17\n",
      "Q-values for current state: [-0.02202955 -0.01305632 -0.01302646 -0.02385688]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 18\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.013026459458550797\n",
      "New Q-value: -0.013305764048145718\n",
      "\n",
      "Step 16\n",
      "Current state: 18\n",
      "Q-values for current state: [-0.01628578 -0.01128587 -0.0061258  -0.01462764]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 10\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.014627637429369319\n",
      "New Q-value: -0.015497687199713474\n",
      "\n",
      "Step 17\n",
      "Current state: 10\n",
      "Q-values for current state: [-0.02269237 -0.01402962 -0.01404319 -0.02394324]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 2\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.023943244840430724\n",
      "New Q-value: -0.024442812847122467\n",
      "\n",
      "Step 18\n",
      "Current state: 2\n",
      "Q-values for current state: [-0.02929945 -0.01993571 -0.02186376 -0.02519614]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 10\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.01993571042878752\n",
      "New Q-value: -0.020274952899189857\n",
      "\n",
      "Step 19\n",
      "Current state: 10\n",
      "Q-values for current state: [-0.02269237 -0.01402962 -0.01404319 -0.02444281]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 2\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.024442812847122467\n",
      "New Q-value: -0.024924652087833257\n",
      "\n",
      "Step 20\n",
      "Current state: 2\n",
      "Q-values for current state: [-0.02929945 -0.02027495 -0.02186376 -0.02519614]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 2\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.02519614452859372\n",
      "New Q-value: -0.025602650601157385\n",
      "\n",
      "Step 21\n",
      "Current state: 2\n",
      "Q-values for current state: [-0.02929945 -0.02027495 -0.02186376 -0.02560265]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 1\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.029299445917000313\n",
      "New Q-value: -0.029728611949832902\n",
      "\n",
      "Step 22\n",
      "Current state: 1\n",
      "Q-values for current state: [-0.03871091 -0.02483274 -0.02611878 -0.03084292]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 0\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.03871091352094687\n",
      "New Q-value: -0.03887240892961395\n",
      "\n",
      "Step 23\n",
      "Current state: 0\n",
      "Q-values for current state: [-0.03904762 -0.03192197 -0.03222651 -0.03912977]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 0\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.03912976513658457\n",
      "New Q-value: -0.03924937538368788\n",
      "\n",
      "Step 24\n",
      "Current state: 0\n",
      "Q-values for current state: [-0.03904762 -0.03192197 -0.03222651 -0.03924938]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 0\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.03924937538368788\n",
      "New Q-value: -0.03935702460608086\n",
      "\n",
      "Step 25\n",
      "Current state: 0\n",
      "Q-values for current state: [-0.03904762 -0.03192197 -0.03222651 -0.03935702]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 1\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.03222651367941232\n",
      "New Q-value: -0.03236297293600371\n",
      "\n",
      "Step 26\n",
      "Current state: 1\n",
      "Q-values for current state: [-0.03887241 -0.02483274 -0.02611878 -0.03084292]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 2\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.026118783221620225\n",
      "New Q-value: -0.02643302542488124\n",
      "\n",
      "Step 27\n",
      "Current state: 2\n",
      "Q-values for current state: [-0.02972861 -0.02027495 -0.02186376 -0.02560265]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 2\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.025602650601157385\n",
      "New Q-value: -0.025968506066464683\n",
      "\n",
      "Step 28\n",
      "Current state: 2\n",
      "Q-values for current state: [-0.02972861 -0.02027495 -0.02186376 -0.02596851]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 3\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.021863760787307986\n",
      "New Q-value: -0.021988815920824658\n",
      "\n",
      "Step 29\n",
      "Current state: 3\n",
      "Q-values for current state: [-0.02268886 -0.01380454 -0.01534104 -0.01923579]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 3\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.019235787280386815\n",
      "New Q-value: -0.019623639764595607\n",
      "\n",
      "Step 30\n",
      "Current state: 3\n",
      "Q-values for current state: [-0.02268886 -0.01380454 -0.01534104 -0.01962364]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 11\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.013804539076289181\n",
      "New Q-value: -0.014156755640338133\n",
      "\n",
      "Step 31\n",
      "Current state: 11\n",
      "Q-values for current state: [-0.01118479 -0.00771232 -0.01162616 -0.01496245]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 3\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.014962448440772016\n",
      "New Q-value: -0.015811095382526938\n",
      "\n",
      "Step 32\n",
      "Current state: 3\n",
      "Q-values for current state: [-0.02268886 -0.01415676 -0.01534104 -0.01962364]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 11\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.014156755640338133\n",
      "New Q-value: -0.01447375054798219\n",
      "\n",
      "Step 33\n",
      "Current state: 11\n",
      "Q-values for current state: [-0.01118479 -0.00771232 -0.01162616 -0.0158111 ]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 12\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.011626164635371689\n",
      "New Q-value: -0.01217289167942402\n",
      "\n",
      "Step 34\n",
      "Current state: 12\n",
      "Q-values for current state: [-0.00937602 -0.00746677 -0.01183822 -0.00769109]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 11\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.009376022182118379\n",
      "New Q-value: -0.010171090435584411\n",
      "\n",
      "Step 35\n",
      "Current state: 11\n",
      "Q-values for current state: [-0.01118479 -0.00771232 -0.01217289 -0.0158111 ]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 10\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.01118479412985649\n",
      "New Q-value: -0.01239912823015193\n",
      "\n",
      "Step 36\n",
      "Current state: 10\n",
      "Q-values for current state: [-0.02269237 -0.01402962 -0.01404319 -0.02492465]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 11\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.014043187678289349\n",
      "New Q-value: -0.014371539382138284\n",
      "\n",
      "Step 37\n",
      "Current state: 11\n",
      "Q-values for current state: [-0.01239913 -0.00771232 -0.01217289 -0.0158111 ]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 12\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.01217289167942402\n",
      "New Q-value: -0.012664946019071118\n",
      "\n",
      "Step 38\n",
      "Current state: 12\n",
      "Q-values for current state: [-0.01017109 -0.00746677 -0.01183822 -0.00769109]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 13\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.011838224270485061\n",
      "New Q-value: -0.012150019788436555\n",
      "\n",
      "Step 39\n",
      "Current state: 13\n",
      "Q-values for current state: [-0.01008285 -0.00786011 -0.00521703 -0.00940755]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 21\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.00786010739125\n",
      "New Q-value: -0.0084014139694\n",
      "\n",
      "Step 40\n",
      "Current state: 21\n",
      "Q-values for current state: [-0.00526713 -0.00468559 -0.00474761 -0.00344545]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 20\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.005267127750000001\n",
      "New Q-value: -0.005997864975000001\n",
      "\n",
      "Step 41\n",
      "Current state: 20\n",
      "Q-values for current state: [-0.00468559 -0.00271    -0.00420711 -0.00529062]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 21\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.004207107902822501\n",
      "New Q-value: -0.005113714429815251\n",
      "\n",
      "Step 42\n",
      "Current state: 21\n",
      "Q-values for current state: [-0.00599786 -0.00468559 -0.00474761 -0.00344545]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 22\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.004747607500000001\n",
      "New Q-value: -0.005616699250000001\n",
      "\n",
      "Step 43\n",
      "Current state: 22\n",
      "Q-values for current state: [-0.0045631  -0.00468559 -0.0036195  -0.0040951 ]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 30\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.00468559\n",
      "New Q-value: -0.005312031\n",
      "\n",
      "Step 44\n",
      "Current state: 30\n",
      "Q-values for current state: [-0.001      -0.00271    -0.001      -0.00435255]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 31\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.001\n",
      "New Q-value: -0.0019950000000000002\n",
      "\n",
      "Step 45\n",
      "Current state: 31\n",
      "Q-values for current state: [-0.00271  -0.001995 -0.001    -0.001   ]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 30\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.00271\n",
      "New Q-value: -0.0035340000000000002\n",
      "\n",
      "Step 46\n",
      "Current state: 30\n",
      "Q-values for current state: [-0.001      -0.00271    -0.001995   -0.00435255]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 22\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.00435255\n",
      "New Q-value: -0.0052611475000000005\n",
      "\n",
      "Step 47\n",
      "Current state: 22\n",
      "Q-values for current state: [-0.0045631  -0.00531203 -0.0036195  -0.0040951 ]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 14\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.0040951\n",
      "New Q-value: -0.00468559\n",
      "\n",
      "Step 48\n",
      "Current state: 14\n",
      "Q-values for current state: [-0.00532997 -0.00518377 -0.0059001   0.        ]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 6\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: -0.001326705\n",
      "\n",
      "Step 49\n",
      "Current state: 6\n",
      "Q-values for current state: [-0.0108229 -0.003439  -0.0057641 -0.0079256]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 5\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.010822900601663367\n",
      "New Q-value: -0.011536695846151492\n",
      "\n",
      "Step 50\n",
      "Current state: 5\n",
      "Q-values for current state: [-0.00837985 -0.01035967 -0.0097495  -0.00956184]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 5\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.009561842752592831\n",
      "New Q-value: -0.01040174378198801\n",
      "\n",
      "Step 51\n",
      "Current state: 5\n",
      "Q-values for current state: [-0.00837985 -0.01035967 -0.0097495  -0.01040174]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 4\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.008379845312152235\n",
      "New Q-value: -0.009533044909250768\n",
      "\n",
      "Step 52\n",
      "Current state: 4\n",
      "Q-values for current state: [-0.01556526 -0.01043352 -0.01125145 -0.01337713]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 3\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.015565262838757865\n",
      "New Q-value: -0.016383742856940386\n",
      "\n",
      "Step 53\n",
      "Current state: 3\n",
      "Q-values for current state: [-0.02268886 -0.01447375 -0.01534104 -0.01962364]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 2\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.022688856718386184\n",
      "New Q-value: -0.023346091571970602\n",
      "\n",
      "Step 54\n",
      "Current state: 2\n",
      "Q-values for current state: [-0.02972861 -0.02027495 -0.02198882 -0.02596851]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 1\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.029728611949832902\n",
      "New Q-value: -0.03011486137938223\n",
      "\n",
      "Step 55\n",
      "Current state: 1\n",
      "Q-values for current state: [-0.03887241 -0.02483274 -0.02643303 -0.03084292]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 1\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.030842922302636896\n",
      "New Q-value: -0.031117740696905826\n",
      "\n",
      "Step 56\n",
      "Current state: 1\n",
      "Q-values for current state: [-0.03887241 -0.02483274 -0.02643303 -0.03111774]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 1\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.031117740696905826\n",
      "New Q-value: -0.031365077251747864\n",
      "\n",
      "Step 57\n",
      "Current state: 1\n",
      "Q-values for current state: [-0.03887241 -0.02483274 -0.02643303 -0.03136508]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 1\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.031365077251747864\n",
      "New Q-value: -0.0315876801511057\n",
      "\n",
      "Step 58\n",
      "Current state: 1\n",
      "Q-values for current state: [-0.03887241 -0.02483274 -0.02643303 -0.03158768]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 0\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.03887240892961395\n",
      "New Q-value: -0.03901775479741433\n",
      "\n",
      "Step 59\n",
      "Current state: 0\n",
      "Q-values for current state: [-0.03904762 -0.03192197 -0.03236297 -0.03935702]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 0\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.03935702460608086\n",
      "New Q-value: -0.03945390890623455\n",
      "\n",
      "Step 60\n",
      "Current state: 0\n",
      "Q-values for current state: [-0.03904762 -0.03192197 -0.03236297 -0.03945391]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 1\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.03236297293600371\n",
      "New Q-value: -0.032485786266935955\n",
      "\n",
      "Step 61\n",
      "Current state: 1\n",
      "Q-values for current state: [-0.03901775 -0.02483274 -0.02643303 -0.03158768]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 2\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.02643302542488124\n",
      "New Q-value: -0.026715843407816152\n",
      "\n",
      "Step 62\n",
      "Current state: 2\n",
      "Q-values for current state: [-0.03011486 -0.02027495 -0.02198882 -0.02596851]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 1\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.03011486137938223\n",
      "New Q-value: -0.030462485865976625\n",
      "\n",
      "Step 63\n",
      "Current state: 1\n",
      "Q-values for current state: [-0.03901775 -0.02483274 -0.02671584 -0.03158768]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 0\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.03901775479741433\n",
      "New Q-value: -0.03914856607843467\n",
      "\n",
      "Step 64\n",
      "Current state: 0\n",
      "Q-values for current state: [-0.03904762 -0.03192197 -0.03248579 -0.03945391]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 1\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.032485786266935955\n",
      "New Q-value: -0.03259631826477498\n",
      "\n",
      "Step 65\n",
      "Current state: 1\n",
      "Q-values for current state: [-0.03914857 -0.02483274 -0.02671584 -0.03158768]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 0\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.03914856607843467\n",
      "New Q-value: -0.03926629623135298\n",
      "\n",
      "Step 66\n",
      "Current state: 0\n",
      "Q-values for current state: [-0.03904762 -0.03192197 -0.03259632 -0.03945391]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 8\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.031921965902755485\n",
      "New Q-value: -0.03201663803640146\n",
      "\n",
      "Step 67\n",
      "Current state: 8\n",
      "Q-values for current state: [-0.03143175 -0.0240723  -0.02646782 -0.0382897 ]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 16\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.02407230235706868\n",
      "New Q-value: -0.024307133707702468\n",
      "\n",
      "Step 68\n",
      "Current state: 16\n",
      "Q-values for current state: [-0.0248445  -0.01728486 -0.0181474  -0.03042097]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 24\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.01728485880358583\n",
      "New Q-value: -0.017599520329015628\n",
      "\n",
      "Step 69\n",
      "Current state: 24\n",
      "Q-values for current state: [-0.01332283 -0.01177751 -0.0109805  -0.01844218]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 24\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.013322828228389327\n",
      "New Q-value: -0.014033692811338776\n",
      "\n",
      "Step 70\n",
      "Current state: 24\n",
      "Q-values for current state: [-0.01403369 -0.01177751 -0.0109805  -0.01844218]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 32\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.011777511235672434\n",
      "New Q-value: -0.012212457946379096\n",
      "\n",
      "Step 71\n",
      "Current state: 32\n",
      "Q-values for current state: [-0.00943307 -0.00792011 -0.00644945 -0.00745262]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 33\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.006449450887093751\n",
      "New Q-value: -0.007031797039009376\n",
      "\n",
      "Step 72\n",
      "Current state: 33\n",
      "Q-values for current state: [-0.00239254 -0.00569533 -0.0052517  -0.00765552]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 32\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.002392539375\n",
      "New Q-value: -0.0038213061562058907\n",
      "\n",
      "Step 73\n",
      "Current state: 32\n",
      "Q-values for current state: [-0.00943307 -0.00792011 -0.0070318  -0.00745262]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 40\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.00792010517064\n",
      "New Q-value: -0.008385544653576\n",
      "\n",
      "Step 74\n",
      "Current state: 40\n",
      "Q-values for current state: [-0.00369645 -0.0042756  -0.00271    -0.00522721]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 41\n",
      "Reward: -0.01\n",
      "Terminated: True\n",
      "Old Q-value: -0.00271\n",
      "New Q-value: -0.003439\n",
      "\n",
      "Episode 76\n",
      "Starting state: 0\n",
      "\n",
      "Step 0\n",
      "Current state: 0\n",
      "Q-values for current state: [-0.03904762 -0.03201664 -0.03259632 -0.03945391]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 1\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.03259631826477498\n",
      "New Q-value: -0.0326957970628301\n",
      "\n",
      "Step 1\n",
      "Current state: 1\n",
      "Q-values for current state: [-0.0392663  -0.02483274 -0.02671584 -0.03158768]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 1\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.0315876801511057\n",
      "New Q-value: -0.031788022760527744\n",
      "\n",
      "Step 2\n",
      "Current state: 1\n",
      "Q-values for current state: [-0.0392663  -0.02483274 -0.02671584 -0.03178802]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 1\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.031788022760527744\n",
      "New Q-value: -0.03196833110900759\n",
      "\n",
      "Step 3\n",
      "Current state: 1\n",
      "Q-values for current state: [-0.0392663  -0.02483274 -0.02671584 -0.03196833]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 2\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.026715843407816152\n",
      "New Q-value: -0.026970379592457572\n",
      "\n",
      "Step 4\n",
      "Current state: 2\n",
      "Q-values for current state: [-0.03046249 -0.02027495 -0.02198882 -0.02596851]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 2\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.025968506066464683\n",
      "New Q-value: -0.026297775985241252\n",
      "\n",
      "Step 5\n",
      "Current state: 2\n",
      "Q-values for current state: [-0.03046249 -0.02027495 -0.02198882 -0.02629778]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 3\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.021988815920824658\n",
      "New Q-value: -0.0221649406308005\n",
      "\n",
      "Step 6\n",
      "Current state: 3\n",
      "Q-values for current state: [-0.02334609 -0.01447375 -0.01534104 -0.01962364]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 2\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.023346091571970602\n",
      "New Q-value: -0.02393760294019658\n",
      "\n",
      "Step 7\n",
      "Current state: 2\n",
      "Q-values for current state: [-0.03046249 -0.02027495 -0.02216494 -0.02629778]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 3\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.0221649406308005\n",
      "New Q-value: -0.022323452869778757\n",
      "\n",
      "Step 8\n",
      "Current state: 3\n",
      "Q-values for current state: [-0.0239376  -0.01447375 -0.01534104 -0.01962364]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 3\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.019623639764595607\n",
      "New Q-value: -0.020036282090194354\n",
      "\n",
      "Step 9\n",
      "Current state: 3\n",
      "Q-values for current state: [-0.0239376  -0.01447375 -0.01534104 -0.02003628]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 2\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.02393760294019658\n",
      "New Q-value: -0.024469963171599957\n",
      "\n",
      "Step 10\n",
      "Current state: 2\n",
      "Q-values for current state: [-0.03046249 -0.02027495 -0.02232345 -0.02629778]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 1\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.030462485865976625\n",
      "New Q-value: -0.03077534790391158\n",
      "\n",
      "Step 11\n",
      "Current state: 1\n",
      "Q-values for current state: [-0.0392663  -0.02483274 -0.02697038 -0.03196833]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 9\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.02483274341613283\n",
      "New Q-value: -0.02523123543564224\n",
      "\n",
      "Step 12\n",
      "Current state: 9\n",
      "Q-values for current state: [-0.02897384 -0.01980807 -0.02157993 -0.03002531]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 1\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.030025312684103075\n",
      "New Q-value: -0.03041974878207878\n",
      "\n",
      "Step 13\n",
      "Current state: 1\n",
      "Q-values for current state: [-0.0392663  -0.02523124 -0.02697038 -0.03196833]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 2\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.026970379592457572\n",
      "New Q-value: -0.027199462158634852\n",
      "\n",
      "Step 14\n",
      "Current state: 2\n",
      "Q-values for current state: [-0.03077535 -0.02027495 -0.02232345 -0.02629778]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 10\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.020274952899189857\n",
      "New Q-value: -0.02058027112255196\n",
      "\n",
      "Step 15\n",
      "Current state: 10\n",
      "Q-values for current state: [-0.02269237 -0.01402962 -0.01437154 -0.02492465]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 9\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.022692365345090018\n",
      "New Q-value: -0.023304895171703706\n",
      "\n",
      "Step 16\n",
      "Current state: 9\n",
      "Q-values for current state: [-0.02897384 -0.01980807 -0.02157993 -0.03041975]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 1\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.03041974878207878\n",
      "New Q-value: -0.030774741270256915\n",
      "\n",
      "Step 17\n",
      "Current state: 1\n",
      "Q-values for current state: [-0.0392663  -0.02523124 -0.02719946 -0.03196833]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 9\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.02523123543564224\n",
      "New Q-value: -0.025589878253200707\n",
      "\n",
      "Step 18\n",
      "Current state: 9\n",
      "Q-values for current state: [-0.02897384 -0.01980807 -0.02157993 -0.03077474]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 17\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.019808066959186223\n",
      "New Q-value: -0.02006761054879296\n",
      "\n",
      "Step 19\n",
      "Current state: 17\n",
      "Q-values for current state: [-0.02202955 -0.01305632 -0.01330576 -0.02385688]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 16\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.022029550083505458\n",
      "New Q-value: -0.0224985495064114\n",
      "\n",
      "Step 20\n",
      "Current state: 16\n",
      "Q-values for current state: [-0.0248445  -0.01759952 -0.0181474  -0.03042097]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 8\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.030420969048461574\n",
      "New Q-value: -0.03068804984584715\n",
      "\n",
      "Step 21\n",
      "Current state: 8\n",
      "Q-values for current state: [-0.03143175 -0.02430713 -0.02646782 -0.0382897 ]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 16\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.024307133707702468\n",
      "New Q-value: -0.024548374768188705\n",
      "\n",
      "Step 22\n",
      "Current state: 16\n",
      "Q-values for current state: [-0.0248445  -0.01759952 -0.0181474  -0.03068805]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 8\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.03068804984584715\n",
      "New Q-value: -0.030951340464240362\n",
      "\n",
      "Step 23\n",
      "Current state: 8\n",
      "Q-values for current state: [-0.03143175 -0.02454837 -0.02646782 -0.0382897 ]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 8\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.0314317461000425\n",
      "New Q-value: -0.031620667093016176\n",
      "\n",
      "Step 24\n",
      "Current state: 8\n",
      "Q-values for current state: [-0.03162067 -0.02454837 -0.02646782 -0.0382897 ]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 9\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.026467818419563974\n",
      "New Q-value: -0.02672745957974291\n",
      "\n",
      "Step 25\n",
      "Current state: 9\n",
      "Q-values for current state: [-0.02897384 -0.02006761 -0.02157993 -0.03077474]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 8\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.028973841609132293\n",
      "New Q-value: -0.02940855305119699\n",
      "\n",
      "Step 26\n",
      "Current state: 8\n",
      "Q-values for current state: [-0.03162067 -0.02454837 -0.02672746 -0.0382897 ]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 0\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.0382896981222109\n",
      "New Q-value: -0.03850230892344795\n",
      "\n",
      "Step 27\n",
      "Current state: 0\n",
      "Q-values for current state: [-0.03904762 -0.03201664 -0.0326958  -0.03945391]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 0\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.039047623094130436\n",
      "New Q-value: -0.039184441398175535\n",
      "\n",
      "Step 28\n",
      "Current state: 0\n",
      "Q-values for current state: [-0.03918444 -0.03201664 -0.0326958  -0.03945391]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 0\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.03945390890623455\n",
      "New Q-value: -0.03955009862906923\n",
      "\n",
      "Step 29\n",
      "Current state: 0\n",
      "Q-values for current state: [-0.03918444 -0.03201664 -0.0326958  -0.0395501 ]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 1\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.0326957970628301\n",
      "New Q-value: -0.032857255790601156\n",
      "\n",
      "Step 30\n",
      "Current state: 1\n",
      "Q-values for current state: [-0.0392663  -0.02558988 -0.02719946 -0.03196833]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 1\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.03196833110900759\n",
      "New Q-value: -0.0322025364321609\n",
      "\n",
      "Step 31\n",
      "Current state: 1\n",
      "Q-values for current state: [-0.0392663  -0.02558988 -0.02719946 -0.03220254]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 2\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.027199462158634852\n",
      "New Q-value: -0.027434641699413804\n",
      "\n",
      "Step 32\n",
      "Current state: 2\n",
      "Q-values for current state: [-0.03077535 -0.02058027 -0.02232345 -0.02629778]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 1\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.03077534790391158\n",
      "New Q-value: -0.03112885154757449\n",
      "\n",
      "Step 33\n",
      "Current state: 1\n",
      "Q-values for current state: [-0.0392663  -0.02558988 -0.02743464 -0.03220254]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 1\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.0322025364321609\n",
      "New Q-value: -0.03241332122299888\n",
      "\n",
      "Step 34\n",
      "Current state: 1\n",
      "Q-values for current state: [-0.0392663  -0.02558988 -0.02743464 -0.03241332]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 2\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.027434641699413804\n",
      "New Q-value: -0.027646303286114858\n",
      "\n",
      "Step 35\n",
      "Current state: 2\n",
      "Q-values for current state: [-0.03112885 -0.02058027 -0.02232345 -0.02629778]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 10\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.02058027112255196\n",
      "New Q-value: -0.02085505752357785\n",
      "\n",
      "Step 36\n",
      "Current state: 10\n",
      "Q-values for current state: [-0.0233049  -0.01402962 -0.01437154 -0.02492465]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 18\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.014029615929274611\n",
      "New Q-value: -0.01420860487179715\n",
      "\n",
      "Step 37\n",
      "Current state: 18\n",
      "Q-values for current state: [-0.01628578 -0.01128587 -0.0061258  -0.01549769]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 10\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.015497687199713474\n",
      "New Q-value: -0.016297735942562855\n",
      "\n",
      "Step 38\n",
      "Current state: 10\n",
      "Q-values for current state: [-0.0233049  -0.0142086  -0.01437154 -0.02492465]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 11\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.014371539382138284\n",
      "New Q-value: -0.014667055915602325\n",
      "\n",
      "Step 39\n",
      "Current state: 11\n",
      "Q-values for current state: [-0.01239913 -0.00771232 -0.01266495 -0.0158111 ]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 19\n",
      "Reward: -0.01\n",
      "Terminated: True\n",
      "Old Q-value: -0.007712320754503899\n",
      "New Q-value: -0.00794108867905351\n",
      "\n",
      "Episode 77\n",
      "Starting state: 0\n",
      "\n",
      "Step 0\n",
      "Current state: 0\n",
      "Q-values for current state: [-0.03918444 -0.03201664 -0.03285726 -0.0395501 ]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 1\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.032857255790601156\n",
      "New Q-value: -0.03300256864559511\n",
      "\n",
      "Step 1\n",
      "Current state: 1\n",
      "Q-values for current state: [-0.0392663  -0.02558988 -0.0276463  -0.03241332]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 1\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.03241332122299888\n",
      "New Q-value: -0.032603027534753054\n",
      "\n",
      "Step 2\n",
      "Current state: 1\n",
      "Q-values for current state: [-0.0392663  -0.02558988 -0.0276463  -0.03260303]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 0\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.03926629623135298\n",
      "New Q-value: -0.03938124722167582\n",
      "\n",
      "Step 3\n",
      "Current state: 0\n",
      "Q-values for current state: [-0.03918444 -0.03201664 -0.03300257 -0.0395501 ]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 0\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.03955009862906923\n",
      "New Q-value: -0.039636669379620446\n",
      "\n",
      "Step 4\n",
      "Current state: 0\n",
      "Q-values for current state: [-0.03918444 -0.03201664 -0.03300257 -0.03963667]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 1\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.03300256864559511\n",
      "New Q-value: -0.03313335021508967\n",
      "\n",
      "Step 5\n",
      "Current state: 1\n",
      "Q-values for current state: [-0.03938125 -0.02558988 -0.0276463  -0.03260303]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 2\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.027646303286114858\n",
      "New Q-value: -0.02786290342224327\n",
      "\n",
      "Step 6\n",
      "Current state: 2\n",
      "Q-values for current state: [-0.03112885 -0.02085506 -0.02232345 -0.02629778]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 1\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.03112885154757449\n",
      "New Q-value: -0.03144700482687111\n",
      "\n",
      "Step 7\n",
      "Current state: 1\n",
      "Q-values for current state: [-0.03938125 -0.02558988 -0.0278629  -0.03260303]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 9\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.025589878253200707\n",
      "New Q-value: -0.025937313430015967\n",
      "\n",
      "Step 8\n",
      "Current state: 9\n",
      "Q-values for current state: [-0.02940855 -0.02006761 -0.02157993 -0.03077474]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 10\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.02157993476608092\n",
      "New Q-value: -0.021771758752293556\n",
      "\n",
      "Step 9\n",
      "Current state: 10\n",
      "Q-values for current state: [-0.0233049  -0.0142086  -0.01466706 -0.02492465]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 11\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.014667055915602325\n",
      "New Q-value: -0.014954753748552176\n",
      "\n",
      "Step 10\n",
      "Current state: 11\n",
      "Q-values for current state: [-0.01239913 -0.00794109 -0.01266495 -0.0158111 ]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 3\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.015811095382526938\n",
      "New Q-value: -0.016604992146332553\n",
      "\n",
      "Step 11\n",
      "Current state: 3\n",
      "Q-values for current state: [-0.02446996 -0.01447375 -0.01534104 -0.02003628]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 4\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.015341043292104157\n",
      "New Q-value: -0.015798123091207497\n",
      "\n",
      "Step 12\n",
      "Current state: 4\n",
      "Q-values for current state: [-0.01638374 -0.01043352 -0.01125145 -0.01337713]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 3\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.016383742856940386\n",
      "New Q-value: -0.017120374873304655\n",
      "\n",
      "Step 13\n",
      "Current state: 3\n",
      "Q-values for current state: [-0.02446996 -0.01447375 -0.01579812 -0.02003628]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 4\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.015798123091207497\n",
      "New Q-value: -0.016209494910400503\n",
      "\n",
      "Step 14\n",
      "Current state: 4\n",
      "Q-values for current state: [-0.01712037 -0.01043352 -0.01125145 -0.01337713]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 3\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.017120374873304655\n",
      "New Q-value: -0.0177833436880325\n",
      "\n",
      "Step 15\n",
      "Current state: 3\n",
      "Q-values for current state: [-0.02446996 -0.01447375 -0.01620949 -0.02003628]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 4\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.016209494910400503\n",
      "New Q-value: -0.01657972954767421\n",
      "\n",
      "Step 16\n",
      "Current state: 4\n",
      "Q-values for current state: [-0.01778334 -0.01043352 -0.01125145 -0.01337713]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 5\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.01125145494020196\n",
      "New Q-value: -0.012031948712560588\n",
      "\n",
      "Step 17\n",
      "Current state: 5\n",
      "Q-values for current state: [-0.00953304 -0.01035967 -0.0097495  -0.01040174]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 13\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.010359668575972559\n",
      "New Q-value: -0.010819319663375303\n",
      "\n",
      "Step 18\n",
      "Current state: 13\n",
      "Q-values for current state: [-0.01008285 -0.00840141 -0.00521703 -0.00940755]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 21\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.0084014139694\n",
      "New Q-value: -0.008888589889735\n",
      "\n",
      "Step 19\n",
      "Current state: 21\n",
      "Q-values for current state: [-0.00599786 -0.00468559 -0.0056167  -0.00344545]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 29\n",
      "Reward: -0.01\n",
      "Terminated: True\n",
      "Old Q-value: -0.00468559\n",
      "New Q-value: -0.005217031\n",
      "\n",
      "Episode 78\n",
      "Starting state: 0\n",
      "\n",
      "Step 0\n",
      "Current state: 0\n",
      "Q-values for current state: [-0.03918444 -0.03201664 -0.03313335 -0.03963667]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 0\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.039184441398175535\n",
      "New Q-value: -0.03930757787181612\n",
      "\n",
      "Step 1\n",
      "Current state: 0\n",
      "Q-values for current state: [-0.03930758 -0.03201664 -0.03313335 -0.03963667]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 8\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.03201663803640146\n",
      "New Q-value: -0.03214706983573924\n",
      "\n",
      "Step 2\n",
      "Current state: 8\n",
      "Q-values for current state: [-0.03162067 -0.02454837 -0.02672746 -0.03850231]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 16\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.024548374768188705\n",
      "New Q-value: -0.02476549172262632\n",
      "\n",
      "Step 3\n",
      "Current state: 16\n",
      "Q-values for current state: [-0.0248445  -0.01759952 -0.0181474  -0.03095134]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 8\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.030951340464240362\n",
      "New Q-value: -0.031208928131465825\n",
      "\n",
      "Step 4\n",
      "Current state: 8\n",
      "Q-values for current state: [-0.03162067 -0.02476549 -0.02672746 -0.03850231]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 0\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.03850230892344795\n",
      "New Q-value: -0.038706049665498385\n",
      "\n",
      "Step 5\n",
      "Current state: 0\n",
      "Q-values for current state: [-0.03930758 -0.03214707 -0.03313335 -0.03963667]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 8\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.03214706983573924\n",
      "New Q-value: -0.03228508456581482\n",
      "\n",
      "Step 6\n",
      "Current state: 8\n",
      "Q-values for current state: [-0.03162067 -0.02476549 -0.02672746 -0.03870605]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 9\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.02672745957974291\n",
      "New Q-value: -0.02696113662390395\n",
      "\n",
      "Step 7\n",
      "Current state: 9\n",
      "Q-values for current state: [-0.02940855 -0.02006761 -0.02177176 -0.03077474]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 1\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.030774741270256915\n",
      "New Q-value: -0.03116131191908274\n",
      "\n",
      "Step 8\n",
      "Current state: 1\n",
      "Q-values for current state: [-0.03938125 -0.02593731 -0.0278629  -0.03260303]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 2\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.02786290342224327\n",
      "New Q-value: -0.028057843544758838\n",
      "\n",
      "Step 9\n",
      "Current state: 2\n",
      "Q-values for current state: [-0.031447   -0.02085506 -0.02232345 -0.02629778]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 2\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.026297775985241252\n",
      "New Q-value: -0.026649228851457023\n",
      "\n",
      "Step 10\n",
      "Current state: 2\n",
      "Q-values for current state: [-0.031447   -0.02085506 -0.02232345 -0.02664923]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 10\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.02085505752357785\n",
      "New Q-value: -0.021119369234040796\n",
      "\n",
      "Step 11\n",
      "Current state: 10\n",
      "Q-values for current state: [-0.0233049  -0.0142086  -0.01495475 -0.02492465]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 11\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.014954753748552176\n",
      "New Q-value: -0.015213681798207042\n",
      "\n",
      "Step 12\n",
      "Current state: 11\n",
      "Q-values for current state: [-0.01239913 -0.00794109 -0.01266495 -0.01660499]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 19\n",
      "Reward: -0.01\n",
      "Terminated: True\n",
      "Old Q-value: -0.00794108867905351\n",
      "New Q-value: -0.00814697981114816\n",
      "\n",
      "Episode 79\n",
      "Starting state: 0\n",
      "\n",
      "Step 0\n",
      "Current state: 0\n",
      "Q-values for current state: [-0.03930758 -0.03228508 -0.03313335 -0.03963667]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 8\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.03228508456581482\n",
      "New Q-value: -0.032409297822882835\n",
      "\n",
      "Step 1\n",
      "Current state: 8\n",
      "Q-values for current state: [-0.03162067 -0.02476549 -0.02696114 -0.03870605]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 0\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.038706049665498385\n",
      "New Q-value: -0.03891432799212242\n",
      "\n",
      "Step 2\n",
      "Current state: 0\n",
      "Q-values for current state: [-0.03930758 -0.0324093  -0.03313335 -0.03963667]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 8\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.032409297822882835\n",
      "New Q-value: -0.03252108975424405\n",
      "\n",
      "Step 3\n",
      "Current state: 8\n",
      "Q-values for current state: [-0.03162067 -0.02476549 -0.02696114 -0.03891433]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 0\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.03891432799212242\n",
      "New Q-value: -0.03911239871956336\n",
      "\n",
      "Step 4\n",
      "Current state: 0\n",
      "Q-values for current state: [-0.03930758 -0.03252109 -0.03313335 -0.03963667]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 0\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.03930757787181612\n",
      "New Q-value: -0.0394663236112877\n",
      "\n",
      "Step 5\n",
      "Current state: 0\n",
      "Q-values for current state: [-0.03946632 -0.03252109 -0.03313335 -0.03963667]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 0\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.0394663236112877\n",
      "New Q-value: -0.039609194776812115\n",
      "\n",
      "Step 6\n",
      "Current state: 0\n",
      "Q-values for current state: [-0.03960919 -0.03252109 -0.03313335 -0.03963667]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 0\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.039636669379620446\n",
      "New Q-value: -0.03976250596831159\n",
      "\n",
      "Step 7\n",
      "Current state: 0\n",
      "Q-values for current state: [-0.03960919 -0.03252109 -0.03313335 -0.03976251]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 1\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.03313335021508967\n",
      "New Q-value: -0.03328405996943222\n",
      "\n",
      "Step 8\n",
      "Current state: 1\n",
      "Q-values for current state: [-0.03938125 -0.02593731 -0.02805784 -0.03260303]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 1\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.032603027534753054\n",
      "New Q-value: -0.032806769557129266\n",
      "\n",
      "Step 9\n",
      "Current state: 1\n",
      "Q-values for current state: [-0.03938125 -0.02593731 -0.02805784 -0.03280677]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 1\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.032806769557129266\n",
      "New Q-value: -0.03299013737726786\n",
      "\n",
      "Step 10\n",
      "Current state: 1\n",
      "Q-values for current state: [-0.03938125 -0.02593731 -0.02805784 -0.03299014]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 0\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.03938124722167582\n",
      "New Q-value: -0.03953262602616142\n",
      "\n",
      "Step 11\n",
      "Current state: 0\n",
      "Q-values for current state: [-0.03960919 -0.03252109 -0.03328406 -0.03976251]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 1\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.03328405996943222\n",
      "New Q-value: -0.03341969874834051\n",
      "\n",
      "Step 12\n",
      "Current state: 1\n",
      "Q-values for current state: [-0.03953263 -0.02593731 -0.02805784 -0.03299014]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 2\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.028057843544758838\n",
      "New Q-value: -0.028258399267516828\n",
      "\n",
      "Step 13\n",
      "Current state: 2\n",
      "Q-values for current state: [-0.031447   -0.02111937 -0.02232345 -0.02664923]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 1\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.03144700482687111\n",
      "New Q-value: -0.031766349120035516\n",
      "\n",
      "Step 14\n",
      "Current state: 1\n",
      "Q-values for current state: [-0.03953263 -0.02593731 -0.0282584  -0.03299014]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 2\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.028258399267516828\n",
      "New Q-value: -0.02843889941799902\n",
      "\n",
      "Step 15\n",
      "Current state: 2\n",
      "Q-values for current state: [-0.03176635 -0.02111937 -0.02232345 -0.02664923]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 2\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.026649228851457023\n",
      "New Q-value: -0.026990646043545195\n",
      "\n",
      "Step 16\n",
      "Current state: 2\n",
      "Q-values for current state: [-0.03176635 -0.02111937 -0.02232345 -0.02699065]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 10\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.021119369234040796\n",
      "New Q-value: -0.021357249773457446\n",
      "\n",
      "Step 17\n",
      "Current state: 10\n",
      "Q-values for current state: [-0.0233049  -0.0142086  -0.01521368 -0.02492465]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 11\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.015213681798207042\n",
      "New Q-value: -0.015466276700445413\n",
      "\n",
      "Step 18\n",
      "Current state: 11\n",
      "Q-values for current state: [-0.01239913 -0.00814698 -0.01266495 -0.01660499]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 19\n",
      "Reward: -0.01\n",
      "Terminated: True\n",
      "Old Q-value: -0.00814697981114816\n",
      "New Q-value: -0.008332281830033343\n",
      "\n",
      "Episode 80\n",
      "Starting state: 0\n",
      "\n",
      "Step 0\n",
      "Current state: 0\n",
      "Q-values for current state: [-0.03960919 -0.03252109 -0.0334197  -0.03976251]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 0\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.039609194776812115\n",
      "New Q-value: -0.03973777882578409\n",
      "\n",
      "Step 1\n",
      "Current state: 0\n",
      "Q-values for current state: [-0.03973778 -0.03252109 -0.0334197  -0.03976251]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 0\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.03973777882578409\n",
      "New Q-value: -0.039853504469858865\n",
      "\n",
      "Step 2\n",
      "Current state: 0\n",
      "Q-values for current state: [-0.0398535  -0.03252109 -0.0334197  -0.03976251]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 1\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.03341969874834051\n",
      "New Q-value: -0.03354177364935798\n",
      "\n",
      "Step 3\n",
      "Current state: 1\n",
      "Q-values for current state: [-0.03953263 -0.02593731 -0.0284389  -0.03299014]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 9\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.025937313430015967\n",
      "New Q-value: -0.0262500050891497\n",
      "\n",
      "Step 4\n",
      "Current state: 9\n",
      "Q-values for current state: [-0.02940855 -0.02006761 -0.02177176 -0.03116131]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 1\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.03116131191908274\n",
      "New Q-value: -0.03153893121064369\n",
      "\n",
      "Step 5\n",
      "Current state: 1\n",
      "Q-values for current state: [-0.03953263 -0.02625001 -0.0284389  -0.03299014]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 2\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.02843889941799902\n",
      "New Q-value: -0.028623948204677577\n",
      "\n",
      "Step 6\n",
      "Current state: 2\n",
      "Q-values for current state: [-0.03176635 -0.02135725 -0.02232345 -0.02699065]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 10\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.021357249773457446\n",
      "New Q-value: -0.02157134225893243\n",
      "\n",
      "Step 7\n",
      "Current state: 10\n",
      "Q-values for current state: [-0.0233049  -0.0142086  -0.01546628 -0.02492465]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 2\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.024924652087833257\n",
      "New Q-value: -0.025481464393648512\n",
      "\n",
      "Step 8\n",
      "Current state: 2\n",
      "Q-values for current state: [-0.03176635 -0.02157134 -0.02232345 -0.02699065]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 1\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.031766349120035516\n",
      "New Q-value: -0.03208346469150118\n",
      "\n",
      "Step 9\n",
      "Current state: 1\n",
      "Q-values for current state: [-0.03953263 -0.02625001 -0.02862395 -0.03299014]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 1\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.03299013737726786\n",
      "New Q-value: -0.03318487412301029\n",
      "\n",
      "Step 10\n",
      "Current state: 1\n",
      "Q-values for current state: [-0.03953263 -0.02625001 -0.02862395 -0.03318487]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 0\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.03953262602616142\n",
      "New Q-value: -0.039668866950198466\n",
      "\n",
      "Step 11\n",
      "Current state: 0\n",
      "Q-values for current state: [-0.0398535  -0.03252109 -0.03354177 -0.03976251]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 0\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.039853504469858865\n",
      "New Q-value: -0.03995765754952616\n",
      "\n",
      "Step 12\n",
      "Current state: 0\n",
      "Q-values for current state: [-0.03995766 -0.03252109 -0.03354177 -0.03976251]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 0\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.03976250596831159\n",
      "New Q-value: -0.03987575889813361\n",
      "\n",
      "Step 13\n",
      "Current state: 0\n",
      "Q-values for current state: [-0.03995766 -0.03252109 -0.03354177 -0.03987576]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 1\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.03354177364935798\n",
      "New Q-value: -0.033681346767891404\n",
      "\n",
      "Step 14\n",
      "Current state: 1\n",
      "Q-values for current state: [-0.03966887 -0.02625001 -0.02862395 -0.03318487]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 0\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.039668866950198466\n",
      "New Q-value: -0.0397914837818318\n",
      "\n",
      "Step 15\n",
      "Current state: 0\n",
      "Q-values for current state: [-0.03995766 -0.03252109 -0.03368135 -0.03987576]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 8\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.03252108975424405\n",
      "New Q-value: -0.032621702492469144\n",
      "\n",
      "Step 16\n",
      "Current state: 8\n",
      "Q-values for current state: [-0.03162067 -0.02476549 -0.02696114 -0.0391124 ]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 8\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.031620667093016176\n",
      "New Q-value: -0.03181132209736406\n",
      "\n",
      "Step 17\n",
      "Current state: 8\n",
      "Q-values for current state: [-0.03181132 -0.02476549 -0.02696114 -0.0391124 ]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 8\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.03181132209736406\n",
      "New Q-value: -0.03198291160127715\n",
      "\n",
      "Step 18\n",
      "Current state: 8\n",
      "Q-values for current state: [-0.03198291 -0.02476549 -0.02696114 -0.0391124 ]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 16\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.02476549172262632\n",
      "New Q-value: -0.024960896981620174\n",
      "\n",
      "Step 19\n",
      "Current state: 16\n",
      "Q-values for current state: [-0.0248445  -0.01759952 -0.0181474  -0.03120893]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 24\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.017599520329015628\n",
      "New Q-value: -0.017882715701902446\n",
      "\n",
      "Step 20\n",
      "Current state: 24\n",
      "Q-values for current state: [-0.01403369 -0.01221246 -0.0109805  -0.01844218]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 16\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.018442184322207282\n",
      "New Q-value: -0.019296823881667285\n",
      "\n",
      "Step 21\n",
      "Current state: 16\n",
      "Q-values for current state: [-0.0248445  -0.01788272 -0.0181474  -0.03120893]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 16\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.02484449833261237\n",
      "New Q-value: -0.025058906491031867\n",
      "\n",
      "Step 22\n",
      "Current state: 16\n",
      "Q-values for current state: [-0.02505891 -0.01788272 -0.0181474  -0.03120893]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 16\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.025058906491031867\n",
      "New Q-value: -0.02525187383360941\n",
      "\n",
      "Step 23\n",
      "Current state: 16\n",
      "Q-values for current state: [-0.02525187 -0.01788272 -0.0181474  -0.03120893]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 17\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.018147395469851953\n",
      "New Q-value: -0.018573006208392116\n",
      "\n",
      "Step 24\n",
      "Current state: 17\n",
      "Q-values for current state: [-0.02249855 -0.01305632 -0.01330576 -0.02385688]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 9\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.023856876574933017\n",
      "New Q-value: -0.024377611919575045\n",
      "\n",
      "Step 25\n",
      "Current state: 9\n",
      "Q-values for current state: [-0.02940855 -0.02006761 -0.02177176 -0.03153893]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 10\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.021771758752293556\n",
      "New Q-value: -0.02194440033988493\n",
      "\n",
      "Step 26\n",
      "Current state: 10\n",
      "Q-values for current state: [-0.0233049  -0.0142086  -0.01546628 -0.02548146]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 2\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.025481464393648512\n",
      "New Q-value: -0.025982595468882243\n",
      "\n",
      "Step 27\n",
      "Current state: 2\n",
      "Q-values for current state: [-0.03208346 -0.02157134 -0.02232345 -0.02699065]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 1\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.03208346469150118\n",
      "New Q-value: -0.032368868705820286\n",
      "\n",
      "Step 28\n",
      "Current state: 1\n",
      "Q-values for current state: [-0.03979148 -0.02625001 -0.02862395 -0.03318487]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 9\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.0262500050891497\n",
      "New Q-value: -0.02653142758237006\n",
      "\n",
      "Step 29\n",
      "Current state: 9\n",
      "Q-values for current state: [-0.02940855 -0.02006761 -0.0219444  -0.03153893]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 1\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.03153893121064369\n",
      "New Q-value: -0.031905523709904474\n",
      "\n",
      "Step 30\n",
      "Current state: 1\n",
      "Q-values for current state: [-0.03979148 -0.02653143 -0.02862395 -0.03318487]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 0\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.0397914837818318\n",
      "New Q-value: -0.039911397140433194\n",
      "\n",
      "Step 31\n",
      "Current state: 0\n",
      "Q-values for current state: [-0.03995766 -0.0326217  -0.03368135 -0.03987576]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 8\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.032621702492469144\n",
      "New Q-value: -0.032730817456476144\n",
      "\n",
      "Step 32\n",
      "Current state: 8\n",
      "Q-values for current state: [-0.03198291 -0.0249609  -0.02696114 -0.0391124 ]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 16\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.024960896981620174\n",
      "New Q-value: -0.02516366527513889\n",
      "\n",
      "Step 33\n",
      "Current state: 16\n",
      "Q-values for current state: [-0.02525187 -0.01788272 -0.01857301 -0.03120893]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 17\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.018573006208392116\n",
      "New Q-value: -0.018956055873078262\n",
      "\n",
      "Step 34\n",
      "Current state: 17\n",
      "Q-values for current state: [-0.02249855 -0.01305632 -0.01330576 -0.02437761]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 18\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.013305764048145718\n",
      "New Q-value: -0.013557138178781146\n",
      "\n",
      "Step 35\n",
      "Current state: 18\n",
      "Q-values for current state: [-0.01628578 -0.01128587 -0.0061258  -0.01629774]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 10\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.016297735942562855\n",
      "New Q-value: -0.017017779811127298\n",
      "\n",
      "Step 36\n",
      "Current state: 10\n",
      "Q-values for current state: [-0.0233049  -0.0142086  -0.01546628 -0.0259826 ]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 18\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.01420860487179715\n",
      "New Q-value: -0.014369694920067434\n",
      "\n",
      "Step 37\n",
      "Current state: 18\n",
      "Q-values for current state: [-0.01628578 -0.01128587 -0.0061258  -0.01701778]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 17\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.016285776008238762\n",
      "New Q-value: -0.016897548692940245\n",
      "\n",
      "Step 38\n",
      "Current state: 17\n",
      "Q-values for current state: [-0.02249855 -0.01305632 -0.01355714 -0.02437761]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 25\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.013056318795003767\n",
      "New Q-value: -0.013473378897091017\n",
      "\n",
      "Step 39\n",
      "Current state: 25\n",
      "Q-values for current state: [-0.01130846 -0.00760728 -0.01158082 -0.01397801]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 17\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.013978014668908849\n",
      "New Q-value: -0.01486018419724161\n",
      "\n",
      "Step 40\n",
      "Current state: 17\n",
      "Q-values for current state: [-0.02249855 -0.01347338 -0.01355714 -0.02437761]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 9\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.024377611919575045\n",
      "New Q-value: -0.02484627372975287\n",
      "\n",
      "Step 41\n",
      "Current state: 9\n",
      "Q-values for current state: [-0.02940855 -0.02006761 -0.0219444  -0.03190552]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 1\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.031905523709904474\n",
      "New Q-value: -0.032235456959239184\n",
      "\n",
      "Step 42\n",
      "Current state: 1\n",
      "Q-values for current state: [-0.0399114  -0.02653143 -0.02862395 -0.03318487]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 2\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.028623948204677577\n",
      "New Q-value: -0.0288108308988084\n",
      "\n",
      "Step 43\n",
      "Current state: 2\n",
      "Q-values for current state: [-0.03236887 -0.02157134 -0.02232345 -0.02699065]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 1\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.032368868705820286\n",
      "New Q-value: -0.03265246745556341\n",
      "\n",
      "Step 44\n",
      "Current state: 1\n",
      "Q-values for current state: [-0.0399114  -0.02653143 -0.02881083 -0.03318487]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 9\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.02653142758237006\n",
      "New Q-value: -0.026784707826268385\n",
      "\n",
      "Step 45\n",
      "Current state: 9\n",
      "Q-values for current state: [-0.02940855 -0.02006761 -0.0219444  -0.03223546]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 1\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.032235456959239184\n",
      "New Q-value: -0.03255645850681076\n",
      "\n",
      "Step 46\n",
      "Current state: 1\n",
      "Q-values for current state: [-0.0399114  -0.02678471 -0.02881083 -0.03318487]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 9\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.026784707826268385\n",
      "New Q-value: -0.027012660045776876\n",
      "\n",
      "Step 47\n",
      "Current state: 9\n",
      "Q-values for current state: [-0.02940855 -0.02006761 -0.0219444  -0.03255646]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 8\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.02940855305119699\n",
      "New Q-value: -0.029858245947215485\n",
      "\n",
      "Step 48\n",
      "Current state: 8\n",
      "Q-values for current state: [-0.03198291 -0.02516367 -0.02696114 -0.0391124 ]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 0\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.03911239871956336\n",
      "New Q-value: -0.03931058650597226\n",
      "\n",
      "Step 49\n",
      "Current state: 0\n",
      "Q-values for current state: [-0.03995766 -0.03273082 -0.03368135 -0.03987576]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 0\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.03995765754952616\n",
      "New Q-value: -0.04007131945293878\n",
      "\n",
      "Step 50\n",
      "Current state: 0\n",
      "Q-values for current state: [-0.04007132 -0.03273082 -0.03368135 -0.03987576]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 8\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.032730817456476144\n",
      "New Q-value: -0.03284828391196672\n",
      "\n",
      "Step 51\n",
      "Current state: 8\n",
      "Q-values for current state: [-0.03198291 -0.02516367 -0.02696114 -0.03931059]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 8\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.03198291160127715\n",
      "New Q-value: -0.03217516864228763\n",
      "\n",
      "Step 52\n",
      "Current state: 8\n",
      "Q-values for current state: [-0.03217517 -0.02516367 -0.02696114 -0.03931059]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 0\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.03931058650597226\n",
      "New Q-value: -0.03950011482701187\n",
      "\n",
      "Step 53\n",
      "Current state: 0\n",
      "Q-values for current state: [-0.04007132 -0.03284828 -0.03368135 -0.03987576]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 0\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.03987575889813361\n",
      "New Q-value: -0.04000876997995709\n",
      "\n",
      "Step 54\n",
      "Current state: 0\n",
      "Q-values for current state: [-0.04007132 -0.03284828 -0.03368135 -0.04000877]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 1\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.033681346767891404\n",
      "New Q-value: -0.03387941479545107\n",
      "\n",
      "Step 55\n",
      "Current state: 1\n",
      "Q-values for current state: [-0.0399114  -0.02701266 -0.02881083 -0.03318487]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 9\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.027012660045776876\n",
      "New Q-value: -0.02721781704333452\n",
      "\n",
      "Step 56\n",
      "Current state: 9\n",
      "Q-values for current state: [-0.02985825 -0.02006761 -0.0219444  -0.03255646]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 17\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.02006761054879296\n",
      "New Q-value: -0.02034082048913731\n",
      "\n",
      "Step 57\n",
      "Current state: 17\n",
      "Q-values for current state: [-0.02249855 -0.01347338 -0.01355714 -0.02484627]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 16\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.0224985495064114\n",
      "New Q-value: -0.02294755254745099\n",
      "\n",
      "Step 58\n",
      "Current state: 16\n",
      "Q-values for current state: [-0.02525187 -0.01788272 -0.01895606 -0.03120893]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 8\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.031208928131465825\n",
      "New Q-value: -0.03147858351945744\n",
      "\n",
      "Step 59\n",
      "Current state: 8\n",
      "Q-values for current state: [-0.03217517 -0.02516367 -0.02696114 -0.03950011]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 9\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.02696113662390395\n",
      "New Q-value: -0.0271974009079816\n",
      "\n",
      "Step 60\n",
      "Current state: 9\n",
      "Q-values for current state: [-0.02985825 -0.02034082 -0.0219444  -0.03255646]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 10\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.02194440033988493\n",
      "New Q-value: -0.02211508132330284\n",
      "\n",
      "Step 61\n",
      "Current state: 10\n",
      "Q-values for current state: [-0.0233049  -0.01436969 -0.01546628 -0.0259826 ]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 2\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.025982595468882243\n",
      "New Q-value: -0.0264336134365926\n",
      "\n",
      "Step 62\n",
      "Current state: 2\n",
      "Q-values for current state: [-0.03265247 -0.02157134 -0.02232345 -0.02699065]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 10\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.02157134225893243\n",
      "New Q-value: -0.021779329050445596\n",
      "\n",
      "Step 63\n",
      "Current state: 10\n",
      "Q-values for current state: [-0.0233049  -0.01436969 -0.01546628 -0.02643361]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 11\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.015466276700445413\n",
      "New Q-value: -0.01571121580425404\n",
      "\n",
      "Step 64\n",
      "Current state: 11\n",
      "Q-values for current state: [-0.01239913 -0.00833228 -0.01266495 -0.01660499]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 3\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.016604992146332553\n",
      "New Q-value: -0.017319499233757606\n",
      "\n",
      "Step 65\n",
      "Current state: 3\n",
      "Q-values for current state: [-0.02446996 -0.01447375 -0.01657973 -0.02003628]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 2\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.024469963171599957\n",
      "New Q-value: -0.025092003114232292\n",
      "\n",
      "Step 66\n",
      "Current state: 2\n",
      "Q-values for current state: [-0.03265247 -0.02177933 -0.02232345 -0.02699065]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 2\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.026990646043545195\n",
      "New Q-value: -0.027360617698983007\n",
      "\n",
      "Step 67\n",
      "Current state: 2\n",
      "Q-values for current state: [-0.03265247 -0.02177933 -0.02232345 -0.02736062]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 2\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.027360617698983007\n",
      "New Q-value: -0.02769359218887704\n",
      "\n",
      "Step 68\n",
      "Current state: 2\n",
      "Q-values for current state: [-0.03265247 -0.02177933 -0.02232345 -0.02769359]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 10\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.021779329050445596\n",
      "New Q-value: -0.021966517162807443\n",
      "\n",
      "Step 69\n",
      "Current state: 10\n",
      "Q-values for current state: [-0.0233049  -0.01436969 -0.01571122 -0.02643361]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 9\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.023304895171703706\n",
      "New Q-value: -0.023906783601001378\n",
      "\n",
      "Step 70\n",
      "Current state: 9\n",
      "Q-values for current state: [-0.02985825 -0.02034082 -0.02211508 -0.03255646]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 1\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.03255645850681076\n",
      "New Q-value: -0.03288650527524647\n",
      "\n",
      "Step 71\n",
      "Current state: 1\n",
      "Q-values for current state: [-0.0399114  -0.02721782 -0.02881083 -0.03318487]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 0\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.039911397140433194\n",
      "New Q-value: -0.040040844398026715\n",
      "\n",
      "Step 72\n",
      "Current state: 0\n",
      "Q-values for current state: [-0.04007132 -0.03284828 -0.03387941 -0.04000877]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 0\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.04000876997995709\n",
      "New Q-value: -0.040128479953598216\n",
      "\n",
      "Step 73\n",
      "Current state: 0\n",
      "Q-values for current state: [-0.04007132 -0.03284828 -0.03387941 -0.04012848]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 1\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.03387941479545107\n",
      "New Q-value: -0.03407716593502274\n",
      "\n",
      "Step 74\n",
      "Current state: 1\n",
      "Q-values for current state: [-0.04004084 -0.02721782 -0.02881083 -0.03318487]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 2\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.0288108308988084\n",
      "New Q-value: -0.029016566939394264\n",
      "\n",
      "Step 75\n",
      "Current state: 2\n",
      "Q-values for current state: [-0.03265247 -0.02196652 -0.02232345 -0.02769359]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 3\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.022323452869778757\n",
      "New Q-value: -0.02246611388485919\n",
      "\n",
      "Step 76\n",
      "Current state: 3\n",
      "Q-values for current state: [-0.025092   -0.01447375 -0.01657973 -0.02003628]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 3\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.020036282090194354\n",
      "New Q-value: -0.02040766018323323\n",
      "\n",
      "Step 77\n",
      "Current state: 3\n",
      "Q-values for current state: [-0.025092   -0.01447375 -0.01657973 -0.02040766]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 11\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.01447375054798219\n",
      "New Q-value: -0.014817942267037138\n",
      "\n",
      "Step 78\n",
      "Current state: 11\n",
      "Q-values for current state: [-0.01239913 -0.00833228 -0.01266495 -0.0173195 ]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 10\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.01239912823015193\n",
      "New Q-value: -0.013524336424543142\n",
      "\n",
      "Step 79\n",
      "Current state: 10\n",
      "Q-values for current state: [-0.02390678 -0.01436969 -0.01571122 -0.02643361]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 18\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.014369694920067434\n",
      "New Q-value: -0.01451467596351069\n",
      "\n",
      "Step 80\n",
      "Current state: 18\n",
      "Q-values for current state: [-0.01689755 -0.01128587 -0.0061258  -0.01701778]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 17\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.016897548692940245\n",
      "New Q-value: -0.017487764818869867\n",
      "\n",
      "Step 81\n",
      "Current state: 17\n",
      "Q-values for current state: [-0.02294755 -0.01347338 -0.01355714 -0.02484627]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 9\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.02484627372975287\n",
      "New Q-value: -0.02529402430324563\n",
      "\n",
      "Step 82\n",
      "Current state: 9\n",
      "Q-values for current state: [-0.02985825 -0.02034082 -0.02211508 -0.03288651]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 17\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.02034082048913731\n",
      "New Q-value: -0.020586709435447225\n",
      "\n",
      "Step 83\n",
      "Current state: 17\n",
      "Q-values for current state: [-0.02294755 -0.01347338 -0.01355714 -0.02529402]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 16\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.02294755254745099\n",
      "New Q-value: -0.023351655284386624\n",
      "\n",
      "Step 84\n",
      "Current state: 16\n",
      "Q-values for current state: [-0.02525187 -0.01788272 -0.01895606 -0.03147858]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 8\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.03147858351945744\n",
      "New Q-value: -0.03172127336864989\n",
      "\n",
      "Step 85\n",
      "Current state: 8\n",
      "Q-values for current state: [-0.03217517 -0.02516367 -0.0271974  -0.03950011]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 0\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.03950011482701187\n",
      "New Q-value: -0.03967069031594753\n",
      "\n",
      "Step 86\n",
      "Current state: 0\n",
      "Q-values for current state: [-0.04007132 -0.03284828 -0.03407717 -0.04012848]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 0\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.04007131945293878\n",
      "New Q-value: -0.04018477447928174\n",
      "\n",
      "Step 87\n",
      "Current state: 0\n",
      "Q-values for current state: [-0.04018477 -0.03284828 -0.03407717 -0.04012848]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 0\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.04018477447928174\n",
      "New Q-value: -0.0402868840029904\n",
      "\n",
      "Step 88\n",
      "Current state: 0\n",
      "Q-values for current state: [-0.04028688 -0.03284828 -0.03407717 -0.04012848]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 8\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.03284828391196672\n",
      "New Q-value: -0.032954003721908244\n",
      "\n",
      "Step 89\n",
      "Current state: 8\n",
      "Q-values for current state: [-0.03217517 -0.02516367 -0.0271974  -0.03967069]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 8\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.03217516864228763\n",
      "New Q-value: -0.032348199979197065\n",
      "\n",
      "Step 90\n",
      "Current state: 8\n",
      "Q-values for current state: [-0.0323482  -0.02516367 -0.0271974  -0.03967069]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 9\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.0271974009079816\n",
      "New Q-value: -0.027433398213550925\n",
      "\n",
      "Step 91\n",
      "Current state: 9\n",
      "Q-values for current state: [-0.02985825 -0.02058671 -0.02211508 -0.03288651]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 10\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.02211508132330284\n",
      "New Q-value: -0.02228246740750607\n",
      "\n",
      "Step 92\n",
      "Current state: 10\n",
      "Q-values for current state: [-0.02390678 -0.01451468 -0.01571122 -0.02643361]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 18\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.01451467596351069\n",
      "New Q-value: -0.01464515890260962\n",
      "\n",
      "Step 93\n",
      "Current state: 18\n",
      "Q-values for current state: [-0.01748776 -0.01128587 -0.0061258  -0.01701778]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 10\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.017017779811127298\n",
      "New Q-value: -0.01770729192576248\n",
      "\n",
      "Step 94\n",
      "Current state: 10\n",
      "Q-values for current state: [-0.02390678 -0.01464516 -0.01571122 -0.02643361]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 11\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.01571121580425404\n",
      "New Q-value: -0.015931660997681802\n",
      "\n",
      "Step 95\n",
      "Current state: 11\n",
      "Q-values for current state: [-0.01352434 -0.00833228 -0.01266495 -0.0173195 ]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 12\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.012664946019071118\n",
      "New Q-value: -0.013107794924753505\n",
      "\n",
      "Step 96\n",
      "Current state: 12\n",
      "Q-values for current state: [-0.01017109 -0.00746677 -0.01215002 -0.00769109]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 20\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.0074667737641\n",
      "New Q-value: -0.00797754638769\n",
      "\n",
      "Step 97\n",
      "Current state: 20\n",
      "Q-values for current state: [-0.00468559 -0.00271    -0.00511371 -0.00529062]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 12\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.0052906180224675\n",
      "New Q-value: -0.006492209949990404\n",
      "\n",
      "Step 98\n",
      "Current state: 12\n",
      "Q-values for current state: [-0.01017109 -0.00797755 -0.01215002 -0.00769109]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 20\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.00797754638769\n",
      "New Q-value: -0.008437241748921\n",
      "\n",
      "Step 99\n",
      "Current state: 20\n",
      "Q-values for current state: [-0.00468559 -0.00271    -0.00511371 -0.00649221]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 12\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.006492209949990404\n",
      "New Q-value: -0.007573642684761017\n",
      "\n",
      "Episode 80 summary:\n",
      "Total reward: -1.0000000000000007\n",
      "Successful episodes so far: []\n",
      "Average reward over last 10 episodes: -0.493\n",
      "\n",
      "Episode 81\n",
      "Starting state: 0\n",
      "\n",
      "Step 0\n",
      "Current state: 0\n",
      "Q-values for current state: [-0.04028688 -0.032954   -0.03407717 -0.04012848]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 8\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.032954003721908244\n",
      "New Q-value: -0.03304915155085562\n",
      "\n",
      "Step 1\n",
      "Current state: 8\n",
      "Q-values for current state: [-0.0323482  -0.02516367 -0.0274334  -0.03967069]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 16\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.02516366527513889\n",
      "New Q-value: -0.025346156739305733\n",
      "\n",
      "Step 2\n",
      "Current state: 16\n",
      "Q-values for current state: [-0.02525187 -0.01788272 -0.01895606 -0.03172127]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 8\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.03172127336864989\n",
      "New Q-value: -0.031957030922018945\n",
      "\n",
      "Step 3\n",
      "Current state: 8\n",
      "Q-values for current state: [-0.0323482  -0.02534616 -0.0274334  -0.03967069]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 8\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.032348199979197065\n",
      "New Q-value: -0.032521264871511406\n",
      "\n",
      "Step 4\n",
      "Current state: 8\n",
      "Q-values for current state: [-0.03252126 -0.02534616 -0.0274334  -0.03967069]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 0\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.03967069031594753\n",
      "New Q-value: -0.03984329068168406\n",
      "\n",
      "Step 5\n",
      "Current state: 0\n",
      "Q-values for current state: [-0.04028688 -0.03304915 -0.03407717 -0.04012848]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 1\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.03407716593502274\n",
      "New Q-value: -0.034255141960637246\n",
      "\n",
      "Step 6\n",
      "Current state: 1\n",
      "Q-values for current state: [-0.04004084 -0.02721782 -0.02901657 -0.03318487]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 0\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.040040844398026715\n",
      "New Q-value: -0.04017642935555533\n",
      "\n",
      "Step 7\n",
      "Current state: 0\n",
      "Q-values for current state: [-0.04028688 -0.03304915 -0.03425514 -0.04012848]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 1\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.034255141960637246\n",
      "New Q-value: -0.0344153203836903\n",
      "\n",
      "Step 8\n",
      "Current state: 1\n",
      "Q-values for current state: [-0.04017643 -0.02721782 -0.02901657 -0.03318487]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 2\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.029016566939394264\n",
      "New Q-value: -0.029201729375921547\n",
      "\n",
      "Step 9\n",
      "Current state: 2\n",
      "Q-values for current state: [-0.03265247 -0.02196652 -0.02246611 -0.02769359]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 10\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.021966517162807443\n",
      "New Q-value: -0.022161155542274612\n",
      "\n",
      "Step 10\n",
      "Current state: 10\n",
      "Q-values for current state: [-0.02390678 -0.01464516 -0.01593166 -0.02643361]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 2\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.0264336134365926\n",
      "New Q-value: -0.026895561869449428\n",
      "\n",
      "Step 11\n",
      "Current state: 2\n",
      "Q-values for current state: [-0.03265247 -0.02216116 -0.02246611 -0.02769359]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 1\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.03265246745556341\n",
      "New Q-value: -0.032972913329123846\n",
      "\n",
      "Step 12\n",
      "Current state: 1\n",
      "Q-values for current state: [-0.04017643 -0.02721782 -0.02920173 -0.03318487]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 2\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.029201729375921547\n",
      "New Q-value: -0.02938686621484548\n",
      "\n",
      "Step 13\n",
      "Current state: 2\n",
      "Q-values for current state: [-0.03297291 -0.02216116 -0.02246611 -0.02769359]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 10\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.022161155542274612\n",
      "New Q-value: -0.022336330083795066\n",
      "\n",
      "Step 14\n",
      "Current state: 10\n",
      "Q-values for current state: [-0.02390678 -0.01464516 -0.01593166 -0.02689556]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 11\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.015931660997681802\n",
      "New Q-value: -0.01613006167176679\n",
      "\n",
      "Step 15\n",
      "Current state: 11\n",
      "Q-values for current state: [-0.01352434 -0.00833228 -0.01310779 -0.0173195 ]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 10\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.013524336424543142\n",
      "New Q-value: -0.014563192877836742\n",
      "\n",
      "Step 16\n",
      "Current state: 10\n",
      "Q-values for current state: [-0.02390678 -0.01464516 -0.01613006 -0.02689556]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 9\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.023906783601001378\n",
      "New Q-value: -0.024471842637268724\n",
      "\n",
      "Step 17\n",
      "Current state: 9\n",
      "Q-values for current state: [-0.02985825 -0.02058671 -0.02228247 -0.03288651]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 1\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.03288650527524647\n",
      "New Q-value: -0.0331835473668386\n",
      "\n",
      "Step 18\n",
      "Current state: 1\n",
      "Q-values for current state: [-0.04017643 -0.02721782 -0.02938687 -0.03318487]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 9\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.02721781704333452\n",
      "New Q-value: -0.027451772735368554\n",
      "\n",
      "Step 19\n",
      "Current state: 9\n",
      "Q-values for current state: [-0.02985825 -0.02058671 -0.02228247 -0.03318355]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 1\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.0331835473668386\n",
      "New Q-value: -0.033473111040014755\n",
      "\n",
      "Step 20\n",
      "Current state: 1\n",
      "Q-values for current state: [-0.04017643 -0.02745177 -0.02938687 -0.03318487]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 0\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.04017642935555533\n",
      "New Q-value: -0.04029845581733108\n",
      "\n",
      "Step 21\n",
      "Current state: 0\n",
      "Q-values for current state: [-0.04028688 -0.03304915 -0.03441532 -0.04012848]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 0\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.040128479953598216\n",
      "New Q-value: -0.04025530135556968\n",
      "\n",
      "Step 22\n",
      "Current state: 0\n",
      "Q-values for current state: [-0.04028688 -0.03304915 -0.03441532 -0.0402553 ]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 0\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.0402868840029904\n",
      "New Q-value: -0.04039786500002265\n",
      "\n",
      "Step 23\n",
      "Current state: 0\n",
      "Q-values for current state: [-0.04039787 -0.03304915 -0.03441532 -0.0402553 ]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 1\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.0344153203836903\n",
      "New Q-value: -0.03458170675518128\n",
      "\n",
      "Step 24\n",
      "Current state: 1\n",
      "Q-values for current state: [-0.04029846 -0.02745177 -0.02938687 -0.03318487]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 0\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.04029845581733108\n",
      "New Q-value: -0.040408279632929256\n",
      "\n",
      "Step 25\n",
      "Current state: 0\n",
      "Q-values for current state: [-0.04039787 -0.03304915 -0.03458171 -0.0402553 ]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 0\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.04039786500002265\n",
      "New Q-value: -0.040497747897351666\n",
      "\n",
      "Step 26\n",
      "Current state: 0\n",
      "Q-values for current state: [-0.04049775 -0.03304915 -0.03458171 -0.0402553 ]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 1\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.03458170675518128\n",
      "New Q-value: -0.03473145448952317\n",
      "\n",
      "Step 27\n",
      "Current state: 1\n",
      "Q-values for current state: [-0.04040828 -0.02745177 -0.02938687 -0.03318487]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 2\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.02938686621484548\n",
      "New Q-value: -0.029570130951321463\n",
      "\n",
      "Step 28\n",
      "Current state: 2\n",
      "Q-values for current state: [-0.03297291 -0.02233633 -0.02246611 -0.02769359]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 2\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.02769359218887704\n",
      "New Q-value: -0.028046184327949867\n",
      "\n",
      "Step 29\n",
      "Current state: 2\n",
      "Q-values for current state: [-0.03297291 -0.02233633 -0.02246611 -0.02804618]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 2\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.028046184327949867\n",
      "New Q-value: -0.028363517253115412\n",
      "\n",
      "Step 30\n",
      "Current state: 2\n",
      "Q-values for current state: [-0.03297291 -0.02233633 -0.02246611 -0.02836352]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 10\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.022336330083795066\n",
      "New Q-value: -0.022493987171163472\n",
      "\n",
      "Step 31\n",
      "Current state: 10\n",
      "Q-values for current state: [-0.02447184 -0.01464516 -0.01613006 -0.02689556]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 18\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.01464515890260962\n",
      "New Q-value: -0.014762593547798658\n",
      "\n",
      "Step 32\n",
      "Current state: 18\n",
      "Q-values for current state: [-0.01748776 -0.01128587 -0.0061258  -0.01770729]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 19\n",
      "Reward: -0.01\n",
      "Terminated: True\n",
      "Old Q-value: -0.00612579511\n",
      "New Q-value: -0.006513215599\n",
      "\n",
      "Episode 82\n",
      "Starting state: 0\n",
      "\n",
      "Step 0\n",
      "Current state: 0\n",
      "Q-values for current state: [-0.04049775 -0.03304915 -0.03473145 -0.0402553 ]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 8\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.03304915155085562\n",
      "New Q-value: -0.0331521212860041\n",
      "\n",
      "Step 1\n",
      "Current state: 8\n",
      "Q-values for current state: [-0.03252126 -0.02534616 -0.0274334  -0.03984329]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 8\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.032521264871511406\n",
      "New Q-value: -0.03267702327459431\n",
      "\n",
      "Step 2\n",
      "Current state: 8\n",
      "Q-values for current state: [-0.03267702 -0.02534616 -0.0274334  -0.03984329]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 0\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.03984329068168406\n",
      "New Q-value: -0.040008413135686045\n",
      "\n",
      "Step 3\n",
      "Current state: 0\n",
      "Q-values for current state: [-0.04049775 -0.03315212 -0.03473145 -0.0402553 ]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 0\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.040497747897351666\n",
      "New Q-value: -0.04059742462978689\n",
      "\n",
      "Step 4\n",
      "Current state: 0\n",
      "Q-values for current state: [-0.04059742 -0.03315212 -0.03473145 -0.0402553 ]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 8\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.0331521212860041\n",
      "New Q-value: -0.033244794047637735\n",
      "\n",
      "Step 5\n",
      "Current state: 8\n",
      "Q-values for current state: [-0.03267702 -0.02534616 -0.0274334  -0.04000841]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 0\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.040008413135686045\n",
      "New Q-value: -0.04016582725664303\n",
      "\n",
      "Step 6\n",
      "Current state: 0\n",
      "Q-values for current state: [-0.04059742 -0.03324479 -0.03473145 -0.0402553 ]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 8\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.033244794047637735\n",
      "New Q-value: -0.033328199533108005\n",
      "\n",
      "Step 7\n",
      "Current state: 8\n",
      "Q-values for current state: [-0.03267702 -0.02534616 -0.0274334  -0.04016583]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 0\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.04016582725664303\n",
      "New Q-value: -0.04031542348662399\n",
      "\n",
      "Step 8\n",
      "Current state: 0\n",
      "Q-values for current state: [-0.04059742 -0.0333282  -0.03473145 -0.0402553 ]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 1\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.03473145448952317\n",
      "New Q-value: -0.03486622745043086\n",
      "\n",
      "Step 9\n",
      "Current state: 1\n",
      "Q-values for current state: [-0.04040828 -0.02745177 -0.02957013 -0.03318487]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 9\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.027451772735368554\n",
      "New Q-value: -0.027662332858199183\n",
      "\n",
      "Step 10\n",
      "Current state: 9\n",
      "Q-values for current state: [-0.02985825 -0.02058671 -0.02228247 -0.03347311]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 8\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.029858245947215485\n",
      "New Q-value: -0.03028030624272798\n",
      "\n",
      "Step 11\n",
      "Current state: 8\n",
      "Q-values for current state: [-0.03267702 -0.02534616 -0.0274334  -0.04031542]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 16\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.025346156739305733\n",
      "New Q-value: -0.02551039905705589\n",
      "\n",
      "Step 12\n",
      "Current state: 16\n",
      "Q-values for current state: [-0.02525187 -0.01788272 -0.01895606 -0.03195703]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 8\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.031957030922018945\n",
      "New Q-value: -0.03218481574023736\n",
      "\n",
      "Step 13\n",
      "Current state: 8\n",
      "Q-values for current state: [-0.03267702 -0.0255104  -0.0274334  -0.04031542]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 16\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.02551039905705589\n",
      "New Q-value: -0.025658217143031035\n",
      "\n",
      "Step 14\n",
      "Current state: 16\n",
      "Q-values for current state: [-0.02525187 -0.01788272 -0.01895606 -0.03218482]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 24\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.017882715701902446\n",
      "New Q-value: -0.018137591537500584\n",
      "\n",
      "Step 15\n",
      "Current state: 24\n",
      "Q-values for current state: [-0.01403369 -0.01221246 -0.0109805  -0.01929682]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 25\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.010980499008298752\n",
      "New Q-value: -0.011605141089056505\n",
      "\n",
      "Step 16\n",
      "Current state: 25\n",
      "Q-values for current state: [-0.01130846 -0.00760728 -0.01158082 -0.01486018]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 26\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.011580821984558353\n",
      "New Q-value: -0.011918357731102518\n",
      "\n",
      "Step 17\n",
      "Current state: 26\n",
      "Q-values for current state: [-0.0084064  -0.00792949 -0.00521703 -0.00938425]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 18\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.00938425112287554\n",
      "New Q-value: -0.010064581492492985\n",
      "\n",
      "Step 18\n",
      "Current state: 18\n",
      "Q-values for current state: [-0.01748776 -0.01128587 -0.00651322 -0.01770729]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 17\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.017487764818869867\n",
      "New Q-value: -0.018018959332206527\n",
      "\n",
      "Step 19\n",
      "Current state: 17\n",
      "Q-values for current state: [-0.02335166 -0.01347338 -0.01355714 -0.02529402]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 25\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.013473378897091017\n",
      "New Q-value: -0.013848732988969544\n",
      "\n",
      "Step 20\n",
      "Current state: 25\n",
      "Q-values for current state: [-0.01130846 -0.00760728 -0.01191836 -0.01486018]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 26\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.011918357731102518\n",
      "New Q-value: -0.012222139902992266\n",
      "\n",
      "Step 21\n",
      "Current state: 26\n",
      "Q-values for current state: [-0.0084064  -0.00792949 -0.00521703 -0.01006458]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 25\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.00840639833471866\n",
      "New Q-value: -0.009288450482834422\n",
      "\n",
      "Step 22\n",
      "Current state: 25\n",
      "Q-values for current state: [-0.01130846 -0.00760728 -0.01222214 -0.01486018]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 26\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.012222139902992266\n",
      "New Q-value: -0.01249554385769304\n",
      "\n",
      "Step 23\n",
      "Current state: 26\n",
      "Q-values for current state: [-0.00928845 -0.00792949 -0.00521703 -0.01006458]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 18\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.010064581492492985\n",
      "New Q-value: -0.010676878825148686\n",
      "\n",
      "Step 24\n",
      "Current state: 18\n",
      "Q-values for current state: [-0.01801896 -0.01128587 -0.00651322 -0.01770729]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 10\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.01770729192576248\n",
      "New Q-value: -0.018339009120227106\n",
      "\n",
      "Step 25\n",
      "Current state: 10\n",
      "Q-values for current state: [-0.02447184 -0.01476259 -0.01613006 -0.02689556]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 18\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.014762593547798658\n",
      "New Q-value: -0.014905089674923792\n",
      "\n",
      "Step 26\n",
      "Current state: 18\n",
      "Q-values for current state: [-0.01801896 -0.01128587 -0.00651322 -0.01833901]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 26\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.011285872488133628\n",
      "New Q-value: -0.011652903184320265\n",
      "\n",
      "Step 27\n",
      "Current state: 26\n",
      "Q-values for current state: [-0.00928845 -0.00792949 -0.00521703 -0.01067688]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 27\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.005217031\n",
      "New Q-value: -0.0056953279\n",
      "\n",
      "Step 28\n",
      "Current state: 27\n",
      "Q-values for current state: [ 0.       -0.001    -0.003439 -0.00271 ]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 26\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: -0.0015410561505000001\n",
      "\n",
      "Step 29\n",
      "Current state: 26\n",
      "Q-values for current state: [-0.00928845 -0.00792949 -0.00569533 -0.01067688]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 25\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.009288450482834422\n",
      "New Q-value: -0.010082297416138608\n",
      "\n",
      "Step 30\n",
      "Current state: 25\n",
      "Q-values for current state: [-0.01130846 -0.00760728 -0.01249554 -0.01486018]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 33\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.007607284016711875\n",
      "New Q-value: -0.008209579699880247\n",
      "\n",
      "Step 31\n",
      "Current state: 33\n",
      "Q-values for current state: [-0.00382131 -0.00569533 -0.0052517  -0.00765552]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 25\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.007655524198346309\n",
      "New Q-value: -0.008669881850000302\n",
      "\n",
      "Step 32\n",
      "Current state: 25\n",
      "Q-values for current state: [-0.01130846 -0.00820958 -0.01249554 -0.01486018]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 17\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.01486018419724161\n",
      "New Q-value: -0.01566209390450166\n",
      "\n",
      "Step 33\n",
      "Current state: 17\n",
      "Q-values for current state: [-0.02335166 -0.01384873 -0.01355714 -0.02529402]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 18\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.013557138178781146\n",
      "New Q-value: -0.013820179842808032\n",
      "\n",
      "Step 34\n",
      "Current state: 18\n",
      "Q-values for current state: [-0.01801896 -0.0116529  -0.00651322 -0.01833901]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 17\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.018018959332206527\n",
      "New Q-value: -0.018529980484052636\n",
      "\n",
      "Step 35\n",
      "Current state: 17\n",
      "Q-values for current state: [-0.02335166 -0.01384873 -0.01382018 -0.02529402]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 9\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.02529402430324563\n",
      "New Q-value: -0.02572035926928855\n",
      "\n",
      "Step 36\n",
      "Current state: 9\n",
      "Q-values for current state: [-0.03028031 -0.02058671 -0.02228247 -0.03347311]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 10\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.02228246740750607\n",
      "New Q-value: -0.022470204185873224\n",
      "\n",
      "Step 37\n",
      "Current state: 10\n",
      "Q-values for current state: [-0.02447184 -0.01490509 -0.01613006 -0.02689556]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 18\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.014905089674923792\n",
      "New Q-value: -0.015033336189336412\n",
      "\n",
      "Step 38\n",
      "Current state: 18\n",
      "Q-values for current state: [-0.01852998 -0.0116529  -0.00651322 -0.01833901]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 17\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.018529980484052636\n",
      "New Q-value: -0.018989899520714136\n",
      "\n",
      "Step 39\n",
      "Current state: 17\n",
      "Q-values for current state: [-0.02335166 -0.01384873 -0.01382018 -0.02572036]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 25\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.013848732988969544\n",
      "New Q-value: -0.014243769761561213\n",
      "\n",
      "Step 40\n",
      "Current state: 25\n",
      "Q-values for current state: [-0.01130846 -0.00820958 -0.01249554 -0.01566209]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 26\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.01249554385769304\n",
      "New Q-value: -0.012787045622423736\n",
      "\n",
      "Step 41\n",
      "Current state: 26\n",
      "Q-values for current state: [-0.0100823  -0.00792949 -0.00569533 -0.01067688]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 27\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.0056953279\n",
      "New Q-value: -0.00622079511\n",
      "\n",
      "Step 42\n",
      "Current state: 27\n",
      "Q-values for current state: [-0.00154106 -0.001      -0.003439   -0.00271   ]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 28\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.003439\n",
      "New Q-value: -0.0041901\n",
      "\n",
      "Step 43\n",
      "Current state: 28\n",
      "Q-values for current state: [-0.001      -0.0019     -0.00271    -0.00119765]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 27\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.001\n",
      "New Q-value: -0.0019950000000000002\n",
      "\n",
      "Step 44\n",
      "Current state: 27\n",
      "Q-values for current state: [-0.00154106 -0.001      -0.0041901  -0.00271   ]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 35\n",
      "Reward: -0.01\n",
      "Terminated: True\n",
      "Old Q-value: -0.001\n",
      "New Q-value: -0.0019000000000000002\n",
      "\n",
      "Episode 83\n",
      "Starting state: 0\n",
      "\n",
      "Step 0\n",
      "Current state: 0\n",
      "Q-values for current state: [-0.04059742 -0.0333282  -0.03486623 -0.0402553 ]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 1\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.03486622745043086\n",
      "New Q-value: -0.03500752632691669\n",
      "\n",
      "Step 1\n",
      "Current state: 1\n",
      "Q-values for current state: [-0.04040828 -0.02766233 -0.02957013 -0.03318487]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 0\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.040408279632929256\n",
      "New Q-value: -0.04053363062528159\n",
      "\n",
      "Step 2\n",
      "Current state: 0\n",
      "Q-values for current state: [-0.04059742 -0.0333282  -0.03500753 -0.0402553 ]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 8\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.033328199533108005\n",
      "New Q-value: -0.03343291020838515\n",
      "\n",
      "Step 3\n",
      "Current state: 8\n",
      "Q-values for current state: [-0.03267702 -0.02565822 -0.0274334  -0.04031542]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 8\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.03267702327459431\n",
      "New Q-value: -0.03284685157572283\n",
      "\n",
      "Step 4\n",
      "Current state: 8\n",
      "Q-values for current state: [-0.03284685 -0.02565822 -0.0274334  -0.04031542]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 8\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.03284685157572283\n",
      "New Q-value: -0.03299969704673849\n",
      "\n",
      "Step 5\n",
      "Current state: 8\n",
      "Q-values for current state: [-0.0329997  -0.02565822 -0.0274334  -0.04031542]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 9\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.027433398213550925\n",
      "New Q-value: -0.02764579578856332\n",
      "\n",
      "Step 6\n",
      "Current state: 9\n",
      "Q-values for current state: [-0.03028031 -0.02058671 -0.0224702  -0.03347311]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 8\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.03028030624272798\n",
      "New Q-value: -0.03068980624704313\n",
      "\n",
      "Step 7\n",
      "Current state: 8\n",
      "Q-values for current state: [-0.0329997  -0.02565822 -0.0276458  -0.04031542]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 16\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.025658217143031035\n",
      "New Q-value: -0.02581546662479049\n",
      "\n",
      "Step 8\n",
      "Current state: 16\n",
      "Q-values for current state: [-0.02525187 -0.01813759 -0.01895606 -0.03218482]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 16\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.02525187383360941\n",
      "New Q-value: -0.025449757646311025\n",
      "\n",
      "Step 9\n",
      "Current state: 16\n",
      "Q-values for current state: [-0.02544976 -0.01813759 -0.01895606 -0.03218482]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 16\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.025449757646311025\n",
      "New Q-value: -0.025627853077742478\n",
      "\n",
      "Step 10\n",
      "Current state: 16\n",
      "Q-values for current state: [-0.02562785 -0.01813759 -0.01895606 -0.03218482]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 17\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.018956055873078262\n",
      "New Q-value: -0.0193733673708372\n",
      "\n",
      "Step 11\n",
      "Current state: 17\n",
      "Q-values for current state: [-0.02335166 -0.01424377 -0.01382018 -0.02572036]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 25\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.014243769761561213\n",
      "New Q-value: -0.014599302856893715\n",
      "\n",
      "Step 12\n",
      "Current state: 25\n",
      "Q-values for current state: [-0.01130846 -0.00820958 -0.01278705 -0.01566209]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 33\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.008209579699880247\n",
      "New Q-value: -0.008751645814731783\n",
      "\n",
      "Step 13\n",
      "Current state: 33\n",
      "Q-values for current state: [-0.00382131 -0.00569533 -0.0052517  -0.00866988]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 34\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.005251695000000001\n",
      "New Q-value: -0.006053230500000001\n",
      "\n",
      "Step 14\n",
      "Current state: 34\n",
      "Q-values for current state: [-0.00456953 -0.00468559 -0.003439   -0.00372548]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 42\n",
      "Reward: -0.01\n",
      "Terminated: True\n",
      "Old Q-value: -0.00468559\n",
      "New Q-value: -0.005217031\n",
      "\n",
      "Episode 84\n",
      "Starting state: 0\n",
      "\n",
      "Step 0\n",
      "Current state: 0\n",
      "Q-values for current state: [-0.04059742 -0.03343291 -0.03500753 -0.0402553 ]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 0\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.04059742462978689\n",
      "New Q-value: -0.04071380863660479\n",
      "\n",
      "Step 1\n",
      "Current state: 0\n",
      "Q-values for current state: [-0.04071381 -0.03343291 -0.03500753 -0.0402553 ]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 1\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.03500752632691669\n",
      "New Q-value: -0.035134695315753944\n",
      "\n",
      "Step 2\n",
      "Current state: 1\n",
      "Q-values for current state: [-0.04053363 -0.02766233 -0.02957013 -0.03318487]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 9\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.027662332858199183\n",
      "New Q-value: -0.02785183696874675\n",
      "\n",
      "Step 3\n",
      "Current state: 9\n",
      "Q-values for current state: [-0.03068981 -0.02058671 -0.0224702  -0.03347311]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 10\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.022470204185873224\n",
      "New Q-value: -0.02265135070527286\n",
      "\n",
      "Step 4\n",
      "Current state: 10\n",
      "Q-values for current state: [-0.02447184 -0.01503334 -0.01613006 -0.02689556]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 9\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.024471842637268724\n",
      "New Q-value: -0.024980395769909337\n",
      "\n",
      "Step 5\n",
      "Current state: 9\n",
      "Q-values for current state: [-0.03068981 -0.02058671 -0.02265135 -0.03347311]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 8\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.03068980624704313\n",
      "New Q-value: -0.031073294951693912\n",
      "\n",
      "Step 6\n",
      "Current state: 8\n",
      "Q-values for current state: [-0.0329997  -0.02581547 -0.0276458  -0.04031542]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 8\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.03299969704673849\n",
      "New Q-value: -0.033152196671419734\n",
      "\n",
      "Step 7\n",
      "Current state: 8\n",
      "Q-values for current state: [-0.0331522  -0.02581547 -0.0276458  -0.04031542]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 0\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.04031542348662399\n",
      "New Q-value: -0.040460007607758176\n",
      "\n",
      "Step 8\n",
      "Current state: 0\n",
      "Q-values for current state: [-0.04071381 -0.03343291 -0.0351347  -0.0402553 ]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 0\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.04025530135556968\n",
      "New Q-value: -0.0404058976898093\n",
      "\n",
      "Step 9\n",
      "Current state: 0\n",
      "Q-values for current state: [-0.04071381 -0.03343291 -0.0351347  -0.0404059 ]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 0\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.04071380863660479\n",
      "New Q-value: -0.0408185542427409\n",
      "\n",
      "Step 10\n",
      "Current state: 0\n",
      "Q-values for current state: [-0.04081855 -0.03343291 -0.0351347  -0.0404059 ]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 1\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.035134695315753944\n",
      "New Q-value: -0.03526715029620949\n",
      "\n",
      "Step 11\n",
      "Current state: 1\n",
      "Q-values for current state: [-0.04053363 -0.02785184 -0.02957013 -0.03318487]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 1\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.03318487412301029\n",
      "New Q-value: -0.033512311222740204\n",
      "\n",
      "Step 12\n",
      "Current state: 1\n",
      "Q-values for current state: [-0.04053363 -0.02785184 -0.02957013 -0.03351231]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 2\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.029570130951321463\n",
      "New Q-value: -0.02974739867525094\n",
      "\n",
      "Step 13\n",
      "Current state: 2\n",
      "Q-values for current state: [-0.03297291 -0.02249399 -0.02246611 -0.02836352]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 3\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.02246611388485919\n",
      "New Q-value: -0.0226272070117418\n",
      "\n",
      "Step 14\n",
      "Current state: 3\n",
      "Q-values for current state: [-0.025092   -0.01481794 -0.01657973 -0.02040766]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 4\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.01657972954767421\n",
      "New Q-value: -0.016912940721220543\n",
      "\n",
      "Step 15\n",
      "Current state: 4\n",
      "Q-values for current state: [-0.01778334 -0.01043352 -0.01203195 -0.01337713]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 3\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.0177833436880325\n",
      "New Q-value: -0.018412713834597777\n",
      "\n",
      "Step 16\n",
      "Current state: 3\n",
      "Q-values for current state: [-0.025092   -0.01481794 -0.01691294 -0.02040766]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 3\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.02040766018323323\n",
      "New Q-value: -0.020774598680278433\n",
      "\n",
      "Step 17\n",
      "Current state: 3\n",
      "Q-values for current state: [-0.025092   -0.01481794 -0.01691294 -0.0207746 ]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 11\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.014817942267037138\n",
      "New Q-value: -0.015127714814186592\n",
      "\n",
      "Step 18\n",
      "Current state: 11\n",
      "Q-values for current state: [-0.01456319 -0.00833228 -0.01310779 -0.0173195 ]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 3\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.017319499233757606\n",
      "New Q-value: -0.01802468221772957\n",
      "\n",
      "Step 19\n",
      "Current state: 3\n",
      "Q-values for current state: [-0.025092   -0.01512771 -0.01691294 -0.0207746 ]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 3\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.020774598680278433\n",
      "New Q-value: -0.021134271719598315\n",
      "\n",
      "Step 20\n",
      "Current state: 3\n",
      "Q-values for current state: [-0.025092   -0.01512771 -0.01691294 -0.02113427]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 3\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.021134271719598315\n",
      "New Q-value: -0.021457977454986208\n",
      "\n",
      "Step 21\n",
      "Current state: 3\n",
      "Q-values for current state: [-0.025092   -0.01512771 -0.01691294 -0.02145798]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 2\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.025092003114232292\n",
      "New Q-value: -0.025719731584069594\n",
      "\n",
      "Step 22\n",
      "Current state: 2\n",
      "Q-values for current state: [-0.03297291 -0.02249399 -0.02262721 -0.02836352]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 1\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.032972913329123846\n",
      "New Q-value: -0.0333215465082424\n",
      "\n",
      "Step 23\n",
      "Current state: 1\n",
      "Q-values for current state: [-0.04053363 -0.02785184 -0.0297474  -0.03351231]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 1\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.033512311222740204\n",
      "New Q-value: -0.033807004612497124\n",
      "\n",
      "Step 24\n",
      "Current state: 1\n",
      "Q-values for current state: [-0.04053363 -0.02785184 -0.0297474  -0.033807  ]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 9\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.02785183696874675\n",
      "New Q-value: -0.028022390668239562\n",
      "\n",
      "Step 25\n",
      "Current state: 9\n",
      "Q-values for current state: [-0.03107329 -0.02058671 -0.02265135 -0.03347311]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 17\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.020586709435447225\n",
      "New Q-value: -0.020840955576969265\n",
      "\n",
      "Step 26\n",
      "Current state: 17\n",
      "Q-values for current state: [-0.02335166 -0.0145993  -0.01382018 -0.02572036]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 16\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.023351655284386624\n",
      "New Q-value: -0.02373956095201052\n",
      "\n",
      "Step 27\n",
      "Current state: 16\n",
      "Q-values for current state: [-0.02562785 -0.01813759 -0.01937337 -0.03218482]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 24\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.018137591537500584\n",
      "New Q-value: -0.018426320787210893\n",
      "\n",
      "Step 28\n",
      "Current state: 24\n",
      "Q-values for current state: [-0.01403369 -0.01221246 -0.01160514 -0.01929682]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 25\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.011605141089056505\n",
      "New Q-value: -0.012276033332550373\n",
      "\n",
      "Step 29\n",
      "Current state: 25\n",
      "Q-values for current state: [-0.01130846 -0.00875165 -0.01278705 -0.01566209]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 33\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.008751645814731783\n",
      "New Q-value: -0.009239505318098164\n",
      "\n",
      "Step 30\n",
      "Current state: 33\n",
      "Q-values for current state: [-0.00382131 -0.00569533 -0.00605323 -0.00866988]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 32\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.0038213061562058907\n",
      "New Q-value: -0.005107196259291192\n",
      "\n",
      "Step 31\n",
      "Current state: 32\n",
      "Q-values for current state: [-0.00943307 -0.00838554 -0.0070318  -0.00745262]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 32\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.00943307331523905\n",
      "New Q-value: -0.010157786702421037\n",
      "\n",
      "Step 32\n",
      "Current state: 32\n",
      "Q-values for current state: [-0.01015779 -0.00838554 -0.0070318  -0.00745262]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 24\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.007452621299181697\n",
      "New Q-value: -0.008867542674169542\n",
      "\n",
      "Step 33\n",
      "Current state: 24\n",
      "Q-values for current state: [-0.01403369 -0.01221246 -0.01227603 -0.01929682]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 25\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.012276033332550373\n",
      "New Q-value: -0.012926183004514662\n",
      "\n",
      "Step 34\n",
      "Current state: 25\n",
      "Q-values for current state: [-0.01130846 -0.00923951 -0.01278705 -0.01566209]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 26\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.012787045622423736\n",
      "New Q-value: -0.013099316595631363\n",
      "\n",
      "Step 35\n",
      "Current state: 26\n",
      "Q-values for current state: [-0.0100823  -0.00792949 -0.0062208  -0.01067688]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 18\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.010676878825148686\n",
      "New Q-value: -0.011227946424538818\n",
      "\n",
      "Step 36\n",
      "Current state: 18\n",
      "Q-values for current state: [-0.0189899  -0.0116529  -0.00651322 -0.01833901]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 26\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.011652903184320265\n",
      "New Q-value: -0.012078588401338238\n",
      "\n",
      "Step 37\n",
      "Current state: 26\n",
      "Q-values for current state: [-0.0100823  -0.00792949 -0.0062208  -0.01122795]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 27\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.00622079511\n",
      "New Q-value: -0.0067451159332974995\n",
      "\n",
      "Step 38\n",
      "Current state: 27\n",
      "Q-values for current state: [-0.00154106 -0.0019     -0.0041901  -0.00271   ]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 35\n",
      "Reward: -0.01\n",
      "Terminated: True\n",
      "Old Q-value: -0.0019000000000000002\n",
      "New Q-value: -0.00271\n",
      "\n",
      "Episode 85\n",
      "Starting state: 0\n",
      "\n",
      "Step 0\n",
      "Current state: 0\n",
      "Q-values for current state: [-0.04081855 -0.03343291 -0.03526715 -0.0404059 ]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 8\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.03343291020838515\n",
      "New Q-value: -0.033542088516901734\n",
      "\n",
      "Step 1\n",
      "Current state: 8\n",
      "Q-values for current state: [-0.0331522  -0.02581547 -0.0276458  -0.04046001]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 0\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.040460007607758176\n",
      "New Q-value: -0.04060050525608802\n",
      "\n",
      "Step 2\n",
      "Current state: 0\n",
      "Q-values for current state: [-0.04081855 -0.03354209 -0.03526715 -0.0404059 ]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 0\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.0408185542427409\n",
      "New Q-value: -0.04092319722757247\n",
      "\n",
      "Step 3\n",
      "Current state: 0\n",
      "Q-values for current state: [-0.0409232  -0.03354209 -0.03526715 -0.0404059 ]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 0\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.0404058976898093\n",
      "New Q-value: -0.040551806329934034\n",
      "\n",
      "Step 4\n",
      "Current state: 0\n",
      "Q-values for current state: [-0.0409232  -0.03354209 -0.03526715 -0.04055181]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 8\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.033542088516901734\n",
      "New Q-value: -0.03364034899456666\n",
      "\n",
      "Step 5\n",
      "Current state: 8\n",
      "Q-values for current state: [-0.0331522  -0.02581547 -0.0276458  -0.04060051]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 9\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.02764579578856332\n",
      "New Q-value: -0.027861106989519067\n",
      "\n",
      "Step 6\n",
      "Current state: 9\n",
      "Q-values for current state: [-0.03107329 -0.02084096 -0.02265135 -0.03347311]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 17\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.020840955576969265\n",
      "New Q-value: -0.0210697771043391\n",
      "\n",
      "Step 7\n",
      "Current state: 17\n",
      "Q-values for current state: [-0.02373956 -0.0145993  -0.01382018 -0.02572036]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 16\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.02373956095201052\n",
      "New Q-value: -0.0241161053315945\n",
      "\n",
      "Step 8\n",
      "Current state: 16\n",
      "Q-values for current state: [-0.02562785 -0.01842632 -0.01937337 -0.03218482]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 17\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.0193733673708372\n",
      "New Q-value: -0.019748947718820242\n",
      "\n",
      "Step 9\n",
      "Current state: 17\n",
      "Q-values for current state: [-0.02411611 -0.0145993  -0.01382018 -0.02572036]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 9\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.02572035926928855\n",
      "New Q-value: -0.026149952167271912\n",
      "\n",
      "Step 10\n",
      "Current state: 9\n",
      "Q-values for current state: [-0.03107329 -0.02106978 -0.02265135 -0.03347311]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 10\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.02265135070527286\n",
      "New Q-value: -0.022814382572732536\n",
      "\n",
      "Step 11\n",
      "Current state: 10\n",
      "Q-values for current state: [-0.0249804  -0.01503334 -0.01613006 -0.02689556]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 9\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.024980395769909337\n",
      "New Q-value: -0.02548398501783062\n",
      "\n",
      "Step 12\n",
      "Current state: 9\n",
      "Q-values for current state: [-0.03107329 -0.02106978 -0.02281438 -0.03347311]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 10\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.022814382572732536\n",
      "New Q-value: -0.022961111253446242\n",
      "\n",
      "Step 13\n",
      "Current state: 10\n",
      "Q-values for current state: [-0.02548399 -0.01503334 -0.01613006 -0.02689556]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 2\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.026895561869449428\n",
      "New Q-value: -0.027342934463765015\n",
      "\n",
      "Step 14\n",
      "Current state: 2\n",
      "Q-values for current state: [-0.03332155 -0.02249399 -0.02262721 -0.02836352]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 10\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.022493987171163472\n",
      "New Q-value: -0.022672755392034085\n",
      "\n",
      "Step 15\n",
      "Current state: 10\n",
      "Q-values for current state: [-0.02548399 -0.01503334 -0.01613006 -0.02734293]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 11\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.01613006167176679\n",
      "New Q-value: -0.016308622278443277\n",
      "\n",
      "Step 16\n",
      "Current state: 11\n",
      "Q-values for current state: [-0.01456319 -0.00833228 -0.01310779 -0.01802468]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 3\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.01802468221772957\n",
      "New Q-value: -0.01865934690330434\n",
      "\n",
      "Step 17\n",
      "Current state: 3\n",
      "Q-values for current state: [-0.02571973 -0.01512771 -0.01691294 -0.02145798]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 2\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.025719731584069594\n",
      "New Q-value: -0.026297343091778107\n",
      "\n",
      "Step 18\n",
      "Current state: 2\n",
      "Q-values for current state: [-0.03332155 -0.02267276 -0.02262721 -0.02836352]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 1\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.0333215465082424\n",
      "New Q-value: -0.03365151897090092\n",
      "\n",
      "Step 19\n",
      "Current state: 1\n",
      "Q-values for current state: [-0.04053363 -0.02802239 -0.0297474  -0.033807  ]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 1\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.033807004612497124\n",
      "New Q-value: -0.03408843126473017\n",
      "\n",
      "Step 20\n",
      "Current state: 1\n",
      "Q-values for current state: [-0.04053363 -0.02802239 -0.0297474  -0.03408843]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 2\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.02974739867525094\n",
      "New Q-value: -0.029922243473841318\n",
      "\n",
      "Step 21\n",
      "Current state: 2\n",
      "Q-values for current state: [-0.03365152 -0.02267276 -0.02262721 -0.02836352]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 3\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.0226272070117418\n",
      "New Q-value: -0.022801619217915344\n",
      "\n",
      "Step 22\n",
      "Current state: 3\n",
      "Q-values for current state: [-0.02629734 -0.01512771 -0.01691294 -0.02145798]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 4\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.016912940721220543\n",
      "New Q-value: -0.017212830777412244\n",
      "\n",
      "Step 23\n",
      "Current state: 4\n",
      "Q-values for current state: [-0.01841271 -0.01043352 -0.01203195 -0.01337713]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 4\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.013377126057624311\n",
      "New Q-value: -0.014030597580175636\n",
      "\n",
      "Step 24\n",
      "Current state: 4\n",
      "Q-values for current state: [-0.01841271 -0.01043352 -0.01203195 -0.0140306 ]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 4\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.014030597580175636\n",
      "New Q-value: -0.014618721950471827\n",
      "\n",
      "Step 25\n",
      "Current state: 4\n",
      "Q-values for current state: [-0.01841271 -0.01043352 -0.01203195 -0.01461872]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 4\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.014618721950471827\n",
      "New Q-value: -0.0151480338837384\n",
      "\n",
      "Step 26\n",
      "Current state: 4\n",
      "Q-values for current state: [-0.01841271 -0.01043352 -0.01203195 -0.01514803]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 5\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.012031948712560588\n",
      "New Q-value: -0.012734393107683352\n",
      "\n",
      "Step 27\n",
      "Current state: 5\n",
      "Q-values for current state: [-0.00953304 -0.01081932 -0.0097495  -0.01040174]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 4\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.009533044909250768\n",
      "New Q-value: -0.010570924546639448\n",
      "\n",
      "Step 28\n",
      "Current state: 4\n",
      "Q-values for current state: [-0.01841271 -0.01043352 -0.01273439 -0.01514803]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 4\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.0151480338837384\n",
      "New Q-value: -0.015624414623678317\n",
      "\n",
      "Step 29\n",
      "Current state: 4\n",
      "Q-values for current state: [-0.01841271 -0.01043352 -0.01273439 -0.01562441]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 3\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.018412713834597777\n",
      "New Q-value: -0.019008575358485727\n",
      "\n",
      "Step 30\n",
      "Current state: 3\n",
      "Q-values for current state: [-0.02629734 -0.01512771 -0.01721283 -0.02145798]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 3\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.021457977454986208\n",
      "New Q-value: -0.021749312616835312\n",
      "\n",
      "Step 31\n",
      "Current state: 3\n",
      "Q-values for current state: [-0.02629734 -0.01512771 -0.01721283 -0.02174931]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 11\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.015127714814186592\n",
      "New Q-value: -0.0154065101066211\n",
      "\n",
      "Step 32\n",
      "Current state: 11\n",
      "Q-values for current state: [-0.01456319 -0.00833228 -0.01310779 -0.01865935]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 12\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.013107794924753505\n",
      "New Q-value: -0.013527669162047808\n",
      "\n",
      "Step 33\n",
      "Current state: 12\n",
      "Q-values for current state: [-0.01017109 -0.00843724 -0.01215002 -0.00769109]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 4\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.007691091892312146\n",
      "New Q-value: -0.008913166831394688\n",
      "\n",
      "Step 34\n",
      "Current state: 4\n",
      "Q-values for current state: [-0.01900858 -0.01043352 -0.01273439 -0.01562441]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 3\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.019008575358485727\n",
      "New Q-value: -0.01957133628276616\n",
      "\n",
      "Step 35\n",
      "Current state: 3\n",
      "Q-values for current state: [-0.02629734 -0.01540651 -0.01721283 -0.02174931]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 2\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.026297343091778107\n",
      "New Q-value: -0.026821520544843533\n",
      "\n",
      "Step 36\n",
      "Current state: 2\n",
      "Q-values for current state: [-0.03365152 -0.02267276 -0.02280162 -0.02836352]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 3\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.022801619217915344\n",
      "New Q-value: -0.022985075756252814\n",
      "\n",
      "Step 37\n",
      "Current state: 3\n",
      "Q-values for current state: [-0.02682152 -0.01540651 -0.01721283 -0.02174931]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 11\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.0154065101066211\n",
      "New Q-value: -0.01565742586981216\n",
      "\n",
      "Step 38\n",
      "Current state: 11\n",
      "Q-values for current state: [-0.01456319 -0.00833228 -0.01352767 -0.01865935]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 3\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.01865934690330434\n",
      "New Q-value: -0.019280867670606064\n",
      "\n",
      "Step 39\n",
      "Current state: 3\n",
      "Q-values for current state: [-0.02682152 -0.01565743 -0.01721283 -0.02174931]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 2\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.026821520544843533\n",
      "New Q-value: -0.027293280252602418\n",
      "\n",
      "Step 40\n",
      "Current state: 2\n",
      "Q-values for current state: [-0.03365152 -0.02267276 -0.02298508 -0.02836352]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 2\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.028363517253115412\n",
      "New Q-value: -0.02868107729004711\n",
      "\n",
      "Step 41\n",
      "Current state: 2\n",
      "Q-values for current state: [-0.03365152 -0.02267276 -0.02298508 -0.02868108]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 2\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.02868107729004711\n",
      "New Q-value: -0.028966881323285638\n",
      "\n",
      "Step 42\n",
      "Current state: 2\n",
      "Q-values for current state: [-0.03365152 -0.02267276 -0.02298508 -0.02896688]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 3\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.022985075756252814\n",
      "New Q-value: -0.023174023638259687\n",
      "\n",
      "Step 43\n",
      "Current state: 3\n",
      "Q-values for current state: [-0.02729328 -0.01565743 -0.01721283 -0.02174931]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 3\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.021749312616835312\n",
      "New Q-value: -0.022061836812783935\n",
      "\n",
      "Step 44\n",
      "Current state: 3\n",
      "Q-values for current state: [-0.02729328 -0.01565743 -0.01721283 -0.02206184]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 11\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.01565742586981216\n",
      "New Q-value: -0.01588325005668411\n",
      "\n",
      "Step 45\n",
      "Current state: 11\n",
      "Q-values for current state: [-0.01456319 -0.00833228 -0.01352767 -0.01928087]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 19\n",
      "Reward: -0.01\n",
      "Terminated: True\n",
      "Old Q-value: -0.008332281830033343\n",
      "New Q-value: -0.008499053647030009\n",
      "\n",
      "Episode 86\n",
      "Starting state: 0\n",
      "\n",
      "Step 0\n",
      "Current state: 0\n",
      "Q-values for current state: [-0.0409232  -0.03364035 -0.03526715 -0.04055181]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 1\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.03526715029620949\n",
      "New Q-value: -0.0354025623800713\n",
      "\n",
      "Step 1\n",
      "Current state: 1\n",
      "Q-values for current state: [-0.04053363 -0.02802239 -0.02992224 -0.03408843]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 1\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.03408843126473017\n",
      "New Q-value: -0.034341715251739915\n",
      "\n",
      "Step 2\n",
      "Current state: 1\n",
      "Q-values for current state: [-0.04053363 -0.02802239 -0.02992224 -0.03434172]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 1\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.034341715251739915\n",
      "New Q-value: -0.03456967084004868\n",
      "\n",
      "Step 3\n",
      "Current state: 1\n",
      "Q-values for current state: [-0.04053363 -0.02802239 -0.02992224 -0.03456967]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 1\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.03456967084004868\n",
      "New Q-value: -0.03477483086952657\n",
      "\n",
      "Step 4\n",
      "Current state: 1\n",
      "Q-values for current state: [-0.04053363 -0.02802239 -0.02992224 -0.03477483]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 0\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.04053363062528159\n",
      "New Q-value: -0.04067610071723726\n",
      "\n",
      "Step 5\n",
      "Current state: 0\n",
      "Q-values for current state: [-0.0409232  -0.03364035 -0.03540256 -0.04055181]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 1\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.0354025623800713\n",
      "New Q-value: -0.035524433255546925\n",
      "\n",
      "Step 6\n",
      "Current state: 1\n",
      "Q-values for current state: [-0.0406761  -0.02802239 -0.02992224 -0.03477483]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 9\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.028022390668239562\n",
      "New Q-value: -0.028221780426327822\n",
      "\n",
      "Step 7\n",
      "Current state: 9\n",
      "Q-values for current state: [-0.03107329 -0.02106978 -0.02296111 -0.03347311]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 1\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.033473111040014755\n",
      "New Q-value: -0.03380686907651442\n",
      "\n",
      "Step 8\n",
      "Current state: 1\n",
      "Q-values for current state: [-0.0406761  -0.02822178 -0.02992224 -0.03477483]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 9\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.028221780426327822\n",
      "New Q-value: -0.028401231208607255\n",
      "\n",
      "Step 9\n",
      "Current state: 9\n",
      "Q-values for current state: [-0.03107329 -0.02106978 -0.02296111 -0.03380687]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 10\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.022961111253446242\n",
      "New Q-value: -0.023093167066088577\n",
      "\n",
      "Step 10\n",
      "Current state: 10\n",
      "Q-values for current state: [-0.02548399 -0.01503334 -0.01630862 -0.02734293]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 11\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.016308622278443277\n",
      "New Q-value: -0.0164851701470668\n",
      "\n",
      "Step 11\n",
      "Current state: 11\n",
      "Q-values for current state: [-0.01456319 -0.00849905 -0.01352767 -0.01928087]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 19\n",
      "Reward: -0.01\n",
      "Terminated: True\n",
      "Old Q-value: -0.008499053647030009\n",
      "New Q-value: -0.008649148282327008\n",
      "\n",
      "Episode 87\n",
      "Starting state: 0\n",
      "\n",
      "Step 0\n",
      "Current state: 0\n",
      "Q-values for current state: [-0.0409232  -0.03364035 -0.03552443 -0.04055181]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 8\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.03364034899456666\n",
      "New Q-value: -0.033728783424465086\n",
      "\n",
      "Step 1\n",
      "Current state: 8\n",
      "Q-values for current state: [-0.0331522  -0.02581547 -0.02786111 -0.04060051]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 16\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.02581546662479049\n",
      "New Q-value: -0.025984420437096475\n",
      "\n",
      "Step 2\n",
      "Current state: 16\n",
      "Q-values for current state: [-0.02562785 -0.01842632 -0.01974895 -0.03218482]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 8\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.03218481574023736\n",
      "New Q-value: -0.03243485410773779\n",
      "\n",
      "Step 3\n",
      "Current state: 8\n",
      "Q-values for current state: [-0.0331522  -0.02598442 -0.02786111 -0.04060051]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 16\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.025984420437096475\n",
      "New Q-value: -0.026136478868171862\n",
      "\n",
      "Step 4\n",
      "Current state: 16\n",
      "Q-values for current state: [-0.02562785 -0.01842632 -0.01974895 -0.03243485]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 16\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.025627853077742478\n",
      "New Q-value: -0.025815568244753267\n",
      "\n",
      "Step 5\n",
      "Current state: 16\n",
      "Q-values for current state: [-0.02581557 -0.01842632 -0.01974895 -0.03243485]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 16\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.025815568244753267\n",
      "New Q-value: -0.025984511895062977\n",
      "\n",
      "Step 6\n",
      "Current state: 16\n",
      "Q-values for current state: [-0.02598451 -0.01842632 -0.01974895 -0.03243485]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 24\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.018426320787210893\n",
      "New Q-value: -0.018743872213395817\n",
      "\n",
      "Step 7\n",
      "Current state: 24\n",
      "Q-values for current state: [-0.01403369 -0.01221246 -0.01292618 -0.01929682]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 24\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.014033692811338776\n",
      "New Q-value: -0.014790507035110912\n",
      "\n",
      "Step 8\n",
      "Current state: 24\n",
      "Q-values for current state: [-0.01479051 -0.01221246 -0.01292618 -0.01929682]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 16\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.019296823881667285\n",
      "New Q-value: -0.02014780935377316\n",
      "\n",
      "Step 9\n",
      "Current state: 16\n",
      "Q-values for current state: [-0.02598451 -0.01874387 -0.01974895 -0.03243485]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 16\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.025984511895062977\n",
      "New Q-value: -0.026166728565829283\n",
      "\n",
      "Step 10\n",
      "Current state: 16\n",
      "Q-values for current state: [-0.02616673 -0.01874387 -0.01974895 -0.03243485]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 17\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.019748947718820242\n",
      "New Q-value: -0.02008697003200498\n",
      "\n",
      "Step 11\n",
      "Current state: 17\n",
      "Q-values for current state: [-0.02411611 -0.0145993  -0.01382018 -0.02614995]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 16\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.0241161053315945\n",
      "New Q-value: -0.024485162658707652\n",
      "\n",
      "Step 12\n",
      "Current state: 16\n",
      "Q-values for current state: [-0.02616673 -0.01874387 -0.02008697 -0.03243485]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 24\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.018743872213395817\n",
      "New Q-value: -0.01902966849696225\n",
      "\n",
      "Step 13\n",
      "Current state: 24\n",
      "Q-values for current state: [-0.01479051 -0.01221246 -0.01292618 -0.02014781]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 24\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.014790507035110912\n",
      "New Q-value: -0.015471639836505836\n",
      "\n",
      "Step 14\n",
      "Current state: 24\n",
      "Q-values for current state: [-0.01547164 -0.01221246 -0.01292618 -0.02014781]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 32\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.012212457946379096\n",
      "New Q-value: -0.012659232870447077\n",
      "\n",
      "Step 15\n",
      "Current state: 32\n",
      "Q-values for current state: [-0.01015779 -0.00838554 -0.0070318  -0.00886754]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 24\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.008867542674169542\n",
      "New Q-value: -0.01018341552944506\n",
      "\n",
      "Step 16\n",
      "Current state: 24\n",
      "Q-values for current state: [-0.01547164 -0.01265923 -0.01292618 -0.02014781]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 16\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.02014780935377316\n",
      "New Q-value: -0.020940846925607257\n",
      "\n",
      "Step 17\n",
      "Current state: 16\n",
      "Q-values for current state: [-0.02616673 -0.01902967 -0.02008697 -0.03243485]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 17\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.02008697003200498\n",
      "New Q-value: -0.020391190113871246\n",
      "\n",
      "Step 18\n",
      "Current state: 17\n",
      "Q-values for current state: [-0.02448516 -0.0145993  -0.01382018 -0.02614995]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 18\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.013820179842808032\n",
      "New Q-value: -0.01405691734043223\n",
      "\n",
      "Step 19\n",
      "Current state: 18\n",
      "Q-values for current state: [-0.0189899  -0.01207859 -0.00651322 -0.01833901]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 10\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.018339009120227106\n",
      "New Q-value: -0.018933275146191354\n",
      "\n",
      "Step 20\n",
      "Current state: 10\n",
      "Q-values for current state: [-0.02548399 -0.01503334 -0.01648517 -0.02734293]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 9\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.02548398501783062\n",
      "New Q-value: -0.025937215340959774\n",
      "\n",
      "Step 21\n",
      "Current state: 9\n",
      "Q-values for current state: [-0.03107329 -0.02106978 -0.02309317 -0.03380687]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 10\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.023093167066088577\n",
      "New Q-value: -0.023212017297466678\n",
      "\n",
      "Step 22\n",
      "Current state: 10\n",
      "Q-values for current state: [-0.02593722 -0.01503334 -0.01648517 -0.02734293]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 11\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.0164851701470668\n",
      "New Q-value: -0.016658322219181187\n",
      "\n",
      "Step 23\n",
      "Current state: 11\n",
      "Q-values for current state: [-0.01456319 -0.00864915 -0.01352767 -0.01928087]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 19\n",
      "Reward: -0.01\n",
      "Terminated: True\n",
      "Old Q-value: -0.008649148282327008\n",
      "New Q-value: -0.008784233454094306\n",
      "\n",
      "Episode 88\n",
      "Starting state: 0\n",
      "\n",
      "Step 0\n",
      "Current state: 0\n",
      "Q-values for current state: [-0.0409232  -0.03372878 -0.03552443 -0.04055181]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 1\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.035524433255546925\n",
      "New Q-value: -0.03567010689480992\n",
      "\n",
      "Step 1\n",
      "Current state: 1\n",
      "Q-values for current state: [-0.0406761  -0.02840123 -0.02992224 -0.03477483]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 0\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.04067610071723726\n",
      "New Q-value: -0.04081272507083772\n",
      "\n",
      "Step 2\n",
      "Current state: 0\n",
      "Q-values for current state: [-0.0409232  -0.03372878 -0.03567011 -0.04055181]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 0\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.04092319722757247\n",
      "New Q-value: -0.04103511193013941\n",
      "\n",
      "Step 3\n",
      "Current state: 0\n",
      "Q-values for current state: [-0.04103511 -0.03372878 -0.03567011 -0.04055181]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 1\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.03567010689480992\n",
      "New Q-value: -0.03580121317014662\n",
      "\n",
      "Step 4\n",
      "Current state: 1\n",
      "Q-values for current state: [-0.04081273 -0.02840123 -0.02992224 -0.03477483]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 9\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.028401231208607255\n",
      "New Q-value: -0.028562736912658745\n",
      "\n",
      "Step 5\n",
      "Current state: 9\n",
      "Q-values for current state: [-0.03107329 -0.02106978 -0.02321202 -0.03380687]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 8\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.031073294951693912\n",
      "New Q-value: -0.03144893094900085\n",
      "\n",
      "Step 6\n",
      "Current state: 8\n",
      "Q-values for current state: [-0.0331522  -0.02613648 -0.02786111 -0.04060051]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 0\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.04060050525608802\n",
      "New Q-value: -0.0407446891558034\n",
      "\n",
      "Step 7\n",
      "Current state: 0\n",
      "Q-values for current state: [-0.04103511 -0.03372878 -0.03580121 -0.04055181]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 0\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.04103511193013941\n",
      "New Q-value: -0.041135835162449653\n",
      "\n",
      "Step 8\n",
      "Current state: 0\n",
      "Q-values for current state: [-0.04113584 -0.03372878 -0.03580121 -0.04055181]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 0\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.040551806329934034\n",
      "New Q-value: -0.040700860122264815\n",
      "\n",
      "Step 9\n",
      "Current state: 0\n",
      "Q-values for current state: [-0.04113584 -0.03372878 -0.03580121 -0.04070086]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 0\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.041135835162449653\n",
      "New Q-value: -0.04122648607152887\n",
      "\n",
      "Step 10\n",
      "Current state: 0\n",
      "Q-values for current state: [-0.04122649 -0.03372878 -0.03580121 -0.04070086]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 1\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.03580121317014662\n",
      "New Q-value: -0.03593455185983454\n",
      "\n",
      "Step 11\n",
      "Current state: 1\n",
      "Q-values for current state: [-0.04081273 -0.02856274 -0.02992224 -0.03477483]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 1\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.03477483086952657\n",
      "New Q-value: -0.03501080778927649\n",
      "\n",
      "Step 12\n",
      "Current state: 1\n",
      "Q-values for current state: [-0.04081273 -0.02856274 -0.02992224 -0.03501081]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 1\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.03501080778927649\n",
      "New Q-value: -0.03522318701705142\n",
      "\n",
      "Step 13\n",
      "Current state: 1\n",
      "Q-values for current state: [-0.04081273 -0.02856274 -0.02992224 -0.03522319]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 2\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.029922243473841318\n",
      "New Q-value: -0.030083930888700424\n",
      "\n",
      "Step 14\n",
      "Current state: 2\n",
      "Q-values for current state: [-0.03365152 -0.02267276 -0.02317402 -0.02896688]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 3\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.023174023638259687\n",
      "New Q-value: -0.02336553002981871\n",
      "\n",
      "Step 15\n",
      "Current state: 3\n",
      "Q-values for current state: [-0.02729328 -0.01588325 -0.01721283 -0.02206184]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 3\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.022061836812783935\n",
      "New Q-value: -0.02236456188689053\n",
      "\n",
      "Step 16\n",
      "Current state: 3\n",
      "Q-values for current state: [-0.02729328 -0.01588325 -0.01721283 -0.02236456]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 11\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.01588325005668411\n",
      "New Q-value: -0.01612942722915466\n",
      "\n",
      "Step 17\n",
      "Current state: 11\n",
      "Q-values for current state: [-0.01456319 -0.00878423 -0.01352767 -0.01928087]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 12\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.013527669162047808\n",
      "New Q-value: -0.013976440211990522\n",
      "\n",
      "Step 18\n",
      "Current state: 12\n",
      "Q-values for current state: [-0.01017109 -0.00843724 -0.01215002 -0.00891317]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 20\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.008437241748921\n",
      "New Q-value: -0.0088509675740289\n",
      "\n",
      "Step 19\n",
      "Current state: 20\n",
      "Q-values for current state: [-0.00468559 -0.00271    -0.00511371 -0.00757364]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 19\n",
      "Reward: -0.01\n",
      "Terminated: True\n",
      "Old Q-value: -0.00468559\n",
      "New Q-value: -0.005217031\n",
      "\n",
      "Episode 89\n",
      "Starting state: 0\n",
      "\n",
      "Step 0\n",
      "Current state: 0\n",
      "Q-values for current state: [-0.04122649 -0.03372878 -0.03593455 -0.04070086]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 0\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.04122648607152887\n",
      "New Q-value: -0.04130807188970017\n",
      "\n",
      "Step 1\n",
      "Current state: 0\n",
      "Q-values for current state: [-0.04130807 -0.03372878 -0.03593455 -0.04070086]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 1\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.03593455185983454\n",
      "New Q-value: -0.036054556680553664\n",
      "\n",
      "Step 2\n",
      "Current state: 1\n",
      "Q-values for current state: [-0.04081273 -0.02856274 -0.03008393 -0.03522319]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 2\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.030083930888700424\n",
      "New Q-value: -0.03022944956207362\n",
      "\n",
      "Step 3\n",
      "Current state: 2\n",
      "Q-values for current state: [-0.03365152 -0.02267276 -0.02336553 -0.02896688]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 3\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.02336553002981871\n",
      "New Q-value: -0.02356127261360653\n",
      "\n",
      "Step 4\n",
      "Current state: 3\n",
      "Q-values for current state: [-0.02729328 -0.01612943 -0.01721283 -0.02236456]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 11\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.01612942722915466\n",
      "New Q-value: -0.016350986684378153\n",
      "\n",
      "Step 5\n",
      "Current state: 11\n",
      "Q-values for current state: [-0.01456319 -0.00878423 -0.01397644 -0.01928087]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 12\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.013976440211990522\n",
      "New Q-value: -0.014419638110324215\n",
      "\n",
      "Step 6\n",
      "Current state: 12\n",
      "Q-values for current state: [-0.01017109 -0.00885097 -0.01215002 -0.00891317]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 11\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.010171090435584411\n",
      "New Q-value: -0.01098848357016493\n",
      "\n",
      "Step 7\n",
      "Current state: 11\n",
      "Q-values for current state: [-0.01456319 -0.00878423 -0.01441964 -0.01928087]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 10\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.014563192877836742\n",
      "New Q-value: -0.015535040528040027\n",
      "\n",
      "Step 8\n",
      "Current state: 10\n",
      "Q-values for current state: [-0.02593722 -0.01503334 -0.01665832 -0.02734293]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 9\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.025937215340959774\n",
      "New Q-value: -0.026345122631776012\n",
      "\n",
      "Step 9\n",
      "Current state: 9\n",
      "Q-values for current state: [-0.03144893 -0.02106978 -0.02321202 -0.03380687]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 17\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.0210697771043391\n",
      "New Q-value: -0.021298206541246253\n",
      "\n",
      "Step 10\n",
      "Current state: 17\n",
      "Q-values for current state: [-0.02448516 -0.0145993  -0.01405692 -0.02614995]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 18\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.01405691734043223\n",
      "New Q-value: -0.014269981088294007\n",
      "\n",
      "Step 11\n",
      "Current state: 18\n",
      "Q-values for current state: [-0.0189899  -0.01207859 -0.00651322 -0.01893328]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 26\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.012078588401338238\n",
      "New Q-value: -0.012511515574867677\n",
      "\n",
      "Step 12\n",
      "Current state: 26\n",
      "Q-values for current state: [-0.0100823  -0.00792949 -0.00674512 -0.01122795]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 25\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.010082297416138608\n",
      "New Q-value: -0.010951820679744073\n",
      "\n",
      "Step 13\n",
      "Current state: 25\n",
      "Q-values for current state: [-0.01130846 -0.00923951 -0.01309932 -0.01566209]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 26\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.013099316595631363\n",
      "New Q-value: -0.01343017094973149\n",
      "\n",
      "Step 14\n",
      "Current state: 26\n",
      "Q-values for current state: [-0.01095182 -0.00792949 -0.00674512 -0.01122795]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 18\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.011227946424538818\n",
      "New Q-value: -0.011723907263989936\n",
      "\n",
      "Step 15\n",
      "Current state: 18\n",
      "Q-values for current state: [-0.0189899  -0.01251152 -0.00651322 -0.01893328]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 17\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.018989899520714136\n",
      "New Q-value: -0.019446557772030654\n",
      "\n",
      "Step 16\n",
      "Current state: 17\n",
      "Q-values for current state: [-0.02448516 -0.0145993  -0.01426998 -0.02614995]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 16\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.024485162658707652\n",
      "New Q-value: -0.0248444649000483\n",
      "\n",
      "Step 17\n",
      "Current state: 16\n",
      "Q-values for current state: [-0.02616673 -0.01902967 -0.02039119 -0.03243485]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 24\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.01902966849696225\n",
      "New Q-value: -0.019329328769958496\n",
      "\n",
      "Step 18\n",
      "Current state: 24\n",
      "Q-values for current state: [-0.01547164 -0.01265923 -0.01292618 -0.02094085]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 32\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.012659232870447077\n",
      "New Q-value: -0.01306133030210826\n",
      "\n",
      "Step 19\n",
      "Current state: 32\n",
      "Q-values for current state: [-0.01015779 -0.00838554 -0.0070318  -0.01018342]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 33\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.007031797039009376\n",
      "New Q-value: -0.007813800979741102\n",
      "\n",
      "Step 20\n",
      "Current state: 33\n",
      "Q-values for current state: [-0.0051072  -0.00569533 -0.00605323 -0.00866988]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 25\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.008669881850000302\n",
      "New Q-value: -0.009680646670219598\n",
      "\n",
      "Step 21\n",
      "Current state: 25\n",
      "Q-values for current state: [-0.01130846 -0.00923951 -0.01343017 -0.01566209]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 33\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.009239505318098164\n",
      "New Q-value: -0.00980073843092101\n",
      "\n",
      "Step 22\n",
      "Current state: 33\n",
      "Q-values for current state: [-0.0051072  -0.00569533 -0.00605323 -0.00968065]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 34\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.006053230500000001\n",
      "New Q-value: -0.006774612450000001\n",
      "\n",
      "Step 23\n",
      "Current state: 34\n",
      "Q-values for current state: [-0.00456953 -0.00521703 -0.003439   -0.00372548]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 35\n",
      "Reward: -0.01\n",
      "Terminated: True\n",
      "Old Q-value: -0.003439\n",
      "New Q-value: -0.0040951\n",
      "\n",
      "Episode 90\n",
      "Starting state: 0\n",
      "\n",
      "Step 0\n",
      "Current state: 0\n",
      "Q-values for current state: [-0.04130807 -0.03372878 -0.03605456 -0.04070086]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 0\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.040700860122264815\n",
      "New Q-value: -0.04083500853536252\n",
      "\n",
      "Step 1\n",
      "Current state: 0\n",
      "Q-values for current state: [-0.04130807 -0.03372878 -0.03605456 -0.04083501]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 8\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.033728783424465086\n",
      "New Q-value: -0.0338388705744949\n",
      "\n",
      "Step 2\n",
      "Current state: 8\n",
      "Q-values for current state: [-0.0331522  -0.02613648 -0.02786111 -0.04074469]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 0\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.0407446891558034\n",
      "New Q-value: -0.040884912944800074\n",
      "\n",
      "Step 3\n",
      "Current state: 0\n",
      "Q-values for current state: [-0.04130807 -0.03383887 -0.03605456 -0.04083501]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 1\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.036054556680553664\n",
      "New Q-value: -0.03616256101920088\n",
      "\n",
      "Step 4\n",
      "Current state: 1\n",
      "Q-values for current state: [-0.04081273 -0.02856274 -0.03022945 -0.03522319]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 1\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.03522318701705142\n",
      "New Q-value: -0.035414328322048855\n",
      "\n",
      "Step 5\n",
      "Current state: 1\n",
      "Q-values for current state: [-0.04081273 -0.02856274 -0.03022945 -0.03541433]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 9\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.028562736912658745\n",
      "New Q-value: -0.028729792842811266\n",
      "\n",
      "Step 6\n",
      "Current state: 9\n",
      "Q-values for current state: [-0.03144893 -0.02129821 -0.02321202 -0.03380687]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 17\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.021298206541246253\n",
      "New Q-value: -0.02152403409050956\n",
      "\n",
      "Step 7\n",
      "Current state: 17\n",
      "Q-values for current state: [-0.02484446 -0.0145993  -0.01426998 -0.02614995]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 16\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.0248444649000483\n",
      "New Q-value: -0.025196304643189525\n",
      "\n",
      "Step 8\n",
      "Current state: 16\n",
      "Q-values for current state: [-0.02616673 -0.01932933 -0.02039119 -0.03243485]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 24\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.019329328769958496\n",
      "New Q-value: -0.01962438327839154\n",
      "\n",
      "Step 9\n",
      "Current state: 24\n",
      "Q-values for current state: [-0.01547164 -0.01306133 -0.01292618 -0.02094085]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 25\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.012926183004514662\n",
      "New Q-value: -0.01356463485500069\n",
      "\n",
      "Step 10\n",
      "Current state: 25\n",
      "Q-values for current state: [-0.01130846 -0.00980074 -0.01343017 -0.01566209]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 24\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.011308460009682924\n",
      "New Q-value: -0.012418440387414915\n",
      "\n",
      "Step 11\n",
      "Current state: 24\n",
      "Q-values for current state: [-0.01547164 -0.01306133 -0.01356463 -0.02094085]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 32\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.01306133030210826\n",
      "New Q-value: -0.013497508364972838\n",
      "\n",
      "Step 12\n",
      "Current state: 32\n",
      "Q-values for current state: [-0.01015779 -0.00838554 -0.0078138  -0.01018342]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 40\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.008385544653576\n",
      "New Q-value: -0.0088736951882184\n",
      "\n",
      "Step 13\n",
      "Current state: 40\n",
      "Q-values for current state: [-0.00369645 -0.0042756  -0.003439   -0.00522721]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 48\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.0042756\n",
      "New Q-value: -0.004952065\n",
      "\n",
      "Step 14\n",
      "Current state: 48\n",
      "Q-values for current state: [-0.0019   -0.0019   -0.0019   -0.001095]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 56\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.0019000000000000002\n",
      "New Q-value: -0.00271\n",
      "\n",
      "Step 15\n",
      "Current state: 56\n",
      "Q-values for current state: [-0.001    -0.001    -0.003439  0.      ]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 48\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: -0.001104025\n",
      "\n",
      "Step 16\n",
      "Current state: 48\n",
      "Q-values for current state: [-0.0019   -0.00271  -0.0019   -0.001095]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 48\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.0019000000000000002\n",
      "New Q-value: -0.0028140250000000004\n",
      "\n",
      "Step 17\n",
      "Current state: 48\n",
      "Q-values for current state: [-0.00281403 -0.00271    -0.0019     -0.001095  ]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 49\n",
      "Reward: -0.01\n",
      "Terminated: True\n",
      "Old Q-value: -0.0019000000000000002\n",
      "New Q-value: -0.00271\n",
      "\n",
      "Episode 90 summary:\n",
      "Total reward: -0.18000000000000002\n",
      "Successful episodes so far: []\n",
      "Average reward over last 10 episodes: -0.276\n",
      "\n",
      "Episode 91\n",
      "Starting state: 0\n",
      "\n",
      "Step 0\n",
      "Current state: 0\n",
      "Q-values for current state: [-0.04130807 -0.03383887 -0.03616256 -0.04083501]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 1\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.03616256101920088\n",
      "New Q-value: -0.036275635237347864\n",
      "\n",
      "Step 1\n",
      "Current state: 1\n",
      "Q-values for current state: [-0.04081273 -0.02872979 -0.03022945 -0.03541433]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 2\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.03022944956207362\n",
      "New Q-value: -0.030360416368109495\n",
      "\n",
      "Step 2\n",
      "Current state: 2\n",
      "Q-values for current state: [-0.03365152 -0.02267276 -0.02356127 -0.02896688]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 10\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.022672755392034085\n",
      "New Q-value: -0.022833646790817636\n",
      "\n",
      "Step 3\n",
      "Current state: 10\n",
      "Q-values for current state: [-0.02634512 -0.01503334 -0.01665832 -0.02734293]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 2\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.027342934463765015\n",
      "New Q-value: -0.027777837462516188\n",
      "\n",
      "Step 4\n",
      "Current state: 2\n",
      "Q-values for current state: [-0.03365152 -0.02283365 -0.02356127 -0.02896688]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 2\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.028966881323285638\n",
      "New Q-value: -0.02923938963608475\n",
      "\n",
      "Step 5\n",
      "Current state: 2\n",
      "Q-values for current state: [-0.03365152 -0.02283365 -0.02356127 -0.02923939]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 2\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.02923938963608475\n",
      "New Q-value: -0.02948464711760395\n",
      "\n",
      "Step 6\n",
      "Current state: 2\n",
      "Q-values for current state: [-0.03365152 -0.02283365 -0.02356127 -0.02948465]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 2\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.02948464711760395\n",
      "New Q-value: -0.029705378850971232\n",
      "\n",
      "Step 7\n",
      "Current state: 2\n",
      "Q-values for current state: [-0.03365152 -0.02283365 -0.02356127 -0.02970538]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 10\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.022833646790817636\n",
      "New Q-value: -0.02297844904972283\n",
      "\n",
      "Step 8\n",
      "Current state: 10\n",
      "Q-values for current state: [-0.02634512 -0.01503334 -0.01665832 -0.02777784]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 11\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.016658322219181187\n",
      "New Q-value: -0.01682699217540203\n",
      "\n",
      "Step 9\n",
      "Current state: 11\n",
      "Q-values for current state: [-0.01553504 -0.00878423 -0.01441964 -0.01928087]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 3\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.019280867670606064\n",
      "New Q-value: -0.01990612463856138\n",
      "\n",
      "Step 10\n",
      "Current state: 3\n",
      "Q-values for current state: [-0.02729328 -0.01635099 -0.01721283 -0.02236456]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 2\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.027293280252602418\n",
      "New Q-value: -0.027746904887065844\n",
      "\n",
      "Step 11\n",
      "Current state: 2\n",
      "Q-values for current state: [-0.03365152 -0.02297845 -0.02356127 -0.02970538]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 1\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.03365151897090092\n",
      "New Q-value: -0.0340156973938779\n",
      "\n",
      "Step 12\n",
      "Current state: 1\n",
      "Q-values for current state: [-0.04081273 -0.02872979 -0.03036042 -0.03541433]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 1\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.035414328322048855\n",
      "New Q-value: -0.03560222580991104\n",
      "\n",
      "Step 13\n",
      "Current state: 1\n",
      "Q-values for current state: [-0.04081273 -0.02872979 -0.03036042 -0.03560223]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 9\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.028729792842811266\n",
      "New Q-value: -0.028901596797128545\n",
      "\n",
      "Step 14\n",
      "Current state: 9\n",
      "Q-values for current state: [-0.03144893 -0.02152403 -0.02321202 -0.03380687]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 1\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.03380686907651442\n",
      "New Q-value: -0.03417183386459019\n",
      "\n",
      "Step 15\n",
      "Current state: 1\n",
      "Q-values for current state: [-0.04081273 -0.0289016  -0.03036042 -0.03560223]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 2\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.030360416368109495\n",
      "New Q-value: -0.030507327391022215\n",
      "\n",
      "Step 16\n",
      "Current state: 2\n",
      "Q-values for current state: [-0.0340157  -0.02297845 -0.02356127 -0.02970538]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 2\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.029705378850971232\n",
      "New Q-value: -0.029917793625597777\n",
      "\n",
      "Step 17\n",
      "Current state: 2\n",
      "Q-values for current state: [-0.0340157  -0.02297845 -0.02356127 -0.02991779]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 3\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.02356127261360653\n",
      "New Q-value: -0.023758489087261803\n",
      "\n",
      "Step 18\n",
      "Current state: 3\n",
      "Q-values for current state: [-0.0277469  -0.01635099 -0.01721283 -0.02236456]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 11\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.016350986684378153\n",
      "New Q-value: -0.016550390194079296\n",
      "\n",
      "Step 19\n",
      "Current state: 11\n",
      "Q-values for current state: [-0.01553504 -0.00878423 -0.01441964 -0.01990612]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 12\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.014419638110324215\n",
      "New Q-value: -0.014818516218824539\n",
      "\n",
      "Step 20\n",
      "Current state: 12\n",
      "Q-values for current state: [-0.01098848 -0.00885097 -0.01215002 -0.00891317]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 11\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.01098848357016493\n",
      "New Q-value: -0.011724137391287396\n",
      "\n",
      "Step 21\n",
      "Current state: 11\n",
      "Q-values for current state: [-0.01553504 -0.00878423 -0.01481852 -0.01990612]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 10\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.015535040528040027\n",
      "New Q-value: -0.016409703413222985\n",
      "\n",
      "Step 22\n",
      "Current state: 10\n",
      "Q-values for current state: [-0.02634512 -0.01503334 -0.01682699 -0.02777784]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 11\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.01682699217540203\n",
      "New Q-value: -0.016978795136000785\n",
      "\n",
      "Step 23\n",
      "Current state: 11\n",
      "Q-values for current state: [-0.0164097  -0.00878423 -0.01481852 -0.01990612]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 10\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.016409703413222985\n",
      "New Q-value: -0.017196900009887647\n",
      "\n",
      "Step 24\n",
      "Current state: 10\n",
      "Q-values for current state: [-0.02634512 -0.01503334 -0.0169788  -0.02777784]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 11\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.016978795136000785\n",
      "New Q-value: -0.017115417800539667\n",
      "\n",
      "Step 25\n",
      "Current state: 11\n",
      "Q-values for current state: [-0.0171969  -0.00878423 -0.01481852 -0.01990612]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 3\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.01990612463856138\n",
      "New Q-value: -0.020487799243142778\n",
      "\n",
      "Step 26\n",
      "Current state: 3\n",
      "Q-values for current state: [-0.0277469  -0.01655039 -0.01721283 -0.02236456]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 3\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.02236456188689053\n",
      "New Q-value: -0.022700392766639012\n",
      "\n",
      "Step 27\n",
      "Current state: 3\n",
      "Q-values for current state: [-0.0277469  -0.01655039 -0.01721283 -0.02270039]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 11\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.016550390194079296\n",
      "New Q-value: -0.016729853352810325\n",
      "\n",
      "Step 28\n",
      "Current state: 11\n",
      "Q-values for current state: [-0.0171969  -0.00878423 -0.01481852 -0.0204878 ]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 10\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.017196900009887647\n",
      "New Q-value: -0.01790537694688584\n",
      "\n",
      "Step 29\n",
      "Current state: 10\n",
      "Q-values for current state: [-0.02634512 -0.01503334 -0.01711542 -0.02777784]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 2\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.027777837462516188\n",
      "New Q-value: -0.028183006375988237\n",
      "\n",
      "Step 30\n",
      "Current state: 2\n",
      "Q-values for current state: [-0.0340157  -0.02297845 -0.02375849 -0.02991779]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 3\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.023758489087261803\n",
      "New Q-value: -0.023971976247052603\n",
      "\n",
      "Step 31\n",
      "Current state: 3\n",
      "Q-values for current state: [-0.0277469  -0.01672985 -0.01721283 -0.02270039]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 4\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.017212830777412244\n",
      "New Q-value: -0.017482731827984776\n",
      "\n",
      "Step 32\n",
      "Current state: 4\n",
      "Q-values for current state: [-0.01957134 -0.01043352 -0.01273439 -0.01562441]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 4\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.015624414623678317\n",
      "New Q-value: -0.01605315728962424\n",
      "\n",
      "Step 33\n",
      "Current state: 4\n",
      "Q-values for current state: [-0.01957134 -0.01043352 -0.01273439 -0.01605316]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 5\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.012734393107683352\n",
      "New Q-value: -0.013387156347741543\n",
      "\n",
      "Step 34\n",
      "Current state: 5\n",
      "Q-values for current state: [-0.01057092 -0.01081932 -0.0097495  -0.01040174]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 5\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.01040174378198801\n",
      "New Q-value: -0.011287771954615736\n",
      "\n",
      "Step 35\n",
      "Current state: 5\n",
      "Q-values for current state: [-0.01057092 -0.01081932 -0.0097495  -0.01128777]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 5\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.011287771954615736\n",
      "New Q-value: -0.012085197309980688\n",
      "\n",
      "Step 36\n",
      "Current state: 5\n",
      "Q-values for current state: [-0.01057092 -0.01081932 -0.0097495  -0.0120852 ]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 4\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.010570924546639448\n",
      "New Q-value: -0.011505016220289259\n",
      "\n",
      "Step 37\n",
      "Current state: 4\n",
      "Q-values for current state: [-0.01957134 -0.01043352 -0.01338716 -0.01605316]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 4\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.01605315728962424\n",
      "New Q-value: -0.01643902568897557\n",
      "\n",
      "Step 38\n",
      "Current state: 4\n",
      "Q-values for current state: [-0.01957134 -0.01043352 -0.01338716 -0.01643903]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 5\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.013387156347741543\n",
      "New Q-value: -0.013974643263793914\n",
      "\n",
      "Step 39\n",
      "Current state: 5\n",
      "Q-values for current state: [-0.01150502 -0.01081932 -0.0097495  -0.0120852 ]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 13\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.010819319663375303\n",
      "New Q-value: -0.011233005642037773\n",
      "\n",
      "Step 40\n",
      "Current state: 13\n",
      "Q-values for current state: [-0.01008285 -0.00888859 -0.00521703 -0.00940755]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 12\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.010082850310922748\n",
      "New Q-value: -0.010915407199363219\n",
      "\n",
      "Step 41\n",
      "Current state: 12\n",
      "Q-values for current state: [-0.01172414 -0.00885097 -0.01215002 -0.00891317]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 4\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.008913166831394688\n",
      "New Q-value: -0.010013034276568975\n",
      "\n",
      "Step 42\n",
      "Current state: 4\n",
      "Q-values for current state: [-0.01957134 -0.01043352 -0.01397464 -0.01643903]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 4\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.01643902568897557\n",
      "New Q-value: -0.01678630724839177\n",
      "\n",
      "Step 43\n",
      "Current state: 4\n",
      "Q-values for current state: [-0.01957134 -0.01043352 -0.01397464 -0.01678631]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 12\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.010433517140144804\n",
      "New Q-value: -0.011231007345663069\n",
      "\n",
      "Step 44\n",
      "Current state: 12\n",
      "Q-values for current state: [-0.01172414 -0.00885097 -0.01215002 -0.01001303]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 11\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.011724137391287396\n",
      "New Q-value: -0.012386225830297616\n",
      "\n",
      "Step 45\n",
      "Current state: 11\n",
      "Q-values for current state: [-0.01790538 -0.00878423 -0.01481852 -0.0204878 ]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 3\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.020487799243142778\n",
      "New Q-value: -0.02102835538734548\n",
      "\n",
      "Step 46\n",
      "Current state: 3\n",
      "Q-values for current state: [-0.0277469  -0.01672985 -0.01748273 -0.02270039]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 2\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.027746904887065844\n",
      "New Q-value: -0.02815516705808293\n",
      "\n",
      "Step 47\n",
      "Current state: 2\n",
      "Q-values for current state: [-0.0340157  -0.02297845 -0.02397198 -0.02991779]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 3\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.023971976247052603\n",
      "New Q-value: -0.024164114690864325\n",
      "\n",
      "Step 48\n",
      "Current state: 3\n",
      "Q-values for current state: [-0.02815517 -0.01672985 -0.01748273 -0.02270039]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 11\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.016729853352810325\n",
      "New Q-value: -0.016891370195668252\n",
      "\n",
      "Step 49\n",
      "Current state: 11\n",
      "Q-values for current state: [-0.01790538 -0.00878423 -0.01481852 -0.02102836]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 19\n",
      "Reward: -0.01\n",
      "Terminated: True\n",
      "Old Q-value: -0.008784233454094306\n",
      "New Q-value: -0.008905810108684875\n",
      "\n",
      "Episode 92\n",
      "Starting state: 0\n",
      "\n",
      "Step 0\n",
      "Current state: 0\n",
      "Q-values for current state: [-0.04130807 -0.03383887 -0.03627564 -0.04083501]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 8\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.0338388705744949\n",
      "New Q-value: -0.03393794900952174\n",
      "\n",
      "Step 1\n",
      "Current state: 8\n",
      "Q-values for current state: [-0.0331522  -0.02613648 -0.02786111 -0.04088491]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 0\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.040884912944800074\n",
      "New Q-value: -0.04102052680622463\n",
      "\n",
      "Step 2\n",
      "Current state: 0\n",
      "Q-values for current state: [-0.04130807 -0.03393795 -0.03627564 -0.04083501]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 8\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.03393794900952174\n",
      "New Q-value: -0.034027119601045895\n",
      "\n",
      "Step 3\n",
      "Current state: 8\n",
      "Q-values for current state: [-0.0331522  -0.02613648 -0.02786111 -0.04102053]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 9\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.027861106989519067\n",
      "New Q-value: -0.02811977952916557\n",
      "\n",
      "Step 4\n",
      "Current state: 9\n",
      "Q-values for current state: [-0.03144893 -0.02152403 -0.02321202 -0.03417183]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 17\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.02152403409050956\n",
      "New Q-value: -0.021727278884846532\n",
      "\n",
      "Step 5\n",
      "Current state: 17\n",
      "Q-values for current state: [-0.0251963  -0.0145993  -0.01426998 -0.02614995]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 25\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.014599302856893715\n",
      "New Q-value: -0.01507044272214184\n",
      "\n",
      "Step 6\n",
      "Current state: 25\n",
      "Q-values for current state: [-0.01241844 -0.00980074 -0.01343017 -0.01566209]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 26\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.01343017094973149\n",
      "New Q-value: -0.013727939868421603\n",
      "\n",
      "Step 7\n",
      "Current state: 26\n",
      "Q-values for current state: [-0.01095182 -0.00792949 -0.00674512 -0.01172391]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 27\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.0067451159332974995\n",
      "New Q-value: -0.0072170046742652495\n",
      "\n",
      "Step 8\n",
      "Current state: 27\n",
      "Q-values for current state: [-0.00154106 -0.00271    -0.0041901  -0.00271   ]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 28\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.0041901\n",
      "New Q-value: -0.004884866512500001\n",
      "\n",
      "Step 9\n",
      "Current state: 28\n",
      "Q-values for current state: [-0.001995   -0.0019     -0.00271    -0.00119765]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 36\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.0019000000000000002\n",
      "New Q-value: -0.00271\n",
      "\n",
      "Step 10\n",
      "Current state: 36\n",
      "Q-values for current state: [ 0.    -0.001 -0.001  0.   ]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 37\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.001\n",
      "New Q-value: -0.0019000000000000002\n",
      "\n",
      "Step 11\n",
      "Current state: 37\n",
      "Q-values for current state: [ 0.      0.      0.     -0.0019]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 45\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: -0.001\n",
      "\n",
      "Step 12\n",
      "Current state: 45\n",
      "Q-values for current state: [0. 0. 0. 0.]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 44\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: 0.0\n",
      "New Q-value: -0.001\n",
      "\n",
      "Step 13\n",
      "Current state: 44\n",
      "Q-values for current state: [ 0.    -0.001  0.     0.   ]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 52\n",
      "Reward: -0.01\n",
      "Terminated: True\n",
      "Old Q-value: -0.001\n",
      "New Q-value: -0.0019000000000000002\n",
      "\n",
      "Episode 93\n",
      "Starting state: 0\n",
      "\n",
      "Step 0\n",
      "Current state: 0\n",
      "Q-values for current state: [-0.04130807 -0.03402712 -0.03627564 -0.04083501]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 8\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.034027119601045895\n",
      "New Q-value: -0.034107373133417634\n",
      "\n",
      "Step 1\n",
      "Current state: 8\n",
      "Q-values for current state: [-0.0331522  -0.02613648 -0.02811978 -0.04102053]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 16\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.026136478868171862\n",
      "New Q-value: -0.026387147392801874\n",
      "\n",
      "Step 2\n",
      "Current state: 16\n",
      "Q-values for current state: [-0.02616673 -0.01962438 -0.02039119 -0.03243485]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 24\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.01962438327839154\n",
      "New Q-value: -0.019944208245224807\n",
      "\n",
      "Step 3\n",
      "Current state: 24\n",
      "Q-values for current state: [-0.01547164 -0.01349751 -0.01356463 -0.02094085]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 32\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.013497508364972838\n",
      "New Q-value: -0.013890068621550958\n",
      "\n",
      "Step 4\n",
      "Current state: 32\n",
      "Q-values for current state: [-0.01015779 -0.0088737  -0.0078138  -0.01018342]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 24\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.01018341552944506\n",
      "New Q-value: -0.01145371428772562\n",
      "\n",
      "Step 5\n",
      "Current state: 24\n",
      "Q-values for current state: [-0.01547164 -0.01389007 -0.01356463 -0.02094085]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 25\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.01356463485500069\n",
      "New Q-value: -0.014139241520438118\n",
      "\n",
      "Step 6\n",
      "Current state: 25\n",
      "Q-values for current state: [-0.01241844 -0.00980074 -0.01372794 -0.01566209]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 26\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.013727939868421603\n",
      "New Q-value: -0.014040761325634641\n",
      "\n",
      "Step 7\n",
      "Current state: 26\n",
      "Q-values for current state: [-0.01095182 -0.00792949 -0.007217   -0.01172391]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 25\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.010951820679744073\n",
      "New Q-value: -0.011787708762707162\n",
      "\n",
      "Step 8\n",
      "Current state: 25\n",
      "Q-values for current state: [-0.01241844 -0.00980074 -0.01404076 -0.01566209]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 17\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.01566209390450166\n",
      "New Q-value: -0.016451532717439425\n",
      "\n",
      "Step 9\n",
      "Current state: 17\n",
      "Q-values for current state: [-0.0251963  -0.01507044 -0.01426998 -0.02614995]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 25\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.01507044272214184\n",
      "New Q-value: -0.015494468600865152\n",
      "\n",
      "Step 10\n",
      "Current state: 25\n",
      "Q-values for current state: [-0.01241844 -0.00980074 -0.01404076 -0.01645153]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 26\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.014040761325634641\n",
      "New Q-value: -0.014322300637126376\n",
      "\n",
      "Step 11\n",
      "Current state: 26\n",
      "Q-values for current state: [-0.01178771 -0.00792949 -0.007217   -0.01172391]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 18\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.011723907263989936\n",
      "New Q-value: -0.012170272019495942\n",
      "\n",
      "Step 12\n",
      "Current state: 18\n",
      "Q-values for current state: [-0.01944656 -0.01251152 -0.00651322 -0.01893328]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 26\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.012511515574867677\n",
      "New Q-value: -0.012945979461436108\n",
      "\n",
      "Step 13\n",
      "Current state: 26\n",
      "Q-values for current state: [-0.01178771 -0.00792949 -0.007217   -0.01217027]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 18\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.012170272019495942\n",
      "New Q-value: -0.012572000299451349\n",
      "\n",
      "Step 14\n",
      "Current state: 18\n",
      "Q-values for current state: [-0.01944656 -0.01294598 -0.00651322 -0.01893328]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 19\n",
      "Reward: -0.01\n",
      "Terminated: True\n",
      "Old Q-value: -0.006513215599\n",
      "New Q-value: -0.0068618940391\n",
      "\n",
      "Episode 94\n",
      "Starting state: 0\n",
      "\n",
      "Step 0\n",
      "Current state: 0\n",
      "Q-values for current state: [-0.04130807 -0.03410737 -0.03627564 -0.04083501]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 0\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.04083500853536252\n",
      "New Q-value: -0.04099170812950094\n",
      "\n",
      "Step 1\n",
      "Current state: 0\n",
      "Q-values for current state: [-0.04130807 -0.03410737 -0.03627564 -0.04099171]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 1\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.036275635237347864\n",
      "New Q-value: -0.03639372340934029\n",
      "\n",
      "Step 2\n",
      "Current state: 1\n",
      "Q-values for current state: [-0.04081273 -0.0289016  -0.03050733 -0.03560223]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 9\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.028901596797128545\n",
      "New Q-value: -0.029075528611476113\n",
      "\n",
      "Step 3\n",
      "Current state: 9\n",
      "Q-values for current state: [-0.03144893 -0.02172728 -0.02321202 -0.03417183]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 10\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.023212017297466678\n",
      "New Q-value: -0.023318982505706968\n",
      "\n",
      "Step 4\n",
      "Current state: 10\n",
      "Q-values for current state: [-0.02634512 -0.01503334 -0.01711542 -0.02818301]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 11\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.017115417800539667\n",
      "New Q-value: -0.017249927980810764\n",
      "\n",
      "Step 5\n",
      "Current state: 11\n",
      "Q-values for current state: [-0.01790538 -0.00890581 -0.01481852 -0.02102836]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 10\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.01790537694688584\n",
      "New Q-value: -0.018543006190184216\n",
      "\n",
      "Step 6\n",
      "Current state: 10\n",
      "Q-values for current state: [-0.02634512 -0.01503334 -0.01724993 -0.02818301]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 9\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.026345122631776012\n",
      "New Q-value: -0.02677470186265883\n",
      "\n",
      "Step 7\n",
      "Current state: 9\n",
      "Q-values for current state: [-0.03144893 -0.02172728 -0.02331898 -0.03417183]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 8\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.03144893094900085\n",
      "New Q-value: -0.03181081685641694\n",
      "\n",
      "Step 8\n",
      "Current state: 8\n",
      "Q-values for current state: [-0.0331522  -0.02638715 -0.02811978 -0.04102053]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 9\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.02811977952916557\n",
      "New Q-value: -0.028371893070309434\n",
      "\n",
      "Step 9\n",
      "Current state: 9\n",
      "Q-values for current state: [-0.03181082 -0.02172728 -0.02331898 -0.03417183]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 10\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.023318982505706968\n",
      "New Q-value: -0.02341525119312323\n",
      "\n",
      "Step 10\n",
      "Current state: 10\n",
      "Q-values for current state: [-0.0267747  -0.01503334 -0.01724993 -0.02818301]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 18\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.015033336189336412\n",
      "New Q-value: -0.015181882504117271\n",
      "\n",
      "Step 11\n",
      "Current state: 18\n",
      "Q-values for current state: [-0.01944656 -0.01294598 -0.00686189 -0.01893328]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 10\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.018933275146191354\n",
      "New Q-value: -0.01948222646946336\n",
      "\n",
      "Step 12\n",
      "Current state: 10\n",
      "Q-values for current state: [-0.0267747  -0.01518188 -0.01724993 -0.02818301]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 18\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.015181882504117271\n",
      "New Q-value: -0.015315574187420043\n",
      "\n",
      "Step 13\n",
      "Current state: 18\n",
      "Q-values for current state: [-0.01944656 -0.01294598 -0.00686189 -0.01948223]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 17\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.019446557772030654\n",
      "New Q-value: -0.01985755019821552\n",
      "\n",
      "Step 14\n",
      "Current state: 17\n",
      "Q-values for current state: [-0.0251963  -0.01549447 -0.01426998 -0.02614995]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 18\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.014269981088294007\n",
      "New Q-value: -0.014494862913179106\n",
      "\n",
      "Step 15\n",
      "Current state: 18\n",
      "Q-values for current state: [-0.01985755 -0.01294598 -0.00686189 -0.01948223]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 17\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.01985755019821552\n",
      "New Q-value: -0.02024880715514598\n",
      "\n",
      "Step 16\n",
      "Current state: 17\n",
      "Q-values for current state: [-0.0251963  -0.01549447 -0.01449486 -0.02614995]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 9\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.026149952167271912\n",
      "New Q-value: -0.02659904844460514\n",
      "\n",
      "Step 17\n",
      "Current state: 9\n",
      "Q-values for current state: [-0.03181082 -0.02172728 -0.02341525 -0.03417183]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 17\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.021727278884846532\n",
      "New Q-value: -0.021931562973113893\n",
      "\n",
      "Step 18\n",
      "Current state: 17\n",
      "Q-values for current state: [-0.0251963  -0.01549447 -0.01449486 -0.02659905]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 18\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.014494862913179106\n",
      "New Q-value: -0.014697256555575695\n",
      "\n",
      "Step 19\n",
      "Current state: 18\n",
      "Q-values for current state: [-0.02024881 -0.01294598 -0.00686189 -0.01948223]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 10\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.01948222646946336\n",
      "New Q-value: -0.019988983370321928\n",
      "\n",
      "Step 20\n",
      "Current state: 10\n",
      "Q-values for current state: [-0.0267747  -0.01531557 -0.01724993 -0.02818301]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 18\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.015315574187420043\n",
      "New Q-value: -0.01543589670239254\n",
      "\n",
      "Step 21\n",
      "Current state: 18\n",
      "Q-values for current state: [-0.02024881 -0.01294598 -0.00686189 -0.01998898]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 10\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.019988983370321928\n",
      "New Q-value: -0.020456495220017027\n",
      "\n",
      "Step 22\n",
      "Current state: 10\n",
      "Q-values for current state: [-0.0267747  -0.0154359  -0.01724993 -0.02818301]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 2\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.028183006375988237\n",
      "New Q-value: -0.02854765839811308\n",
      "\n",
      "Step 23\n",
      "Current state: 2\n",
      "Q-values for current state: [-0.0340157  -0.02297845 -0.02416411 -0.02991779]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 3\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.024164114690864325\n",
      "New Q-value: -0.024352383390366376\n",
      "\n",
      "Step 24\n",
      "Current state: 3\n",
      "Q-values for current state: [-0.02815517 -0.01689137 -0.01748273 -0.02270039]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 2\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.02815516705808293\n",
      "New Q-value: -0.028522603011998306\n",
      "\n",
      "Step 25\n",
      "Current state: 2\n",
      "Q-values for current state: [-0.0340157  -0.02297845 -0.02435238 -0.02991779]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 2\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.029917793625597777\n",
      "New Q-value: -0.030108966922761667\n",
      "\n",
      "Step 26\n",
      "Current state: 2\n",
      "Q-values for current state: [-0.0340157  -0.02297845 -0.02435238 -0.03010897]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 10\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.02297844904972283\n",
      "New Q-value: -0.02314701433147784\n",
      "\n",
      "Step 27\n",
      "Current state: 10\n",
      "Q-values for current state: [-0.0267747  -0.0154359  -0.01724993 -0.02854766]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 18\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.01543589670239254\n",
      "New Q-value: -0.015544186965867785\n",
      "\n",
      "Step 28\n",
      "Current state: 18\n",
      "Q-values for current state: [-0.02024881 -0.01294598 -0.00686189 -0.0204565 ]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 26\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.012945979461436108\n",
      "New Q-value: -0.013336996959347695\n",
      "\n",
      "Step 29\n",
      "Current state: 26\n",
      "Q-values for current state: [-0.01178771 -0.00792949 -0.007217   -0.012572  ]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 18\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.012572000299451349\n",
      "New Q-value: -0.012966680203220714\n",
      "\n",
      "Step 30\n",
      "Current state: 18\n",
      "Q-values for current state: [-0.02024881 -0.013337   -0.00686189 -0.0204565 ]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 10\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.020456495220017027\n",
      "New Q-value: -0.020887543459772763\n",
      "\n",
      "Step 31\n",
      "Current state: 10\n",
      "Q-values for current state: [-0.0267747  -0.01554419 -0.01724993 -0.02854766]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 11\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.017249927980810764\n",
      "New Q-value: -0.01737098714305475\n",
      "\n",
      "Step 32\n",
      "Current state: 11\n",
      "Q-values for current state: [-0.01854301 -0.00890581 -0.01481852 -0.02102836]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 19\n",
      "Reward: -0.01\n",
      "Terminated: True\n",
      "Old Q-value: -0.008905810108684875\n",
      "New Q-value: -0.009015229097816388\n",
      "\n",
      "Episode 95\n",
      "Starting state: 0\n",
      "\n",
      "Step 0\n",
      "Current state: 0\n",
      "Q-values for current state: [-0.04130807 -0.03410737 -0.03639372 -0.04099171]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 0\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.04130807188970017\n",
      "New Q-value: -0.04141746514840483\n",
      "\n",
      "Step 1\n",
      "Current state: 0\n",
      "Q-values for current state: [-0.04141747 -0.03410737 -0.03639372 -0.04099171]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 0\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.04141746514840483\n",
      "New Q-value: -0.04151591908123902\n",
      "\n",
      "Step 2\n",
      "Current state: 0\n",
      "Q-values for current state: [-0.04151592 -0.03410737 -0.03639372 -0.04099171]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 8\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.034107373133417634\n",
      "New Q-value: -0.03420341482239205\n",
      "\n",
      "Step 3\n",
      "Current state: 8\n",
      "Q-values for current state: [-0.0331522  -0.02638715 -0.02837189 -0.04102053]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 16\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.026387147392801874\n",
      "New Q-value: -0.026643132436818043\n",
      "\n",
      "Step 4\n",
      "Current state: 16\n",
      "Q-values for current state: [-0.02616673 -0.01994421 -0.02039119 -0.03243485]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 16\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.026166728565829283\n",
      "New Q-value: -0.026444755492542712\n",
      "\n",
      "Step 5\n",
      "Current state: 16\n",
      "Q-values for current state: [-0.02644476 -0.01994421 -0.02039119 -0.03243485]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 17\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.020391190113871246\n",
      "New Q-value: -0.020748310475263813\n",
      "\n",
      "Step 6\n",
      "Current state: 17\n",
      "Q-values for current state: [-0.0251963  -0.01549447 -0.01469726 -0.02659905]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 16\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.025196304643189525\n",
      "New Q-value: -0.02557137396216693\n",
      "\n",
      "Step 7\n",
      "Current state: 16\n",
      "Q-values for current state: [-0.02644476 -0.01994421 -0.02074831 -0.03243485]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 24\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.019944208245224807\n",
      "New Q-value: -0.02026934393974967\n",
      "\n",
      "Step 8\n",
      "Current state: 24\n",
      "Q-values for current state: [-0.01547164 -0.01389007 -0.01413924 -0.02094085]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 16\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.020940846925607257\n",
      "New Q-value: -0.021772349907322748\n",
      "\n",
      "Step 9\n",
      "Current state: 16\n",
      "Q-values for current state: [-0.02644476 -0.02026934 -0.02074831 -0.03243485]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 24\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.02026934393974967\n",
      "New Q-value: -0.020561966064822045\n",
      "\n",
      "Step 10\n",
      "Current state: 24\n",
      "Q-values for current state: [-0.01547164 -0.01389007 -0.01413924 -0.02177235]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 16\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.021772349907322748\n",
      "New Q-value: -0.02254850169274857\n",
      "\n",
      "Step 11\n",
      "Current state: 16\n",
      "Q-values for current state: [-0.02644476 -0.02056197 -0.02074831 -0.03243485]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 24\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.020561966064822045\n",
      "New Q-value: -0.02082532597738718\n",
      "\n",
      "Step 12\n",
      "Current state: 24\n",
      "Q-values for current state: [-0.01547164 -0.01389007 -0.01413924 -0.0225485 ]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 25\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.014139241520438118\n",
      "New Q-value: -0.014656387519331803\n",
      "\n",
      "Step 13\n",
      "Current state: 25\n",
      "Q-values for current state: [-0.01241844 -0.00980074 -0.0143223  -0.01645153]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 26\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.014322300637126376\n",
      "New Q-value: -0.014575686017468937\n",
      "\n",
      "Step 14\n",
      "Current state: 26\n",
      "Q-values for current state: [-0.01178771 -0.00792949 -0.007217   -0.01296668]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 34\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.00792949068519\n",
      "New Q-value: -0.008490462526382874\n",
      "\n",
      "Step 15\n",
      "Current state: 34\n",
      "Q-values for current state: [-0.00456953 -0.00521703 -0.0040951  -0.00372548]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 33\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.004569528990625\n",
      "New Q-value: -0.005597759736195164\n",
      "\n",
      "Step 16\n",
      "Current state: 33\n",
      "Q-values for current state: [-0.0051072  -0.00569533 -0.00677461 -0.00968065]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 25\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.009680646670219598\n",
      "New Q-value: -0.010643652154135133\n",
      "\n",
      "Step 17\n",
      "Current state: 25\n",
      "Q-values for current state: [-0.01241844 -0.00980074 -0.01457569 -0.01645153]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 17\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.016451532717439425\n",
      "New Q-value: -0.017202618818475172\n",
      "\n",
      "Step 18\n",
      "Current state: 17\n",
      "Q-values for current state: [-0.02557137 -0.01549447 -0.01469726 -0.02659905]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 9\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.02659904844460514\n",
      "New Q-value: -0.027022642082590448\n",
      "\n",
      "Step 19\n",
      "Current state: 9\n",
      "Q-values for current state: [-0.03181082 -0.02193156 -0.02341525 -0.03417183]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 10\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.02341525119312323\n",
      "New Q-value: -0.023550423835568348\n",
      "\n",
      "Step 20\n",
      "Current state: 10\n",
      "Q-values for current state: [-0.0267747  -0.01554419 -0.01737099 -0.02854766]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 9\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.02677470186265883\n",
      "New Q-value: -0.02718073015883877\n",
      "\n",
      "Step 21\n",
      "Current state: 9\n",
      "Q-values for current state: [-0.03181082 -0.02193156 -0.02355042 -0.03417183]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 8\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.03181081685641694\n",
      "New Q-value: -0.032160832752272966\n",
      "\n",
      "Step 22\n",
      "Current state: 8\n",
      "Q-values for current state: [-0.0331522  -0.02664313 -0.02837189 -0.04102053]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 9\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.028371893070309434\n",
      "New Q-value: -0.02861820224572431\n",
      "\n",
      "Step 23\n",
      "Current state: 9\n",
      "Q-values for current state: [-0.03216083 -0.02193156 -0.02355042 -0.03417183]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 8\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.032160832752272966\n",
      "New Q-value: -0.03247584705854338\n",
      "\n",
      "Step 24\n",
      "Current state: 8\n",
      "Q-values for current state: [-0.0331522  -0.02664313 -0.0286182  -0.04102053]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 9\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.02861820224572431\n",
      "New Q-value: -0.0288398805035977\n",
      "\n",
      "Step 25\n",
      "Current state: 9\n",
      "Q-values for current state: [-0.03247585 -0.02193156 -0.02355042 -0.03417183]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 17\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.021931562973113893\n",
      "New Q-value: -0.022134646048582195\n",
      "\n",
      "Step 26\n",
      "Current state: 17\n",
      "Q-values for current state: [-0.02557137 -0.01549447 -0.01469726 -0.02702264]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 9\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.027022642082590448\n",
      "New Q-value: -0.02742316924894671\n",
      "\n",
      "Step 27\n",
      "Current state: 9\n",
      "Q-values for current state: [-0.03247585 -0.02213465 -0.02355042 -0.03417183]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 1\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.03417183386459019\n",
      "New Q-value: -0.0345168256962214\n",
      "\n",
      "Step 28\n",
      "Current state: 1\n",
      "Q-values for current state: [-0.04081273 -0.02907553 -0.03050733 -0.03560223]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 9\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.029075528611476113\n",
      "New Q-value: -0.02927076712494381\n",
      "\n",
      "Step 29\n",
      "Current state: 9\n",
      "Q-values for current state: [-0.03247585 -0.02213465 -0.02355042 -0.03451683]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 10\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.023550423835568348\n",
      "New Q-value: -0.023672079213768953\n",
      "\n",
      "Step 30\n",
      "Current state: 10\n",
      "Q-values for current state: [-0.02718073 -0.01554419 -0.01737099 -0.02854766]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 9\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.02718073015883877\n",
      "New Q-value: -0.0275654485175702\n",
      "\n",
      "Step 31\n",
      "Current state: 9\n",
      "Q-values for current state: [-0.03247585 -0.02213465 -0.02367208 -0.03451683]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 17\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.022134646048582195\n",
      "New Q-value: -0.022317420816503668\n",
      "\n",
      "Step 32\n",
      "Current state: 17\n",
      "Q-values for current state: [-0.02557137 -0.01549447 -0.01469726 -0.02742317]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 16\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.02557137396216693\n",
      "New Q-value: -0.0259853260611003\n",
      "\n",
      "Step 33\n",
      "Current state: 16\n",
      "Q-values for current state: [-0.02644476 -0.02082533 -0.02074831 -0.03243485]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 17\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.020748310475263813\n",
      "New Q-value: -0.021069718800517123\n",
      "\n",
      "Step 34\n",
      "Current state: 17\n",
      "Q-values for current state: [-0.02598533 -0.01549447 -0.01469726 -0.02742317]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 18\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.014697256555575695\n",
      "New Q-value: -0.014879410833732626\n",
      "\n",
      "Step 35\n",
      "Current state: 18\n",
      "Q-values for current state: [-0.02024881 -0.013337   -0.00686189 -0.02088754]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 26\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.013336996959347695\n",
      "New Q-value: -0.013688912707468125\n",
      "\n",
      "Step 36\n",
      "Current state: 26\n",
      "Q-values for current state: [-0.01178771 -0.00849046 -0.007217   -0.01296668]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 27\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.0072170046742652495\n",
      "New Q-value: -0.007641704541136224\n",
      "\n",
      "Step 37\n",
      "Current state: 27\n",
      "Q-values for current state: [-0.00154106 -0.00271    -0.00488487 -0.00271   ]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 28\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.004884866512500001\n",
      "New Q-value: -0.005510156373750001\n",
      "\n",
      "Step 38\n",
      "Current state: 28\n",
      "Q-values for current state: [-0.001995   -0.00271    -0.00271    -0.00119765]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 20\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.0011976475000000002\n",
      "New Q-value: -0.0023353327500000003\n",
      "\n",
      "Step 39\n",
      "Current state: 20\n",
      "Q-values for current state: [-0.00521703 -0.00271    -0.00511371 -0.00757364]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 12\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.007573642684761017\n",
      "New Q-value: -0.008657120335817661\n",
      "\n",
      "Step 40\n",
      "Current state: 12\n",
      "Q-values for current state: [-0.01238623 -0.00885097 -0.01215002 -0.01001303]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 13\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.012150019788436555\n",
      "New Q-value: -0.0124306357545929\n",
      "\n",
      "Step 41\n",
      "Current state: 13\n",
      "Q-values for current state: [-0.01091541 -0.00888859 -0.00521703 -0.00940755]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 12\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.010915407199363219\n",
      "New Q-value: -0.011664708398959642\n",
      "\n",
      "Step 42\n",
      "Current state: 12\n",
      "Q-values for current state: [-0.01238623 -0.00885097 -0.01243064 -0.01001303]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 13\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.0124306357545929\n",
      "New Q-value: -0.01268319012413361\n",
      "\n",
      "Step 43\n",
      "Current state: 13\n",
      "Q-values for current state: [-0.01166471 -0.00888859 -0.00521703 -0.00940755]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 21\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.008888589889735\n",
      "New Q-value: -0.0093270482180365\n",
      "\n",
      "Step 44\n",
      "Current state: 21\n",
      "Q-values for current state: [-0.00599786 -0.00521703 -0.0056167  -0.00344545]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 13\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.003445445445\n",
      "New Q-value: -0.0045965188455\n",
      "\n",
      "Step 45\n",
      "Current state: 13\n",
      "Q-values for current state: [-0.01166471 -0.00932705 -0.00521703 -0.00940755]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 12\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.011664708398959642\n",
      "New Q-value: -0.012339079478596424\n",
      "\n",
      "Step 46\n",
      "Current state: 12\n",
      "Q-values for current state: [-0.01238623 -0.00885097 -0.01268319 -0.01001303]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 13\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.01268319012413361\n",
      "New Q-value: -0.012910489056720249\n",
      "\n",
      "Step 47\n",
      "Current state: 13\n",
      "Q-values for current state: [-0.01233908 -0.00932705 -0.00521703 -0.00940755]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 21\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.0093270482180365\n",
      "New Q-value: -0.009831012686555349\n",
      "\n",
      "Step 48\n",
      "Current state: 21\n",
      "Q-values for current state: [-0.00599786 -0.00521703 -0.0056167  -0.00459652]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 22\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.005616699250000001\n",
      "New Q-value: -0.006398881825000001\n",
      "\n",
      "Step 49\n",
      "Current state: 22\n",
      "Q-values for current state: [-0.0045631  -0.00531203 -0.0036195  -0.00468559]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 21\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.0045630987375\n",
      "New Q-value: -0.0055434581540725\n",
      "\n",
      "Step 50\n",
      "Current state: 21\n",
      "Q-values for current state: [-0.00599786 -0.00521703 -0.00639888 -0.00459652]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 13\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.0045965188455\n",
      "New Q-value: -0.00563248490595\n",
      "\n",
      "Step 51\n",
      "Current state: 13\n",
      "Q-values for current state: [-0.01233908 -0.00983101 -0.00521703 -0.00940755]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 12\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.012339079478596424\n",
      "New Q-value: -0.012946013450269528\n",
      "\n",
      "Step 52\n",
      "Current state: 12\n",
      "Q-values for current state: [-0.01238623 -0.00885097 -0.01291049 -0.01001303]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 13\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.012910489056720249\n",
      "New Q-value: -0.013115058096048224\n",
      "\n",
      "Step 53\n",
      "Current state: 13\n",
      "Q-values for current state: [-0.01294601 -0.00983101 -0.00521703 -0.00940755]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 12\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.012946013450269528\n",
      "New Q-value: -0.01349225402477532\n",
      "\n",
      "Step 54\n",
      "Current state: 12\n",
      "Q-values for current state: [-0.01238623 -0.00885097 -0.01311506 -0.01001303]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 11\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.012386225830297616\n",
      "New Q-value: -0.013004050011560412\n",
      "\n",
      "Step 55\n",
      "Current state: 11\n",
      "Q-values for current state: [-0.01854301 -0.00901523 -0.01481852 -0.02102836]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 3\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.02102835538734548\n",
      "New Q-value: -0.021530200017199415\n",
      "\n",
      "Step 56\n",
      "Current state: 3\n",
      "Q-values for current state: [-0.0285226  -0.01689137 -0.01748273 -0.02270039]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 2\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.028522603011998306\n",
      "New Q-value: -0.02886930907228887\n",
      "\n",
      "Step 57\n",
      "Current state: 2\n",
      "Q-values for current state: [-0.0340157  -0.02314701 -0.02435238 -0.03010897]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 3\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.024352383390366376\n",
      "New Q-value: -0.024521825219918223\n",
      "\n",
      "Step 58\n",
      "Current state: 3\n",
      "Q-values for current state: [-0.02886931 -0.01689137 -0.01748273 -0.02270039]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 2\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.02886930907228887\n",
      "New Q-value: -0.029181344526550378\n",
      "\n",
      "Step 59\n",
      "Current state: 2\n",
      "Q-values for current state: [-0.0340157  -0.02314701 -0.02452183 -0.03010897]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 2\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.030108966922761667\n",
      "New Q-value: -0.030297036591975895\n",
      "\n",
      "Step 60\n",
      "Current state: 2\n",
      "Q-values for current state: [-0.0340157  -0.02314701 -0.02452183 -0.03029704]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 3\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.024521825219918223\n",
      "New Q-value: -0.024674322866514886\n",
      "\n",
      "Step 61\n",
      "Current state: 3\n",
      "Q-values for current state: [-0.02918134 -0.01689137 -0.01748273 -0.02270039]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 11\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.016891370195668252\n",
      "New Q-value: -0.017058679940393985\n",
      "\n",
      "Step 62\n",
      "Current state: 11\n",
      "Q-values for current state: [-0.01854301 -0.00901523 -0.01481852 -0.0215302 ]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 3\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.021530200017199415\n",
      "New Q-value: -0.021997754609816903\n",
      "\n",
      "Step 63\n",
      "Current state: 3\n",
      "Q-values for current state: [-0.02918134 -0.01705868 -0.01748273 -0.02270039]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 11\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.017058679940393985\n",
      "New Q-value: -0.017209258710647143\n",
      "\n",
      "Step 64\n",
      "Current state: 11\n",
      "Q-values for current state: [-0.01854301 -0.00901523 -0.01481852 -0.02199775]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 10\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.018543006190184216\n",
      "New Q-value: -0.019165403332923235\n",
      "\n",
      "Step 65\n",
      "Current state: 10\n",
      "Q-values for current state: [-0.02756545 -0.01554419 -0.01737099 -0.02854766]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 2\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.02854765839811308\n",
      "New Q-value: -0.02889185891979217\n",
      "\n",
      "Step 66\n",
      "Current state: 2\n",
      "Q-values for current state: [-0.0340157  -0.02314701 -0.02467432 -0.03029704]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 10\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.02314701433147784\n",
      "New Q-value: -0.023309010660087493\n",
      "\n",
      "Step 67\n",
      "Current state: 10\n",
      "Q-values for current state: [-0.02756545 -0.01554419 -0.01737099 -0.02889186]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 18\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.015544186965867785\n",
      "New Q-value: -0.015641648202995507\n",
      "\n",
      "Step 68\n",
      "Current state: 18\n",
      "Q-values for current state: [-0.02024881 -0.01368891 -0.00686189 -0.02088754]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 17\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.02024880715514598\n",
      "New Q-value: -0.020637470468835982\n",
      "\n",
      "Step 69\n",
      "Current state: 17\n",
      "Q-values for current state: [-0.02598533 -0.01549447 -0.01487941 -0.02742317]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 9\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.02742316924894671\n",
      "New Q-value: -0.027801007301619888\n",
      "\n",
      "Step 70\n",
      "Current state: 9\n",
      "Q-values for current state: [-0.03247585 -0.02231742 -0.02367208 -0.03451683]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 17\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.022317420816503668\n",
      "New Q-value: -0.0224992227640579\n",
      "\n",
      "Step 71\n",
      "Current state: 17\n",
      "Q-values for current state: [-0.02598533 -0.01549447 -0.01487941 -0.02780101]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 25\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.015494468600865152\n",
      "New Q-value: -0.015876091891716133\n",
      "\n",
      "Step 72\n",
      "Current state: 25\n",
      "Q-values for current state: [-0.01241844 -0.00980074 -0.01457569 -0.01720262]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 26\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.014575686017468937\n",
      "New Q-value: -0.014844079347129984\n",
      "\n",
      "Step 73\n",
      "Current state: 26\n",
      "Q-values for current state: [-0.01178771 -0.00849046 -0.0076417  -0.01296668]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 25\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.011787708762707162\n",
      "New Q-value: -0.012540008037373943\n",
      "\n",
      "Step 74\n",
      "Current state: 25\n",
      "Q-values for current state: [-0.01241844 -0.00980074 -0.01484408 -0.01720262]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 33\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.00980073843092101\n",
      "New Q-value: -0.010305848232461572\n",
      "\n",
      "Step 75\n",
      "Current state: 33\n",
      "Q-values for current state: [-0.0051072  -0.00569533 -0.00677461 -0.01064365]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 34\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.006774612450000001\n",
      "New Q-value: -0.007451072114711876\n",
      "\n",
      "Step 76\n",
      "Current state: 34\n",
      "Q-values for current state: [-0.00559776 -0.00521703 -0.0040951  -0.00372548]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 33\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.005597759736195164\n",
      "New Q-value: -0.00652316740720831\n",
      "\n",
      "Step 77\n",
      "Current state: 33\n",
      "Q-values for current state: [-0.0051072  -0.00569533 -0.00745107 -0.01064365]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 25\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.010643652154135133\n",
      "New Q-value: -0.01155834252080547\n",
      "\n",
      "Step 78\n",
      "Current state: 25\n",
      "Q-values for current state: [-0.01241844 -0.01030585 -0.01484408 -0.01720262]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 33\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.010305848232461572\n",
      "New Q-value: -0.010760447053848078\n",
      "\n",
      "Step 79\n",
      "Current state: 33\n",
      "Q-values for current state: [-0.0051072  -0.00569533 -0.00745107 -0.01155834]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 32\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.005107196259291192\n",
      "New Q-value: -0.006338787726437478\n",
      "\n",
      "Step 80\n",
      "Current state: 32\n",
      "Q-values for current state: [-0.01015779 -0.0088737  -0.0078138  -0.01145371]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 32\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.010157786702421037\n",
      "New Q-value: -0.010884319125254338\n",
      "\n",
      "Step 81\n",
      "Current state: 32\n",
      "Q-values for current state: [-0.01088432 -0.0088737  -0.0078138  -0.01145371]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 32\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.010884319125254338\n",
      "New Q-value: -0.011538198305804308\n",
      "\n",
      "Step 82\n",
      "Current state: 32\n",
      "Q-values for current state: [-0.0115382  -0.0088737  -0.0078138  -0.01145371]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 33\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.007813800979741102\n",
      "New Q-value: -0.00857347703226699\n",
      "\n",
      "Step 83\n",
      "Current state: 33\n",
      "Q-values for current state: [-0.00633879 -0.00569533 -0.00745107 -0.01155834]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 34\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.007451072114711876\n",
      "New Q-value: -0.008059885812952564\n",
      "\n",
      "Step 84\n",
      "Current state: 34\n",
      "Q-values for current state: [-0.00652317 -0.00521703 -0.0040951  -0.00372548]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 35\n",
      "Reward: -0.01\n",
      "Terminated: True\n",
      "Old Q-value: -0.0040951\n",
      "New Q-value: -0.00468559\n",
      "\n",
      "Episode 96\n",
      "Starting state: 0\n",
      "\n",
      "Step 0\n",
      "Current state: 0\n",
      "Q-values for current state: [-0.04151592 -0.03420341 -0.03639372 -0.04099171]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 0\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.04099170812950094\n",
      "New Q-value: -0.04114186172467809\n",
      "\n",
      "Step 1\n",
      "Current state: 0\n",
      "Q-values for current state: [-0.04151592 -0.03420341 -0.03639372 -0.04114186]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 1\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.03639372340934029\n",
      "New Q-value: -0.03653507394527592\n",
      "\n",
      "Step 2\n",
      "Current state: 1\n",
      "Q-values for current state: [-0.04081273 -0.02927077 -0.03050733 -0.03560223]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 2\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.030507327391022215\n",
      "New Q-value: -0.030670950664628306\n",
      "\n",
      "Step 3\n",
      "Current state: 2\n",
      "Q-values for current state: [-0.0340157  -0.02330901 -0.02467432 -0.03029704]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 1\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.0340156973938779\n",
      "New Q-value: -0.03439485053135977\n",
      "\n",
      "Step 4\n",
      "Current state: 1\n",
      "Q-values for current state: [-0.04081273 -0.02927077 -0.03067095 -0.03560223]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 9\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.02927076712494381\n",
      "New Q-value: -0.029481116575034928\n",
      "\n",
      "Step 5\n",
      "Current state: 9\n",
      "Q-values for current state: [-0.03247585 -0.02249922 -0.02367208 -0.03451683]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 8\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.03247584705854338\n",
      "New Q-value: -0.03275935993418676\n",
      "\n",
      "Step 6\n",
      "Current state: 8\n",
      "Q-values for current state: [-0.0331522  -0.02664313 -0.02883988 -0.04102053]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 16\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.026643132436818043\n",
      "New Q-value: -0.026957225160988023\n",
      "\n",
      "Step 7\n",
      "Current state: 16\n",
      "Q-values for current state: [-0.02644476 -0.02082533 -0.02106972 -0.03243485]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 16\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.026444755492542712\n",
      "New Q-value: -0.026778685911140222\n",
      "\n",
      "Step 8\n",
      "Current state: 16\n",
      "Q-values for current state: [-0.02677869 -0.02082533 -0.02106972 -0.03243485]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 16\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.026778685911140222\n",
      "New Q-value: -0.02707922328787798\n",
      "\n",
      "Step 9\n",
      "Current state: 16\n",
      "Q-values for current state: [-0.02707922 -0.02082533 -0.02106972 -0.03243485]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 17\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.021069718800517123\n",
      "New Q-value: -0.02137629094967001\n",
      "\n",
      "Step 10\n",
      "Current state: 17\n",
      "Q-values for current state: [-0.02598533 -0.01587609 -0.01487941 -0.02780101]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 25\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.015876091891716133\n",
      "New Q-value: -0.016310725172660086\n",
      "\n",
      "Step 11\n",
      "Current state: 25\n",
      "Q-values for current state: [-0.01241844 -0.01076045 -0.01484408 -0.01720262]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 17\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.017202618818475172\n",
      "New Q-value: -0.017895900965832254\n",
      "\n",
      "Step 12\n",
      "Current state: 17\n",
      "Q-values for current state: [-0.02598533 -0.01631073 -0.01487941 -0.02780101]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 25\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.016310725172660086\n",
      "New Q-value: -0.016701895125509646\n",
      "\n",
      "Step 13\n",
      "Current state: 25\n",
      "Q-values for current state: [-0.01241844 -0.01076045 -0.01484408 -0.0178959 ]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 24\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.012418440387414915\n",
      "New Q-value: -0.013496152867720764\n",
      "\n",
      "Step 14\n",
      "Current state: 24\n",
      "Q-values for current state: [-0.01547164 -0.01389007 -0.01465639 -0.0225485 ]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 32\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.013890068621550958\n",
      "New Q-value: -0.014315542077461227\n",
      "\n",
      "Step 15\n",
      "Current state: 32\n",
      "Q-values for current state: [-0.0115382  -0.0088737  -0.00857348 -0.01145371]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 40\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.0088736951882184\n",
      "New Q-value: -0.00931303066939656\n",
      "\n",
      "Step 16\n",
      "Current state: 40\n",
      "Q-values for current state: [-0.00369645 -0.00495207 -0.003439   -0.00522721]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 48\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.004952065\n",
      "New Q-value: -0.005560883500000001\n",
      "\n",
      "Step 17\n",
      "Current state: 48\n",
      "Q-values for current state: [-0.00281403 -0.00271    -0.00271    -0.001095  ]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 48\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.0028140250000000004\n",
      "New Q-value: -0.0036366475000000004\n",
      "\n",
      "Step 18\n",
      "Current state: 48\n",
      "Q-values for current state: [-0.00363665 -0.00271    -0.00271    -0.001095  ]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 49\n",
      "Reward: -0.01\n",
      "Terminated: True\n",
      "Old Q-value: -0.00271\n",
      "New Q-value: -0.003439\n",
      "\n",
      "Episode 97\n",
      "Starting state: 0\n",
      "\n",
      "Step 0\n",
      "Current state: 0\n",
      "Q-values for current state: [-0.04151592 -0.03420341 -0.03653507 -0.04114186]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 0\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.04114186172467809\n",
      "New Q-value: -0.041276999960337526\n",
      "\n",
      "Step 1\n",
      "Current state: 0\n",
      "Q-values for current state: [-0.04151592 -0.03420341 -0.03653507 -0.041277  ]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 1\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.03653507394527592\n",
      "New Q-value: -0.03668227262537665\n",
      "\n",
      "Step 2\n",
      "Current state: 1\n",
      "Q-values for current state: [-0.04081273 -0.02948112 -0.03067095 -0.03560223]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 1\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.03560222580991104\n",
      "New Q-value: -0.035842709303548256\n",
      "\n",
      "Step 3\n",
      "Current state: 1\n",
      "Q-values for current state: [-0.04081273 -0.02948112 -0.03067095 -0.03584271]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 9\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.029481116575034928\n",
      "New Q-value: -0.029670431080116935\n",
      "\n",
      "Step 4\n",
      "Current state: 9\n",
      "Q-values for current state: [-0.03275936 -0.02249922 -0.02367208 -0.03451683]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 8\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.03275935993418676\n",
      "New Q-value: -0.033044360331061944\n",
      "\n",
      "Step 5\n",
      "Current state: 8\n",
      "Q-values for current state: [-0.0331522  -0.02695723 -0.02883988 -0.04102053]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 16\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.026957225160988023\n",
      "New Q-value: -0.027239908612741004\n",
      "\n",
      "Step 6\n",
      "Current state: 16\n",
      "Q-values for current state: [-0.02707922 -0.02082533 -0.02137629 -0.03243485]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 17\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.02137629094967001\n",
      "New Q-value: -0.021652205883907608\n",
      "\n",
      "Step 7\n",
      "Current state: 17\n",
      "Q-values for current state: [-0.02598533 -0.0167019  -0.01487941 -0.02780101]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 16\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.0259853260611003\n",
      "New Q-value: -0.026365199422842052\n",
      "\n",
      "Step 8\n",
      "Current state: 16\n",
      "Q-values for current state: [-0.02707922 -0.02082533 -0.02165221 -0.03243485]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 17\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.021652205883907608\n",
      "New Q-value: -0.021900529324721446\n",
      "\n",
      "Step 9\n",
      "Current state: 17\n",
      "Q-values for current state: [-0.0263652  -0.0167019  -0.01487941 -0.02780101]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 16\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.026365199422842052\n",
      "New Q-value: -0.026707085448409627\n",
      "\n",
      "Step 10\n",
      "Current state: 16\n",
      "Q-values for current state: [-0.02707922 -0.02082533 -0.02190053 -0.03243485]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 24\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.02082532597738718\n",
      "New Q-value: -0.02110276987700728\n",
      "\n",
      "Step 11\n",
      "Current state: 24\n",
      "Q-values for current state: [-0.01547164 -0.01431554 -0.01465639 -0.0225485 ]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 32\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.014315542077461227\n",
      "New Q-value: -0.014698468187780468\n",
      "\n",
      "Step 12\n",
      "Current state: 32\n",
      "Q-values for current state: [-0.0115382  -0.00931303 -0.00857348 -0.01145371]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 24\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.01145371428772562\n",
      "New Q-value: -0.01270069967328958\n",
      "\n",
      "Step 13\n",
      "Current state: 24\n",
      "Q-values for current state: [-0.01547164 -0.01469847 -0.01465639 -0.0225485 ]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 32\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.014698468187780468\n",
      "New Q-value: -0.015043101687067784\n",
      "\n",
      "Step 14\n",
      "Current state: 32\n",
      "Q-values for current state: [-0.0115382  -0.00931303 -0.00857348 -0.0127007 ]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 40\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.00931303066939656\n",
      "New Q-value: -0.009708432602456905\n",
      "\n",
      "Step 15\n",
      "Current state: 40\n",
      "Q-values for current state: [-0.00369645 -0.00556088 -0.003439   -0.00522721]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 40\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.0036964500000000004\n",
      "New Q-value: -0.0046535100000000005\n",
      "\n",
      "Step 16\n",
      "Current state: 40\n",
      "Q-values for current state: [-0.00465351 -0.00556088 -0.003439   -0.00522721]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 41\n",
      "Reward: -0.01\n",
      "Terminated: True\n",
      "Old Q-value: -0.003439\n",
      "New Q-value: -0.0040951\n",
      "\n",
      "Episode 98\n",
      "Starting state: 0\n",
      "\n",
      "Step 0\n",
      "Current state: 0\n",
      "Q-values for current state: [-0.04151592 -0.03420341 -0.03668227 -0.041277  ]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 0\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.04151591908123902\n",
      "New Q-value: -0.04161365158124236\n",
      "\n",
      "Step 1\n",
      "Current state: 0\n",
      "Q-values for current state: [-0.04161365 -0.03420341 -0.03668227 -0.041277  ]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 0\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.041276999960337526\n",
      "New Q-value: -0.04139862437243102\n",
      "\n",
      "Step 2\n",
      "Current state: 0\n",
      "Q-values for current state: [-0.04161365 -0.03420341 -0.03668227 -0.04139862]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 1\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.03668227262537665\n",
      "New Q-value: -0.036832736315450094\n",
      "\n",
      "Step 3\n",
      "Current state: 1\n",
      "Q-values for current state: [-0.04081273 -0.02967043 -0.03067095 -0.03584271]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 9\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.029670431080116935\n",
      "New Q-value: -0.02984081413469074\n",
      "\n",
      "Step 4\n",
      "Current state: 9\n",
      "Q-values for current state: [-0.03304436 -0.02249922 -0.02367208 -0.03451683]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 1\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.0345168256962214\n",
      "New Q-value: -0.03490002046939488\n",
      "\n",
      "Step 5\n",
      "Current state: 1\n",
      "Q-values for current state: [-0.04081273 -0.02984081 -0.03067095 -0.03584271]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 0\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.04081272507083772\n",
      "New Q-value: -0.04098077697188119\n",
      "\n",
      "Step 6\n",
      "Current state: 0\n",
      "Q-values for current state: [-0.04161365 -0.03420341 -0.03683274 -0.04139862]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 0\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.04139862437243102\n",
      "New Q-value: -0.04150808634331516\n",
      "\n",
      "Step 7\n",
      "Current state: 0\n",
      "Q-values for current state: [-0.04161365 -0.03420341 -0.03683274 -0.04150809]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 0\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.04150808634331516\n",
      "New Q-value: -0.04160660211711089\n",
      "\n",
      "Step 8\n",
      "Current state: 0\n",
      "Q-values for current state: [-0.04161365 -0.03420341 -0.03683274 -0.0416066 ]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 0\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.04160660211711089\n",
      "New Q-value: -0.041695266313527046\n",
      "\n",
      "Step 9\n",
      "Current state: 0\n",
      "Q-values for current state: [-0.04161365 -0.03420341 -0.03683274 -0.04169527]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 0\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.041695266313527046\n",
      "New Q-value: -0.041775064090301586\n",
      "\n",
      "Step 10\n",
      "Current state: 0\n",
      "Q-values for current state: [-0.04161365 -0.03420341 -0.03683274 -0.04177506]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 8\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.03420341482239205\n",
      "New Q-value: -0.03437086465836324\n",
      "\n",
      "Step 11\n",
      "Current state: 8\n",
      "Q-values for current state: [-0.0331522  -0.02723991 -0.02883988 -0.04102053]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 0\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.04102052680622463\n",
      "New Q-value: -0.04118370626814667\n",
      "\n",
      "Step 12\n",
      "Current state: 0\n",
      "Q-values for current state: [-0.04161365 -0.03437086 -0.03683274 -0.04177506]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 0\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.04161365158124236\n",
      "New Q-value: -0.04171751856566263\n",
      "\n",
      "Step 13\n",
      "Current state: 0\n",
      "Q-values for current state: [-0.04171752 -0.03437086 -0.03683274 -0.04177506]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 0\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.041775064090301586\n",
      "New Q-value: -0.04186278982381594\n",
      "\n",
      "Step 14\n",
      "Current state: 0\n",
      "Q-values for current state: [-0.04171752 -0.03437086 -0.03683274 -0.04186279]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 0\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.04186278982381594\n",
      "New Q-value: -0.04194174298397885\n",
      "\n",
      "Step 15\n",
      "Current state: 0\n",
      "Q-values for current state: [-0.04171752 -0.03437086 -0.03683274 -0.04194174]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 0\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.04194174298397885\n",
      "New Q-value: -0.04201280082812547\n",
      "\n",
      "Step 16\n",
      "Current state: 0\n",
      "Q-values for current state: [-0.04171752 -0.03437086 -0.03683274 -0.0420128 ]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 0\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.04171751856566263\n",
      "New Q-value: -0.04181099885164087\n",
      "\n",
      "Step 17\n",
      "Current state: 0\n",
      "Q-values for current state: [-0.041811   -0.03437086 -0.03683274 -0.0420128 ]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 8\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.03437086465836324\n",
      "New Q-value: -0.03452156951073731\n",
      "\n",
      "Step 18\n",
      "Current state: 8\n",
      "Q-values for current state: [-0.0331522  -0.02723991 -0.02883988 -0.04118371]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 0\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.04118370626814667\n",
      "New Q-value: -0.041344884744852047\n",
      "\n",
      "Step 19\n",
      "Current state: 0\n",
      "Q-values for current state: [-0.041811   -0.03452157 -0.03683274 -0.0420128 ]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 0\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.04181099885164087\n",
      "New Q-value: -0.04190944806999683\n",
      "\n",
      "Step 20\n",
      "Current state: 0\n",
      "Q-values for current state: [-0.04190945 -0.03452157 -0.03683274 -0.0420128 ]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 0\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.04190944806999683\n",
      "New Q-value: -0.04199805236651719\n",
      "\n",
      "Step 21\n",
      "Current state: 0\n",
      "Q-values for current state: [-0.04199805 -0.03452157 -0.03683274 -0.0420128 ]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 8\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.03452156951073731\n",
      "New Q-value: -0.03465720387787397\n",
      "\n",
      "Step 22\n",
      "Current state: 8\n",
      "Q-values for current state: [-0.0331522  -0.02723991 -0.02883988 -0.04134488]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 0\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.041344884744852047\n",
      "New Q-value: -0.04150283063876487\n",
      "\n",
      "Step 23\n",
      "Current state: 0\n",
      "Q-values for current state: [-0.04199805 -0.0346572  -0.03683274 -0.0420128 ]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 8\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.03465720387787397\n",
      "New Q-value: -0.03477927480829697\n",
      "\n",
      "Step 24\n",
      "Current state: 8\n",
      "Q-values for current state: [-0.0331522  -0.02723991 -0.02883988 -0.04150283]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 0\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.04150283063876487\n",
      "New Q-value: -0.0416565786816766\n",
      "\n",
      "Step 25\n",
      "Current state: 0\n",
      "Q-values for current state: [-0.04199805 -0.03477927 -0.03683274 -0.0420128 ]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 0\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.04201280082812547\n",
      "New Q-value: -0.042115551852101134\n",
      "\n",
      "Step 26\n",
      "Current state: 0\n",
      "Q-values for current state: [-0.04199805 -0.03477927 -0.03683274 -0.04211555]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 0\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.04199805236651719\n",
      "New Q-value: -0.04210227823665368\n",
      "\n",
      "Step 27\n",
      "Current state: 0\n",
      "Q-values for current state: [-0.04210228 -0.03477927 -0.03683274 -0.04211555]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 1\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.036832736315450094\n",
      "New Q-value: -0.0369843400267007\n",
      "\n",
      "Step 28\n",
      "Current state: 1\n",
      "Q-values for current state: [-0.04098078 -0.02984081 -0.03067095 -0.03584271]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 2\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.030670950664628306\n",
      "New Q-value: -0.030818211610873787\n",
      "\n",
      "Step 29\n",
      "Current state: 2\n",
      "Q-values for current state: [-0.03439485 -0.02330901 -0.02467432 -0.03029704]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 1\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.03439485053135977\n",
      "New Q-value: -0.03479024282101942\n",
      "\n",
      "Step 30\n",
      "Current state: 1\n",
      "Q-values for current state: [-0.04098078 -0.02984081 -0.03081821 -0.03584271]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 0\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.04098077697188119\n",
      "New Q-value: -0.04118673038148128\n",
      "\n",
      "Step 31\n",
      "Current state: 0\n",
      "Q-values for current state: [-0.04210228 -0.03477927 -0.03698434 -0.04211555]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 0\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.04210227823665368\n",
      "New Q-value: -0.042196081519776524\n",
      "\n",
      "Step 32\n",
      "Current state: 0\n",
      "Q-values for current state: [-0.04219608 -0.03477927 -0.03698434 -0.04211555]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 1\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.0369843400267007\n",
      "New Q-value: -0.037120783366826254\n",
      "\n",
      "Step 33\n",
      "Current state: 1\n",
      "Q-values for current state: [-0.04118673 -0.02984081 -0.03081821 -0.03584271]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 9\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.02984081413469074\n",
      "New Q-value: -0.029994158883807166\n",
      "\n",
      "Step 34\n",
      "Current state: 9\n",
      "Q-values for current state: [-0.03304436 -0.02249922 -0.02367208 -0.03490002]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 10\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.023672079213768953\n",
      "New Q-value: -0.02379082787167663\n",
      "\n",
      "Step 35\n",
      "Current state: 10\n",
      "Q-values for current state: [-0.02756545 -0.01564165 -0.01737099 -0.02889186]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 2\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.02889185891979217\n",
      "New Q-value: -0.029217029040521265\n",
      "\n",
      "Step 36\n",
      "Current state: 2\n",
      "Q-values for current state: [-0.03479024 -0.02330901 -0.02467432 -0.03029704]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 2\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.030297036591975895\n",
      "New Q-value: -0.030481688945486616\n",
      "\n",
      "Step 37\n",
      "Current state: 2\n",
      "Q-values for current state: [-0.03479024 -0.02330901 -0.02467432 -0.03048169]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 1\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.03479024282101942\n",
      "New Q-value: -0.03516066363287916\n",
      "\n",
      "Step 38\n",
      "Current state: 1\n",
      "Q-values for current state: [-0.04118673 -0.02999416 -0.03081821 -0.03584271]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 9\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.029994158883807166\n",
      "New Q-value: -0.03013216915801195\n",
      "\n",
      "Step 39\n",
      "Current state: 9\n",
      "Q-values for current state: [-0.03304436 -0.02249922 -0.02379083 -0.03490002]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 10\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.02379082787167663\n",
      "New Q-value: -0.02389770166379354\n",
      "\n",
      "Step 40\n",
      "Current state: 10\n",
      "Q-values for current state: [-0.02756545 -0.01564165 -0.01737099 -0.02921703]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 9\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.0275654485175702\n",
      "New Q-value: -0.027946329828398682\n",
      "\n",
      "Step 41\n",
      "Current state: 9\n",
      "Q-values for current state: [-0.03304436 -0.02249922 -0.0238977  -0.03490002]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 17\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.0224992227640579\n",
      "New Q-value: -0.022662844516856708\n",
      "\n",
      "Step 42\n",
      "Current state: 17\n",
      "Q-values for current state: [-0.02670709 -0.0167019  -0.01487941 -0.02780101]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 16\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.026707085448409627\n",
      "New Q-value: -0.027041140041884355\n",
      "\n",
      "Step 43\n",
      "Current state: 16\n",
      "Q-values for current state: [-0.02707922 -0.02110277 -0.02190053 -0.03243485]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 16\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.02707922328787798\n",
      "New Q-value: -0.027376064097405874\n",
      "\n",
      "Step 44\n",
      "Current state: 16\n",
      "Q-values for current state: [-0.02737606 -0.02110277 -0.02190053 -0.03243485]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 8\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.03243485410773779\n",
      "New Q-value: -0.032779160015174405\n",
      "\n",
      "Step 45\n",
      "Current state: 8\n",
      "Q-values for current state: [-0.0331522  -0.02723991 -0.02883988 -0.04165658]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 0\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.0416565786816766\n",
      "New Q-value: -0.04179495192029715\n",
      "\n",
      "Step 46\n",
      "Current state: 0\n",
      "Q-values for current state: [-0.04219608 -0.03477927 -0.03712078 -0.04211555]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 8\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.03477927480829697\n",
      "New Q-value: -0.03488913864567767\n",
      "\n",
      "Step 47\n",
      "Current state: 8\n",
      "Q-values for current state: [-0.0331522  -0.02723991 -0.02883988 -0.04179495]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 0\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.04179495192029715\n",
      "New Q-value: -0.04192992489960681\n",
      "\n",
      "Step 48\n",
      "Current state: 0\n",
      "Q-values for current state: [-0.04219608 -0.03488914 -0.03712078 -0.04211555]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 0\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.042196081519776524\n",
      "New Q-value: -0.042290941539138246\n",
      "\n",
      "Step 49\n",
      "Current state: 0\n",
      "Q-values for current state: [-0.04229094 -0.03488914 -0.03712078 -0.04211555]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 0\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.042290941539138246\n",
      "New Q-value: -0.0423763155565638\n",
      "\n",
      "Step 50\n",
      "Current state: 0\n",
      "Q-values for current state: [-0.04237632 -0.03488914 -0.03712078 -0.04211555]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 0\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.0423763155565638\n",
      "New Q-value: -0.0424531521722468\n",
      "\n",
      "Step 51\n",
      "Current state: 0\n",
      "Q-values for current state: [-0.04245315 -0.03488914 -0.03712078 -0.04211555]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 8\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.03488913864567767\n",
      "New Q-value: -0.0349880160993203\n",
      "\n",
      "Step 52\n",
      "Current state: 8\n",
      "Q-values for current state: [-0.0331522  -0.02723991 -0.02883988 -0.04192992]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 8\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.033152196671419734\n",
      "New Q-value: -0.033424768322488155\n",
      "\n",
      "Step 53\n",
      "Current state: 8\n",
      "Q-values for current state: [-0.03342477 -0.02723991 -0.02883988 -0.04192992]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 16\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.027239908612741004\n",
      "New Q-value: -0.027520680889782594\n",
      "\n",
      "Step 54\n",
      "Current state: 16\n",
      "Q-values for current state: [-0.02737606 -0.02110277 -0.02190053 -0.03277916]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 17\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.021900529324721446\n",
      "New Q-value: -0.0221240204214539\n",
      "\n",
      "Step 55\n",
      "Current state: 17\n",
      "Q-values for current state: [-0.02704114 -0.0167019  -0.01487941 -0.02780101]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 18\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.014879410833732626\n",
      "New Q-value: -0.015043349684073863\n",
      "\n",
      "Step 56\n",
      "Current state: 18\n",
      "Q-values for current state: [-0.02063747 -0.01368891 -0.00686189 -0.02088754]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 19\n",
      "Reward: -0.01\n",
      "Terminated: True\n",
      "Old Q-value: -0.0068618940391\n",
      "New Q-value: -0.007175704635189999\n",
      "\n",
      "Episode 99\n",
      "Starting state: 0\n",
      "\n",
      "Step 0\n",
      "Current state: 0\n",
      "Q-values for current state: [-0.04245315 -0.03498802 -0.03712078 -0.04211555]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 0\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.0424531521722468\n",
      "New Q-value: -0.04253169848445755\n",
      "\n",
      "Step 1\n",
      "Current state: 0\n",
      "Q-values for current state: [-0.0425317  -0.03498802 -0.03712078 -0.04211555]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 8\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.0349880160993203\n",
      "New Q-value: -0.035103679173917615\n",
      "\n",
      "Step 2\n",
      "Current state: 8\n",
      "Q-values for current state: [-0.03342477 -0.02752068 -0.02883988 -0.04192992]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 9\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.0288398805035977\n",
      "New Q-value: -0.029108862682339316\n",
      "\n",
      "Step 3\n",
      "Current state: 9\n",
      "Q-values for current state: [-0.03304436 -0.02266284 -0.0238977  -0.03490002]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 1\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.03490002046939488\n",
      "New Q-value: -0.03527257449246653\n",
      "\n",
      "Step 4\n",
      "Current state: 1\n",
      "Q-values for current state: [-0.04118673 -0.03013217 -0.03081821 -0.03584271]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 9\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.03013216915801195\n",
      "New Q-value: -0.030271922471312142\n",
      "\n",
      "Step 5\n",
      "Current state: 9\n",
      "Q-values for current state: [-0.03304436 -0.02266284 -0.0238977  -0.03527257]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 1\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.03527257449246653\n",
      "New Q-value: -0.035621149677994526\n",
      "\n",
      "Step 6\n",
      "Current state: 1\n",
      "Q-values for current state: [-0.04118673 -0.03027192 -0.03081821 -0.03584271]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 9\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.030271922471312142\n",
      "New Q-value: -0.030397700453282314\n",
      "\n",
      "Step 7\n",
      "Current state: 9\n",
      "Q-values for current state: [-0.03304436 -0.02266284 -0.0238977  -0.03562115]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 17\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.022662844516856708\n",
      "New Q-value: -0.022825678285158055\n",
      "\n",
      "Step 8\n",
      "Current state: 17\n",
      "Q-values for current state: [-0.02704114 -0.0167019  -0.01504335 -0.02780101]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 25\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.016701895125509646\n",
      "New Q-value: -0.017053948083074248\n",
      "\n",
      "Step 9\n",
      "Current state: 25\n",
      "Q-values for current state: [-0.01349615 -0.01076045 -0.01484408 -0.0178959 ]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 26\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.014844079347129984\n",
      "New Q-value: -0.015085633343824926\n",
      "\n",
      "Step 10\n",
      "Current state: 26\n",
      "Q-values for current state: [-0.01254001 -0.00849046 -0.0076417  -0.01296668]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 34\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.008490462526382874\n",
      "New Q-value: -0.008995337183456461\n",
      "\n",
      "Step 11\n",
      "Current state: 34\n",
      "Q-values for current state: [-0.00652317 -0.00521703 -0.00468559 -0.00372548]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 33\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.00652316740720831\n",
      "New Q-value: -0.00741190681698748\n",
      "\n",
      "Step 12\n",
      "Current state: 33\n",
      "Q-values for current state: [-0.00633879 -0.00569533 -0.00805989 -0.01155834]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 32\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.006338787726437478\n",
      "New Q-value: -0.007519389271859094\n",
      "\n",
      "Step 13\n",
      "Current state: 32\n",
      "Q-values for current state: [-0.0115382  -0.00970843 -0.00857348 -0.0127007 ]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 32\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.011538198305804308\n",
      "New Q-value: -0.012198858793289242\n",
      "\n",
      "Step 14\n",
      "Current state: 32\n",
      "Q-values for current state: [-0.01219886 -0.00970843 -0.00857348 -0.0127007 ]\n",
      "Exploring - Chosen action: 0\n",
      "New state: 32\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.012198858793289242\n",
      "New Q-value: -0.012793453232025681\n",
      "\n",
      "Step 15\n",
      "Current state: 32\n",
      "Q-values for current state: [-0.01279345 -0.00970843 -0.00857348 -0.0127007 ]\n",
      "Exploring - Chosen action: 3\n",
      "New state: 24\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.01270069967328958\n",
      "New Q-value: -0.013822986520297143\n",
      "\n",
      "Step 16\n",
      "Current state: 24\n",
      "Q-values for current state: [-0.01547164 -0.0150431  -0.01465639 -0.0225485 ]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 25\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.014656387519331803\n",
      "New Q-value: -0.015212991237514191\n",
      "\n",
      "Step 17\n",
      "Current state: 25\n",
      "Q-values for current state: [-0.01349615 -0.01076045 -0.01508563 -0.0178959 ]\n",
      "Exploring - Chosen action: 2\n",
      "New state: 26\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.015085633343824926\n",
      "New Q-value: -0.015303031940850375\n",
      "\n",
      "Step 18\n",
      "Current state: 26\n",
      "Q-values for current state: [-0.01254001 -0.00899534 -0.0076417  -0.01296668]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 34\n",
      "Reward: -0.01\n",
      "Terminated: False\n",
      "Old Q-value: -0.008995337183456461\n",
      "New Q-value: -0.00944972437482269\n",
      "\n",
      "Step 19\n",
      "Current state: 34\n",
      "Q-values for current state: [-0.00741191 -0.00521703 -0.00468559 -0.00372548]\n",
      "Exploring - Chosen action: 1\n",
      "New state: 42\n",
      "Reward: -0.01\n",
      "Terminated: True\n",
      "Old Q-value: -0.005217031\n",
      "New Q-value: -0.0056953279\n"
     ]
    }
   ],
   "execution_count": 17
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-17T12:38:45.821650Z",
     "start_time": "2025-02-17T12:38:45.405056Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def print_policy(Qtable, shape=(8, 8)):\n",
    "    \"\"\"Prints the policy based on the Q-table\"\"\"\n",
    "    policy = np.array([np.argmax(Qtable[i]) for i in range(len(Qtable))])\n",
    "    arrows = ['', '', '', '']\n",
    "    policy_arrows = np.array([arrows[a] for a in policy]).reshape(shape)\n",
    "    print(\"\\nCurrent Policy:\")\n",
    "    print(policy_arrows)\n",
    "\n",
    "def improved_debug_train_v2(n_training_episodes, min_epsilon, max_epsilon, decay_rate, env, max_steps, Qtable):\n",
    "    episode_rewards = []\n",
    "    successful_episodes = []\n",
    "\n",
    "    # Dictionary to track state visitation frequency\n",
    "    state_visits = {i: 0 for i in range(64)}\n",
    "\n",
    "    for episode in range(n_training_episodes):\n",
    "        epsilon = min_epsilon + (max_epsilon - min_epsilon)*np.exp(-decay_rate*episode)\n",
    "\n",
    "        state, info = env.reset()\n",
    "        total_reward = 0\n",
    "        episode_steps = []\n",
    "\n",
    "        for step in range(max_steps):\n",
    "            state_visits[state] += 1\n",
    "\n",
    "            # Epsilon-greedy action selection\n",
    "            random_num = random.uniform(0, 1)\n",
    "            if random_num > epsilon:\n",
    "                action = np.argmax(Qtable[state])\n",
    "            else:\n",
    "                action = env.action_space.sample()\n",
    "\n",
    "            # Take action\n",
    "            new_state, reward, terminated, truncated, info = env.step(action)\n",
    "\n",
    "            # Modified reward structure\n",
    "            if reward == 1:  # Reached goal\n",
    "                reward = 10  # Increase reward for reaching goal\n",
    "            elif terminated:  # Fell in hole\n",
    "                reward = -5  # Larger penalty for falling in hole\n",
    "            else:\n",
    "                reward = -0.01  # Small step penalty\n",
    "\n",
    "            # Q-value update\n",
    "            old_q = Qtable[state][action]\n",
    "            next_max = np.max(Qtable[new_state]) if not terminated else 0\n",
    "            new_q = Qtable[state][action] + learning_rate * (\n",
    "                    reward + gamma * next_max - Qtable[state][action]\n",
    "            )\n",
    "            Qtable[state][action] = new_q\n",
    "\n",
    "            episode_steps.append((state, action, reward, new_state))\n",
    "            total_reward += reward\n",
    "\n",
    "            if reward == 10:  # Reached goal\n",
    "                successful_episodes.append(episode)\n",
    "                print(f\"\\nGOAL REACHED in episode {episode}!\")\n",
    "                print(f\"Path taken: {[(s[0], s[1]) for s in episode_steps]}\")\n",
    "                print_policy(Qtable)\n",
    "                break\n",
    "\n",
    "            if terminated or truncated:\n",
    "                break\n",
    "\n",
    "            state = new_state\n",
    "\n",
    "        episode_rewards.append(total_reward)\n",
    "\n",
    "        # Print progress every 100 episodes\n",
    "        if episode % 100 == 0:\n",
    "            print(f\"\\nEpisode {episode}\")\n",
    "            print(f\"Average reward: {np.mean(episode_rewards[-100:]):.3f}\")\n",
    "            print(f\"Success rate: {len([x for x in successful_episodes if x > episode-100])/100:.2%}\")\n",
    "            print_policy(Qtable)\n",
    "\n",
    "            # Print least visited states\n",
    "            least_visited = sorted(state_visits.items(), key=lambda x: x[1])[:5]\n",
    "            print(\"\\nLeast visited states:\")\n",
    "            for state, visits in least_visited:\n",
    "                print(f\"State {state}: {visits} visits\")\n",
    "\n",
    "    return Qtable, successful_episodes, episode_rewards, state_visits\n",
    "\n",
    "# Reset parameters with modifications\n",
    "learning_rate = 0.1\n",
    "gamma = 0.99  # Increased from 0.95 to give more weight to future rewards\n",
    "max_steps = 200\n",
    "max_epsilon = 1.0\n",
    "min_epsilon = 0.1  # Increased from 0.01 to maintain some exploration\n",
    "decay_rate = 0.002  # Increased for faster decay\n",
    "\n",
    "# Initialize fresh Q-table\n",
    "Qtable_frozenlake = initialize_q_table(state_space, action_space)\n",
    "\n",
    "# Run improved training\n",
    "print(\"\\nStarting improved training...\")\n",
    "Qtable_frozenlake, successful_episodes, episode_rewards, state_visits = improved_debug_train_v2(\n",
    "    n_training_episodes=1000,  # Increased episodes\n",
    "    min_epsilon=min_epsilon,\n",
    "    max_epsilon=max_epsilon,\n",
    "    decay_rate=decay_rate,\n",
    "    env=env,\n",
    "    max_steps=max_steps,\n",
    "    Qtable=Qtable_frozenlake\n",
    ")\n",
    "\n",
    "# Final policy visualization\n",
    "print(\"\\nFinal Policy:\")\n",
    "print_policy(Qtable_frozenlake)\n",
    "\n",
    "# Plot training results\n",
    "print(\"\\nPlotting results...\")\n",
    "plt.figure(figsize=(12, 4))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(episode_rewards)\n",
    "plt.title('Rewards per Episode')\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Total Reward')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "window_size = 100\n",
    "moving_avg = [np.mean(episode_rewards[i:i+window_size])\n",
    "              for i in range(0, len(episode_rewards)-window_size+1)]\n",
    "plt.plot(moving_avg)\n",
    "plt.title(f'Moving Average ({window_size} episodes)')\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Average Reward')\n",
    "plt.show()"
   ],
   "id": "c57888e946799e62",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting improved training...\n",
      "\n",
      "Episode 0\n",
      "Average reward: -5.180\n",
      "Success rate: 0.00%\n",
      "\n",
      "Current Policy:\n",
      "[['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']]\n",
      "\n",
      "Least visited states:\n",
      "State 3: 0 visits\n",
      "State 4: 0 visits\n",
      "State 5: 0 visits\n",
      "State 6: 0 visits\n",
      "State 7: 0 visits\n",
      "\n",
      "Episode 100\n",
      "Average reward: -5.182\n",
      "Success rate: 0.00%\n",
      "\n",
      "Current Policy:\n",
      "[['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']]\n",
      "\n",
      "Least visited states:\n",
      "State 19: 0 visits\n",
      "State 29: 0 visits\n",
      "State 35: 0 visits\n",
      "State 41: 0 visits\n",
      "State 42: 0 visits\n",
      "\n",
      "GOAL REACHED in episode 123!\n",
      "Path taken: [(0, np.int64(2)), (1, np.int64(0)), (0, np.int64(1)), (8, np.int64(3)), (0, np.int64(0)), (0, np.int64(3)), (0, np.int64(0)), (0, np.int64(0)), (0, np.int64(0)), (0, np.int64(2)), (1, np.int64(2)), (2, np.int64(2)), (3, np.int64(2)), (4, np.int64(3)), (4, np.int64(2)), (5, np.int64(0)), (4, np.int64(0)), (3, np.int64(2)), (4, np.int64(0)), (3, np.int64(0)), (2, np.int64(2)), (3, np.int64(0)), (2, np.int64(2)), (3, np.int64(2)), (4, np.int64(1)), (12, np.int64(3)), (4, np.int64(3)), (4, np.int64(3)), (4, np.int64(1)), (12, np.int64(2)), (13, np.int64(3)), (5, np.int64(2)), (6, np.int64(1)), (14, np.int64(1)), (22, np.int64(1)), (30, np.int64(2)), (31, np.int64(2)), (31, np.int64(0)), (30, np.int64(2)), (31, np.int64(2)), (31, np.int64(1)), (39, np.int64(1)), (47, np.int64(2)), (47, np.int64(1)), (55, np.int64(1))]\n",
      "\n",
      "Current Policy:\n",
      "[['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']]\n",
      "\n",
      "GOAL REACHED in episode 144!\n",
      "Path taken: [(0, np.int64(1)), (8, np.int64(0)), (8, np.int64(0)), (8, np.int64(1)), (16, np.int64(3)), (8, np.int64(1)), (16, np.int64(1)), (24, np.int64(2)), (25, np.int64(0)), (24, np.int64(3)), (16, np.int64(2)), (17, np.int64(0)), (16, np.int64(3)), (8, np.int64(0)), (8, np.int64(0)), (8, np.int64(3)), (0, np.int64(2)), (1, np.int64(1)), (9, np.int64(0)), (8, np.int64(1)), (16, np.int64(3)), (8, np.int64(3)), (0, np.int64(1)), (8, np.int64(2)), (9, np.int64(3)), (1, np.int64(3)), (1, np.int64(2)), (2, np.int64(1)), (10, np.int64(3)), (2, np.int64(2)), (3, np.int64(2)), (4, np.int64(1)), (12, np.int64(1)), (20, np.int64(3)), (12, np.int64(1)), (20, np.int64(2)), (21, np.int64(2)), (22, np.int64(0)), (21, np.int64(3)), (13, np.int64(2)), (14, np.int64(2)), (15, np.int64(1)), (23, np.int64(1)), (31, np.int64(1)), (39, np.int64(0)), (38, np.int64(3)), (30, np.int64(2)), (31, np.int64(1)), (39, np.int64(2)), (39, np.int64(1)), (47, np.int64(1)), (55, np.int64(1))]\n",
      "\n",
      "Current Policy:\n",
      "[['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']]\n",
      "\n",
      "GOAL REACHED in episode 197!\n",
      "Path taken: [(0, np.int64(0)), (0, np.int64(2)), (1, np.int64(1)), (9, np.int64(1)), (17, np.int64(2)), (18, np.int64(1)), (26, np.int64(2)), (27, np.int64(0)), (26, np.int64(2)), (27, np.int64(2)), (28, np.int64(0)), (27, np.int64(2)), (28, np.int64(1)), (36, np.int64(3)), (28, np.int64(1)), (36, np.int64(1)), (44, np.int64(2)), (45, np.int64(0)), (44, np.int64(3)), (36, np.int64(1)), (44, np.int64(3)), (36, np.int64(3)), (28, np.int64(0)), (27, np.int64(2)), (28, np.int64(3)), (20, np.int64(2)), (21, np.int64(2)), (22, np.int64(1)), (30, np.int64(1)), (38, np.int64(2)), (39, np.int64(2)), (39, np.int64(0)), (38, np.int64(3)), (30, np.int64(2)), (31, np.int64(0)), (30, np.int64(3)), (22, np.int64(1)), (30, np.int64(3)), (22, np.int64(3)), (14, np.int64(0)), (13, np.int64(3)), (5, np.int64(0)), (4, np.int64(2)), (5, np.int64(2)), (6, np.int64(1)), (14, np.int64(2)), (15, np.int64(2)), (15, np.int64(0)), (14, np.int64(1)), (22, np.int64(3)), (14, np.int64(1)), (22, np.int64(2)), (23, np.int64(1)), (31, np.int64(2)), (31, np.int64(1)), (39, np.int64(1)), (47, np.int64(2)), (47, np.int64(2)), (47, np.int64(1)), (55, np.int64(1))]\n",
      "\n",
      "Current Policy:\n",
      "[['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']]\n",
      "\n",
      "Episode 200\n",
      "Average reward: -4.782\n",
      "Success rate: 3.00%\n",
      "\n",
      "Current Policy:\n",
      "[['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']]\n",
      "\n",
      "Least visited states:\n",
      "State 19: 0 visits\n",
      "State 29: 0 visits\n",
      "State 35: 0 visits\n",
      "State 41: 0 visits\n",
      "State 42: 0 visits\n",
      "\n",
      "GOAL REACHED in episode 213!\n",
      "Path taken: [(0, np.int64(2)), (1, np.int64(1)), (9, np.int64(2)), (10, np.int64(2)), (11, np.int64(2)), (12, np.int64(1)), (20, np.int64(2)), (21, np.int64(2)), (22, np.int64(1)), (30, np.int64(2)), (31, np.int64(1)), (39, np.int64(1)), (47, np.int64(1)), (55, np.int64(1))]\n",
      "\n",
      "Current Policy:\n",
      "[['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']]\n",
      "\n",
      "GOAL REACHED in episode 282!\n",
      "Path taken: [(0, np.int64(0)), (0, np.int64(2)), (1, np.int64(2)), (2, np.int64(2)), (3, np.int64(1)), (11, np.int64(3)), (3, np.int64(1)), (11, np.int64(0)), (10, np.int64(1)), (18, np.int64(1)), (26, np.int64(2)), (27, np.int64(2)), (28, np.int64(1)), (36, np.int64(2)), (37, np.int64(2)), (38, np.int64(0)), (37, np.int64(2)), (38, np.int64(2)), (39, np.int64(3)), (31, np.int64(1)), (39, np.int64(0)), (38, np.int64(2)), (39, np.int64(1)), (47, np.int64(1)), (55, np.int64(1))]\n",
      "\n",
      "Current Policy:\n",
      "[['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']]\n",
      "\n",
      "Episode 300\n",
      "Average reward: -4.880\n",
      "Success rate: 2.00%\n",
      "\n",
      "Current Policy:\n",
      "[['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']]\n",
      "\n",
      "Least visited states:\n",
      "State 19: 0 visits\n",
      "State 29: 0 visits\n",
      "State 35: 0 visits\n",
      "State 41: 0 visits\n",
      "State 42: 0 visits\n",
      "\n",
      "GOAL REACHED in episode 315!\n",
      "Path taken: [(0, np.int64(2)), (1, np.int64(2)), (2, np.int64(1)), (10, np.int64(2)), (11, np.int64(2)), (12, np.int64(0)), (11, np.int64(3)), (3, np.int64(2)), (4, np.int64(0)), (3, np.int64(3)), (3, np.int64(2)), (4, np.int64(0)), (3, np.int64(3)), (3, np.int64(1)), (11, np.int64(3)), (3, np.int64(2)), (4, np.int64(2)), (5, np.int64(0)), (4, np.int64(1)), (12, np.int64(3)), (4, np.int64(2)), (5, np.int64(2)), (6, np.int64(2)), (7, np.int64(0)), (6, np.int64(2)), (7, np.int64(3)), (7, np.int64(1)), (15, np.int64(1)), (23, np.int64(3)), (15, np.int64(1)), (23, np.int64(1)), (31, np.int64(1)), (39, np.int64(1)), (47, np.int64(1)), (55, np.int64(1))]\n",
      "\n",
      "Current Policy:\n",
      "[['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']]\n",
      "\n",
      "GOAL REACHED in episode 352!\n",
      "Path taken: [(0, np.int64(2)), (1, np.int64(2)), (2, np.int64(3)), (2, np.int64(2)), (3, np.int64(2)), (4, np.int64(2)), (5, np.int64(1)), (13, np.int64(1)), (21, np.int64(3)), (13, np.int64(2)), (14, np.int64(2)), (15, np.int64(3)), (7, np.int64(0)), (6, np.int64(2)), (7, np.int64(1)), (15, np.int64(1)), (23, np.int64(1)), (31, np.int64(1)), (39, np.int64(1)), (47, np.int64(1)), (55, np.int64(1))]\n",
      "\n",
      "Current Policy:\n",
      "[['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']]\n",
      "\n",
      "GOAL REACHED in episode 360!\n",
      "Path taken: [(0, np.int64(0)), (0, np.int64(2)), (1, np.int64(2)), (2, np.int64(3)), (2, np.int64(3)), (2, np.int64(0)), (1, np.int64(0)), (0, np.int64(2)), (1, np.int64(2)), (2, np.int64(3)), (2, np.int64(1)), (10, np.int64(2)), (11, np.int64(2)), (12, np.int64(2)), (13, np.int64(2)), (14, np.int64(1)), (22, np.int64(1)), (30, np.int64(2)), (31, np.int64(3)), (23, np.int64(1)), (31, np.int64(1)), (39, np.int64(1)), (47, np.int64(2)), (47, np.int64(3)), (39, np.int64(1)), (47, np.int64(2)), (47, np.int64(3)), (39, np.int64(3)), (31, np.int64(1)), (39, np.int64(1)), (47, np.int64(1)), (55, np.int64(3)), (47, np.int64(3)), (39, np.int64(1)), (47, np.int64(1)), (55, np.int64(1))]\n",
      "\n",
      "Current Policy:\n",
      "[['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']]\n",
      "\n",
      "GOAL REACHED in episode 362!\n",
      "Path taken: [(0, np.int64(1)), (8, np.int64(1)), (16, np.int64(1)), (24, np.int64(3)), (16, np.int64(3)), (8, np.int64(1)), (16, np.int64(1)), (24, np.int64(2)), (25, np.int64(3)), (17, np.int64(2)), (18, np.int64(0)), (17, np.int64(3)), (9, np.int64(2)), (10, np.int64(2)), (11, np.int64(2)), (12, np.int64(2)), (13, np.int64(3)), (5, np.int64(2)), (6, np.int64(3)), (6, np.int64(2)), (7, np.int64(2)), (7, np.int64(3)), (7, np.int64(2)), (7, np.int64(1)), (15, np.int64(1)), (23, np.int64(1)), (31, np.int64(1)), (39, np.int64(1)), (47, np.int64(3)), (39, np.int64(1)), (47, np.int64(3)), (39, np.int64(1)), (47, np.int64(1)), (55, np.int64(1))]\n",
      "\n",
      "Current Policy:\n",
      "[['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']]\n",
      "\n",
      "GOAL REACHED in episode 366!\n",
      "Path taken: [(0, np.int64(2)), (1, np.int64(3)), (1, np.int64(2)), (2, np.int64(1)), (10, np.int64(2)), (11, np.int64(2)), (12, np.int64(1)), (20, np.int64(2)), (21, np.int64(2)), (22, np.int64(0)), (21, np.int64(2)), (22, np.int64(1)), (30, np.int64(1)), (38, np.int64(2)), (39, np.int64(1)), (47, np.int64(3)), (39, np.int64(1)), (47, np.int64(3)), (39, np.int64(1)), (47, np.int64(1)), (55, np.int64(1))]\n",
      "\n",
      "Current Policy:\n",
      "[['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']]\n",
      "\n",
      "GOAL REACHED in episode 379!\n",
      "Path taken: [(0, np.int64(2)), (1, np.int64(2)), (2, np.int64(3)), (2, np.int64(2)), (3, np.int64(1)), (11, np.int64(2)), (12, np.int64(0)), (11, np.int64(2)), (12, np.int64(3)), (4, np.int64(1)), (12, np.int64(0)), (11, np.int64(2)), (12, np.int64(2)), (13, np.int64(1)), (21, np.int64(2)), (22, np.int64(1)), (30, np.int64(1)), (38, np.int64(0)), (37, np.int64(2)), (38, np.int64(2)), (39, np.int64(1)), (47, np.int64(2)), (47, np.int64(1)), (55, np.int64(1))]\n",
      "\n",
      "Current Policy:\n",
      "[['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']]\n",
      "\n",
      "GOAL REACHED in episode 381!\n",
      "Path taken: [(0, np.int64(3)), (0, np.int64(2)), (1, np.int64(2)), (2, np.int64(3)), (2, np.int64(2)), (3, np.int64(1)), (11, np.int64(2)), (12, np.int64(1)), (20, np.int64(1)), (28, np.int64(0)), (27, np.int64(2)), (28, np.int64(0)), (27, np.int64(2)), (28, np.int64(1)), (36, np.int64(2)), (37, np.int64(2)), (38, np.int64(3)), (30, np.int64(1)), (38, np.int64(2)), (39, np.int64(1)), (47, np.int64(2)), (47, np.int64(1)), (55, np.int64(1))]\n",
      "\n",
      "Current Policy:\n",
      "[['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']]\n",
      "\n",
      "GOAL REACHED in episode 382!\n",
      "Path taken: [(0, np.int64(2)), (1, np.int64(2)), (2, np.int64(2)), (3, np.int64(2)), (4, np.int64(2)), (5, np.int64(2)), (6, np.int64(1)), (14, np.int64(1)), (22, np.int64(1)), (30, np.int64(1)), (38, np.int64(2)), (39, np.int64(1)), (47, np.int64(1)), (55, np.int64(1))]\n",
      "\n",
      "Current Policy:\n",
      "[['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']]\n",
      "\n",
      "GOAL REACHED in episode 385!\n",
      "Path taken: [(0, np.int64(2)), (1, np.int64(1)), (9, np.int64(1)), (17, np.int64(1)), (25, np.int64(2)), (26, np.int64(2)), (27, np.int64(2)), (28, np.int64(1)), (36, np.int64(2)), (37, np.int64(2)), (38, np.int64(3)), (30, np.int64(1)), (38, np.int64(2)), (39, np.int64(1)), (47, np.int64(3)), (39, np.int64(3)), (31, np.int64(1)), (39, np.int64(1)), (47, np.int64(3)), (39, np.int64(1)), (47, np.int64(1)), (55, np.int64(1))]\n",
      "\n",
      "Current Policy:\n",
      "[['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']]\n",
      "\n",
      "GOAL REACHED in episode 388!\n",
      "Path taken: [(0, np.int64(2)), (1, np.int64(1)), (9, np.int64(2)), (10, np.int64(3)), (2, np.int64(1)), (10, np.int64(2)), (11, np.int64(0)), (10, np.int64(2)), (11, np.int64(2)), (12, np.int64(1)), (20, np.int64(2)), (21, np.int64(3)), (13, np.int64(2)), (14, np.int64(2)), (15, np.int64(1)), (23, np.int64(3)), (15, np.int64(1)), (23, np.int64(3)), (15, np.int64(1)), (23, np.int64(0)), (22, np.int64(2)), (23, np.int64(1)), (31, np.int64(1)), (39, np.int64(1)), (47, np.int64(1)), (55, np.int64(2)), (55, np.int64(1))]\n",
      "\n",
      "Current Policy:\n",
      "[['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']]\n",
      "\n",
      "GOAL REACHED in episode 391!\n",
      "Path taken: [(0, np.int64(3)), (0, np.int64(2)), (1, np.int64(2)), (2, np.int64(3)), (2, np.int64(2)), (3, np.int64(2)), (4, np.int64(1)), (12, np.int64(2)), (13, np.int64(3)), (5, np.int64(0)), (4, np.int64(0)), (3, np.int64(2)), (4, np.int64(2)), (5, np.int64(1)), (13, np.int64(1)), (21, np.int64(2)), (22, np.int64(1)), (30, np.int64(3)), (22, np.int64(1)), (30, np.int64(1)), (38, np.int64(2)), (39, np.int64(1)), (47, np.int64(1)), (55, np.int64(2)), (55, np.int64(1))]\n",
      "\n",
      "Current Policy:\n",
      "[['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']]\n",
      "\n",
      "GOAL REACHED in episode 392!\n",
      "Path taken: [(0, np.int64(0)), (0, np.int64(1)), (8, np.int64(3)), (0, np.int64(2)), (1, np.int64(0)), (0, np.int64(1)), (8, np.int64(0)), (8, np.int64(1)), (16, np.int64(1)), (24, np.int64(2)), (25, np.int64(1)), (33, np.int64(2)), (34, np.int64(0)), (33, np.int64(3)), (25, np.int64(2)), (26, np.int64(0)), (25, np.int64(2)), (26, np.int64(2)), (27, np.int64(2)), (28, np.int64(3)), (20, np.int64(3)), (12, np.int64(1)), (20, np.int64(1)), (28, np.int64(1)), (36, np.int64(2)), (37, np.int64(2)), (38, np.int64(2)), (39, np.int64(3)), (31, np.int64(1)), (39, np.int64(1)), (47, np.int64(1)), (55, np.int64(1))]\n",
      "\n",
      "Current Policy:\n",
      "[['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']]\n",
      "\n",
      "GOAL REACHED in episode 393!\n",
      "Path taken: [(0, np.int64(3)), (0, np.int64(0)), (0, np.int64(1)), (8, np.int64(2)), (9, np.int64(0)), (8, np.int64(3)), (0, np.int64(2)), (1, np.int64(2)), (2, np.int64(2)), (3, np.int64(3)), (3, np.int64(2)), (4, np.int64(3)), (4, np.int64(1)), (12, np.int64(1)), (20, np.int64(3)), (12, np.int64(2)), (13, np.int64(2)), (14, np.int64(1)), (22, np.int64(1)), (30, np.int64(1)), (38, np.int64(2)), (39, np.int64(1)), (47, np.int64(2)), (47, np.int64(1)), (55, np.int64(1))]\n",
      "\n",
      "Current Policy:\n",
      "[['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']]\n",
      "\n",
      "Episode 400\n",
      "Average reward: -3.226\n",
      "Success rate: 13.00%\n",
      "\n",
      "Current Policy:\n",
      "[['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']]\n",
      "\n",
      "Least visited states:\n",
      "State 19: 0 visits\n",
      "State 29: 0 visits\n",
      "State 35: 0 visits\n",
      "State 41: 0 visits\n",
      "State 42: 0 visits\n",
      "\n",
      "GOAL REACHED in episode 410!\n",
      "Path taken: [(0, np.int64(1)), (8, np.int64(3)), (0, np.int64(2)), (1, np.int64(2)), (2, np.int64(2)), (3, np.int64(2)), (4, np.int64(1)), (12, np.int64(1)), (20, np.int64(2)), (21, np.int64(2)), (22, np.int64(1)), (30, np.int64(1)), (38, np.int64(0)), (37, np.int64(0)), (36, np.int64(2)), (37, np.int64(2)), (38, np.int64(2)), (39, np.int64(1)), (47, np.int64(1)), (55, np.int64(1))]\n",
      "\n",
      "Current Policy:\n",
      "[['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']]\n",
      "\n",
      "GOAL REACHED in episode 412!\n",
      "Path taken: [(0, np.int64(1)), (8, np.int64(2)), (9, np.int64(1)), (17, np.int64(3)), (9, np.int64(3)), (1, np.int64(1)), (9, np.int64(0)), (8, np.int64(2)), (9, np.int64(2)), (10, np.int64(2)), (11, np.int64(3)), (3, np.int64(0)), (2, np.int64(2)), (3, np.int64(1)), (11, np.int64(2)), (12, np.int64(2)), (13, np.int64(0)), (12, np.int64(1)), (20, np.int64(2)), (21, np.int64(2)), (22, np.int64(1)), (30, np.int64(1)), (38, np.int64(2)), (39, np.int64(1)), (47, np.int64(3)), (39, np.int64(3)), (31, np.int64(1)), (39, np.int64(1)), (47, np.int64(3)), (39, np.int64(1)), (47, np.int64(1)), (55, np.int64(1))]\n",
      "\n",
      "Current Policy:\n",
      "[['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']]\n",
      "\n",
      "GOAL REACHED in episode 421!\n",
      "Path taken: [(0, np.int64(1)), (8, np.int64(3)), (0, np.int64(2)), (1, np.int64(1)), (9, np.int64(2)), (10, np.int64(2)), (11, np.int64(2)), (12, np.int64(1)), (20, np.int64(2)), (21, np.int64(2)), (22, np.int64(1)), (30, np.int64(2)), (31, np.int64(0)), (30, np.int64(1)), (38, np.int64(2)), (39, np.int64(2)), (39, np.int64(1)), (47, np.int64(1)), (55, np.int64(1))]\n",
      "\n",
      "Current Policy:\n",
      "[['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']]\n",
      "\n",
      "GOAL REACHED in episode 431!\n",
      "Path taken: [(0, np.int64(1)), (8, np.int64(2)), (9, np.int64(0)), (8, np.int64(2)), (9, np.int64(2)), (10, np.int64(3)), (2, np.int64(1)), (10, np.int64(2)), (11, np.int64(3)), (3, np.int64(2)), (4, np.int64(2)), (5, np.int64(3)), (5, np.int64(0)), (4, np.int64(3)), (4, np.int64(2)), (5, np.int64(0)), (4, np.int64(1)), (12, np.int64(0)), (11, np.int64(2)), (12, np.int64(2)), (13, np.int64(0)), (12, np.int64(1)), (20, np.int64(3)), (12, np.int64(1)), (20, np.int64(2)), (21, np.int64(2)), (22, np.int64(1)), (30, np.int64(2)), (31, np.int64(3)), (23, np.int64(1)), (31, np.int64(1)), (39, np.int64(2)), (39, np.int64(1)), (47, np.int64(1)), (55, np.int64(1))]\n",
      "\n",
      "Current Policy:\n",
      "[['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']]\n",
      "\n",
      "GOAL REACHED in episode 434!\n",
      "Path taken: [(0, np.int64(1)), (8, np.int64(1)), (16, np.int64(1)), (24, np.int64(1)), (32, np.int64(0)), (32, np.int64(3)), (24, np.int64(2)), (25, np.int64(2)), (26, np.int64(2)), (27, np.int64(2)), (28, np.int64(1)), (36, np.int64(2)), (37, np.int64(2)), (38, np.int64(2)), (39, np.int64(1)), (47, np.int64(3)), (39, np.int64(1)), (47, np.int64(1)), (55, np.int64(1))]\n",
      "\n",
      "Current Policy:\n",
      "[['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']]\n",
      "\n",
      "GOAL REACHED in episode 436!\n",
      "Path taken: [(0, np.int64(2)), (1, np.int64(1)), (9, np.int64(1)), (17, np.int64(1)), (25, np.int64(3)), (17, np.int64(1)), (25, np.int64(2)), (26, np.int64(2)), (27, np.int64(2)), (28, np.int64(1)), (36, np.int64(2)), (37, np.int64(2)), (38, np.int64(2)), (39, np.int64(1)), (47, np.int64(2)), (47, np.int64(1)), (55, np.int64(1))]\n",
      "\n",
      "Current Policy:\n",
      "[['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']]\n",
      "\n",
      "GOAL REACHED in episode 440!\n",
      "Path taken: [(0, np.int64(2)), (1, np.int64(1)), (9, np.int64(3)), (1, np.int64(3)), (1, np.int64(2)), (2, np.int64(2)), (3, np.int64(2)), (4, np.int64(1)), (12, np.int64(1)), (20, np.int64(2)), (21, np.int64(2)), (22, np.int64(1)), (30, np.int64(3)), (22, np.int64(1)), (30, np.int64(1)), (38, np.int64(0)), (37, np.int64(2)), (38, np.int64(2)), (39, np.int64(2)), (39, np.int64(3)), (31, np.int64(1)), (39, np.int64(1)), (47, np.int64(1)), (55, np.int64(3)), (47, np.int64(1)), (55, np.int64(1))]\n",
      "\n",
      "Current Policy:\n",
      "[['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']]\n",
      "\n",
      "GOAL REACHED in episode 444!\n",
      "Path taken: [(0, np.int64(2)), (1, np.int64(2)), (2, np.int64(3)), (2, np.int64(2)), (3, np.int64(0)), (2, np.int64(2)), (3, np.int64(2)), (4, np.int64(2)), (5, np.int64(3)), (5, np.int64(0)), (4, np.int64(2)), (5, np.int64(1)), (13, np.int64(3)), (5, np.int64(1)), (13, np.int64(1)), (21, np.int64(3)), (13, np.int64(1)), (21, np.int64(2)), (22, np.int64(1)), (30, np.int64(1)), (38, np.int64(2)), (39, np.int64(2)), (39, np.int64(1)), (47, np.int64(1)), (55, np.int64(1))]\n",
      "\n",
      "Current Policy:\n",
      "[['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']]\n",
      "\n",
      "GOAL REACHED in episode 446!\n",
      "Path taken: [(0, np.int64(2)), (1, np.int64(0)), (0, np.int64(0)), (0, np.int64(1)), (8, np.int64(1)), (16, np.int64(2)), (17, np.int64(2)), (18, np.int64(1)), (26, np.int64(1)), (34, np.int64(3)), (26, np.int64(3)), (18, np.int64(3)), (10, np.int64(2)), (11, np.int64(3)), (3, np.int64(0)), (2, np.int64(2)), (3, np.int64(2)), (4, np.int64(3)), (4, np.int64(1)), (12, np.int64(1)), (20, np.int64(2)), (21, np.int64(2)), (22, np.int64(1)), (30, np.int64(1)), (38, np.int64(2)), (39, np.int64(1)), (47, np.int64(3)), (39, np.int64(3)), (31, np.int64(1)), (39, np.int64(1)), (47, np.int64(1)), (55, np.int64(1))]\n",
      "\n",
      "Current Policy:\n",
      "[['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']]\n",
      "\n",
      "GOAL REACHED in episode 451!\n",
      "Path taken: [(0, np.int64(1)), (8, np.int64(0)), (8, np.int64(2)), (9, np.int64(2)), (10, np.int64(2)), (11, np.int64(2)), (12, np.int64(1)), (20, np.int64(2)), (21, np.int64(2)), (22, np.int64(3)), (14, np.int64(3)), (6, np.int64(1)), (14, np.int64(1)), (22, np.int64(1)), (30, np.int64(1)), (38, np.int64(2)), (39, np.int64(1)), (47, np.int64(1)), (55, np.int64(1))]\n",
      "\n",
      "Current Policy:\n",
      "[['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']]\n",
      "\n",
      "GOAL REACHED in episode 452!\n",
      "Path taken: [(0, np.int64(1)), (8, np.int64(1)), (16, np.int64(0)), (16, np.int64(1)), (24, np.int64(2)), (25, np.int64(2)), (26, np.int64(0)), (25, np.int64(2)), (26, np.int64(2)), (27, np.int64(2)), (28, np.int64(1)), (36, np.int64(2)), (37, np.int64(0)), (36, np.int64(2)), (37, np.int64(2)), (38, np.int64(2)), (39, np.int64(1)), (47, np.int64(1)), (55, np.int64(2)), (55, np.int64(1))]\n",
      "\n",
      "Current Policy:\n",
      "[['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']]\n",
      "\n",
      "GOAL REACHED in episode 455!\n",
      "Path taken: [(0, np.int64(1)), (8, np.int64(1)), (16, np.int64(0)), (16, np.int64(1)), (24, np.int64(2)), (25, np.int64(1)), (33, np.int64(3)), (25, np.int64(0)), (24, np.int64(2)), (25, np.int64(2)), (26, np.int64(0)), (25, np.int64(2)), (26, np.int64(1)), (34, np.int64(3)), (26, np.int64(3)), (18, np.int64(1)), (26, np.int64(0)), (25, np.int64(2)), (26, np.int64(1)), (34, np.int64(3)), (26, np.int64(1)), (34, np.int64(3)), (26, np.int64(2)), (27, np.int64(2)), (28, np.int64(1)), (36, np.int64(2)), (37, np.int64(2)), (38, np.int64(0)), (37, np.int64(1)), (45, np.int64(3)), (37, np.int64(2)), (38, np.int64(2)), (39, np.int64(3)), (31, np.int64(1)), (39, np.int64(1)), (47, np.int64(1)), (55, np.int64(2)), (55, np.int64(3)), (47, np.int64(1)), (55, np.int64(1))]\n",
      "\n",
      "Current Policy:\n",
      "[['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']]\n",
      "\n",
      "GOAL REACHED in episode 458!\n",
      "Path taken: [(0, np.int64(0)), (0, np.int64(1)), (8, np.int64(2)), (9, np.int64(1)), (17, np.int64(1)), (25, np.int64(1)), (33, np.int64(2)), (34, np.int64(3)), (26, np.int64(2)), (27, np.int64(2)), (28, np.int64(1)), (36, np.int64(2)), (37, np.int64(2)), (38, np.int64(2)), (39, np.int64(1)), (47, np.int64(3)), (39, np.int64(1)), (47, np.int64(1)), (55, np.int64(2)), (55, np.int64(2)), (55, np.int64(1))]\n",
      "\n",
      "Current Policy:\n",
      "[['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']]\n",
      "\n",
      "GOAL REACHED in episode 459!\n",
      "Path taken: [(0, np.int64(0)), (0, np.int64(1)), (8, np.int64(1)), (16, np.int64(0)), (16, np.int64(2)), (17, np.int64(0)), (16, np.int64(1)), (24, np.int64(3)), (16, np.int64(2)), (17, np.int64(0)), (16, np.int64(1)), (24, np.int64(0)), (24, np.int64(2)), (25, np.int64(2)), (26, np.int64(2)), (27, np.int64(2)), (28, np.int64(1)), (36, np.int64(2)), (37, np.int64(2)), (38, np.int64(2)), (39, np.int64(1)), (47, np.int64(1)), (55, np.int64(1))]\n",
      "\n",
      "Current Policy:\n",
      "[['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']]\n",
      "\n",
      "GOAL REACHED in episode 461!\n",
      "Path taken: [(0, np.int64(2)), (1, np.int64(3)), (1, np.int64(2)), (2, np.int64(2)), (3, np.int64(0)), (2, np.int64(0)), (1, np.int64(2)), (2, np.int64(2)), (3, np.int64(2)), (4, np.int64(1)), (12, np.int64(1)), (20, np.int64(3)), (12, np.int64(3)), (4, np.int64(1)), (12, np.int64(3)), (4, np.int64(0)), (3, np.int64(2)), (4, np.int64(3)), (4, np.int64(2)), (5, np.int64(1)), (13, np.int64(1)), (21, np.int64(3)), (13, np.int64(1)), (21, np.int64(2)), (22, np.int64(1)), (30, np.int64(1)), (38, np.int64(2)), (39, np.int64(1)), (47, np.int64(2)), (47, np.int64(1)), (55, np.int64(3)), (47, np.int64(1)), (55, np.int64(1))]\n",
      "\n",
      "Current Policy:\n",
      "[['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']]\n",
      "\n",
      "GOAL REACHED in episode 468!\n",
      "Path taken: [(0, np.int64(2)), (1, np.int64(2)), (2, np.int64(2)), (3, np.int64(3)), (3, np.int64(2)), (4, np.int64(3)), (4, np.int64(1)), (12, np.int64(1)), (20, np.int64(2)), (21, np.int64(2)), (22, np.int64(3)), (14, np.int64(0)), (13, np.int64(1)), (21, np.int64(0)), (20, np.int64(1)), (28, np.int64(1)), (36, np.int64(2)), (37, np.int64(2)), (38, np.int64(2)), (39, np.int64(0)), (38, np.int64(2)), (39, np.int64(3)), (31, np.int64(1)), (39, np.int64(2)), (39, np.int64(1)), (47, np.int64(1)), (55, np.int64(1))]\n",
      "\n",
      "Current Policy:\n",
      "[['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']]\n",
      "\n",
      "GOAL REACHED in episode 471!\n",
      "Path taken: [(0, np.int64(2)), (1, np.int64(2)), (2, np.int64(2)), (3, np.int64(1)), (11, np.int64(2)), (12, np.int64(1)), (20, np.int64(2)), (21, np.int64(2)), (22, np.int64(1)), (30, np.int64(1)), (38, np.int64(2)), (39, np.int64(1)), (47, np.int64(3)), (39, np.int64(0)), (38, np.int64(2)), (39, np.int64(1)), (47, np.int64(1)), (55, np.int64(1))]\n",
      "\n",
      "Current Policy:\n",
      "[['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']]\n",
      "\n",
      "GOAL REACHED in episode 481!\n",
      "Path taken: [(0, np.int64(0)), (0, np.int64(2)), (1, np.int64(2)), (2, np.int64(2)), (3, np.int64(2)), (4, np.int64(0)), (3, np.int64(1)), (11, np.int64(2)), (12, np.int64(1)), (20, np.int64(2)), (21, np.int64(2)), (22, np.int64(1)), (30, np.int64(2)), (31, np.int64(1)), (39, np.int64(0)), (38, np.int64(2)), (39, np.int64(3)), (31, np.int64(1)), (39, np.int64(1)), (47, np.int64(1)), (55, np.int64(1))]\n",
      "\n",
      "Current Policy:\n",
      "[['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']]\n",
      "\n",
      "GOAL REACHED in episode 482!\n",
      "Path taken: [(0, np.int64(1)), (8, np.int64(2)), (9, np.int64(2)), (10, np.int64(2)), (11, np.int64(2)), (12, np.int64(1)), (20, np.int64(2)), (21, np.int64(2)), (22, np.int64(1)), (30, np.int64(1)), (38, np.int64(0)), (37, np.int64(2)), (38, np.int64(2)), (39, np.int64(3)), (31, np.int64(0)), (30, np.int64(1)), (38, np.int64(2)), (39, np.int64(3)), (31, np.int64(1)), (39, np.int64(1)), (47, np.int64(2)), (47, np.int64(2)), (47, np.int64(3)), (39, np.int64(1)), (47, np.int64(3)), (39, np.int64(3)), (31, np.int64(1)), (39, np.int64(1)), (47, np.int64(1)), (55, np.int64(1))]\n",
      "\n",
      "Current Policy:\n",
      "[['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']]\n",
      "\n",
      "GOAL REACHED in episode 484!\n",
      "Path taken: [(0, np.int64(2)), (1, np.int64(3)), (1, np.int64(2)), (2, np.int64(0)), (1, np.int64(2)), (2, np.int64(1)), (10, np.int64(2)), (11, np.int64(2)), (12, np.int64(1)), (20, np.int64(2)), (21, np.int64(2)), (22, np.int64(1)), (30, np.int64(1)), (38, np.int64(2)), (39, np.int64(2)), (39, np.int64(2)), (39, np.int64(1)), (47, np.int64(2)), (47, np.int64(1)), (55, np.int64(1))]\n",
      "\n",
      "Current Policy:\n",
      "[['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']]\n",
      "\n",
      "GOAL REACHED in episode 485!\n",
      "Path taken: [(0, np.int64(2)), (1, np.int64(2)), (2, np.int64(2)), (3, np.int64(3)), (3, np.int64(2)), (4, np.int64(1)), (12, np.int64(1)), (20, np.int64(3)), (12, np.int64(1)), (20, np.int64(2)), (21, np.int64(2)), (22, np.int64(1)), (30, np.int64(3)), (22, np.int64(1)), (30, np.int64(1)), (38, np.int64(2)), (39, np.int64(1)), (47, np.int64(1)), (55, np.int64(3)), (47, np.int64(1)), (55, np.int64(1))]\n",
      "\n",
      "Current Policy:\n",
      "[['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']]\n",
      "\n",
      "GOAL REACHED in episode 490!\n",
      "Path taken: [(0, np.int64(2)), (1, np.int64(1)), (9, np.int64(2)), (10, np.int64(2)), (11, np.int64(2)), (12, np.int64(2)), (13, np.int64(1)), (21, np.int64(2)), (22, np.int64(2)), (23, np.int64(1)), (31, np.int64(3)), (23, np.int64(1)), (31, np.int64(1)), (39, np.int64(1)), (47, np.int64(1)), (55, np.int64(1))]\n",
      "\n",
      "Current Policy:\n",
      "[['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']]\n",
      "\n",
      "GOAL REACHED in episode 494!\n",
      "Path taken: [(0, np.int64(2)), (1, np.int64(2)), (2, np.int64(0)), (1, np.int64(2)), (2, np.int64(2)), (3, np.int64(1)), (11, np.int64(2)), (12, np.int64(0)), (11, np.int64(2)), (12, np.int64(1)), (20, np.int64(2)), (21, np.int64(2)), (22, np.int64(2)), (23, np.int64(2)), (23, np.int64(2)), (23, np.int64(0)), (22, np.int64(1)), (30, np.int64(1)), (38, np.int64(2)), (39, np.int64(1)), (47, np.int64(1)), (55, np.int64(1))]\n",
      "\n",
      "Current Policy:\n",
      "[['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']]\n",
      "\n",
      "GOAL REACHED in episode 496!\n",
      "Path taken: [(0, np.int64(1)), (8, np.int64(2)), (9, np.int64(0)), (8, np.int64(3)), (0, np.int64(0)), (0, np.int64(2)), (1, np.int64(0)), (0, np.int64(2)), (1, np.int64(2)), (2, np.int64(1)), (10, np.int64(2)), (11, np.int64(2)), (12, np.int64(1)), (20, np.int64(2)), (21, np.int64(3)), (13, np.int64(1)), (21, np.int64(2)), (22, np.int64(2)), (23, np.int64(1)), (31, np.int64(1)), (39, np.int64(1)), (47, np.int64(1)), (55, np.int64(1))]\n",
      "\n",
      "Current Policy:\n",
      "[['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']]\n",
      "\n",
      "Episode 500\n",
      "Average reward: -1.567\n",
      "Success rate: 24.00%\n",
      "\n",
      "Current Policy:\n",
      "[['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']]\n",
      "\n",
      "Least visited states:\n",
      "State 19: 0 visits\n",
      "State 29: 0 visits\n",
      "State 35: 0 visits\n",
      "State 41: 0 visits\n",
      "State 42: 0 visits\n",
      "\n",
      "GOAL REACHED in episode 501!\n",
      "Path taken: [(0, np.int64(2)), (1, np.int64(2)), (2, np.int64(2)), (3, np.int64(1)), (11, np.int64(2)), (12, np.int64(3)), (4, np.int64(2)), (5, np.int64(1)), (13, np.int64(1)), (21, np.int64(2)), (22, np.int64(0)), (21, np.int64(2)), (22, np.int64(1)), (30, np.int64(1)), (38, np.int64(2)), (39, np.int64(1)), (47, np.int64(3)), (39, np.int64(1)), (47, np.int64(1)), (55, np.int64(1))]\n",
      "\n",
      "Current Policy:\n",
      "[['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']]\n",
      "\n",
      "GOAL REACHED in episode 504!\n",
      "Path taken: [(0, np.int64(2)), (1, np.int64(0)), (0, np.int64(2)), (1, np.int64(1)), (9, np.int64(2)), (10, np.int64(2)), (11, np.int64(2)), (12, np.int64(1)), (20, np.int64(2)), (21, np.int64(2)), (22, np.int64(1)), (30, np.int64(1)), (38, np.int64(2)), (39, np.int64(1)), (47, np.int64(2)), (47, np.int64(1)), (55, np.int64(1))]\n",
      "\n",
      "Current Policy:\n",
      "[['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']]\n",
      "\n",
      "GOAL REACHED in episode 507!\n",
      "Path taken: [(0, np.int64(2)), (1, np.int64(2)), (2, np.int64(3)), (2, np.int64(2)), (3, np.int64(2)), (4, np.int64(3)), (4, np.int64(1)), (12, np.int64(0)), (11, np.int64(2)), (12, np.int64(1)), (20, np.int64(3)), (12, np.int64(3)), (4, np.int64(3)), (4, np.int64(0)), (3, np.int64(2)), (4, np.int64(1)), (12, np.int64(1)), (20, np.int64(2)), (21, np.int64(2)), (22, np.int64(2)), (23, np.int64(1)), (31, np.int64(3)), (23, np.int64(1)), (31, np.int64(1)), (39, np.int64(1)), (47, np.int64(3)), (39, np.int64(1)), (47, np.int64(1)), (55, np.int64(2)), (55, np.int64(1))]\n",
      "\n",
      "Current Policy:\n",
      "[['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']]\n",
      "\n",
      "GOAL REACHED in episode 508!\n",
      "Path taken: [(0, np.int64(2)), (1, np.int64(2)), (2, np.int64(2)), (3, np.int64(2)), (4, np.int64(1)), (12, np.int64(2)), (13, np.int64(1)), (21, np.int64(2)), (22, np.int64(1)), (30, np.int64(3)), (22, np.int64(1)), (30, np.int64(1)), (38, np.int64(3)), (30, np.int64(1)), (38, np.int64(2)), (39, np.int64(1)), (47, np.int64(1)), (55, np.int64(1))]\n",
      "\n",
      "Current Policy:\n",
      "[['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']]\n",
      "\n",
      "GOAL REACHED in episode 511!\n",
      "Path taken: [(0, np.int64(2)), (1, np.int64(0)), (0, np.int64(2)), (1, np.int64(0)), (0, np.int64(1)), (8, np.int64(2)), (9, np.int64(2)), (10, np.int64(3)), (2, np.int64(2)), (3, np.int64(3)), (3, np.int64(2)), (4, np.int64(1)), (12, np.int64(1)), (20, np.int64(2)), (21, np.int64(2)), (22, np.int64(2)), (23, np.int64(1)), (31, np.int64(1)), (39, np.int64(1)), (47, np.int64(1)), (55, np.int64(1))]\n",
      "\n",
      "Current Policy:\n",
      "[['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']]\n",
      "\n",
      "GOAL REACHED in episode 513!\n",
      "Path taken: [(0, np.int64(2)), (1, np.int64(2)), (2, np.int64(2)), (3, np.int64(3)), (3, np.int64(3)), (3, np.int64(1)), (11, np.int64(3)), (3, np.int64(3)), (3, np.int64(2)), (4, np.int64(1)), (12, np.int64(1)), (20, np.int64(2)), (21, np.int64(0)), (20, np.int64(2)), (21, np.int64(2)), (22, np.int64(3)), (14, np.int64(1)), (22, np.int64(1)), (30, np.int64(3)), (22, np.int64(1)), (30, np.int64(2)), (31, np.int64(1)), (39, np.int64(1)), (47, np.int64(2)), (47, np.int64(1)), (55, np.int64(1))]\n",
      "\n",
      "Current Policy:\n",
      "[['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']]\n",
      "\n",
      "GOAL REACHED in episode 519!\n",
      "Path taken: [(0, np.int64(2)), (1, np.int64(2)), (2, np.int64(2)), (3, np.int64(2)), (4, np.int64(0)), (3, np.int64(0)), (2, np.int64(0)), (1, np.int64(2)), (2, np.int64(2)), (3, np.int64(2)), (4, np.int64(1)), (12, np.int64(1)), (20, np.int64(1)), (28, np.int64(1)), (36, np.int64(2)), (37, np.int64(1)), (45, np.int64(3)), (37, np.int64(2)), (38, np.int64(2)), (39, np.int64(3)), (31, np.int64(0)), (30, np.int64(2)), (31, np.int64(1)), (39, np.int64(1)), (47, np.int64(3)), (39, np.int64(1)), (47, np.int64(3)), (39, np.int64(1)), (47, np.int64(1)), (55, np.int64(1))]\n",
      "\n",
      "Current Policy:\n",
      "[['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']]\n",
      "\n",
      "GOAL REACHED in episode 523!\n",
      "Path taken: [(0, np.int64(2)), (1, np.int64(2)), (2, np.int64(2)), (3, np.int64(3)), (3, np.int64(0)), (2, np.int64(3)), (2, np.int64(2)), (3, np.int64(1)), (11, np.int64(3)), (3, np.int64(2)), (4, np.int64(3)), (4, np.int64(1)), (12, np.int64(1)), (20, np.int64(2)), (21, np.int64(3)), (13, np.int64(1)), (21, np.int64(2)), (22, np.int64(1)), (30, np.int64(1)), (38, np.int64(2)), (39, np.int64(1)), (47, np.int64(1)), (55, np.int64(1))]\n",
      "\n",
      "Current Policy:\n",
      "[['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']]\n",
      "\n",
      "GOAL REACHED in episode 526!\n",
      "Path taken: [(0, np.int64(2)), (1, np.int64(2)), (2, np.int64(2)), (3, np.int64(2)), (4, np.int64(3)), (4, np.int64(1)), (12, np.int64(1)), (20, np.int64(2)), (21, np.int64(2)), (22, np.int64(1)), (30, np.int64(3)), (22, np.int64(3)), (14, np.int64(1)), (22, np.int64(1)), (30, np.int64(1)), (38, np.int64(0)), (37, np.int64(2)), (38, np.int64(0)), (37, np.int64(2)), (38, np.int64(2)), (39, np.int64(1)), (47, np.int64(1)), (55, np.int64(1))]\n",
      "\n",
      "Current Policy:\n",
      "[['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']]\n",
      "\n",
      "GOAL REACHED in episode 528!\n",
      "Path taken: [(0, np.int64(2)), (1, np.int64(2)), (2, np.int64(2)), (3, np.int64(1)), (11, np.int64(0)), (10, np.int64(0)), (9, np.int64(2)), (10, np.int64(2)), (11, np.int64(2)), (12, np.int64(1)), (20, np.int64(2)), (21, np.int64(2)), (22, np.int64(2)), (23, np.int64(1)), (31, np.int64(0)), (30, np.int64(1)), (38, np.int64(2)), (39, np.int64(3)), (31, np.int64(3)), (23, np.int64(1)), (31, np.int64(2)), (31, np.int64(1)), (39, np.int64(0)), (38, np.int64(2)), (39, np.int64(1)), (47, np.int64(1)), (55, np.int64(1))]\n",
      "\n",
      "Current Policy:\n",
      "[['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']]\n",
      "\n",
      "GOAL REACHED in episode 531!\n",
      "Path taken: [(0, np.int64(2)), (1, np.int64(0)), (0, np.int64(2)), (1, np.int64(2)), (2, np.int64(2)), (3, np.int64(2)), (4, np.int64(1)), (12, np.int64(1)), (20, np.int64(2)), (21, np.int64(2)), (22, np.int64(1)), (30, np.int64(1)), (38, np.int64(2)), (39, np.int64(1)), (47, np.int64(1)), (55, np.int64(1))]\n",
      "\n",
      "Current Policy:\n",
      "[['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']]\n",
      "\n",
      "GOAL REACHED in episode 532!\n",
      "Path taken: [(0, np.int64(2)), (1, np.int64(3)), (1, np.int64(2)), (2, np.int64(2)), (3, np.int64(2)), (4, np.int64(1)), (12, np.int64(1)), (20, np.int64(2)), (21, np.int64(2)), (22, np.int64(1)), (30, np.int64(1)), (38, np.int64(2)), (39, np.int64(2)), (39, np.int64(1)), (47, np.int64(1)), (55, np.int64(1))]\n",
      "\n",
      "Current Policy:\n",
      "[['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']]\n",
      "\n",
      "GOAL REACHED in episode 533!\n",
      "Path taken: [(0, np.int64(2)), (1, np.int64(2)), (2, np.int64(2)), (3, np.int64(2)), (4, np.int64(0)), (3, np.int64(2)), (4, np.int64(0)), (3, np.int64(2)), (4, np.int64(1)), (12, np.int64(3)), (4, np.int64(3)), (4, np.int64(1)), (12, np.int64(1)), (20, np.int64(1)), (28, np.int64(1)), (36, np.int64(2)), (37, np.int64(2)), (38, np.int64(2)), (39, np.int64(1)), (47, np.int64(1)), (55, np.int64(1))]\n",
      "\n",
      "Current Policy:\n",
      "[['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']]\n",
      "\n",
      "GOAL REACHED in episode 535!\n",
      "Path taken: [(0, np.int64(2)), (1, np.int64(1)), (9, np.int64(3)), (1, np.int64(1)), (9, np.int64(0)), (8, np.int64(3)), (0, np.int64(2)), (1, np.int64(3)), (1, np.int64(1)), (9, np.int64(2)), (10, np.int64(3)), (2, np.int64(2)), (3, np.int64(3)), (3, np.int64(2)), (4, np.int64(1)), (12, np.int64(1)), (20, np.int64(1)), (28, np.int64(1)), (36, np.int64(2)), (37, np.int64(2)), (38, np.int64(2)), (39, np.int64(1)), (47, np.int64(1)), (55, np.int64(1))]\n",
      "\n",
      "Current Policy:\n",
      "[['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']]\n",
      "\n",
      "GOAL REACHED in episode 538!\n",
      "Path taken: [(0, np.int64(2)), (1, np.int64(2)), (2, np.int64(2)), (3, np.int64(2)), (4, np.int64(1)), (12, np.int64(1)), (20, np.int64(2)), (21, np.int64(3)), (13, np.int64(2)), (14, np.int64(1)), (22, np.int64(1)), (30, np.int64(1)), (38, np.int64(2)), (39, np.int64(3)), (31, np.int64(1)), (39, np.int64(1)), (47, np.int64(3)), (39, np.int64(1)), (47, np.int64(2)), (47, np.int64(1)), (55, np.int64(1))]\n",
      "\n",
      "Current Policy:\n",
      "[['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']]\n",
      "\n",
      "GOAL REACHED in episode 540!\n",
      "Path taken: [(0, np.int64(2)), (1, np.int64(2)), (2, np.int64(2)), (3, np.int64(3)), (3, np.int64(2)), (4, np.int64(1)), (12, np.int64(1)), (20, np.int64(2)), (21, np.int64(2)), (22, np.int64(2)), (23, np.int64(1)), (31, np.int64(1)), (39, np.int64(2)), (39, np.int64(1)), (47, np.int64(3)), (39, np.int64(1)), (47, np.int64(1)), (55, np.int64(1))]\n",
      "\n",
      "Current Policy:\n",
      "[['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']]\n",
      "\n",
      "GOAL REACHED in episode 541!\n",
      "Path taken: [(0, np.int64(1)), (8, np.int64(3)), (0, np.int64(2)), (1, np.int64(2)), (2, np.int64(0)), (1, np.int64(2)), (2, np.int64(0)), (1, np.int64(2)), (2, np.int64(3)), (2, np.int64(2)), (3, np.int64(2)), (4, np.int64(1)), (12, np.int64(1)), (20, np.int64(2)), (21, np.int64(2)), (22, np.int64(1)), (30, np.int64(1)), (38, np.int64(3)), (30, np.int64(1)), (38, np.int64(2)), (39, np.int64(2)), (39, np.int64(3)), (31, np.int64(1)), (39, np.int64(1)), (47, np.int64(3)), (39, np.int64(1)), (47, np.int64(1)), (55, np.int64(1))]\n",
      "\n",
      "Current Policy:\n",
      "[['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']]\n",
      "\n",
      "GOAL REACHED in episode 545!\n",
      "Path taken: [(0, np.int64(2)), (1, np.int64(2)), (2, np.int64(2)), (3, np.int64(2)), (4, np.int64(1)), (12, np.int64(0)), (11, np.int64(3)), (3, np.int64(3)), (3, np.int64(2)), (4, np.int64(1)), (12, np.int64(1)), (20, np.int64(2)), (21, np.int64(2)), (22, np.int64(1)), (30, np.int64(1)), (38, np.int64(2)), (39, np.int64(1)), (47, np.int64(1)), (55, np.int64(3)), (47, np.int64(1)), (55, np.int64(2)), (55, np.int64(1))]\n",
      "\n",
      "Current Policy:\n",
      "[['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']]\n",
      "\n",
      "GOAL REACHED in episode 546!\n",
      "Path taken: [(0, np.int64(1)), (8, np.int64(2)), (9, np.int64(3)), (1, np.int64(2)), (2, np.int64(1)), (10, np.int64(1)), (18, np.int64(1)), (26, np.int64(2)), (27, np.int64(2)), (28, np.int64(3)), (20, np.int64(1)), (28, np.int64(1)), (36, np.int64(2)), (37, np.int64(2)), (38, np.int64(2)), (39, np.int64(1)), (47, np.int64(2)), (47, np.int64(1)), (55, np.int64(1))]\n",
      "\n",
      "Current Policy:\n",
      "[['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']]\n",
      "\n",
      "GOAL REACHED in episode 547!\n",
      "Path taken: [(0, np.int64(2)), (1, np.int64(3)), (1, np.int64(2)), (2, np.int64(2)), (3, np.int64(2)), (4, np.int64(1)), (12, np.int64(2)), (13, np.int64(1)), (21, np.int64(2)), (22, np.int64(1)), (30, np.int64(2)), (31, np.int64(1)), (39, np.int64(1)), (47, np.int64(1)), (55, np.int64(1))]\n",
      "\n",
      "Current Policy:\n",
      "[['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']]\n",
      "\n",
      "GOAL REACHED in episode 550!\n",
      "Path taken: [(0, np.int64(2)), (1, np.int64(2)), (2, np.int64(2)), (3, np.int64(2)), (4, np.int64(1)), (12, np.int64(1)), (20, np.int64(3)), (12, np.int64(1)), (20, np.int64(3)), (12, np.int64(1)), (20, np.int64(3)), (12, np.int64(1)), (20, np.int64(2)), (21, np.int64(2)), (22, np.int64(0)), (21, np.int64(3)), (13, np.int64(1)), (21, np.int64(3)), (13, np.int64(1)), (21, np.int64(2)), (22, np.int64(1)), (30, np.int64(1)), (38, np.int64(2)), (39, np.int64(1)), (47, np.int64(1)), (55, np.int64(3)), (47, np.int64(1)), (55, np.int64(2)), (55, np.int64(1))]\n",
      "\n",
      "Current Policy:\n",
      "[['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']]\n",
      "\n",
      "GOAL REACHED in episode 552!\n",
      "Path taken: [(0, np.int64(2)), (1, np.int64(0)), (0, np.int64(2)), (1, np.int64(0)), (0, np.int64(2)), (1, np.int64(2)), (2, np.int64(2)), (3, np.int64(2)), (4, np.int64(1)), (12, np.int64(1)), (20, np.int64(3)), (12, np.int64(0)), (11, np.int64(2)), (12, np.int64(1)), (20, np.int64(2)), (21, np.int64(2)), (22, np.int64(2)), (23, np.int64(1)), (31, np.int64(1)), (39, np.int64(1)), (47, np.int64(1)), (55, np.int64(1))]\n",
      "\n",
      "Current Policy:\n",
      "[['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']]\n",
      "\n",
      "GOAL REACHED in episode 553!\n",
      "Path taken: [(0, np.int64(2)), (1, np.int64(2)), (2, np.int64(2)), (3, np.int64(2)), (4, np.int64(1)), (12, np.int64(1)), (20, np.int64(2)), (21, np.int64(3)), (13, np.int64(2)), (14, np.int64(1)), (22, np.int64(1)), (30, np.int64(3)), (22, np.int64(1)), (30, np.int64(1)), (38, np.int64(2)), (39, np.int64(1)), (47, np.int64(1)), (55, np.int64(1))]\n",
      "\n",
      "Current Policy:\n",
      "[['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']]\n",
      "\n",
      "GOAL REACHED in episode 554!\n",
      "Path taken: [(0, np.int64(2)), (1, np.int64(2)), (2, np.int64(2)), (3, np.int64(2)), (4, np.int64(2)), (5, np.int64(1)), (13, np.int64(1)), (21, np.int64(2)), (22, np.int64(3)), (14, np.int64(1)), (22, np.int64(1)), (30, np.int64(1)), (38, np.int64(2)), (39, np.int64(0)), (38, np.int64(2)), (39, np.int64(2)), (39, np.int64(1)), (47, np.int64(1)), (55, np.int64(1))]\n",
      "\n",
      "Current Policy:\n",
      "[['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']]\n",
      "\n",
      "GOAL REACHED in episode 568!\n",
      "Path taken: [(0, np.int64(2)), (1, np.int64(2)), (2, np.int64(2)), (3, np.int64(2)), (4, np.int64(1)), (12, np.int64(0)), (11, np.int64(2)), (12, np.int64(2)), (13, np.int64(1)), (21, np.int64(2)), (22, np.int64(1)), (30, np.int64(1)), (38, np.int64(2)), (39, np.int64(1)), (47, np.int64(1)), (55, np.int64(1))]\n",
      "\n",
      "Current Policy:\n",
      "[['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']]\n",
      "\n",
      "GOAL REACHED in episode 569!\n",
      "Path taken: [(0, np.int64(2)), (1, np.int64(1)), (9, np.int64(2)), (10, np.int64(2)), (11, np.int64(2)), (12, np.int64(1)), (20, np.int64(2)), (21, np.int64(2)), (22, np.int64(0)), (21, np.int64(2)), (22, np.int64(3)), (14, np.int64(1)), (22, np.int64(1)), (30, np.int64(3)), (22, np.int64(1)), (30, np.int64(1)), (38, np.int64(2)), (39, np.int64(3)), (31, np.int64(1)), (39, np.int64(1)), (47, np.int64(2)), (47, np.int64(1)), (55, np.int64(3)), (47, np.int64(1)), (55, np.int64(3)), (47, np.int64(1)), (55, np.int64(1))]\n",
      "\n",
      "Current Policy:\n",
      "[['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']]\n",
      "\n",
      "GOAL REACHED in episode 574!\n",
      "Path taken: [(0, np.int64(2)), (1, np.int64(2)), (2, np.int64(2)), (3, np.int64(3)), (3, np.int64(2)), (4, np.int64(1)), (12, np.int64(1)), (20, np.int64(2)), (21, np.int64(2)), (22, np.int64(0)), (21, np.int64(2)), (22, np.int64(3)), (14, np.int64(1)), (22, np.int64(1)), (30, np.int64(1)), (38, np.int64(2)), (39, np.int64(1)), (47, np.int64(1)), (55, np.int64(3)), (47, np.int64(3)), (39, np.int64(3)), (31, np.int64(1)), (39, np.int64(0)), (38, np.int64(2)), (39, np.int64(2)), (39, np.int64(1)), (47, np.int64(1)), (55, np.int64(2)), (55, np.int64(1))]\n",
      "\n",
      "Current Policy:\n",
      "[['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']]\n",
      "\n",
      "GOAL REACHED in episode 575!\n",
      "Path taken: [(0, np.int64(2)), (1, np.int64(0)), (0, np.int64(2)), (1, np.int64(2)), (2, np.int64(2)), (3, np.int64(2)), (4, np.int64(1)), (12, np.int64(2)), (13, np.int64(1)), (21, np.int64(2)), (22, np.int64(1)), (30, np.int64(1)), (38, np.int64(2)), (39, np.int64(1)), (47, np.int64(3)), (39, np.int64(1)), (47, np.int64(1)), (55, np.int64(1))]\n",
      "\n",
      "Current Policy:\n",
      "[['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']]\n",
      "\n",
      "GOAL REACHED in episode 576!\n",
      "Path taken: [(0, np.int64(2)), (1, np.int64(2)), (2, np.int64(2)), (3, np.int64(2)), (4, np.int64(3)), (4, np.int64(1)), (12, np.int64(2)), (13, np.int64(1)), (21, np.int64(2)), (22, np.int64(2)), (23, np.int64(0)), (22, np.int64(1)), (30, np.int64(1)), (38, np.int64(2)), (39, np.int64(1)), (47, np.int64(1)), (55, np.int64(2)), (55, np.int64(3)), (47, np.int64(1)), (55, np.int64(1))]\n",
      "\n",
      "Current Policy:\n",
      "[['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']]\n",
      "\n",
      "GOAL REACHED in episode 580!\n",
      "Path taken: [(0, np.int64(2)), (1, np.int64(0)), (0, np.int64(3)), (0, np.int64(2)), (1, np.int64(2)), (2, np.int64(2)), (3, np.int64(2)), (4, np.int64(0)), (3, np.int64(3)), (3, np.int64(2)), (4, np.int64(1)), (12, np.int64(1)), (20, np.int64(2)), (21, np.int64(2)), (22, np.int64(3)), (14, np.int64(2)), (15, np.int64(1)), (23, np.int64(1)), (31, np.int64(1)), (39, np.int64(1)), (47, np.int64(2)), (47, np.int64(3)), (39, np.int64(1)), (47, np.int64(1)), (55, np.int64(2)), (55, np.int64(1))]\n",
      "\n",
      "Current Policy:\n",
      "[['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']]\n",
      "\n",
      "GOAL REACHED in episode 583!\n",
      "Path taken: [(0, np.int64(2)), (1, np.int64(2)), (2, np.int64(1)), (10, np.int64(2)), (11, np.int64(2)), (12, np.int64(1)), (20, np.int64(1)), (28, np.int64(1)), (36, np.int64(2)), (37, np.int64(2)), (38, np.int64(2)), (39, np.int64(3)), (31, np.int64(1)), (39, np.int64(1)), (47, np.int64(1)), (55, np.int64(1))]\n",
      "\n",
      "Current Policy:\n",
      "[['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']]\n",
      "\n",
      "GOAL REACHED in episode 588!\n",
      "Path taken: [(0, np.int64(2)), (1, np.int64(2)), (2, np.int64(2)), (3, np.int64(2)), (4, np.int64(1)), (12, np.int64(1)), (20, np.int64(2)), (21, np.int64(2)), (22, np.int64(1)), (30, np.int64(1)), (38, np.int64(2)), (39, np.int64(2)), (39, np.int64(1)), (47, np.int64(1)), (55, np.int64(1))]\n",
      "\n",
      "Current Policy:\n",
      "[['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']]\n",
      "\n",
      "GOAL REACHED in episode 589!\n",
      "Path taken: [(0, np.int64(2)), (1, np.int64(2)), (2, np.int64(0)), (1, np.int64(2)), (2, np.int64(3)), (2, np.int64(2)), (3, np.int64(2)), (4, np.int64(1)), (12, np.int64(1)), (20, np.int64(2)), (21, np.int64(2)), (22, np.int64(1)), (30, np.int64(1)), (38, np.int64(2)), (39, np.int64(2)), (39, np.int64(1)), (47, np.int64(1)), (55, np.int64(1))]\n",
      "\n",
      "Current Policy:\n",
      "[['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']]\n",
      "\n",
      "GOAL REACHED in episode 590!\n",
      "Path taken: [(0, np.int64(2)), (1, np.int64(2)), (2, np.int64(2)), (3, np.int64(2)), (4, np.int64(1)), (12, np.int64(1)), (20, np.int64(2)), (21, np.int64(2)), (22, np.int64(3)), (14, np.int64(2)), (15, np.int64(1)), (23, np.int64(0)), (22, np.int64(1)), (30, np.int64(1)), (38, np.int64(2)), (39, np.int64(1)), (47, np.int64(2)), (47, np.int64(3)), (39, np.int64(1)), (47, np.int64(1)), (55, np.int64(1))]\n",
      "\n",
      "Current Policy:\n",
      "[['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']]\n",
      "\n",
      "GOAL REACHED in episode 591!\n",
      "Path taken: [(0, np.int64(2)), (1, np.int64(2)), (2, np.int64(2)), (3, np.int64(2)), (4, np.int64(0)), (3, np.int64(3)), (3, np.int64(2)), (4, np.int64(1)), (12, np.int64(2)), (13, np.int64(1)), (21, np.int64(2)), (22, np.int64(1)), (30, np.int64(1)), (38, np.int64(2)), (39, np.int64(1)), (47, np.int64(1)), (55, np.int64(1))]\n",
      "\n",
      "Current Policy:\n",
      "[['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']]\n",
      "\n",
      "GOAL REACHED in episode 594!\n",
      "Path taken: [(0, np.int64(2)), (1, np.int64(2)), (2, np.int64(2)), (3, np.int64(2)), (4, np.int64(1)), (12, np.int64(1)), (20, np.int64(1)), (28, np.int64(1)), (36, np.int64(2)), (37, np.int64(2)), (38, np.int64(3)), (30, np.int64(1)), (38, np.int64(2)), (39, np.int64(1)), (47, np.int64(1)), (55, np.int64(1))]\n",
      "\n",
      "Current Policy:\n",
      "[['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']]\n",
      "\n",
      "GOAL REACHED in episode 596!\n",
      "Path taken: [(0, np.int64(2)), (1, np.int64(0)), (0, np.int64(2)), (1, np.int64(0)), (0, np.int64(2)), (1, np.int64(2)), (2, np.int64(3)), (2, np.int64(0)), (1, np.int64(2)), (2, np.int64(0)), (1, np.int64(2)), (2, np.int64(1)), (10, np.int64(0)), (9, np.int64(2)), (10, np.int64(2)), (11, np.int64(2)), (12, np.int64(1)), (20, np.int64(1)), (28, np.int64(1)), (36, np.int64(2)), (37, np.int64(2)), (38, np.int64(2)), (39, np.int64(1)), (47, np.int64(1)), (55, np.int64(1))]\n",
      "\n",
      "Current Policy:\n",
      "[['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']]\n",
      "\n",
      "Episode 600\n",
      "Average reward: 0.389\n",
      "Success rate: 37.00%\n",
      "\n",
      "Current Policy:\n",
      "[['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']]\n",
      "\n",
      "Least visited states:\n",
      "State 19: 0 visits\n",
      "State 29: 0 visits\n",
      "State 35: 0 visits\n",
      "State 41: 0 visits\n",
      "State 42: 0 visits\n",
      "\n",
      "GOAL REACHED in episode 609!\n",
      "Path taken: [(0, np.int64(3)), (0, np.int64(0)), (0, np.int64(2)), (1, np.int64(2)), (2, np.int64(3)), (2, np.int64(2)), (3, np.int64(2)), (4, np.int64(1)), (12, np.int64(1)), (20, np.int64(2)), (21, np.int64(2)), (22, np.int64(1)), (30, np.int64(1)), (38, np.int64(0)), (37, np.int64(2)), (38, np.int64(2)), (39, np.int64(3)), (31, np.int64(2)), (31, np.int64(1)), (39, np.int64(1)), (47, np.int64(1)), (55, np.int64(2)), (55, np.int64(1))]\n",
      "\n",
      "Current Policy:\n",
      "[['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']]\n",
      "\n",
      "GOAL REACHED in episode 612!\n",
      "Path taken: [(0, np.int64(2)), (1, np.int64(0)), (0, np.int64(2)), (1, np.int64(2)), (2, np.int64(2)), (3, np.int64(2)), (4, np.int64(1)), (12, np.int64(0)), (11, np.int64(2)), (12, np.int64(1)), (20, np.int64(2)), (21, np.int64(2)), (22, np.int64(2)), (23, np.int64(1)), (31, np.int64(1)), (39, np.int64(1)), (47, np.int64(1)), (55, np.int64(1))]\n",
      "\n",
      "Current Policy:\n",
      "[['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']]\n",
      "\n",
      "GOAL REACHED in episode 616!\n",
      "Path taken: [(0, np.int64(3)), (0, np.int64(2)), (1, np.int64(2)), (2, np.int64(2)), (3, np.int64(2)), (4, np.int64(1)), (12, np.int64(1)), (20, np.int64(1)), (28, np.int64(1)), (36, np.int64(2)), (37, np.int64(2)), (38, np.int64(2)), (39, np.int64(2)), (39, np.int64(1)), (47, np.int64(1)), (55, np.int64(1))]\n",
      "\n",
      "Current Policy:\n",
      "[['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']]\n",
      "\n",
      "GOAL REACHED in episode 617!\n",
      "Path taken: [(0, np.int64(0)), (0, np.int64(1)), (8, np.int64(3)), (0, np.int64(0)), (0, np.int64(1)), (8, np.int64(3)), (0, np.int64(2)), (1, np.int64(0)), (0, np.int64(0)), (0, np.int64(2)), (1, np.int64(2)), (2, np.int64(2)), (3, np.int64(2)), (4, np.int64(1)), (12, np.int64(0)), (11, np.int64(2)), (12, np.int64(1)), (20, np.int64(2)), (21, np.int64(2)), (22, np.int64(1)), (30, np.int64(1)), (38, np.int64(2)), (39, np.int64(2)), (39, np.int64(1)), (47, np.int64(1)), (55, np.int64(1))]\n",
      "\n",
      "Current Policy:\n",
      "[['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']]\n",
      "\n",
      "GOAL REACHED in episode 621!\n",
      "Path taken: [(0, np.int64(2)), (1, np.int64(2)), (2, np.int64(2)), (3, np.int64(2)), (4, np.int64(0)), (3, np.int64(1)), (11, np.int64(2)), (12, np.int64(1)), (20, np.int64(2)), (21, np.int64(2)), (22, np.int64(3)), (14, np.int64(1)), (22, np.int64(1)), (30, np.int64(1)), (38, np.int64(2)), (39, np.int64(1)), (47, np.int64(1)), (55, np.int64(1))]\n",
      "\n",
      "Current Policy:\n",
      "[['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']]\n",
      "\n",
      "GOAL REACHED in episode 623!\n",
      "Path taken: [(0, np.int64(2)), (1, np.int64(2)), (2, np.int64(2)), (3, np.int64(2)), (4, np.int64(3)), (4, np.int64(1)), (12, np.int64(1)), (20, np.int64(2)), (21, np.int64(2)), (22, np.int64(0)), (21, np.int64(2)), (22, np.int64(0)), (21, np.int64(2)), (22, np.int64(1)), (30, np.int64(1)), (38, np.int64(2)), (39, np.int64(1)), (47, np.int64(1)), (55, np.int64(1))]\n",
      "\n",
      "Current Policy:\n",
      "[['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']]\n",
      "\n",
      "GOAL REACHED in episode 624!\n",
      "Path taken: [(0, np.int64(2)), (1, np.int64(2)), (2, np.int64(2)), (3, np.int64(0)), (2, np.int64(3)), (2, np.int64(1)), (10, np.int64(3)), (2, np.int64(3)), (2, np.int64(2)), (3, np.int64(2)), (4, np.int64(1)), (12, np.int64(0)), (11, np.int64(2)), (12, np.int64(1)), (20, np.int64(2)), (21, np.int64(2)), (22, np.int64(1)), (30, np.int64(3)), (22, np.int64(0)), (21, np.int64(2)), (22, np.int64(1)), (30, np.int64(3)), (22, np.int64(1)), (30, np.int64(3)), (22, np.int64(1)), (30, np.int64(2)), (31, np.int64(1)), (39, np.int64(1)), (47, np.int64(1)), (55, np.int64(1))]\n",
      "\n",
      "Current Policy:\n",
      "[['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']]\n",
      "\n",
      "GOAL REACHED in episode 626!\n",
      "Path taken: [(0, np.int64(2)), (1, np.int64(2)), (2, np.int64(0)), (1, np.int64(2)), (2, np.int64(0)), (1, np.int64(3)), (1, np.int64(0)), (0, np.int64(2)), (1, np.int64(1)), (9, np.int64(2)), (10, np.int64(2)), (11, np.int64(2)), (12, np.int64(1)), (20, np.int64(2)), (21, np.int64(2)), (22, np.int64(1)), (30, np.int64(1)), (38, np.int64(2)), (39, np.int64(1)), (47, np.int64(2)), (47, np.int64(2)), (47, np.int64(1)), (55, np.int64(1))]\n",
      "\n",
      "Current Policy:\n",
      "[['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']]\n",
      "\n",
      "GOAL REACHED in episode 627!\n",
      "Path taken: [(0, np.int64(2)), (1, np.int64(1)), (9, np.int64(2)), (10, np.int64(2)), (11, np.int64(2)), (12, np.int64(1)), (20, np.int64(2)), (21, np.int64(2)), (22, np.int64(1)), (30, np.int64(1)), (38, np.int64(2)), (39, np.int64(1)), (47, np.int64(1)), (55, np.int64(1))]\n",
      "\n",
      "Current Policy:\n",
      "[['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']]\n",
      "\n",
      "GOAL REACHED in episode 629!\n",
      "Path taken: [(0, np.int64(0)), (0, np.int64(1)), (8, np.int64(3)), (0, np.int64(2)), (1, np.int64(2)), (2, np.int64(2)), (3, np.int64(2)), (4, np.int64(1)), (12, np.int64(0)), (11, np.int64(2)), (12, np.int64(2)), (13, np.int64(1)), (21, np.int64(2)), (22, np.int64(1)), (30, np.int64(1)), (38, np.int64(2)), (39, np.int64(0)), (38, np.int64(2)), (39, np.int64(3)), (31, np.int64(1)), (39, np.int64(1)), (47, np.int64(1)), (55, np.int64(1))]\n",
      "\n",
      "Current Policy:\n",
      "[['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']]\n",
      "\n",
      "GOAL REACHED in episode 631!\n",
      "Path taken: [(0, np.int64(2)), (1, np.int64(2)), (2, np.int64(2)), (3, np.int64(2)), (4, np.int64(1)), (12, np.int64(1)), (20, np.int64(2)), (21, np.int64(2)), (22, np.int64(1)), (30, np.int64(1)), (38, np.int64(2)), (39, np.int64(3)), (31, np.int64(1)), (39, np.int64(1)), (47, np.int64(3)), (39, np.int64(0)), (38, np.int64(2)), (39, np.int64(1)), (47, np.int64(1)), (55, np.int64(1))]\n",
      "\n",
      "Current Policy:\n",
      "[['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']]\n",
      "\n",
      "GOAL REACHED in episode 637!\n",
      "Path taken: [(0, np.int64(2)), (1, np.int64(2)), (2, np.int64(2)), (3, np.int64(3)), (3, np.int64(2)), (4, np.int64(0)), (3, np.int64(2)), (4, np.int64(1)), (12, np.int64(1)), (20, np.int64(2)), (21, np.int64(3)), (13, np.int64(1)), (21, np.int64(2)), (22, np.int64(1)), (30, np.int64(1)), (38, np.int64(2)), (39, np.int64(0)), (38, np.int64(2)), (39, np.int64(1)), (47, np.int64(1)), (55, np.int64(2)), (55, np.int64(1))]\n",
      "\n",
      "Current Policy:\n",
      "[['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']]\n",
      "\n",
      "GOAL REACHED in episode 638!\n",
      "Path taken: [(0, np.int64(2)), (1, np.int64(3)), (1, np.int64(2)), (2, np.int64(2)), (3, np.int64(2)), (4, np.int64(0)), (3, np.int64(2)), (4, np.int64(1)), (12, np.int64(0)), (11, np.int64(0)), (10, np.int64(2)), (11, np.int64(2)), (12, np.int64(1)), (20, np.int64(2)), (21, np.int64(3)), (13, np.int64(2)), (14, np.int64(1)), (22, np.int64(2)), (23, np.int64(1)), (31, np.int64(1)), (39, np.int64(3)), (31, np.int64(1)), (39, np.int64(1)), (47, np.int64(1)), (55, np.int64(2)), (55, np.int64(2)), (55, np.int64(1))]\n",
      "\n",
      "Current Policy:\n",
      "[['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']]\n",
      "\n",
      "GOAL REACHED in episode 639!\n",
      "Path taken: [(0, np.int64(3)), (0, np.int64(2)), (1, np.int64(2)), (2, np.int64(0)), (1, np.int64(2)), (2, np.int64(2)), (3, np.int64(2)), (4, np.int64(3)), (4, np.int64(1)), (12, np.int64(1)), (20, np.int64(2)), (21, np.int64(2)), (22, np.int64(1)), (30, np.int64(1)), (38, np.int64(2)), (39, np.int64(1)), (47, np.int64(1)), (55, np.int64(1))]\n",
      "\n",
      "Current Policy:\n",
      "[['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']]\n",
      "\n",
      "GOAL REACHED in episode 641!\n",
      "Path taken: [(0, np.int64(2)), (1, np.int64(2)), (2, np.int64(2)), (3, np.int64(2)), (4, np.int64(1)), (12, np.int64(1)), (20, np.int64(2)), (21, np.int64(2)), (22, np.int64(1)), (30, np.int64(1)), (38, np.int64(2)), (39, np.int64(1)), (47, np.int64(1)), (55, np.int64(1))]\n",
      "\n",
      "Current Policy:\n",
      "[['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']]\n",
      "\n",
      "GOAL REACHED in episode 643!\n",
      "Path taken: [(0, np.int64(0)), (0, np.int64(2)), (1, np.int64(2)), (2, np.int64(2)), (3, np.int64(2)), (4, np.int64(2)), (5, np.int64(1)), (13, np.int64(1)), (21, np.int64(2)), (22, np.int64(1)), (30, np.int64(1)), (38, np.int64(2)), (39, np.int64(3)), (31, np.int64(1)), (39, np.int64(2)), (39, np.int64(1)), (47, np.int64(1)), (55, np.int64(1))]\n",
      "\n",
      "Current Policy:\n",
      "[['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']]\n",
      "\n",
      "GOAL REACHED in episode 644!\n",
      "Path taken: [(0, np.int64(1)), (8, np.int64(3)), (0, np.int64(2)), (1, np.int64(0)), (0, np.int64(2)), (1, np.int64(2)), (2, np.int64(2)), (3, np.int64(2)), (4, np.int64(2)), (5, np.int64(1)), (13, np.int64(1)), (21, np.int64(2)), (22, np.int64(2)), (23, np.int64(1)), (31, np.int64(1)), (39, np.int64(3)), (31, np.int64(1)), (39, np.int64(1)), (47, np.int64(1)), (55, np.int64(1))]\n",
      "\n",
      "Current Policy:\n",
      "[['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']]\n",
      "\n",
      "GOAL REACHED in episode 646!\n",
      "Path taken: [(0, np.int64(2)), (1, np.int64(2)), (2, np.int64(2)), (3, np.int64(2)), (4, np.int64(1)), (12, np.int64(1)), (20, np.int64(2)), (21, np.int64(2)), (22, np.int64(1)), (30, np.int64(1)), (38, np.int64(2)), (39, np.int64(1)), (47, np.int64(1)), (55, np.int64(1))]\n",
      "\n",
      "Current Policy:\n",
      "[['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']]\n",
      "\n",
      "GOAL REACHED in episode 650!\n",
      "Path taken: [(0, np.int64(3)), (0, np.int64(1)), (8, np.int64(3)), (0, np.int64(2)), (1, np.int64(2)), (2, np.int64(2)), (3, np.int64(2)), (4, np.int64(1)), (12, np.int64(1)), (20, np.int64(3)), (12, np.int64(1)), (20, np.int64(2)), (21, np.int64(2)), (22, np.int64(1)), (30, np.int64(1)), (38, np.int64(0)), (37, np.int64(2)), (38, np.int64(3)), (30, np.int64(1)), (38, np.int64(2)), (39, np.int64(1)), (47, np.int64(1)), (55, np.int64(1))]\n",
      "\n",
      "Current Policy:\n",
      "[['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']]\n",
      "\n",
      "GOAL REACHED in episode 652!\n",
      "Path taken: [(0, np.int64(2)), (1, np.int64(2)), (2, np.int64(2)), (3, np.int64(2)), (4, np.int64(3)), (4, np.int64(1)), (12, np.int64(0)), (11, np.int64(2)), (12, np.int64(1)), (20, np.int64(2)), (21, np.int64(2)), (22, np.int64(1)), (30, np.int64(1)), (38, np.int64(2)), (39, np.int64(1)), (47, np.int64(1)), (55, np.int64(1))]\n",
      "\n",
      "Current Policy:\n",
      "[['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']]\n",
      "\n",
      "GOAL REACHED in episode 653!\n",
      "Path taken: [(0, np.int64(2)), (1, np.int64(3)), (1, np.int64(2)), (2, np.int64(2)), (3, np.int64(2)), (4, np.int64(3)), (4, np.int64(1)), (12, np.int64(1)), (20, np.int64(2)), (21, np.int64(2)), (22, np.int64(0)), (21, np.int64(2)), (22, np.int64(1)), (30, np.int64(3)), (22, np.int64(1)), (30, np.int64(1)), (38, np.int64(2)), (39, np.int64(1)), (47, np.int64(1)), (55, np.int64(3)), (47, np.int64(1)), (55, np.int64(1))]\n",
      "\n",
      "Current Policy:\n",
      "[['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']]\n",
      "\n",
      "GOAL REACHED in episode 655!\n",
      "Path taken: [(0, np.int64(2)), (1, np.int64(0)), (0, np.int64(2)), (1, np.int64(2)), (2, np.int64(2)), (3, np.int64(1)), (11, np.int64(2)), (12, np.int64(1)), (20, np.int64(3)), (12, np.int64(1)), (20, np.int64(2)), (21, np.int64(2)), (22, np.int64(3)), (14, np.int64(2)), (15, np.int64(1)), (23, np.int64(1)), (31, np.int64(1)), (39, np.int64(1)), (47, np.int64(2)), (47, np.int64(1)), (55, np.int64(1))]\n",
      "\n",
      "Current Policy:\n",
      "[['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']]\n",
      "\n",
      "GOAL REACHED in episode 656!\n",
      "Path taken: [(0, np.int64(2)), (1, np.int64(2)), (2, np.int64(2)), (3, np.int64(2)), (4, np.int64(1)), (12, np.int64(2)), (13, np.int64(3)), (5, np.int64(3)), (5, np.int64(1)), (13, np.int64(1)), (21, np.int64(2)), (22, np.int64(1)), (30, np.int64(1)), (38, np.int64(2)), (39, np.int64(1)), (47, np.int64(1)), (55, np.int64(1))]\n",
      "\n",
      "Current Policy:\n",
      "[['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']]\n",
      "\n",
      "GOAL REACHED in episode 657!\n",
      "Path taken: [(0, np.int64(2)), (1, np.int64(2)), (2, np.int64(2)), (3, np.int64(2)), (4, np.int64(1)), (12, np.int64(3)), (4, np.int64(1)), (12, np.int64(1)), (20, np.int64(2)), (21, np.int64(2)), (22, np.int64(1)), (30, np.int64(3)), (22, np.int64(1)), (30, np.int64(1)), (38, np.int64(2)), (39, np.int64(1)), (47, np.int64(1)), (55, np.int64(1))]\n",
      "\n",
      "Current Policy:\n",
      "[['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']]\n",
      "\n",
      "GOAL REACHED in episode 660!\n",
      "Path taken: [(0, np.int64(2)), (1, np.int64(2)), (2, np.int64(2)), (3, np.int64(2)), (4, np.int64(1)), (12, np.int64(1)), (20, np.int64(2)), (21, np.int64(2)), (22, np.int64(1)), (30, np.int64(1)), (38, np.int64(2)), (39, np.int64(1)), (47, np.int64(1)), (55, np.int64(1))]\n",
      "\n",
      "Current Policy:\n",
      "[['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']]\n",
      "\n",
      "GOAL REACHED in episode 662!\n",
      "Path taken: [(0, np.int64(1)), (8, np.int64(1)), (16, np.int64(2)), (17, np.int64(2)), (18, np.int64(1)), (26, np.int64(3)), (18, np.int64(1)), (26, np.int64(3)), (18, np.int64(1)), (26, np.int64(3)), (18, np.int64(3)), (10, np.int64(2)), (11, np.int64(2)), (12, np.int64(1)), (20, np.int64(2)), (21, np.int64(0)), (20, np.int64(2)), (21, np.int64(2)), (22, np.int64(1)), (30, np.int64(1)), (38, np.int64(2)), (39, np.int64(1)), (47, np.int64(1)), (55, np.int64(1))]\n",
      "\n",
      "Current Policy:\n",
      "[['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']]\n",
      "\n",
      "GOAL REACHED in episode 663!\n",
      "Path taken: [(0, np.int64(2)), (1, np.int64(2)), (2, np.int64(0)), (1, np.int64(0)), (0, np.int64(0)), (0, np.int64(0)), (0, np.int64(0)), (0, np.int64(2)), (1, np.int64(1)), (9, np.int64(2)), (10, np.int64(1)), (18, np.int64(3)), (10, np.int64(2)), (11, np.int64(2)), (12, np.int64(1)), (20, np.int64(2)), (21, np.int64(2)), (22, np.int64(3)), (14, np.int64(2)), (15, np.int64(1)), (23, np.int64(3)), (15, np.int64(1)), (23, np.int64(1)), (31, np.int64(2)), (31, np.int64(3)), (23, np.int64(1)), (31, np.int64(1)), (39, np.int64(1)), (47, np.int64(3)), (39, np.int64(1)), (47, np.int64(1)), (55, np.int64(1))]\n",
      "\n",
      "Current Policy:\n",
      "[['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']]\n",
      "\n",
      "GOAL REACHED in episode 664!\n",
      "Path taken: [(0, np.int64(2)), (1, np.int64(1)), (9, np.int64(2)), (10, np.int64(2)), (11, np.int64(2)), (12, np.int64(1)), (20, np.int64(2)), (21, np.int64(3)), (13, np.int64(1)), (21, np.int64(0)), (20, np.int64(2)), (21, np.int64(2)), (22, np.int64(1)), (30, np.int64(1)), (38, np.int64(2)), (39, np.int64(2)), (39, np.int64(0)), (38, np.int64(2)), (39, np.int64(1)), (47, np.int64(1)), (55, np.int64(1))]\n",
      "\n",
      "Current Policy:\n",
      "[['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']]\n",
      "\n",
      "GOAL REACHED in episode 665!\n",
      "Path taken: [(0, np.int64(2)), (1, np.int64(2)), (2, np.int64(1)), (10, np.int64(2)), (11, np.int64(2)), (12, np.int64(1)), (20, np.int64(2)), (21, np.int64(0)), (20, np.int64(2)), (21, np.int64(2)), (22, np.int64(1)), (30, np.int64(1)), (38, np.int64(2)), (39, np.int64(1)), (47, np.int64(1)), (55, np.int64(3)), (47, np.int64(1)), (55, np.int64(1))]\n",
      "\n",
      "Current Policy:\n",
      "[['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']]\n",
      "\n",
      "GOAL REACHED in episode 669!\n",
      "Path taken: [(0, np.int64(0)), (0, np.int64(2)), (1, np.int64(3)), (1, np.int64(1)), (9, np.int64(0)), (8, np.int64(2)), (9, np.int64(2)), (10, np.int64(2)), (11, np.int64(2)), (12, np.int64(1)), (20, np.int64(2)), (21, np.int64(2)), (22, np.int64(1)), (30, np.int64(1)), (38, np.int64(2)), (39, np.int64(1)), (47, np.int64(1)), (55, np.int64(1))]\n",
      "\n",
      "Current Policy:\n",
      "[['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']]\n",
      "\n",
      "GOAL REACHED in episode 676!\n",
      "Path taken: [(0, np.int64(2)), (1, np.int64(2)), (2, np.int64(2)), (3, np.int64(3)), (3, np.int64(2)), (4, np.int64(1)), (12, np.int64(1)), (20, np.int64(2)), (21, np.int64(0)), (20, np.int64(2)), (21, np.int64(2)), (22, np.int64(1)), (30, np.int64(2)), (31, np.int64(3)), (23, np.int64(1)), (31, np.int64(1)), (39, np.int64(1)), (47, np.int64(1)), (55, np.int64(2)), (55, np.int64(1))]\n",
      "\n",
      "Current Policy:\n",
      "[['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']]\n",
      "\n",
      "GOAL REACHED in episode 677!\n",
      "Path taken: [(0, np.int64(1)), (8, np.int64(3)), (0, np.int64(2)), (1, np.int64(2)), (2, np.int64(2)), (3, np.int64(2)), (4, np.int64(0)), (3, np.int64(2)), (4, np.int64(3)), (4, np.int64(2)), (5, np.int64(0)), (4, np.int64(3)), (4, np.int64(1)), (12, np.int64(1)), (20, np.int64(2)), (21, np.int64(2)), (22, np.int64(0)), (21, np.int64(2)), (22, np.int64(1)), (30, np.int64(2)), (31, np.int64(1)), (39, np.int64(1)), (47, np.int64(1)), (55, np.int64(1))]\n",
      "\n",
      "Current Policy:\n",
      "[['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']]\n",
      "\n",
      "GOAL REACHED in episode 678!\n",
      "Path taken: [(0, np.int64(1)), (8, np.int64(3)), (0, np.int64(0)), (0, np.int64(2)), (1, np.int64(2)), (2, np.int64(2)), (3, np.int64(2)), (4, np.int64(1)), (12, np.int64(1)), (20, np.int64(2)), (21, np.int64(0)), (20, np.int64(2)), (21, np.int64(3)), (13, np.int64(1)), (21, np.int64(2)), (22, np.int64(1)), (30, np.int64(1)), (38, np.int64(2)), (39, np.int64(1)), (47, np.int64(1)), (55, np.int64(1))]\n",
      "\n",
      "Current Policy:\n",
      "[['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']]\n",
      "\n",
      "GOAL REACHED in episode 682!\n",
      "Path taken: [(0, np.int64(3)), (0, np.int64(1)), (8, np.int64(3)), (0, np.int64(3)), (0, np.int64(1)), (8, np.int64(3)), (0, np.int64(1)), (8, np.int64(3)), (0, np.int64(2)), (1, np.int64(2)), (2, np.int64(2)), (3, np.int64(2)), (4, np.int64(1)), (12, np.int64(1)), (20, np.int64(2)), (21, np.int64(0)), (20, np.int64(2)), (21, np.int64(2)), (22, np.int64(1)), (30, np.int64(1)), (38, np.int64(2)), (39, np.int64(2)), (39, np.int64(0)), (38, np.int64(2)), (39, np.int64(1)), (47, np.int64(1)), (55, np.int64(1))]\n",
      "\n",
      "Current Policy:\n",
      "[['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']]\n",
      "\n",
      "GOAL REACHED in episode 684!\n",
      "Path taken: [(0, np.int64(2)), (1, np.int64(2)), (2, np.int64(1)), (10, np.int64(2)), (11, np.int64(2)), (12, np.int64(1)), (20, np.int64(2)), (21, np.int64(2)), (22, np.int64(1)), (30, np.int64(1)), (38, np.int64(2)), (39, np.int64(1)), (47, np.int64(1)), (55, np.int64(2)), (55, np.int64(1))]\n",
      "\n",
      "Current Policy:\n",
      "[['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']]\n",
      "\n",
      "GOAL REACHED in episode 685!\n",
      "Path taken: [(0, np.int64(2)), (1, np.int64(1)), (9, np.int64(2)), (10, np.int64(2)), (11, np.int64(2)), (12, np.int64(1)), (20, np.int64(2)), (21, np.int64(2)), (22, np.int64(1)), (30, np.int64(1)), (38, np.int64(2)), (39, np.int64(1)), (47, np.int64(1)), (55, np.int64(1))]\n",
      "\n",
      "Current Policy:\n",
      "[['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']]\n",
      "\n",
      "GOAL REACHED in episode 686!\n",
      "Path taken: [(0, np.int64(3)), (0, np.int64(2)), (1, np.int64(2)), (2, np.int64(2)), (3, np.int64(2)), (4, np.int64(2)), (5, np.int64(1)), (13, np.int64(2)), (14, np.int64(2)), (15, np.int64(1)), (23, np.int64(1)), (31, np.int64(1)), (39, np.int64(1)), (47, np.int64(3)), (39, np.int64(1)), (47, np.int64(2)), (47, np.int64(1)), (55, np.int64(1))]\n",
      "\n",
      "Current Policy:\n",
      "[['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']]\n",
      "\n",
      "GOAL REACHED in episode 687!\n",
      "Path taken: [(0, np.int64(2)), (1, np.int64(1)), (9, np.int64(1)), (17, np.int64(2)), (18, np.int64(3)), (10, np.int64(2)), (11, np.int64(2)), (12, np.int64(3)), (4, np.int64(0)), (3, np.int64(2)), (4, np.int64(1)), (12, np.int64(1)), (20, np.int64(2)), (21, np.int64(2)), (22, np.int64(1)), (30, np.int64(1)), (38, np.int64(0)), (37, np.int64(2)), (38, np.int64(2)), (39, np.int64(1)), (47, np.int64(2)), (47, np.int64(1)), (55, np.int64(1))]\n",
      "\n",
      "Current Policy:\n",
      "[['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']]\n",
      "\n",
      "GOAL REACHED in episode 690!\n",
      "Path taken: [(0, np.int64(3)), (0, np.int64(2)), (1, np.int64(2)), (2, np.int64(2)), (3, np.int64(2)), (4, np.int64(1)), (12, np.int64(1)), (20, np.int64(2)), (21, np.int64(2)), (22, np.int64(1)), (30, np.int64(1)), (38, np.int64(2)), (39, np.int64(1)), (47, np.int64(1)), (55, np.int64(1))]\n",
      "\n",
      "Current Policy:\n",
      "[['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']]\n",
      "\n",
      "GOAL REACHED in episode 691!\n",
      "Path taken: [(0, np.int64(2)), (1, np.int64(2)), (2, np.int64(2)), (3, np.int64(3)), (3, np.int64(2)), (4, np.int64(1)), (12, np.int64(1)), (20, np.int64(2)), (21, np.int64(2)), (22, np.int64(1)), (30, np.int64(1)), (38, np.int64(2)), (39, np.int64(1)), (47, np.int64(1)), (55, np.int64(1))]\n",
      "\n",
      "Current Policy:\n",
      "[['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']]\n",
      "\n",
      "GOAL REACHED in episode 693!\n",
      "Path taken: [(0, np.int64(2)), (1, np.int64(2)), (2, np.int64(1)), (10, np.int64(2)), (11, np.int64(2)), (12, np.int64(1)), (20, np.int64(2)), (21, np.int64(2)), (22, np.int64(1)), (30, np.int64(1)), (38, np.int64(2)), (39, np.int64(1)), (47, np.int64(1)), (55, np.int64(1))]\n",
      "\n",
      "Current Policy:\n",
      "[['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']]\n",
      "\n",
      "GOAL REACHED in episode 695!\n",
      "Path taken: [(0, np.int64(2)), (1, np.int64(2)), (2, np.int64(2)), (3, np.int64(2)), (4, np.int64(0)), (3, np.int64(2)), (4, np.int64(1)), (12, np.int64(1)), (20, np.int64(2)), (21, np.int64(2)), (22, np.int64(1)), (30, np.int64(1)), (38, np.int64(2)), (39, np.int64(1)), (47, np.int64(1)), (55, np.int64(1))]\n",
      "\n",
      "Current Policy:\n",
      "[['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']]\n",
      "\n",
      "GOAL REACHED in episode 696!\n",
      "Path taken: [(0, np.int64(2)), (1, np.int64(2)), (2, np.int64(2)), (3, np.int64(2)), (4, np.int64(1)), (12, np.int64(1)), (20, np.int64(2)), (21, np.int64(2)), (22, np.int64(3)), (14, np.int64(3)), (6, np.int64(1)), (14, np.int64(1)), (22, np.int64(1)), (30, np.int64(1)), (38, np.int64(2)), (39, np.int64(0)), (38, np.int64(2)), (39, np.int64(1)), (47, np.int64(1)), (55, np.int64(1))]\n",
      "\n",
      "Current Policy:\n",
      "[['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']]\n",
      "\n",
      "GOAL REACHED in episode 697!\n",
      "Path taken: [(0, np.int64(1)), (8, np.int64(3)), (0, np.int64(2)), (1, np.int64(2)), (2, np.int64(3)), (2, np.int64(2)), (3, np.int64(3)), (3, np.int64(2)), (4, np.int64(1)), (12, np.int64(0)), (11, np.int64(2)), (12, np.int64(1)), (20, np.int64(2)), (21, np.int64(2)), (22, np.int64(1)), (30, np.int64(1)), (38, np.int64(2)), (39, np.int64(1)), (47, np.int64(1)), (55, np.int64(1))]\n",
      "\n",
      "Current Policy:\n",
      "[['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']]\n",
      "\n",
      "GOAL REACHED in episode 700!\n",
      "Path taken: [(0, np.int64(2)), (1, np.int64(1)), (9, np.int64(2)), (10, np.int64(2)), (11, np.int64(2)), (12, np.int64(1)), (20, np.int64(3)), (12, np.int64(1)), (20, np.int64(3)), (12, np.int64(1)), (20, np.int64(2)), (21, np.int64(2)), (22, np.int64(1)), (30, np.int64(1)), (38, np.int64(2)), (39, np.int64(1)), (47, np.int64(2)), (47, np.int64(1)), (55, np.int64(1))]\n",
      "\n",
      "Current Policy:\n",
      "[['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']]\n",
      "\n",
      "Episode 700\n",
      "Average reward: 1.596\n",
      "Success rate: 45.00%\n",
      "\n",
      "Current Policy:\n",
      "[['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']]\n",
      "\n",
      "Least visited states:\n",
      "State 19: 0 visits\n",
      "State 29: 0 visits\n",
      "State 35: 0 visits\n",
      "State 41: 0 visits\n",
      "State 42: 0 visits\n",
      "\n",
      "GOAL REACHED in episode 702!\n",
      "Path taken: [(0, np.int64(2)), (1, np.int64(2)), (2, np.int64(3)), (2, np.int64(2)), (3, np.int64(2)), (4, np.int64(2)), (5, np.int64(2)), (6, np.int64(1)), (14, np.int64(1)), (22, np.int64(1)), (30, np.int64(1)), (38, np.int64(3)), (30, np.int64(3)), (22, np.int64(1)), (30, np.int64(1)), (38, np.int64(2)), (39, np.int64(1)), (47, np.int64(1)), (55, np.int64(1))]\n",
      "\n",
      "Current Policy:\n",
      "[['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']]\n",
      "\n",
      "GOAL REACHED in episode 708!\n",
      "Path taken: [(0, np.int64(2)), (1, np.int64(2)), (2, np.int64(2)), (3, np.int64(2)), (4, np.int64(1)), (12, np.int64(1)), (20, np.int64(2)), (21, np.int64(2)), (22, np.int64(1)), (30, np.int64(1)), (38, np.int64(2)), (39, np.int64(2)), (39, np.int64(1)), (47, np.int64(1)), (55, np.int64(1))]\n",
      "\n",
      "Current Policy:\n",
      "[['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']]\n",
      "\n",
      "GOAL REACHED in episode 711!\n",
      "Path taken: [(0, np.int64(2)), (1, np.int64(2)), (2, np.int64(1)), (10, np.int64(2)), (11, np.int64(2)), (12, np.int64(1)), (20, np.int64(2)), (21, np.int64(0)), (20, np.int64(2)), (21, np.int64(2)), (22, np.int64(1)), (30, np.int64(1)), (38, np.int64(2)), (39, np.int64(1)), (47, np.int64(1)), (55, np.int64(3)), (47, np.int64(1)), (55, np.int64(1))]\n",
      "\n",
      "Current Policy:\n",
      "[['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']]\n",
      "\n",
      "GOAL REACHED in episode 713!\n",
      "Path taken: [(0, np.int64(0)), (0, np.int64(2)), (1, np.int64(2)), (2, np.int64(2)), (3, np.int64(2)), (4, np.int64(3)), (4, np.int64(3)), (4, np.int64(1)), (12, np.int64(1)), (20, np.int64(2)), (21, np.int64(2)), (22, np.int64(1)), (30, np.int64(3)), (22, np.int64(1)), (30, np.int64(2)), (31, np.int64(1)), (39, np.int64(2)), (39, np.int64(2)), (39, np.int64(1)), (47, np.int64(2)), (47, np.int64(1)), (55, np.int64(1))]\n",
      "\n",
      "Current Policy:\n",
      "[['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']]\n",
      "\n",
      "GOAL REACHED in episode 715!\n",
      "Path taken: [(0, np.int64(2)), (1, np.int64(0)), (0, np.int64(2)), (1, np.int64(0)), (0, np.int64(2)), (1, np.int64(2)), (2, np.int64(3)), (2, np.int64(2)), (3, np.int64(2)), (4, np.int64(1)), (12, np.int64(1)), (20, np.int64(2)), (21, np.int64(0)), (20, np.int64(2)), (21, np.int64(2)), (22, np.int64(1)), (30, np.int64(2)), (31, np.int64(1)), (39, np.int64(1)), (47, np.int64(1)), (55, np.int64(1))]\n",
      "\n",
      "Current Policy:\n",
      "[['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']]\n",
      "\n",
      "GOAL REACHED in episode 719!\n",
      "Path taken: [(0, np.int64(0)), (0, np.int64(0)), (0, np.int64(3)), (0, np.int64(2)), (1, np.int64(2)), (2, np.int64(2)), (3, np.int64(2)), (4, np.int64(1)), (12, np.int64(3)), (4, np.int64(3)), (4, np.int64(1)), (12, np.int64(1)), (20, np.int64(2)), (21, np.int64(2)), (22, np.int64(1)), (30, np.int64(1)), (38, np.int64(2)), (39, np.int64(1)), (47, np.int64(1)), (55, np.int64(1))]\n",
      "\n",
      "Current Policy:\n",
      "[['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']]\n",
      "\n",
      "GOAL REACHED in episode 721!\n",
      "Path taken: [(0, np.int64(2)), (1, np.int64(2)), (2, np.int64(3)), (2, np.int64(2)), (3, np.int64(2)), (4, np.int64(1)), (12, np.int64(1)), (20, np.int64(2)), (21, np.int64(2)), (22, np.int64(3)), (14, np.int64(0)), (13, np.int64(1)), (21, np.int64(2)), (22, np.int64(1)), (30, np.int64(1)), (38, np.int64(2)), (39, np.int64(3)), (31, np.int64(1)), (39, np.int64(1)), (47, np.int64(1)), (55, np.int64(1))]\n",
      "\n",
      "Current Policy:\n",
      "[['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']]\n",
      "\n",
      "GOAL REACHED in episode 725!\n",
      "Path taken: [(0, np.int64(2)), (1, np.int64(1)), (9, np.int64(2)), (10, np.int64(2)), (11, np.int64(2)), (12, np.int64(1)), (20, np.int64(2)), (21, np.int64(2)), (22, np.int64(1)), (30, np.int64(1)), (38, np.int64(2)), (39, np.int64(2)), (39, np.int64(1)), (47, np.int64(1)), (55, np.int64(1))]\n",
      "\n",
      "Current Policy:\n",
      "[['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']]\n",
      "\n",
      "GOAL REACHED in episode 726!\n",
      "Path taken: [(0, np.int64(2)), (1, np.int64(0)), (0, np.int64(2)), (1, np.int64(2)), (2, np.int64(2)), (3, np.int64(2)), (4, np.int64(3)), (4, np.int64(1)), (12, np.int64(0)), (11, np.int64(2)), (12, np.int64(1)), (20, np.int64(2)), (21, np.int64(2)), (22, np.int64(3)), (14, np.int64(1)), (22, np.int64(1)), (30, np.int64(1)), (38, np.int64(2)), (39, np.int64(1)), (47, np.int64(1)), (55, np.int64(1))]\n",
      "\n",
      "Current Policy:\n",
      "[['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']]\n",
      "\n",
      "GOAL REACHED in episode 727!\n",
      "Path taken: [(0, np.int64(2)), (1, np.int64(2)), (2, np.int64(2)), (3, np.int64(2)), (4, np.int64(1)), (12, np.int64(1)), (20, np.int64(2)), (21, np.int64(2)), (22, np.int64(1)), (30, np.int64(2)), (31, np.int64(1)), (39, np.int64(1)), (47, np.int64(1)), (55, np.int64(3)), (47, np.int64(1)), (55, np.int64(1))]\n",
      "\n",
      "Current Policy:\n",
      "[['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']]\n",
      "\n",
      "GOAL REACHED in episode 731!\n",
      "Path taken: [(0, np.int64(0)), (0, np.int64(2)), (1, np.int64(2)), (2, np.int64(2)), (3, np.int64(3)), (3, np.int64(0)), (2, np.int64(1)), (10, np.int64(2)), (11, np.int64(2)), (12, np.int64(1)), (20, np.int64(2)), (21, np.int64(2)), (22, np.int64(1)), (30, np.int64(2)), (31, np.int64(1)), (39, np.int64(1)), (47, np.int64(3)), (39, np.int64(1)), (47, np.int64(1)), (55, np.int64(1))]\n",
      "\n",
      "Current Policy:\n",
      "[['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']]\n",
      "\n",
      "GOAL REACHED in episode 733!\n",
      "Path taken: [(0, np.int64(3)), (0, np.int64(2)), (1, np.int64(2)), (2, np.int64(3)), (2, np.int64(2)), (3, np.int64(2)), (4, np.int64(1)), (12, np.int64(1)), (20, np.int64(3)), (12, np.int64(1)), (20, np.int64(2)), (21, np.int64(2)), (22, np.int64(0)), (21, np.int64(2)), (22, np.int64(0)), (21, np.int64(2)), (22, np.int64(1)), (30, np.int64(1)), (38, np.int64(3)), (30, np.int64(1)), (38, np.int64(3)), (30, np.int64(1)), (38, np.int64(0)), (37, np.int64(2)), (38, np.int64(2)), (39, np.int64(0)), (38, np.int64(2)), (39, np.int64(2)), (39, np.int64(1)), (47, np.int64(1)), (55, np.int64(1))]\n",
      "\n",
      "Current Policy:\n",
      "[['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']]\n",
      "\n",
      "GOAL REACHED in episode 734!\n",
      "Path taken: [(0, np.int64(2)), (1, np.int64(2)), (2, np.int64(2)), (3, np.int64(2)), (4, np.int64(1)), (12, np.int64(3)), (4, np.int64(1)), (12, np.int64(1)), (20, np.int64(2)), (21, np.int64(2)), (22, np.int64(1)), (30, np.int64(1)), (38, np.int64(2)), (39, np.int64(2)), (39, np.int64(1)), (47, np.int64(1)), (55, np.int64(1))]\n",
      "\n",
      "Current Policy:\n",
      "[['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']]\n",
      "\n",
      "GOAL REACHED in episode 735!\n",
      "Path taken: [(0, np.int64(2)), (1, np.int64(2)), (2, np.int64(0)), (1, np.int64(2)), (2, np.int64(2)), (3, np.int64(3)), (3, np.int64(2)), (4, np.int64(1)), (12, np.int64(3)), (4, np.int64(1)), (12, np.int64(1)), (20, np.int64(2)), (21, np.int64(2)), (22, np.int64(1)), (30, np.int64(2)), (31, np.int64(1)), (39, np.int64(1)), (47, np.int64(1)), (55, np.int64(1))]\n",
      "\n",
      "Current Policy:\n",
      "[['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']]\n",
      "\n",
      "GOAL REACHED in episode 738!\n",
      "Path taken: [(0, np.int64(3)), (0, np.int64(2)), (1, np.int64(0)), (0, np.int64(2)), (1, np.int64(2)), (2, np.int64(2)), (3, np.int64(2)), (4, np.int64(1)), (12, np.int64(1)), (20, np.int64(2)), (21, np.int64(2)), (22, np.int64(1)), (30, np.int64(1)), (38, np.int64(2)), (39, np.int64(1)), (47, np.int64(1)), (55, np.int64(1))]\n",
      "\n",
      "Current Policy:\n",
      "[['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']]\n",
      "\n",
      "GOAL REACHED in episode 740!\n",
      "Path taken: [(0, np.int64(2)), (1, np.int64(2)), (2, np.int64(2)), (3, np.int64(2)), (4, np.int64(3)), (4, np.int64(1)), (12, np.int64(1)), (20, np.int64(2)), (21, np.int64(2)), (22, np.int64(1)), (30, np.int64(1)), (38, np.int64(2)), (39, np.int64(1)), (47, np.int64(1)), (55, np.int64(1))]\n",
      "\n",
      "Current Policy:\n",
      "[['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']]\n",
      "\n",
      "GOAL REACHED in episode 742!\n",
      "Path taken: [(0, np.int64(2)), (1, np.int64(2)), (2, np.int64(3)), (2, np.int64(2)), (3, np.int64(2)), (4, np.int64(1)), (12, np.int64(1)), (20, np.int64(1)), (28, np.int64(1)), (36, np.int64(2)), (37, np.int64(2)), (38, np.int64(2)), (39, np.int64(1)), (47, np.int64(1)), (55, np.int64(1))]\n",
      "\n",
      "Current Policy:\n",
      "[['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']]\n",
      "\n",
      "GOAL REACHED in episode 745!\n",
      "Path taken: [(0, np.int64(3)), (0, np.int64(2)), (1, np.int64(2)), (2, np.int64(2)), (3, np.int64(2)), (4, np.int64(1)), (12, np.int64(0)), (11, np.int64(3)), (3, np.int64(0)), (2, np.int64(2)), (3, np.int64(2)), (4, np.int64(1)), (12, np.int64(1)), (20, np.int64(2)), (21, np.int64(2)), (22, np.int64(2)), (23, np.int64(1)), (31, np.int64(1)), (39, np.int64(1)), (47, np.int64(2)), (47, np.int64(2)), (47, np.int64(1)), (55, np.int64(1))]\n",
      "\n",
      "Current Policy:\n",
      "[['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']]\n",
      "\n",
      "GOAL REACHED in episode 749!\n",
      "Path taken: [(0, np.int64(2)), (1, np.int64(3)), (1, np.int64(2)), (2, np.int64(2)), (3, np.int64(2)), (4, np.int64(2)), (5, np.int64(1)), (13, np.int64(2)), (14, np.int64(1)), (22, np.int64(1)), (30, np.int64(3)), (22, np.int64(3)), (14, np.int64(1)), (22, np.int64(1)), (30, np.int64(1)), (38, np.int64(2)), (39, np.int64(1)), (47, np.int64(1)), (55, np.int64(1))]\n",
      "\n",
      "Current Policy:\n",
      "[['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']]\n",
      "\n",
      "GOAL REACHED in episode 750!\n",
      "Path taken: [(0, np.int64(3)), (0, np.int64(2)), (1, np.int64(1)), (9, np.int64(2)), (10, np.int64(3)), (2, np.int64(0)), (1, np.int64(2)), (2, np.int64(2)), (3, np.int64(2)), (4, np.int64(1)), (12, np.int64(1)), (20, np.int64(2)), (21, np.int64(2)), (22, np.int64(1)), (30, np.int64(1)), (38, np.int64(2)), (39, np.int64(3)), (31, np.int64(1)), (39, np.int64(1)), (47, np.int64(1)), (55, np.int64(1))]\n",
      "\n",
      "Current Policy:\n",
      "[['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']]\n",
      "\n",
      "GOAL REACHED in episode 751!\n",
      "Path taken: [(0, np.int64(2)), (1, np.int64(2)), (2, np.int64(2)), (3, np.int64(2)), (4, np.int64(3)), (4, np.int64(1)), (12, np.int64(1)), (20, np.int64(2)), (21, np.int64(2)), (22, np.int64(1)), (30, np.int64(1)), (38, np.int64(3)), (30, np.int64(2)), (31, np.int64(0)), (30, np.int64(1)), (38, np.int64(0)), (37, np.int64(2)), (38, np.int64(2)), (39, np.int64(2)), (39, np.int64(1)), (47, np.int64(1)), (55, np.int64(2)), (55, np.int64(1))]\n",
      "\n",
      "Current Policy:\n",
      "[['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']]\n",
      "\n",
      "GOAL REACHED in episode 753!\n",
      "Path taken: [(0, np.int64(2)), (1, np.int64(2)), (2, np.int64(2)), (3, np.int64(2)), (4, np.int64(1)), (12, np.int64(2)), (13, np.int64(3)), (5, np.int64(1)), (13, np.int64(1)), (21, np.int64(2)), (22, np.int64(1)), (30, np.int64(1)), (38, np.int64(2)), (39, np.int64(1)), (47, np.int64(1)), (55, np.int64(3)), (47, np.int64(3)), (39, np.int64(1)), (47, np.int64(3)), (39, np.int64(3)), (31, np.int64(1)), (39, np.int64(1)), (47, np.int64(1)), (55, np.int64(1))]\n",
      "\n",
      "Current Policy:\n",
      "[['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']]\n",
      "\n",
      "GOAL REACHED in episode 754!\n",
      "Path taken: [(0, np.int64(2)), (1, np.int64(2)), (2, np.int64(2)), (3, np.int64(2)), (4, np.int64(1)), (12, np.int64(2)), (13, np.int64(1)), (21, np.int64(2)), (22, np.int64(1)), (30, np.int64(2)), (31, np.int64(1)), (39, np.int64(1)), (47, np.int64(1)), (55, np.int64(1))]\n",
      "\n",
      "Current Policy:\n",
      "[['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']]\n",
      "\n",
      "GOAL REACHED in episode 755!\n",
      "Path taken: [(0, np.int64(3)), (0, np.int64(2)), (1, np.int64(2)), (2, np.int64(2)), (3, np.int64(2)), (4, np.int64(1)), (12, np.int64(1)), (20, np.int64(2)), (21, np.int64(2)), (22, np.int64(3)), (14, np.int64(1)), (22, np.int64(2)), (23, np.int64(1)), (31, np.int64(0)), (30, np.int64(1)), (38, np.int64(2)), (39, np.int64(1)), (47, np.int64(1)), (55, np.int64(1))]\n",
      "\n",
      "Current Policy:\n",
      "[['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']]\n",
      "\n",
      "GOAL REACHED in episode 757!\n",
      "Path taken: [(0, np.int64(2)), (1, np.int64(0)), (0, np.int64(2)), (1, np.int64(2)), (2, np.int64(2)), (3, np.int64(1)), (11, np.int64(0)), (10, np.int64(2)), (11, np.int64(2)), (12, np.int64(1)), (20, np.int64(2)), (21, np.int64(2)), (22, np.int64(1)), (30, np.int64(1)), (38, np.int64(0)), (37, np.int64(2)), (38, np.int64(3)), (30, np.int64(2)), (31, np.int64(1)), (39, np.int64(2)), (39, np.int64(1)), (47, np.int64(1)), (55, np.int64(1))]\n",
      "\n",
      "Current Policy:\n",
      "[['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']]\n",
      "\n",
      "GOAL REACHED in episode 758!\n",
      "Path taken: [(0, np.int64(2)), (1, np.int64(2)), (2, np.int64(2)), (3, np.int64(2)), (4, np.int64(1)), (12, np.int64(1)), (20, np.int64(2)), (21, np.int64(2)), (22, np.int64(1)), (30, np.int64(3)), (22, np.int64(1)), (30, np.int64(2)), (31, np.int64(1)), (39, np.int64(1)), (47, np.int64(1)), (55, np.int64(1))]\n",
      "\n",
      "Current Policy:\n",
      "[['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']]\n",
      "\n",
      "GOAL REACHED in episode 759!\n",
      "Path taken: [(0, np.int64(1)), (8, np.int64(3)), (0, np.int64(2)), (1, np.int64(2)), (2, np.int64(2)), (3, np.int64(2)), (4, np.int64(1)), (12, np.int64(1)), (20, np.int64(2)), (21, np.int64(2)), (22, np.int64(1)), (30, np.int64(1)), (38, np.int64(2)), (39, np.int64(1)), (47, np.int64(1)), (55, np.int64(1))]\n",
      "\n",
      "Current Policy:\n",
      "[['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']]\n",
      "\n",
      "GOAL REACHED in episode 763!\n",
      "Path taken: [(0, np.int64(3)), (0, np.int64(2)), (1, np.int64(2)), (2, np.int64(2)), (3, np.int64(2)), (4, np.int64(3)), (4, np.int64(3)), (4, np.int64(1)), (12, np.int64(1)), (20, np.int64(2)), (21, np.int64(2)), (22, np.int64(3)), (14, np.int64(1)), (22, np.int64(1)), (30, np.int64(1)), (38, np.int64(2)), (39, np.int64(3)), (31, np.int64(1)), (39, np.int64(1)), (47, np.int64(1)), (55, np.int64(1))]\n",
      "\n",
      "Current Policy:\n",
      "[['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']]\n",
      "\n",
      "GOAL REACHED in episode 764!\n",
      "Path taken: [(0, np.int64(2)), (1, np.int64(2)), (2, np.int64(2)), (3, np.int64(2)), (4, np.int64(2)), (5, np.int64(1)), (13, np.int64(1)), (21, np.int64(2)), (22, np.int64(1)), (30, np.int64(2)), (31, np.int64(1)), (39, np.int64(1)), (47, np.int64(1)), (55, np.int64(1))]\n",
      "\n",
      "Current Policy:\n",
      "[['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']]\n",
      "\n",
      "GOAL REACHED in episode 765!\n",
      "Path taken: [(0, np.int64(2)), (1, np.int64(2)), (2, np.int64(2)), (3, np.int64(2)), (4, np.int64(3)), (4, np.int64(1)), (12, np.int64(1)), (20, np.int64(1)), (28, np.int64(1)), (36, np.int64(2)), (37, np.int64(2)), (38, np.int64(2)), (39, np.int64(1)), (47, np.int64(1)), (55, np.int64(1))]\n",
      "\n",
      "Current Policy:\n",
      "[['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']]\n",
      "\n",
      "GOAL REACHED in episode 767!\n",
      "Path taken: [(0, np.int64(2)), (1, np.int64(2)), (2, np.int64(2)), (3, np.int64(2)), (4, np.int64(0)), (3, np.int64(3)), (3, np.int64(2)), (4, np.int64(1)), (12, np.int64(2)), (13, np.int64(1)), (21, np.int64(3)), (13, np.int64(1)), (21, np.int64(2)), (22, np.int64(1)), (30, np.int64(1)), (38, np.int64(3)), (30, np.int64(1)), (38, np.int64(2)), (39, np.int64(1)), (47, np.int64(1)), (55, np.int64(1))]\n",
      "\n",
      "Current Policy:\n",
      "[['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']]\n",
      "\n",
      "GOAL REACHED in episode 769!\n",
      "Path taken: [(0, np.int64(0)), (0, np.int64(1)), (8, np.int64(3)), (0, np.int64(2)), (1, np.int64(2)), (2, np.int64(2)), (3, np.int64(2)), (4, np.int64(0)), (3, np.int64(2)), (4, np.int64(1)), (12, np.int64(1)), (20, np.int64(2)), (21, np.int64(2)), (22, np.int64(1)), (30, np.int64(1)), (38, np.int64(2)), (39, np.int64(1)), (47, np.int64(1)), (55, np.int64(1))]\n",
      "\n",
      "Current Policy:\n",
      "[['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']]\n",
      "\n",
      "GOAL REACHED in episode 770!\n",
      "Path taken: [(0, np.int64(2)), (1, np.int64(2)), (2, np.int64(3)), (2, np.int64(2)), (3, np.int64(2)), (4, np.int64(2)), (5, np.int64(1)), (13, np.int64(1)), (21, np.int64(2)), (22, np.int64(1)), (30, np.int64(1)), (38, np.int64(2)), (39, np.int64(3)), (31, np.int64(1)), (39, np.int64(1)), (47, np.int64(1)), (55, np.int64(1))]\n",
      "\n",
      "Current Policy:\n",
      "[['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']]\n",
      "\n",
      "GOAL REACHED in episode 771!\n",
      "Path taken: [(0, np.int64(2)), (1, np.int64(2)), (2, np.int64(3)), (2, np.int64(2)), (3, np.int64(2)), (4, np.int64(1)), (12, np.int64(1)), (20, np.int64(3)), (12, np.int64(1)), (20, np.int64(2)), (21, np.int64(2)), (22, np.int64(1)), (30, np.int64(2)), (31, np.int64(1)), (39, np.int64(1)), (47, np.int64(2)), (47, np.int64(1)), (55, np.int64(1))]\n",
      "\n",
      "Current Policy:\n",
      "[['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']]\n",
      "\n",
      "GOAL REACHED in episode 774!\n",
      "Path taken: [(0, np.int64(2)), (1, np.int64(0)), (0, np.int64(2)), (1, np.int64(2)), (2, np.int64(2)), (3, np.int64(3)), (3, np.int64(2)), (4, np.int64(1)), (12, np.int64(1)), (20, np.int64(2)), (21, np.int64(3)), (13, np.int64(2)), (14, np.int64(0)), (13, np.int64(1)), (21, np.int64(2)), (22, np.int64(1)), (30, np.int64(1)), (38, np.int64(0)), (37, np.int64(2)), (38, np.int64(2)), (39, np.int64(1)), (47, np.int64(1)), (55, np.int64(1))]\n",
      "\n",
      "Current Policy:\n",
      "[['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']]\n",
      "\n",
      "GOAL REACHED in episode 776!\n",
      "Path taken: [(0, np.int64(2)), (1, np.int64(2)), (2, np.int64(2)), (3, np.int64(2)), (4, np.int64(3)), (4, np.int64(1)), (12, np.int64(1)), (20, np.int64(2)), (21, np.int64(2)), (22, np.int64(2)), (23, np.int64(1)), (31, np.int64(1)), (39, np.int64(1)), (47, np.int64(1)), (55, np.int64(1))]\n",
      "\n",
      "Current Policy:\n",
      "[['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']]\n",
      "\n",
      "GOAL REACHED in episode 777!\n",
      "Path taken: [(0, np.int64(2)), (1, np.int64(2)), (2, np.int64(2)), (3, np.int64(2)), (4, np.int64(2)), (5, np.int64(1)), (13, np.int64(1)), (21, np.int64(2)), (22, np.int64(0)), (21, np.int64(2)), (22, np.int64(1)), (30, np.int64(1)), (38, np.int64(2)), (39, np.int64(1)), (47, np.int64(1)), (55, np.int64(2)), (55, np.int64(1))]\n",
      "\n",
      "Current Policy:\n",
      "[['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']]\n",
      "\n",
      "GOAL REACHED in episode 778!\n",
      "Path taken: [(0, np.int64(2)), (1, np.int64(2)), (2, np.int64(2)), (3, np.int64(0)), (2, np.int64(2)), (3, np.int64(2)), (4, np.int64(2)), (5, np.int64(3)), (5, np.int64(1)), (13, np.int64(2)), (14, np.int64(3)), (6, np.int64(1)), (14, np.int64(1)), (22, np.int64(1)), (30, np.int64(1)), (38, np.int64(2)), (39, np.int64(1)), (47, np.int64(1)), (55, np.int64(1))]\n",
      "\n",
      "Current Policy:\n",
      "[['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']]\n",
      "\n",
      "GOAL REACHED in episode 779!\n",
      "Path taken: [(0, np.int64(2)), (1, np.int64(3)), (1, np.int64(3)), (1, np.int64(2)), (2, np.int64(1)), (10, np.int64(2)), (11, np.int64(3)), (3, np.int64(2)), (4, np.int64(0)), (3, np.int64(1)), (11, np.int64(2)), (12, np.int64(1)), (20, np.int64(2)), (21, np.int64(0)), (20, np.int64(2)), (21, np.int64(2)), (22, np.int64(1)), (30, np.int64(1)), (38, np.int64(2)), (39, np.int64(1)), (47, np.int64(1)), (55, np.int64(1))]\n",
      "\n",
      "Current Policy:\n",
      "[['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']]\n",
      "\n",
      "GOAL REACHED in episode 780!\n",
      "Path taken: [(0, np.int64(2)), (1, np.int64(2)), (2, np.int64(2)), (3, np.int64(2)), (4, np.int64(1)), (12, np.int64(1)), (20, np.int64(2)), (21, np.int64(0)), (20, np.int64(2)), (21, np.int64(2)), (22, np.int64(1)), (30, np.int64(1)), (38, np.int64(2)), (39, np.int64(1)), (47, np.int64(1)), (55, np.int64(1))]\n",
      "\n",
      "Current Policy:\n",
      "[['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']]\n",
      "\n",
      "GOAL REACHED in episode 781!\n",
      "Path taken: [(0, np.int64(2)), (1, np.int64(2)), (2, np.int64(2)), (3, np.int64(2)), (4, np.int64(2)), (5, np.int64(1)), (13, np.int64(1)), (21, np.int64(2)), (22, np.int64(1)), (30, np.int64(1)), (38, np.int64(2)), (39, np.int64(1)), (47, np.int64(1)), (55, np.int64(1))]\n",
      "\n",
      "Current Policy:\n",
      "[['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']]\n",
      "\n",
      "GOAL REACHED in episode 782!\n",
      "Path taken: [(0, np.int64(2)), (1, np.int64(2)), (2, np.int64(2)), (3, np.int64(2)), (4, np.int64(1)), (12, np.int64(0)), (11, np.int64(2)), (12, np.int64(1)), (20, np.int64(2)), (21, np.int64(2)), (22, np.int64(1)), (30, np.int64(1)), (38, np.int64(2)), (39, np.int64(1)), (47, np.int64(2)), (47, np.int64(1)), (55, np.int64(1))]\n",
      "\n",
      "Current Policy:\n",
      "[['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']]\n",
      "\n",
      "GOAL REACHED in episode 787!\n",
      "Path taken: [(0, np.int64(2)), (1, np.int64(3)), (1, np.int64(2)), (2, np.int64(2)), (3, np.int64(2)), (4, np.int64(1)), (12, np.int64(2)), (13, np.int64(3)), (5, np.int64(1)), (13, np.int64(1)), (21, np.int64(2)), (22, np.int64(1)), (30, np.int64(1)), (38, np.int64(2)), (39, np.int64(1)), (47, np.int64(1)), (55, np.int64(3)), (47, np.int64(3)), (39, np.int64(1)), (47, np.int64(2)), (47, np.int64(1)), (55, np.int64(1))]\n",
      "\n",
      "Current Policy:\n",
      "[['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']]\n",
      "\n",
      "GOAL REACHED in episode 789!\n",
      "Path taken: [(0, np.int64(2)), (1, np.int64(0)), (0, np.int64(1)), (8, np.int64(3)), (0, np.int64(2)), (1, np.int64(2)), (2, np.int64(2)), (3, np.int64(2)), (4, np.int64(1)), (12, np.int64(1)), (20, np.int64(2)), (21, np.int64(2)), (22, np.int64(1)), (30, np.int64(1)), (38, np.int64(2)), (39, np.int64(1)), (47, np.int64(3)), (39, np.int64(1)), (47, np.int64(1)), (55, np.int64(1))]\n",
      "\n",
      "Current Policy:\n",
      "[['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']]\n",
      "\n",
      "GOAL REACHED in episode 790!\n",
      "Path taken: [(0, np.int64(1)), (8, np.int64(3)), (0, np.int64(2)), (1, np.int64(2)), (2, np.int64(1)), (10, np.int64(0)), (9, np.int64(2)), (10, np.int64(1)), (18, np.int64(3)), (10, np.int64(2)), (11, np.int64(0)), (10, np.int64(2)), (11, np.int64(2)), (12, np.int64(1)), (20, np.int64(3)), (12, np.int64(1)), (20, np.int64(2)), (21, np.int64(2)), (22, np.int64(1)), (30, np.int64(1)), (38, np.int64(2)), (39, np.int64(1)), (47, np.int64(2)), (47, np.int64(1)), (55, np.int64(1))]\n",
      "\n",
      "Current Policy:\n",
      "[['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']]\n",
      "\n",
      "GOAL REACHED in episode 791!\n",
      "Path taken: [(0, np.int64(2)), (1, np.int64(2)), (2, np.int64(2)), (3, np.int64(2)), (4, np.int64(1)), (12, np.int64(1)), (20, np.int64(2)), (21, np.int64(2)), (22, np.int64(1)), (30, np.int64(1)), (38, np.int64(2)), (39, np.int64(1)), (47, np.int64(1)), (55, np.int64(1))]\n",
      "\n",
      "Current Policy:\n",
      "[['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']]\n",
      "\n",
      "GOAL REACHED in episode 793!\n",
      "Path taken: [(0, np.int64(2)), (1, np.int64(0)), (0, np.int64(2)), (1, np.int64(2)), (2, np.int64(2)), (3, np.int64(2)), (4, np.int64(1)), (12, np.int64(0)), (11, np.int64(2)), (12, np.int64(1)), (20, np.int64(2)), (21, np.int64(2)), (22, np.int64(1)), (30, np.int64(1)), (38, np.int64(2)), (39, np.int64(1)), (47, np.int64(1)), (55, np.int64(1))]\n",
      "\n",
      "Current Policy:\n",
      "[['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']]\n",
      "\n",
      "GOAL REACHED in episode 795!\n",
      "Path taken: [(0, np.int64(2)), (1, np.int64(2)), (2, np.int64(2)), (3, np.int64(2)), (4, np.int64(1)), (12, np.int64(1)), (20, np.int64(2)), (21, np.int64(2)), (22, np.int64(1)), (30, np.int64(2)), (31, np.int64(1)), (39, np.int64(0)), (38, np.int64(2)), (39, np.int64(1)), (47, np.int64(1)), (55, np.int64(1))]\n",
      "\n",
      "Current Policy:\n",
      "[['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']]\n",
      "\n",
      "GOAL REACHED in episode 796!\n",
      "Path taken: [(0, np.int64(2)), (1, np.int64(2)), (2, np.int64(2)), (3, np.int64(3)), (3, np.int64(2)), (4, np.int64(1)), (12, np.int64(1)), (20, np.int64(2)), (21, np.int64(2)), (22, np.int64(1)), (30, np.int64(1)), (38, np.int64(2)), (39, np.int64(1)), (47, np.int64(3)), (39, np.int64(1)), (47, np.int64(1)), (55, np.int64(1))]\n",
      "\n",
      "Current Policy:\n",
      "[['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']]\n",
      "\n",
      "GOAL REACHED in episode 797!\n",
      "Path taken: [(0, np.int64(0)), (0, np.int64(2)), (1, np.int64(2)), (2, np.int64(2)), (3, np.int64(2)), (4, np.int64(1)), (12, np.int64(0)), (11, np.int64(2)), (12, np.int64(1)), (20, np.int64(2)), (21, np.int64(2)), (22, np.int64(1)), (30, np.int64(1)), (38, np.int64(0)), (37, np.int64(2)), (38, np.int64(2)), (39, np.int64(1)), (47, np.int64(1)), (55, np.int64(1))]\n",
      "\n",
      "Current Policy:\n",
      "[['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']]\n",
      "\n",
      "GOAL REACHED in episode 798!\n",
      "Path taken: [(0, np.int64(2)), (1, np.int64(2)), (2, np.int64(2)), (3, np.int64(2)), (4, np.int64(3)), (4, np.int64(2)), (5, np.int64(3)), (5, np.int64(1)), (13, np.int64(1)), (21, np.int64(3)), (13, np.int64(1)), (21, np.int64(2)), (22, np.int64(1)), (30, np.int64(1)), (38, np.int64(2)), (39, np.int64(1)), (47, np.int64(1)), (55, np.int64(1))]\n",
      "\n",
      "Current Policy:\n",
      "[['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']]\n",
      "\n",
      "GOAL REACHED in episode 799!\n",
      "Path taken: [(0, np.int64(2)), (1, np.int64(2)), (2, np.int64(2)), (3, np.int64(0)), (2, np.int64(2)), (3, np.int64(0)), (2, np.int64(2)), (3, np.int64(2)), (4, np.int64(1)), (12, np.int64(1)), (20, np.int64(2)), (21, np.int64(2)), (22, np.int64(1)), (30, np.int64(1)), (38, np.int64(0)), (37, np.int64(2)), (38, np.int64(0)), (37, np.int64(2)), (38, np.int64(2)), (39, np.int64(3)), (31, np.int64(1)), (39, np.int64(1)), (47, np.int64(1)), (55, np.int64(1))]\n",
      "\n",
      "Current Policy:\n",
      "[['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']]\n",
      "\n",
      "Episode 800\n",
      "Average reward: 2.648\n",
      "Success rate: 52.00%\n",
      "\n",
      "Current Policy:\n",
      "[['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']]\n",
      "\n",
      "Least visited states:\n",
      "State 19: 0 visits\n",
      "State 29: 0 visits\n",
      "State 35: 0 visits\n",
      "State 41: 0 visits\n",
      "State 42: 0 visits\n",
      "\n",
      "GOAL REACHED in episode 801!\n",
      "Path taken: [(0, np.int64(2)), (1, np.int64(1)), (9, np.int64(2)), (10, np.int64(2)), (11, np.int64(2)), (12, np.int64(1)), (20, np.int64(2)), (21, np.int64(2)), (22, np.int64(2)), (23, np.int64(0)), (22, np.int64(1)), (30, np.int64(1)), (38, np.int64(2)), (39, np.int64(1)), (47, np.int64(1)), (55, np.int64(1))]\n",
      "\n",
      "Current Policy:\n",
      "[['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']]\n",
      "\n",
      "GOAL REACHED in episode 802!\n",
      "Path taken: [(0, np.int64(2)), (1, np.int64(2)), (2, np.int64(2)), (3, np.int64(2)), (4, np.int64(3)), (4, np.int64(1)), (12, np.int64(1)), (20, np.int64(2)), (21, np.int64(2)), (22, np.int64(2)), (23, np.int64(1)), (31, np.int64(0)), (30, np.int64(1)), (38, np.int64(2)), (39, np.int64(1)), (47, np.int64(1)), (55, np.int64(1))]\n",
      "\n",
      "Current Policy:\n",
      "[['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']]\n",
      "\n",
      "GOAL REACHED in episode 803!\n",
      "Path taken: [(0, np.int64(2)), (1, np.int64(3)), (1, np.int64(2)), (2, np.int64(2)), (3, np.int64(2)), (4, np.int64(1)), (12, np.int64(1)), (20, np.int64(2)), (21, np.int64(2)), (22, np.int64(1)), (30, np.int64(3)), (22, np.int64(1)), (30, np.int64(1)), (38, np.int64(2)), (39, np.int64(1)), (47, np.int64(1)), (55, np.int64(1))]\n",
      "\n",
      "Current Policy:\n",
      "[['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']]\n",
      "\n",
      "GOAL REACHED in episode 804!\n",
      "Path taken: [(0, np.int64(2)), (1, np.int64(2)), (2, np.int64(2)), (3, np.int64(2)), (4, np.int64(3)), (4, np.int64(1)), (12, np.int64(1)), (20, np.int64(1)), (28, np.int64(3)), (20, np.int64(2)), (21, np.int64(2)), (22, np.int64(3)), (14, np.int64(1)), (22, np.int64(1)), (30, np.int64(1)), (38, np.int64(2)), (39, np.int64(1)), (47, np.int64(1)), (55, np.int64(3)), (47, np.int64(1)), (55, np.int64(1))]\n",
      "\n",
      "Current Policy:\n",
      "[['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']]\n",
      "\n",
      "GOAL REACHED in episode 805!\n",
      "Path taken: [(0, np.int64(2)), (1, np.int64(2)), (2, np.int64(2)), (3, np.int64(2)), (4, np.int64(1)), (12, np.int64(2)), (13, np.int64(1)), (21, np.int64(3)), (13, np.int64(1)), (21, np.int64(3)), (13, np.int64(1)), (21, np.int64(0)), (20, np.int64(2)), (21, np.int64(2)), (22, np.int64(2)), (23, np.int64(1)), (31, np.int64(1)), (39, np.int64(1)), (47, np.int64(1)), (55, np.int64(1))]\n",
      "\n",
      "Current Policy:\n",
      "[['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']]\n",
      "\n",
      "GOAL REACHED in episode 806!\n",
      "Path taken: [(0, np.int64(2)), (1, np.int64(0)), (0, np.int64(3)), (0, np.int64(2)), (1, np.int64(2)), (2, np.int64(2)), (3, np.int64(0)), (2, np.int64(3)), (2, np.int64(2)), (3, np.int64(2)), (4, np.int64(0)), (3, np.int64(1)), (11, np.int64(2)), (12, np.int64(1)), (20, np.int64(2)), (21, np.int64(2)), (22, np.int64(1)), (30, np.int64(1)), (38, np.int64(2)), (39, np.int64(2)), (39, np.int64(1)), (47, np.int64(3)), (39, np.int64(2)), (39, np.int64(1)), (47, np.int64(1)), (55, np.int64(2)), (55, np.int64(2)), (55, np.int64(1))]\n",
      "\n",
      "Current Policy:\n",
      "[['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']]\n",
      "\n",
      "GOAL REACHED in episode 808!\n",
      "Path taken: [(0, np.int64(2)), (1, np.int64(2)), (2, np.int64(2)), (3, np.int64(2)), (4, np.int64(1)), (12, np.int64(3)), (4, np.int64(2)), (5, np.int64(1)), (13, np.int64(1)), (21, np.int64(2)), (22, np.int64(1)), (30, np.int64(1)), (38, np.int64(2)), (39, np.int64(1)), (47, np.int64(1)), (55, np.int64(2)), (55, np.int64(1))]\n",
      "\n",
      "Current Policy:\n",
      "[['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']]\n",
      "\n",
      "GOAL REACHED in episode 810!\n",
      "Path taken: [(0, np.int64(2)), (1, np.int64(2)), (2, np.int64(3)), (2, np.int64(2)), (3, np.int64(2)), (4, np.int64(1)), (12, np.int64(1)), (20, np.int64(2)), (21, np.int64(2)), (22, np.int64(1)), (30, np.int64(1)), (38, np.int64(2)), (39, np.int64(1)), (47, np.int64(1)), (55, np.int64(3)), (47, np.int64(1)), (55, np.int64(1))]\n",
      "\n",
      "Current Policy:\n",
      "[['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']]\n",
      "\n",
      "GOAL REACHED in episode 811!\n",
      "Path taken: [(0, np.int64(2)), (1, np.int64(2)), (2, np.int64(0)), (1, np.int64(2)), (2, np.int64(2)), (3, np.int64(0)), (2, np.int64(2)), (3, np.int64(1)), (11, np.int64(2)), (12, np.int64(3)), (4, np.int64(1)), (12, np.int64(1)), (20, np.int64(2)), (21, np.int64(3)), (13, np.int64(1)), (21, np.int64(2)), (22, np.int64(3)), (14, np.int64(1)), (22, np.int64(1)), (30, np.int64(1)), (38, np.int64(2)), (39, np.int64(1)), (47, np.int64(1)), (55, np.int64(1))]\n",
      "\n",
      "Current Policy:\n",
      "[['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']]\n",
      "\n",
      "GOAL REACHED in episode 812!\n",
      "Path taken: [(0, np.int64(2)), (1, np.int64(3)), (1, np.int64(0)), (0, np.int64(2)), (1, np.int64(0)), (0, np.int64(2)), (1, np.int64(2)), (2, np.int64(2)), (3, np.int64(2)), (4, np.int64(1)), (12, np.int64(0)), (11, np.int64(2)), (12, np.int64(3)), (4, np.int64(0)), (3, np.int64(2)), (4, np.int64(1)), (12, np.int64(1)), (20, np.int64(2)), (21, np.int64(2)), (22, np.int64(1)), (30, np.int64(3)), (22, np.int64(1)), (30, np.int64(1)), (38, np.int64(2)), (39, np.int64(1)), (47, np.int64(1)), (55, np.int64(1))]\n",
      "\n",
      "Current Policy:\n",
      "[['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']]\n",
      "\n",
      "GOAL REACHED in episode 813!\n",
      "Path taken: [(0, np.int64(3)), (0, np.int64(2)), (1, np.int64(0)), (0, np.int64(2)), (1, np.int64(2)), (2, np.int64(2)), (3, np.int64(1)), (11, np.int64(2)), (12, np.int64(1)), (20, np.int64(2)), (21, np.int64(2)), (22, np.int64(1)), (30, np.int64(1)), (38, np.int64(2)), (39, np.int64(1)), (47, np.int64(1)), (55, np.int64(1))]\n",
      "\n",
      "Current Policy:\n",
      "[['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']]\n",
      "\n",
      "GOAL REACHED in episode 815!\n",
      "Path taken: [(0, np.int64(2)), (1, np.int64(2)), (2, np.int64(2)), (3, np.int64(2)), (4, np.int64(1)), (12, np.int64(1)), (20, np.int64(2)), (21, np.int64(2)), (22, np.int64(1)), (30, np.int64(1)), (38, np.int64(2)), (39, np.int64(1)), (47, np.int64(1)), (55, np.int64(1))]\n",
      "\n",
      "Current Policy:\n",
      "[['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']]\n",
      "\n",
      "GOAL REACHED in episode 816!\n",
      "Path taken: [(0, np.int64(2)), (1, np.int64(2)), (2, np.int64(0)), (1, np.int64(2)), (2, np.int64(2)), (3, np.int64(2)), (4, np.int64(0)), (3, np.int64(0)), (2, np.int64(2)), (3, np.int64(2)), (4, np.int64(1)), (12, np.int64(1)), (20, np.int64(2)), (21, np.int64(2)), (22, np.int64(1)), (30, np.int64(3)), (22, np.int64(2)), (23, np.int64(1)), (31, np.int64(1)), (39, np.int64(1)), (47, np.int64(1)), (55, np.int64(1))]\n",
      "\n",
      "Current Policy:\n",
      "[['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']]\n",
      "\n",
      "GOAL REACHED in episode 817!\n",
      "Path taken: [(0, np.int64(2)), (1, np.int64(0)), (0, np.int64(2)), (1, np.int64(2)), (2, np.int64(2)), (3, np.int64(2)), (4, np.int64(1)), (12, np.int64(1)), (20, np.int64(2)), (21, np.int64(2)), (22, np.int64(1)), (30, np.int64(1)), (38, np.int64(2)), (39, np.int64(1)), (47, np.int64(1)), (55, np.int64(3)), (47, np.int64(3)), (39, np.int64(1)), (47, np.int64(1)), (55, np.int64(1))]\n",
      "\n",
      "Current Policy:\n",
      "[['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']]\n",
      "\n",
      "GOAL REACHED in episode 818!\n",
      "Path taken: [(0, np.int64(2)), (1, np.int64(2)), (2, np.int64(2)), (3, np.int64(2)), (4, np.int64(1)), (12, np.int64(1)), (20, np.int64(2)), (21, np.int64(2)), (22, np.int64(1)), (30, np.int64(1)), (38, np.int64(2)), (39, np.int64(1)), (47, np.int64(1)), (55, np.int64(1))]\n",
      "\n",
      "Current Policy:\n",
      "[['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']]\n",
      "\n",
      "GOAL REACHED in episode 821!\n",
      "Path taken: [(0, np.int64(2)), (1, np.int64(2)), (2, np.int64(2)), (3, np.int64(2)), (4, np.int64(1)), (12, np.int64(0)), (11, np.int64(2)), (12, np.int64(1)), (20, np.int64(2)), (21, np.int64(2)), (22, np.int64(1)), (30, np.int64(1)), (38, np.int64(0)), (37, np.int64(2)), (38, np.int64(2)), (39, np.int64(1)), (47, np.int64(1)), (55, np.int64(1))]\n",
      "\n",
      "Current Policy:\n",
      "[['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']]\n",
      "\n",
      "GOAL REACHED in episode 822!\n",
      "Path taken: [(0, np.int64(2)), (1, np.int64(2)), (2, np.int64(2)), (3, np.int64(1)), (11, np.int64(2)), (12, np.int64(1)), (20, np.int64(2)), (21, np.int64(2)), (22, np.int64(1)), (30, np.int64(1)), (38, np.int64(2)), (39, np.int64(1)), (47, np.int64(1)), (55, np.int64(2)), (55, np.int64(1))]\n",
      "\n",
      "Current Policy:\n",
      "[['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']]\n",
      "\n",
      "GOAL REACHED in episode 823!\n",
      "Path taken: [(0, np.int64(2)), (1, np.int64(2)), (2, np.int64(2)), (3, np.int64(2)), (4, np.int64(1)), (12, np.int64(3)), (4, np.int64(1)), (12, np.int64(1)), (20, np.int64(2)), (21, np.int64(2)), (22, np.int64(1)), (30, np.int64(1)), (38, np.int64(2)), (39, np.int64(1)), (47, np.int64(1)), (55, np.int64(1))]\n",
      "\n",
      "Current Policy:\n",
      "[['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']]\n",
      "\n",
      "GOAL REACHED in episode 824!\n",
      "Path taken: [(0, np.int64(2)), (1, np.int64(3)), (1, np.int64(2)), (2, np.int64(3)), (2, np.int64(2)), (3, np.int64(2)), (4, np.int64(1)), (12, np.int64(1)), (20, np.int64(2)), (21, np.int64(2)), (22, np.int64(1)), (30, np.int64(1)), (38, np.int64(2)), (39, np.int64(1)), (47, np.int64(1)), (55, np.int64(1))]\n",
      "\n",
      "Current Policy:\n",
      "[['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']]\n",
      "\n",
      "GOAL REACHED in episode 827!\n",
      "Path taken: [(0, np.int64(2)), (1, np.int64(3)), (1, np.int64(1)), (9, np.int64(2)), (10, np.int64(3)), (2, np.int64(2)), (3, np.int64(2)), (4, np.int64(1)), (12, np.int64(1)), (20, np.int64(3)), (12, np.int64(1)), (20, np.int64(2)), (21, np.int64(2)), (22, np.int64(1)), (30, np.int64(1)), (38, np.int64(3)), (30, np.int64(1)), (38, np.int64(2)), (39, np.int64(1)), (47, np.int64(1)), (55, np.int64(1))]\n",
      "\n",
      "Current Policy:\n",
      "[['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']]\n",
      "\n",
      "GOAL REACHED in episode 828!\n",
      "Path taken: [(0, np.int64(2)), (1, np.int64(2)), (2, np.int64(2)), (3, np.int64(2)), (4, np.int64(1)), (12, np.int64(1)), (20, np.int64(2)), (21, np.int64(0)), (20, np.int64(2)), (21, np.int64(2)), (22, np.int64(1)), (30, np.int64(1)), (38, np.int64(0)), (37, np.int64(2)), (38, np.int64(2)), (39, np.int64(1)), (47, np.int64(1)), (55, np.int64(1))]\n",
      "\n",
      "Current Policy:\n",
      "[['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']]\n",
      "\n",
      "GOAL REACHED in episode 829!\n",
      "Path taken: [(0, np.int64(2)), (1, np.int64(2)), (2, np.int64(2)), (3, np.int64(0)), (2, np.int64(2)), (3, np.int64(2)), (4, np.int64(1)), (12, np.int64(1)), (20, np.int64(2)), (21, np.int64(2)), (22, np.int64(0)), (21, np.int64(2)), (22, np.int64(1)), (30, np.int64(1)), (38, np.int64(2)), (39, np.int64(1)), (47, np.int64(3)), (39, np.int64(2)), (39, np.int64(2)), (39, np.int64(1)), (47, np.int64(1)), (55, np.int64(1))]\n",
      "\n",
      "Current Policy:\n",
      "[['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']]\n",
      "\n",
      "GOAL REACHED in episode 831!\n",
      "Path taken: [(0, np.int64(2)), (1, np.int64(1)), (9, np.int64(2)), (10, np.int64(1)), (18, np.int64(3)), (10, np.int64(2)), (11, np.int64(2)), (12, np.int64(1)), (20, np.int64(2)), (21, np.int64(2)), (22, np.int64(1)), (30, np.int64(3)), (22, np.int64(1)), (30, np.int64(1)), (38, np.int64(2)), (39, np.int64(1)), (47, np.int64(1)), (55, np.int64(1))]\n",
      "\n",
      "Current Policy:\n",
      "[['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']]\n",
      "\n",
      "GOAL REACHED in episode 832!\n",
      "Path taken: [(0, np.int64(2)), (1, np.int64(2)), (2, np.int64(2)), (3, np.int64(2)), (4, np.int64(2)), (5, np.int64(0)), (4, np.int64(0)), (3, np.int64(2)), (4, np.int64(1)), (12, np.int64(1)), (20, np.int64(2)), (21, np.int64(2)), (22, np.int64(1)), (30, np.int64(1)), (38, np.int64(2)), (39, np.int64(1)), (47, np.int64(3)), (39, np.int64(1)), (47, np.int64(1)), (55, np.int64(1))]\n",
      "\n",
      "Current Policy:\n",
      "[['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']]\n",
      "\n",
      "GOAL REACHED in episode 833!\n",
      "Path taken: [(0, np.int64(2)), (1, np.int64(2)), (2, np.int64(2)), (3, np.int64(2)), (4, np.int64(1)), (12, np.int64(1)), (20, np.int64(1)), (28, np.int64(1)), (36, np.int64(2)), (37, np.int64(2)), (38, np.int64(2)), (39, np.int64(1)), (47, np.int64(1)), (55, np.int64(2)), (55, np.int64(3)), (47, np.int64(1)), (55, np.int64(1))]\n",
      "\n",
      "Current Policy:\n",
      "[['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']]\n",
      "\n",
      "GOAL REACHED in episode 834!\n",
      "Path taken: [(0, np.int64(2)), (1, np.int64(2)), (2, np.int64(2)), (3, np.int64(2)), (4, np.int64(1)), (12, np.int64(0)), (11, np.int64(2)), (12, np.int64(1)), (20, np.int64(2)), (21, np.int64(2)), (22, np.int64(1)), (30, np.int64(1)), (38, np.int64(3)), (30, np.int64(1)), (38, np.int64(2)), (39, np.int64(1)), (47, np.int64(1)), (55, np.int64(1))]\n",
      "\n",
      "Current Policy:\n",
      "[['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']]\n",
      "\n",
      "GOAL REACHED in episode 836!\n",
      "Path taken: [(0, np.int64(2)), (1, np.int64(2)), (2, np.int64(2)), (3, np.int64(2)), (4, np.int64(2)), (5, np.int64(1)), (13, np.int64(1)), (21, np.int64(2)), (22, np.int64(3)), (14, np.int64(1)), (22, np.int64(2)), (23, np.int64(1)), (31, np.int64(1)), (39, np.int64(1)), (47, np.int64(3)), (39, np.int64(3)), (31, np.int64(1)), (39, np.int64(2)), (39, np.int64(2)), (39, np.int64(1)), (47, np.int64(1)), (55, np.int64(1))]\n",
      "\n",
      "Current Policy:\n",
      "[['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']]\n",
      "\n",
      "GOAL REACHED in episode 837!\n",
      "Path taken: [(0, np.int64(2)), (1, np.int64(2)), (2, np.int64(2)), (3, np.int64(2)), (4, np.int64(1)), (12, np.int64(1)), (20, np.int64(2)), (21, np.int64(2)), (22, np.int64(0)), (21, np.int64(2)), (22, np.int64(1)), (30, np.int64(1)), (38, np.int64(2)), (39, np.int64(1)), (47, np.int64(1)), (55, np.int64(1))]\n",
      "\n",
      "Current Policy:\n",
      "[['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']]\n",
      "\n",
      "GOAL REACHED in episode 840!\n",
      "Path taken: [(0, np.int64(2)), (1, np.int64(2)), (2, np.int64(2)), (3, np.int64(2)), (4, np.int64(0)), (3, np.int64(3)), (3, np.int64(2)), (4, np.int64(3)), (4, np.int64(1)), (12, np.int64(1)), (20, np.int64(3)), (12, np.int64(1)), (20, np.int64(2)), (21, np.int64(2)), (22, np.int64(1)), (30, np.int64(1)), (38, np.int64(2)), (39, np.int64(2)), (39, np.int64(1)), (47, np.int64(1)), (55, np.int64(1))]\n",
      "\n",
      "Current Policy:\n",
      "[['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']]\n",
      "\n",
      "GOAL REACHED in episode 842!\n",
      "Path taken: [(0, np.int64(2)), (1, np.int64(2)), (2, np.int64(2)), (3, np.int64(2)), (4, np.int64(1)), (12, np.int64(1)), (20, np.int64(2)), (21, np.int64(2)), (22, np.int64(1)), (30, np.int64(1)), (38, np.int64(2)), (39, np.int64(1)), (47, np.int64(1)), (55, np.int64(1))]\n",
      "\n",
      "Current Policy:\n",
      "[['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']]\n",
      "\n",
      "GOAL REACHED in episode 843!\n",
      "Path taken: [(0, np.int64(2)), (1, np.int64(2)), (2, np.int64(0)), (1, np.int64(3)), (1, np.int64(2)), (2, np.int64(2)), (3, np.int64(2)), (4, np.int64(1)), (12, np.int64(1)), (20, np.int64(2)), (21, np.int64(2)), (22, np.int64(1)), (30, np.int64(1)), (38, np.int64(0)), (37, np.int64(2)), (38, np.int64(3)), (30, np.int64(1)), (38, np.int64(2)), (39, np.int64(1)), (47, np.int64(1)), (55, np.int64(1))]\n",
      "\n",
      "Current Policy:\n",
      "[['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']]\n",
      "\n",
      "GOAL REACHED in episode 847!\n",
      "Path taken: [(0, np.int64(2)), (1, np.int64(2)), (2, np.int64(2)), (3, np.int64(2)), (4, np.int64(1)), (12, np.int64(1)), (20, np.int64(2)), (21, np.int64(3)), (13, np.int64(1)), (21, np.int64(2)), (22, np.int64(1)), (30, np.int64(1)), (38, np.int64(3)), (30, np.int64(1)), (38, np.int64(2)), (39, np.int64(1)), (47, np.int64(1)), (55, np.int64(1))]\n",
      "\n",
      "Current Policy:\n",
      "[['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']]\n",
      "\n",
      "GOAL REACHED in episode 848!\n",
      "Path taken: [(0, np.int64(2)), (1, np.int64(2)), (2, np.int64(2)), (3, np.int64(2)), (4, np.int64(0)), (3, np.int64(2)), (4, np.int64(1)), (12, np.int64(1)), (20, np.int64(2)), (21, np.int64(2)), (22, np.int64(1)), (30, np.int64(1)), (38, np.int64(2)), (39, np.int64(1)), (47, np.int64(1)), (55, np.int64(1))]\n",
      "\n",
      "Current Policy:\n",
      "[['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']]\n",
      "\n",
      "GOAL REACHED in episode 849!\n",
      "Path taken: [(0, np.int64(2)), (1, np.int64(2)), (2, np.int64(2)), (3, np.int64(2)), (4, np.int64(2)), (5, np.int64(3)), (5, np.int64(1)), (13, np.int64(1)), (21, np.int64(2)), (22, np.int64(1)), (30, np.int64(3)), (22, np.int64(1)), (30, np.int64(1)), (38, np.int64(2)), (39, np.int64(1)), (47, np.int64(1)), (55, np.int64(1))]\n",
      "\n",
      "Current Policy:\n",
      "[['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']]\n",
      "\n",
      "GOAL REACHED in episode 850!\n",
      "Path taken: [(0, np.int64(2)), (1, np.int64(2)), (2, np.int64(2)), (3, np.int64(2)), (4, np.int64(3)), (4, np.int64(0)), (3, np.int64(2)), (4, np.int64(1)), (12, np.int64(1)), (20, np.int64(2)), (21, np.int64(2)), (22, np.int64(1)), (30, np.int64(1)), (38, np.int64(2)), (39, np.int64(3)), (31, np.int64(1)), (39, np.int64(1)), (47, np.int64(1)), (55, np.int64(1))]\n",
      "\n",
      "Current Policy:\n",
      "[['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']]\n",
      "\n",
      "GOAL REACHED in episode 851!\n",
      "Path taken: [(0, np.int64(2)), (1, np.int64(2)), (2, np.int64(2)), (3, np.int64(3)), (3, np.int64(2)), (4, np.int64(3)), (4, np.int64(1)), (12, np.int64(1)), (20, np.int64(1)), (28, np.int64(3)), (20, np.int64(1)), (28, np.int64(1)), (36, np.int64(2)), (37, np.int64(2)), (38, np.int64(2)), (39, np.int64(1)), (47, np.int64(1)), (55, np.int64(1))]\n",
      "\n",
      "Current Policy:\n",
      "[['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']]\n",
      "\n",
      "GOAL REACHED in episode 852!\n",
      "Path taken: [(0, np.int64(2)), (1, np.int64(2)), (2, np.int64(2)), (3, np.int64(2)), (4, np.int64(1)), (12, np.int64(1)), (20, np.int64(2)), (21, np.int64(0)), (20, np.int64(2)), (21, np.int64(2)), (22, np.int64(3)), (14, np.int64(1)), (22, np.int64(1)), (30, np.int64(1)), (38, np.int64(3)), (30, np.int64(1)), (38, np.int64(2)), (39, np.int64(1)), (47, np.int64(1)), (55, np.int64(1))]\n",
      "\n",
      "Current Policy:\n",
      "[['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']]\n",
      "\n",
      "GOAL REACHED in episode 853!\n",
      "Path taken: [(0, np.int64(0)), (0, np.int64(2)), (1, np.int64(2)), (2, np.int64(1)), (10, np.int64(2)), (11, np.int64(2)), (12, np.int64(1)), (20, np.int64(2)), (21, np.int64(2)), (22, np.int64(1)), (30, np.int64(1)), (38, np.int64(0)), (37, np.int64(2)), (38, np.int64(2)), (39, np.int64(3)), (31, np.int64(1)), (39, np.int64(1)), (47, np.int64(1)), (55, np.int64(2)), (55, np.int64(1))]\n",
      "\n",
      "Current Policy:\n",
      "[['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']]\n",
      "\n",
      "GOAL REACHED in episode 854!\n",
      "Path taken: [(0, np.int64(2)), (1, np.int64(0)), (0, np.int64(2)), (1, np.int64(2)), (2, np.int64(2)), (3, np.int64(2)), (4, np.int64(3)), (4, np.int64(1)), (12, np.int64(1)), (20, np.int64(2)), (21, np.int64(2)), (22, np.int64(1)), (30, np.int64(2)), (31, np.int64(2)), (31, np.int64(1)), (39, np.int64(1)), (47, np.int64(1)), (55, np.int64(1))]\n",
      "\n",
      "Current Policy:\n",
      "[['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']]\n",
      "\n",
      "GOAL REACHED in episode 855!\n",
      "Path taken: [(0, np.int64(2)), (1, np.int64(2)), (2, np.int64(2)), (3, np.int64(2)), (4, np.int64(1)), (12, np.int64(0)), (11, np.int64(2)), (12, np.int64(1)), (20, np.int64(2)), (21, np.int64(2)), (22, np.int64(0)), (21, np.int64(2)), (22, np.int64(1)), (30, np.int64(1)), (38, np.int64(2)), (39, np.int64(1)), (47, np.int64(1)), (55, np.int64(1))]\n",
      "\n",
      "Current Policy:\n",
      "[['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']]\n",
      "\n",
      "GOAL REACHED in episode 856!\n",
      "Path taken: [(0, np.int64(2)), (1, np.int64(3)), (1, np.int64(2)), (2, np.int64(2)), (3, np.int64(2)), (4, np.int64(1)), (12, np.int64(2)), (13, np.int64(1)), (21, np.int64(2)), (22, np.int64(1)), (30, np.int64(1)), (38, np.int64(2)), (39, np.int64(1)), (47, np.int64(1)), (55, np.int64(1))]\n",
      "\n",
      "Current Policy:\n",
      "[['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']]\n",
      "\n",
      "GOAL REACHED in episode 860!\n",
      "Path taken: [(0, np.int64(3)), (0, np.int64(2)), (1, np.int64(2)), (2, np.int64(2)), (3, np.int64(2)), (4, np.int64(2)), (5, np.int64(1)), (13, np.int64(1)), (21, np.int64(2)), (22, np.int64(1)), (30, np.int64(1)), (38, np.int64(2)), (39, np.int64(1)), (47, np.int64(2)), (47, np.int64(1)), (55, np.int64(3)), (47, np.int64(1)), (55, np.int64(3)), (47, np.int64(1)), (55, np.int64(1))]\n",
      "\n",
      "Current Policy:\n",
      "[['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']]\n",
      "\n",
      "GOAL REACHED in episode 861!\n",
      "Path taken: [(0, np.int64(2)), (1, np.int64(2)), (2, np.int64(3)), (2, np.int64(1)), (10, np.int64(1)), (18, np.int64(3)), (10, np.int64(2)), (11, np.int64(2)), (12, np.int64(1)), (20, np.int64(2)), (21, np.int64(2)), (22, np.int64(1)), (30, np.int64(1)), (38, np.int64(2)), (39, np.int64(1)), (47, np.int64(1)), (55, np.int64(1))]\n",
      "\n",
      "Current Policy:\n",
      "[['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']]\n",
      "\n",
      "GOAL REACHED in episode 863!\n",
      "Path taken: [(0, np.int64(1)), (8, np.int64(3)), (0, np.int64(2)), (1, np.int64(2)), (2, np.int64(2)), (3, np.int64(2)), (4, np.int64(1)), (12, np.int64(1)), (20, np.int64(2)), (21, np.int64(3)), (13, np.int64(1)), (21, np.int64(2)), (22, np.int64(1)), (30, np.int64(1)), (38, np.int64(2)), (39, np.int64(1)), (47, np.int64(1)), (55, np.int64(1))]\n",
      "\n",
      "Current Policy:\n",
      "[['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']]\n",
      "\n",
      "GOAL REACHED in episode 865!\n",
      "Path taken: [(0, np.int64(2)), (1, np.int64(2)), (2, np.int64(2)), (3, np.int64(2)), (4, np.int64(0)), (3, np.int64(3)), (3, np.int64(2)), (4, np.int64(3)), (4, np.int64(0)), (3, np.int64(2)), (4, np.int64(1)), (12, np.int64(1)), (20, np.int64(2)), (21, np.int64(3)), (13, np.int64(1)), (21, np.int64(2)), (22, np.int64(1)), (30, np.int64(1)), (38, np.int64(2)), (39, np.int64(1)), (47, np.int64(1)), (55, np.int64(1))]\n",
      "\n",
      "Current Policy:\n",
      "[['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']]\n",
      "\n",
      "GOAL REACHED in episode 866!\n",
      "Path taken: [(0, np.int64(1)), (8, np.int64(3)), (0, np.int64(1)), (8, np.int64(3)), (0, np.int64(2)), (1, np.int64(2)), (2, np.int64(2)), (3, np.int64(2)), (4, np.int64(1)), (12, np.int64(1)), (20, np.int64(1)), (28, np.int64(1)), (36, np.int64(2)), (37, np.int64(2)), (38, np.int64(2)), (39, np.int64(1)), (47, np.int64(1)), (55, np.int64(1))]\n",
      "\n",
      "Current Policy:\n",
      "[['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']]\n",
      "\n",
      "GOAL REACHED in episode 869!\n",
      "Path taken: [(0, np.int64(2)), (1, np.int64(2)), (2, np.int64(2)), (3, np.int64(2)), (4, np.int64(1)), (12, np.int64(1)), (20, np.int64(2)), (21, np.int64(2)), (22, np.int64(1)), (30, np.int64(2)), (31, np.int64(1)), (39, np.int64(1)), (47, np.int64(2)), (47, np.int64(2)), (47, np.int64(1)), (55, np.int64(1))]\n",
      "\n",
      "Current Policy:\n",
      "[['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']]\n",
      "\n",
      "GOAL REACHED in episode 870!\n",
      "Path taken: [(0, np.int64(0)), (0, np.int64(2)), (1, np.int64(2)), (2, np.int64(2)), (3, np.int64(2)), (4, np.int64(1)), (12, np.int64(3)), (4, np.int64(1)), (12, np.int64(1)), (20, np.int64(2)), (21, np.int64(2)), (22, np.int64(1)), (30, np.int64(1)), (38, np.int64(2)), (39, np.int64(1)), (47, np.int64(1)), (55, np.int64(1))]\n",
      "\n",
      "Current Policy:\n",
      "[['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']]\n",
      "\n",
      "GOAL REACHED in episode 871!\n",
      "Path taken: [(0, np.int64(2)), (1, np.int64(3)), (1, np.int64(2)), (2, np.int64(2)), (3, np.int64(2)), (4, np.int64(1)), (12, np.int64(1)), (20, np.int64(2)), (21, np.int64(2)), (22, np.int64(1)), (30, np.int64(1)), (38, np.int64(2)), (39, np.int64(1)), (47, np.int64(1)), (55, np.int64(1))]\n",
      "\n",
      "Current Policy:\n",
      "[['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']]\n",
      "\n",
      "GOAL REACHED in episode 872!\n",
      "Path taken: [(0, np.int64(2)), (1, np.int64(2)), (2, np.int64(2)), (3, np.int64(2)), (4, np.int64(1)), (12, np.int64(0)), (11, np.int64(2)), (12, np.int64(1)), (20, np.int64(2)), (21, np.int64(2)), (22, np.int64(0)), (21, np.int64(2)), (22, np.int64(1)), (30, np.int64(1)), (38, np.int64(2)), (39, np.int64(1)), (47, np.int64(1)), (55, np.int64(1))]\n",
      "\n",
      "Current Policy:\n",
      "[['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']]\n",
      "\n",
      "GOAL REACHED in episode 874!\n",
      "Path taken: [(0, np.int64(0)), (0, np.int64(3)), (0, np.int64(2)), (1, np.int64(3)), (1, np.int64(2)), (2, np.int64(0)), (1, np.int64(0)), (0, np.int64(2)), (1, np.int64(2)), (2, np.int64(2)), (3, np.int64(2)), (4, np.int64(1)), (12, np.int64(1)), (20, np.int64(2)), (21, np.int64(2)), (22, np.int64(1)), (30, np.int64(1)), (38, np.int64(2)), (39, np.int64(1)), (47, np.int64(1)), (55, np.int64(1))]\n",
      "\n",
      "Current Policy:\n",
      "[['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']]\n",
      "\n",
      "GOAL REACHED in episode 877!\n",
      "Path taken: [(0, np.int64(2)), (1, np.int64(2)), (2, np.int64(2)), (3, np.int64(1)), (11, np.int64(3)), (3, np.int64(0)), (2, np.int64(2)), (3, np.int64(2)), (4, np.int64(1)), (12, np.int64(1)), (20, np.int64(2)), (21, np.int64(2)), (22, np.int64(1)), (30, np.int64(2)), (31, np.int64(1)), (39, np.int64(1)), (47, np.int64(1)), (55, np.int64(2)), (55, np.int64(1))]\n",
      "\n",
      "Current Policy:\n",
      "[['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']]\n",
      "\n",
      "GOAL REACHED in episode 879!\n",
      "Path taken: [(0, np.int64(2)), (1, np.int64(2)), (2, np.int64(2)), (3, np.int64(2)), (4, np.int64(0)), (3, np.int64(3)), (3, np.int64(1)), (11, np.int64(2)), (12, np.int64(1)), (20, np.int64(2)), (21, np.int64(2)), (22, np.int64(1)), (30, np.int64(1)), (38, np.int64(2)), (39, np.int64(1)), (47, np.int64(1)), (55, np.int64(2)), (55, np.int64(2)), (55, np.int64(1))]\n",
      "\n",
      "Current Policy:\n",
      "[['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']]\n",
      "\n",
      "GOAL REACHED in episode 880!\n",
      "Path taken: [(0, np.int64(2)), (1, np.int64(2)), (2, np.int64(2)), (3, np.int64(2)), (4, np.int64(0)), (3, np.int64(1)), (11, np.int64(2)), (12, np.int64(3)), (4, np.int64(1)), (12, np.int64(3)), (4, np.int64(1)), (12, np.int64(1)), (20, np.int64(2)), (21, np.int64(2)), (22, np.int64(1)), (30, np.int64(1)), (38, np.int64(2)), (39, np.int64(3)), (31, np.int64(1)), (39, np.int64(1)), (47, np.int64(1)), (55, np.int64(1))]\n",
      "\n",
      "Current Policy:\n",
      "[['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']]\n",
      "\n",
      "GOAL REACHED in episode 881!\n",
      "Path taken: [(0, np.int64(1)), (8, np.int64(3)), (0, np.int64(2)), (1, np.int64(2)), (2, np.int64(2)), (3, np.int64(2)), (4, np.int64(1)), (12, np.int64(1)), (20, np.int64(1)), (28, np.int64(1)), (36, np.int64(2)), (37, np.int64(2)), (38, np.int64(2)), (39, np.int64(1)), (47, np.int64(1)), (55, np.int64(2)), (55, np.int64(1))]\n",
      "\n",
      "Current Policy:\n",
      "[['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']]\n",
      "\n",
      "GOAL REACHED in episode 882!\n",
      "Path taken: [(0, np.int64(2)), (1, np.int64(2)), (2, np.int64(2)), (3, np.int64(2)), (4, np.int64(1)), (12, np.int64(1)), (20, np.int64(2)), (21, np.int64(2)), (22, np.int64(1)), (30, np.int64(1)), (38, np.int64(2)), (39, np.int64(1)), (47, np.int64(1)), (55, np.int64(1))]\n",
      "\n",
      "Current Policy:\n",
      "[['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']]\n",
      "\n",
      "GOAL REACHED in episode 883!\n",
      "Path taken: [(0, np.int64(2)), (1, np.int64(2)), (2, np.int64(3)), (2, np.int64(2)), (3, np.int64(2)), (4, np.int64(1)), (12, np.int64(1)), (20, np.int64(1)), (28, np.int64(1)), (36, np.int64(2)), (37, np.int64(2)), (38, np.int64(2)), (39, np.int64(1)), (47, np.int64(1)), (55, np.int64(1))]\n",
      "\n",
      "Current Policy:\n",
      "[['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']]\n",
      "\n",
      "GOAL REACHED in episode 884!\n",
      "Path taken: [(0, np.int64(2)), (1, np.int64(3)), (1, np.int64(2)), (2, np.int64(2)), (3, np.int64(2)), (4, np.int64(1)), (12, np.int64(1)), (20, np.int64(2)), (21, np.int64(2)), (22, np.int64(1)), (30, np.int64(3)), (22, np.int64(1)), (30, np.int64(2)), (31, np.int64(1)), (39, np.int64(1)), (47, np.int64(1)), (55, np.int64(1))]\n",
      "\n",
      "Current Policy:\n",
      "[['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']]\n",
      "\n",
      "GOAL REACHED in episode 885!\n",
      "Path taken: [(0, np.int64(2)), (1, np.int64(2)), (2, np.int64(2)), (3, np.int64(2)), (4, np.int64(1)), (12, np.int64(1)), (20, np.int64(1)), (28, np.int64(1)), (36, np.int64(1)), (44, np.int64(2)), (45, np.int64(3)), (37, np.int64(2)), (38, np.int64(2)), (39, np.int64(1)), (47, np.int64(1)), (55, np.int64(1))]\n",
      "\n",
      "Current Policy:\n",
      "[['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']]\n",
      "\n",
      "GOAL REACHED in episode 886!\n",
      "Path taken: [(0, np.int64(2)), (1, np.int64(2)), (2, np.int64(2)), (3, np.int64(2)), (4, np.int64(1)), (12, np.int64(1)), (20, np.int64(2)), (21, np.int64(2)), (22, np.int64(1)), (30, np.int64(1)), (38, np.int64(2)), (39, np.int64(0)), (38, np.int64(2)), (39, np.int64(1)), (47, np.int64(1)), (55, np.int64(1))]\n",
      "\n",
      "Current Policy:\n",
      "[['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']]\n",
      "\n",
      "GOAL REACHED in episode 888!\n",
      "Path taken: [(0, np.int64(2)), (1, np.int64(2)), (2, np.int64(0)), (1, np.int64(2)), (2, np.int64(2)), (3, np.int64(2)), (4, np.int64(1)), (12, np.int64(1)), (20, np.int64(2)), (21, np.int64(0)), (20, np.int64(2)), (21, np.int64(2)), (22, np.int64(1)), (30, np.int64(1)), (38, np.int64(2)), (39, np.int64(2)), (39, np.int64(1)), (47, np.int64(1)), (55, np.int64(1))]\n",
      "\n",
      "Current Policy:\n",
      "[['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']]\n",
      "\n",
      "GOAL REACHED in episode 889!\n",
      "Path taken: [(0, np.int64(2)), (1, np.int64(0)), (0, np.int64(2)), (1, np.int64(2)), (2, np.int64(2)), (3, np.int64(1)), (11, np.int64(2)), (12, np.int64(1)), (20, np.int64(2)), (21, np.int64(2)), (22, np.int64(0)), (21, np.int64(2)), (22, np.int64(1)), (30, np.int64(1)), (38, np.int64(2)), (39, np.int64(3)), (31, np.int64(1)), (39, np.int64(1)), (47, np.int64(1)), (55, np.int64(1))]\n",
      "\n",
      "Current Policy:\n",
      "[['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']]\n",
      "\n",
      "GOAL REACHED in episode 890!\n",
      "Path taken: [(0, np.int64(2)), (1, np.int64(2)), (2, np.int64(0)), (1, np.int64(2)), (2, np.int64(2)), (3, np.int64(2)), (4, np.int64(1)), (12, np.int64(1)), (20, np.int64(2)), (21, np.int64(2)), (22, np.int64(1)), (30, np.int64(3)), (22, np.int64(1)), (30, np.int64(1)), (38, np.int64(2)), (39, np.int64(3)), (31, np.int64(1)), (39, np.int64(1)), (47, np.int64(2)), (47, np.int64(1)), (55, np.int64(1))]\n",
      "\n",
      "Current Policy:\n",
      "[['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']]\n",
      "\n",
      "GOAL REACHED in episode 891!\n",
      "Path taken: [(0, np.int64(2)), (1, np.int64(2)), (2, np.int64(1)), (10, np.int64(2)), (11, np.int64(2)), (12, np.int64(1)), (20, np.int64(2)), (21, np.int64(2)), (22, np.int64(3)), (14, np.int64(1)), (22, np.int64(1)), (30, np.int64(1)), (38, np.int64(2)), (39, np.int64(1)), (47, np.int64(1)), (55, np.int64(1))]\n",
      "\n",
      "Current Policy:\n",
      "[['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']]\n",
      "\n",
      "GOAL REACHED in episode 895!\n",
      "Path taken: [(0, np.int64(2)), (1, np.int64(2)), (2, np.int64(2)), (3, np.int64(2)), (4, np.int64(0)), (3, np.int64(2)), (4, np.int64(1)), (12, np.int64(1)), (20, np.int64(2)), (21, np.int64(2)), (22, np.int64(0)), (21, np.int64(2)), (22, np.int64(1)), (30, np.int64(1)), (38, np.int64(2)), (39, np.int64(1)), (47, np.int64(1)), (55, np.int64(1))]\n",
      "\n",
      "Current Policy:\n",
      "[['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']]\n",
      "\n",
      "GOAL REACHED in episode 896!\n",
      "Path taken: [(0, np.int64(3)), (0, np.int64(2)), (1, np.int64(2)), (2, np.int64(2)), (3, np.int64(2)), (4, np.int64(1)), (12, np.int64(1)), (20, np.int64(2)), (21, np.int64(2)), (22, np.int64(1)), (30, np.int64(3)), (22, np.int64(1)), (30, np.int64(1)), (38, np.int64(2)), (39, np.int64(3)), (31, np.int64(1)), (39, np.int64(1)), (47, np.int64(1)), (55, np.int64(3)), (47, np.int64(1)), (55, np.int64(1))]\n",
      "\n",
      "Current Policy:\n",
      "[['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']]\n",
      "\n",
      "GOAL REACHED in episode 899!\n",
      "Path taken: [(0, np.int64(2)), (1, np.int64(2)), (2, np.int64(2)), (3, np.int64(3)), (3, np.int64(2)), (4, np.int64(1)), (12, np.int64(1)), (20, np.int64(2)), (21, np.int64(2)), (22, np.int64(1)), (30, np.int64(1)), (38, np.int64(2)), (39, np.int64(1)), (47, np.int64(1)), (55, np.int64(1))]\n",
      "\n",
      "Current Policy:\n",
      "[['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']]\n",
      "\n",
      "GOAL REACHED in episode 900!\n",
      "Path taken: [(0, np.int64(3)), (0, np.int64(3)), (0, np.int64(2)), (1, np.int64(0)), (0, np.int64(2)), (1, np.int64(2)), (2, np.int64(2)), (3, np.int64(0)), (2, np.int64(2)), (3, np.int64(2)), (4, np.int64(1)), (12, np.int64(1)), (20, np.int64(2)), (21, np.int64(2)), (22, np.int64(0)), (21, np.int64(3)), (13, np.int64(0)), (12, np.int64(1)), (20, np.int64(2)), (21, np.int64(2)), (22, np.int64(2)), (23, np.int64(1)), (31, np.int64(1)), (39, np.int64(1)), (47, np.int64(1)), (55, np.int64(1))]\n",
      "\n",
      "Current Policy:\n",
      "[['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']]\n",
      "\n",
      "Episode 900\n",
      "Average reward: 5.043\n",
      "Success rate: 68.00%\n",
      "\n",
      "Current Policy:\n",
      "[['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']]\n",
      "\n",
      "Least visited states:\n",
      "State 19: 0 visits\n",
      "State 29: 0 visits\n",
      "State 35: 0 visits\n",
      "State 41: 0 visits\n",
      "State 42: 0 visits\n",
      "\n",
      "GOAL REACHED in episode 901!\n",
      "Path taken: [(0, np.int64(2)), (1, np.int64(2)), (2, np.int64(3)), (2, np.int64(2)), (3, np.int64(2)), (4, np.int64(1)), (12, np.int64(1)), (20, np.int64(2)), (21, np.int64(2)), (22, np.int64(2)), (23, np.int64(1)), (31, np.int64(1)), (39, np.int64(1)), (47, np.int64(1)), (55, np.int64(1))]\n",
      "\n",
      "Current Policy:\n",
      "[['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']]\n",
      "\n",
      "GOAL REACHED in episode 903!\n",
      "Path taken: [(0, np.int64(1)), (8, np.int64(3)), (0, np.int64(2)), (1, np.int64(2)), (2, np.int64(2)), (3, np.int64(2)), (4, np.int64(1)), (12, np.int64(1)), (20, np.int64(2)), (21, np.int64(2)), (22, np.int64(0)), (21, np.int64(0)), (20, np.int64(2)), (21, np.int64(2)), (22, np.int64(1)), (30, np.int64(1)), (38, np.int64(2)), (39, np.int64(1)), (47, np.int64(1)), (55, np.int64(1))]\n",
      "\n",
      "Current Policy:\n",
      "[['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']]\n",
      "\n",
      "GOAL REACHED in episode 904!\n",
      "Path taken: [(0, np.int64(1)), (8, np.int64(3)), (0, np.int64(0)), (0, np.int64(2)), (1, np.int64(2)), (2, np.int64(2)), (3, np.int64(2)), (4, np.int64(1)), (12, np.int64(1)), (20, np.int64(2)), (21, np.int64(2)), (22, np.int64(1)), (30, np.int64(1)), (38, np.int64(2)), (39, np.int64(1)), (47, np.int64(3)), (39, np.int64(1)), (47, np.int64(1)), (55, np.int64(3)), (47, np.int64(1)), (55, np.int64(1))]\n",
      "\n",
      "Current Policy:\n",
      "[['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']]\n",
      "\n",
      "GOAL REACHED in episode 905!\n",
      "Path taken: [(0, np.int64(2)), (1, np.int64(2)), (2, np.int64(2)), (3, np.int64(2)), (4, np.int64(1)), (12, np.int64(1)), (20, np.int64(2)), (21, np.int64(2)), (22, np.int64(1)), (30, np.int64(1)), (38, np.int64(2)), (39, np.int64(1)), (47, np.int64(1)), (55, np.int64(1))]\n",
      "\n",
      "Current Policy:\n",
      "[['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']]\n",
      "\n",
      "GOAL REACHED in episode 906!\n",
      "Path taken: [(0, np.int64(2)), (1, np.int64(3)), (1, np.int64(2)), (2, np.int64(2)), (3, np.int64(2)), (4, np.int64(1)), (12, np.int64(3)), (4, np.int64(0)), (3, np.int64(2)), (4, np.int64(2)), (5, np.int64(1)), (13, np.int64(1)), (21, np.int64(2)), (22, np.int64(1)), (30, np.int64(1)), (38, np.int64(2)), (39, np.int64(1)), (47, np.int64(1)), (55, np.int64(1))]\n",
      "\n",
      "Current Policy:\n",
      "[['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']]\n",
      "\n",
      "GOAL REACHED in episode 909!\n",
      "Path taken: [(0, np.int64(2)), (1, np.int64(3)), (1, np.int64(2)), (2, np.int64(2)), (3, np.int64(0)), (2, np.int64(2)), (3, np.int64(2)), (4, np.int64(1)), (12, np.int64(1)), (20, np.int64(1)), (28, np.int64(1)), (36, np.int64(2)), (37, np.int64(2)), (38, np.int64(2)), (39, np.int64(1)), (47, np.int64(1)), (55, np.int64(1))]\n",
      "\n",
      "Current Policy:\n",
      "[['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']]\n",
      "\n",
      "GOAL REACHED in episode 910!\n",
      "Path taken: [(0, np.int64(2)), (1, np.int64(2)), (2, np.int64(2)), (3, np.int64(2)), (4, np.int64(2)), (5, np.int64(1)), (13, np.int64(1)), (21, np.int64(2)), (22, np.int64(1)), (30, np.int64(1)), (38, np.int64(2)), (39, np.int64(1)), (47, np.int64(1)), (55, np.int64(1))]\n",
      "\n",
      "Current Policy:\n",
      "[['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']]\n",
      "\n",
      "GOAL REACHED in episode 913!\n",
      "Path taken: [(0, np.int64(2)), (1, np.int64(3)), (1, np.int64(2)), (2, np.int64(2)), (3, np.int64(2)), (4, np.int64(1)), (12, np.int64(1)), (20, np.int64(2)), (21, np.int64(2)), (22, np.int64(1)), (30, np.int64(2)), (31, np.int64(1)), (39, np.int64(2)), (39, np.int64(1)), (47, np.int64(1)), (55, np.int64(1))]\n",
      "\n",
      "Current Policy:\n",
      "[['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']]\n",
      "\n",
      "GOAL REACHED in episode 914!\n",
      "Path taken: [(0, np.int64(2)), (1, np.int64(2)), (2, np.int64(2)), (3, np.int64(2)), (4, np.int64(1)), (12, np.int64(1)), (20, np.int64(2)), (21, np.int64(2)), (22, np.int64(1)), (30, np.int64(1)), (38, np.int64(2)), (39, np.int64(1)), (47, np.int64(1)), (55, np.int64(1))]\n",
      "\n",
      "Current Policy:\n",
      "[['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']]\n",
      "\n",
      "GOAL REACHED in episode 916!\n",
      "Path taken: [(0, np.int64(0)), (0, np.int64(2)), (1, np.int64(2)), (2, np.int64(2)), (3, np.int64(1)), (11, np.int64(2)), (12, np.int64(2)), (13, np.int64(1)), (21, np.int64(3)), (13, np.int64(1)), (21, np.int64(2)), (22, np.int64(1)), (30, np.int64(1)), (38, np.int64(2)), (39, np.int64(1)), (47, np.int64(1)), (55, np.int64(1))]\n",
      "\n",
      "Current Policy:\n",
      "[['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']]\n",
      "\n",
      "GOAL REACHED in episode 919!\n",
      "Path taken: [(0, np.int64(2)), (1, np.int64(2)), (2, np.int64(2)), (3, np.int64(2)), (4, np.int64(1)), (12, np.int64(1)), (20, np.int64(2)), (21, np.int64(2)), (22, np.int64(1)), (30, np.int64(1)), (38, np.int64(2)), (39, np.int64(0)), (38, np.int64(2)), (39, np.int64(1)), (47, np.int64(3)), (39, np.int64(1)), (47, np.int64(1)), (55, np.int64(1))]\n",
      "\n",
      "Current Policy:\n",
      "[['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']]\n",
      "\n",
      "GOAL REACHED in episode 920!\n",
      "Path taken: [(0, np.int64(2)), (1, np.int64(3)), (1, np.int64(2)), (2, np.int64(2)), (3, np.int64(3)), (3, np.int64(2)), (4, np.int64(1)), (12, np.int64(1)), (20, np.int64(2)), (21, np.int64(3)), (13, np.int64(1)), (21, np.int64(2)), (22, np.int64(1)), (30, np.int64(3)), (22, np.int64(1)), (30, np.int64(1)), (38, np.int64(2)), (39, np.int64(1)), (47, np.int64(1)), (55, np.int64(1))]\n",
      "\n",
      "Current Policy:\n",
      "[['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']]\n",
      "\n",
      "GOAL REACHED in episode 921!\n",
      "Path taken: [(0, np.int64(3)), (0, np.int64(2)), (1, np.int64(2)), (2, np.int64(2)), (3, np.int64(2)), (4, np.int64(1)), (12, np.int64(1)), (20, np.int64(2)), (21, np.int64(2)), (22, np.int64(1)), (30, np.int64(1)), (38, np.int64(2)), (39, np.int64(1)), (47, np.int64(1)), (55, np.int64(1))]\n",
      "\n",
      "Current Policy:\n",
      "[['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']]\n",
      "\n",
      "GOAL REACHED in episode 924!\n",
      "Path taken: [(0, np.int64(2)), (1, np.int64(1)), (9, np.int64(2)), (10, np.int64(3)), (2, np.int64(2)), (3, np.int64(2)), (4, np.int64(1)), (12, np.int64(1)), (20, np.int64(2)), (21, np.int64(2)), (22, np.int64(1)), (30, np.int64(1)), (38, np.int64(2)), (39, np.int64(1)), (47, np.int64(1)), (55, np.int64(1))]\n",
      "\n",
      "Current Policy:\n",
      "[['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']]\n",
      "\n",
      "GOAL REACHED in episode 926!\n",
      "Path taken: [(0, np.int64(2)), (1, np.int64(2)), (2, np.int64(0)), (1, np.int64(2)), (2, np.int64(2)), (3, np.int64(2)), (4, np.int64(1)), (12, np.int64(2)), (13, np.int64(1)), (21, np.int64(3)), (13, np.int64(1)), (21, np.int64(2)), (22, np.int64(1)), (30, np.int64(1)), (38, np.int64(2)), (39, np.int64(1)), (47, np.int64(1)), (55, np.int64(1))]\n",
      "\n",
      "Current Policy:\n",
      "[['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']]\n",
      "\n",
      "GOAL REACHED in episode 927!\n",
      "Path taken: [(0, np.int64(2)), (1, np.int64(0)), (0, np.int64(2)), (1, np.int64(2)), (2, np.int64(2)), (3, np.int64(2)), (4, np.int64(0)), (3, np.int64(3)), (3, np.int64(2)), (4, np.int64(0)), (3, np.int64(2)), (4, np.int64(1)), (12, np.int64(1)), (20, np.int64(1)), (28, np.int64(1)), (36, np.int64(2)), (37, np.int64(2)), (38, np.int64(2)), (39, np.int64(3)), (31, np.int64(3)), (23, np.int64(1)), (31, np.int64(1)), (39, np.int64(1)), (47, np.int64(1)), (55, np.int64(1))]\n",
      "\n",
      "Current Policy:\n",
      "[['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']]\n",
      "\n",
      "GOAL REACHED in episode 928!\n",
      "Path taken: [(0, np.int64(2)), (1, np.int64(2)), (2, np.int64(2)), (3, np.int64(2)), (4, np.int64(3)), (4, np.int64(1)), (12, np.int64(1)), (20, np.int64(2)), (21, np.int64(3)), (13, np.int64(1)), (21, np.int64(2)), (22, np.int64(1)), (30, np.int64(1)), (38, np.int64(2)), (39, np.int64(1)), (47, np.int64(1)), (55, np.int64(1))]\n",
      "\n",
      "Current Policy:\n",
      "[['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']]\n",
      "\n",
      "GOAL REACHED in episode 930!\n",
      "Path taken: [(0, np.int64(2)), (1, np.int64(2)), (2, np.int64(1)), (10, np.int64(2)), (11, np.int64(2)), (12, np.int64(1)), (20, np.int64(2)), (21, np.int64(2)), (22, np.int64(1)), (30, np.int64(1)), (38, np.int64(2)), (39, np.int64(1)), (47, np.int64(1)), (55, np.int64(1))]\n",
      "\n",
      "Current Policy:\n",
      "[['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']]\n",
      "\n",
      "GOAL REACHED in episode 932!\n",
      "Path taken: [(0, np.int64(2)), (1, np.int64(2)), (2, np.int64(1)), (10, np.int64(3)), (2, np.int64(2)), (3, np.int64(2)), (4, np.int64(1)), (12, np.int64(1)), (20, np.int64(3)), (12, np.int64(1)), (20, np.int64(2)), (21, np.int64(2)), (22, np.int64(1)), (30, np.int64(1)), (38, np.int64(2)), (39, np.int64(1)), (47, np.int64(3)), (39, np.int64(1)), (47, np.int64(1)), (55, np.int64(1))]\n",
      "\n",
      "Current Policy:\n",
      "[['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']]\n",
      "\n",
      "GOAL REACHED in episode 933!\n",
      "Path taken: [(0, np.int64(2)), (1, np.int64(2)), (2, np.int64(2)), (3, np.int64(2)), (4, np.int64(1)), (12, np.int64(1)), (20, np.int64(2)), (21, np.int64(2)), (22, np.int64(1)), (30, np.int64(2)), (31, np.int64(1)), (39, np.int64(1)), (47, np.int64(1)), (55, np.int64(2)), (55, np.int64(1))]\n",
      "\n",
      "Current Policy:\n",
      "[['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']]\n",
      "\n",
      "GOAL REACHED in episode 935!\n",
      "Path taken: [(0, np.int64(3)), (0, np.int64(2)), (1, np.int64(2)), (2, np.int64(2)), (3, np.int64(2)), (4, np.int64(1)), (12, np.int64(1)), (20, np.int64(3)), (12, np.int64(1)), (20, np.int64(2)), (21, np.int64(2)), (22, np.int64(1)), (30, np.int64(1)), (38, np.int64(2)), (39, np.int64(1)), (47, np.int64(1)), (55, np.int64(1))]\n",
      "\n",
      "Current Policy:\n",
      "[['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']]\n",
      "\n",
      "GOAL REACHED in episode 936!\n",
      "Path taken: [(0, np.int64(2)), (1, np.int64(2)), (2, np.int64(2)), (3, np.int64(2)), (4, np.int64(1)), (12, np.int64(1)), (20, np.int64(2)), (21, np.int64(0)), (20, np.int64(2)), (21, np.int64(2)), (22, np.int64(1)), (30, np.int64(1)), (38, np.int64(2)), (39, np.int64(1)), (47, np.int64(1)), (55, np.int64(1))]\n",
      "\n",
      "Current Policy:\n",
      "[['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']]\n",
      "\n",
      "GOAL REACHED in episode 937!\n",
      "Path taken: [(0, np.int64(2)), (1, np.int64(3)), (1, np.int64(1)), (9, np.int64(2)), (10, np.int64(0)), (9, np.int64(2)), (10, np.int64(2)), (11, np.int64(2)), (12, np.int64(0)), (11, np.int64(3)), (3, np.int64(2)), (4, np.int64(1)), (12, np.int64(1)), (20, np.int64(2)), (21, np.int64(2)), (22, np.int64(1)), (30, np.int64(1)), (38, np.int64(2)), (39, np.int64(1)), (47, np.int64(1)), (55, np.int64(1))]\n",
      "\n",
      "Current Policy:\n",
      "[['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']]\n",
      "\n",
      "GOAL REACHED in episode 938!\n",
      "Path taken: [(0, np.int64(2)), (1, np.int64(2)), (2, np.int64(2)), (3, np.int64(2)), (4, np.int64(1)), (12, np.int64(2)), (13, np.int64(1)), (21, np.int64(2)), (22, np.int64(2)), (23, np.int64(0)), (22, np.int64(0)), (21, np.int64(2)), (22, np.int64(1)), (30, np.int64(1)), (38, np.int64(2)), (39, np.int64(1)), (47, np.int64(1)), (55, np.int64(1))]\n",
      "\n",
      "Current Policy:\n",
      "[['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']]\n",
      "\n",
      "GOAL REACHED in episode 940!\n",
      "Path taken: [(0, np.int64(2)), (1, np.int64(2)), (2, np.int64(2)), (3, np.int64(2)), (4, np.int64(0)), (3, np.int64(2)), (4, np.int64(1)), (12, np.int64(1)), (20, np.int64(2)), (21, np.int64(2)), (22, np.int64(1)), (30, np.int64(1)), (38, np.int64(2)), (39, np.int64(1)), (47, np.int64(1)), (55, np.int64(1))]\n",
      "\n",
      "Current Policy:\n",
      "[['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']]\n",
      "\n",
      "GOAL REACHED in episode 944!\n",
      "Path taken: [(0, np.int64(2)), (1, np.int64(2)), (2, np.int64(2)), (3, np.int64(2)), (4, np.int64(1)), (12, np.int64(1)), (20, np.int64(2)), (21, np.int64(2)), (22, np.int64(1)), (30, np.int64(2)), (31, np.int64(1)), (39, np.int64(1)), (47, np.int64(1)), (55, np.int64(1))]\n",
      "\n",
      "Current Policy:\n",
      "[['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']]\n",
      "\n",
      "GOAL REACHED in episode 945!\n",
      "Path taken: [(0, np.int64(2)), (1, np.int64(3)), (1, np.int64(3)), (1, np.int64(2)), (2, np.int64(3)), (2, np.int64(2)), (3, np.int64(2)), (4, np.int64(1)), (12, np.int64(1)), (20, np.int64(2)), (21, np.int64(2)), (22, np.int64(1)), (30, np.int64(1)), (38, np.int64(2)), (39, np.int64(1)), (47, np.int64(1)), (55, np.int64(1))]\n",
      "\n",
      "Current Policy:\n",
      "[['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']]\n",
      "\n",
      "GOAL REACHED in episode 946!\n",
      "Path taken: [(0, np.int64(2)), (1, np.int64(2)), (2, np.int64(2)), (3, np.int64(2)), (4, np.int64(1)), (12, np.int64(3)), (4, np.int64(1)), (12, np.int64(1)), (20, np.int64(2)), (21, np.int64(2)), (22, np.int64(1)), (30, np.int64(1)), (38, np.int64(2)), (39, np.int64(1)), (47, np.int64(1)), (55, np.int64(3)), (47, np.int64(1)), (55, np.int64(1))]\n",
      "\n",
      "Current Policy:\n",
      "[['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']]\n",
      "\n",
      "GOAL REACHED in episode 951!\n",
      "Path taken: [(0, np.int64(2)), (1, np.int64(2)), (2, np.int64(3)), (2, np.int64(2)), (3, np.int64(0)), (2, np.int64(2)), (3, np.int64(2)), (4, np.int64(1)), (12, np.int64(1)), (20, np.int64(2)), (21, np.int64(2)), (22, np.int64(1)), (30, np.int64(1)), (38, np.int64(2)), (39, np.int64(1)), (47, np.int64(1)), (55, np.int64(1))]\n",
      "\n",
      "Current Policy:\n",
      "[['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']]\n",
      "\n",
      "GOAL REACHED in episode 952!\n",
      "Path taken: [(0, np.int64(2)), (1, np.int64(2)), (2, np.int64(2)), (3, np.int64(2)), (4, np.int64(1)), (12, np.int64(1)), (20, np.int64(3)), (12, np.int64(1)), (20, np.int64(2)), (21, np.int64(2)), (22, np.int64(1)), (30, np.int64(1)), (38, np.int64(2)), (39, np.int64(0)), (38, np.int64(2)), (39, np.int64(1)), (47, np.int64(1)), (55, np.int64(3)), (47, np.int64(1)), (55, np.int64(1))]\n",
      "\n",
      "Current Policy:\n",
      "[['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']]\n",
      "\n",
      "GOAL REACHED in episode 953!\n",
      "Path taken: [(0, np.int64(2)), (1, np.int64(2)), (2, np.int64(2)), (3, np.int64(2)), (4, np.int64(1)), (12, np.int64(1)), (20, np.int64(3)), (12, np.int64(1)), (20, np.int64(2)), (21, np.int64(3)), (13, np.int64(1)), (21, np.int64(2)), (22, np.int64(1)), (30, np.int64(1)), (38, np.int64(2)), (39, np.int64(1)), (47, np.int64(1)), (55, np.int64(1))]\n",
      "\n",
      "Current Policy:\n",
      "[['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']]\n",
      "\n",
      "GOAL REACHED in episode 954!\n",
      "Path taken: [(0, np.int64(2)), (1, np.int64(2)), (2, np.int64(1)), (10, np.int64(1)), (18, np.int64(3)), (10, np.int64(2)), (11, np.int64(2)), (12, np.int64(1)), (20, np.int64(2)), (21, np.int64(2)), (22, np.int64(1)), (30, np.int64(1)), (38, np.int64(2)), (39, np.int64(1)), (47, np.int64(1)), (55, np.int64(1))]\n",
      "\n",
      "Current Policy:\n",
      "[['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']]\n",
      "\n",
      "GOAL REACHED in episode 958!\n",
      "Path taken: [(0, np.int64(2)), (1, np.int64(2)), (2, np.int64(2)), (3, np.int64(2)), (4, np.int64(1)), (12, np.int64(3)), (4, np.int64(2)), (5, np.int64(1)), (13, np.int64(1)), (21, np.int64(2)), (22, np.int64(1)), (30, np.int64(1)), (38, np.int64(2)), (39, np.int64(2)), (39, np.int64(1)), (47, np.int64(1)), (55, np.int64(1))]\n",
      "\n",
      "Current Policy:\n",
      "[['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']]\n",
      "\n",
      "GOAL REACHED in episode 960!\n",
      "Path taken: [(0, np.int64(2)), (1, np.int64(3)), (1, np.int64(2)), (2, np.int64(2)), (3, np.int64(2)), (4, np.int64(2)), (5, np.int64(1)), (13, np.int64(1)), (21, np.int64(3)), (13, np.int64(1)), (21, np.int64(2)), (22, np.int64(3)), (14, np.int64(3)), (6, np.int64(1)), (14, np.int64(1)), (22, np.int64(1)), (30, np.int64(1)), (38, np.int64(2)), (39, np.int64(1)), (47, np.int64(1)), (55, np.int64(1))]\n",
      "\n",
      "Current Policy:\n",
      "[['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']]\n",
      "\n",
      "GOAL REACHED in episode 961!\n",
      "Path taken: [(0, np.int64(2)), (1, np.int64(0)), (0, np.int64(2)), (1, np.int64(2)), (2, np.int64(2)), (3, np.int64(2)), (4, np.int64(1)), (12, np.int64(3)), (4, np.int64(1)), (12, np.int64(1)), (20, np.int64(2)), (21, np.int64(0)), (20, np.int64(2)), (21, np.int64(2)), (22, np.int64(3)), (14, np.int64(1)), (22, np.int64(1)), (30, np.int64(1)), (38, np.int64(2)), (39, np.int64(1)), (47, np.int64(1)), (55, np.int64(1))]\n",
      "\n",
      "Current Policy:\n",
      "[['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']]\n",
      "\n",
      "GOAL REACHED in episode 962!\n",
      "Path taken: [(0, np.int64(2)), (1, np.int64(2)), (2, np.int64(2)), (3, np.int64(2)), (4, np.int64(1)), (12, np.int64(1)), (20, np.int64(2)), (21, np.int64(2)), (22, np.int64(1)), (30, np.int64(1)), (38, np.int64(2)), (39, np.int64(3)), (31, np.int64(1)), (39, np.int64(3)), (31, np.int64(1)), (39, np.int64(1)), (47, np.int64(1)), (55, np.int64(1))]\n",
      "\n",
      "Current Policy:\n",
      "[['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']]\n",
      "\n",
      "GOAL REACHED in episode 963!\n",
      "Path taken: [(0, np.int64(2)), (1, np.int64(2)), (2, np.int64(2)), (3, np.int64(2)), (4, np.int64(1)), (12, np.int64(1)), (20, np.int64(2)), (21, np.int64(2)), (22, np.int64(1)), (30, np.int64(1)), (38, np.int64(2)), (39, np.int64(1)), (47, np.int64(1)), (55, np.int64(1))]\n",
      "\n",
      "Current Policy:\n",
      "[['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']]\n",
      "\n",
      "GOAL REACHED in episode 965!\n",
      "Path taken: [(0, np.int64(2)), (1, np.int64(2)), (2, np.int64(2)), (3, np.int64(2)), (4, np.int64(1)), (12, np.int64(1)), (20, np.int64(2)), (21, np.int64(2)), (22, np.int64(1)), (30, np.int64(1)), (38, np.int64(2)), (39, np.int64(1)), (47, np.int64(2)), (47, np.int64(1)), (55, np.int64(1))]\n",
      "\n",
      "Current Policy:\n",
      "[['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']]\n",
      "\n",
      "GOAL REACHED in episode 969!\n",
      "Path taken: [(0, np.int64(2)), (1, np.int64(2)), (2, np.int64(2)), (3, np.int64(2)), (4, np.int64(1)), (12, np.int64(1)), (20, np.int64(2)), (21, np.int64(2)), (22, np.int64(1)), (30, np.int64(1)), (38, np.int64(2)), (39, np.int64(1)), (47, np.int64(1)), (55, np.int64(1))]\n",
      "\n",
      "Current Policy:\n",
      "[['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']]\n",
      "\n",
      "GOAL REACHED in episode 970!\n",
      "Path taken: [(0, np.int64(2)), (1, np.int64(2)), (2, np.int64(2)), (3, np.int64(2)), (4, np.int64(1)), (12, np.int64(1)), (20, np.int64(2)), (21, np.int64(2)), (22, np.int64(1)), (30, np.int64(1)), (38, np.int64(2)), (39, np.int64(2)), (39, np.int64(1)), (47, np.int64(3)), (39, np.int64(3)), (31, np.int64(1)), (39, np.int64(1)), (47, np.int64(1)), (55, np.int64(2)), (55, np.int64(3)), (47, np.int64(3)), (39, np.int64(1)), (47, np.int64(1)), (55, np.int64(1))]\n",
      "\n",
      "Current Policy:\n",
      "[['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']]\n",
      "\n",
      "GOAL REACHED in episode 971!\n",
      "Path taken: [(0, np.int64(2)), (1, np.int64(2)), (2, np.int64(2)), (3, np.int64(1)), (11, np.int64(2)), (12, np.int64(1)), (20, np.int64(2)), (21, np.int64(2)), (22, np.int64(1)), (30, np.int64(1)), (38, np.int64(2)), (39, np.int64(1)), (47, np.int64(1)), (55, np.int64(1))]\n",
      "\n",
      "Current Policy:\n",
      "[['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']]\n",
      "\n",
      "GOAL REACHED in episode 972!\n",
      "Path taken: [(0, np.int64(2)), (1, np.int64(0)), (0, np.int64(2)), (1, np.int64(2)), (2, np.int64(2)), (3, np.int64(2)), (4, np.int64(1)), (12, np.int64(1)), (20, np.int64(1)), (28, np.int64(1)), (36, np.int64(2)), (37, np.int64(2)), (38, np.int64(2)), (39, np.int64(1)), (47, np.int64(3)), (39, np.int64(1)), (47, np.int64(2)), (47, np.int64(3)), (39, np.int64(1)), (47, np.int64(3)), (39, np.int64(1)), (47, np.int64(1)), (55, np.int64(1))]\n",
      "\n",
      "Current Policy:\n",
      "[['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']]\n",
      "\n",
      "GOAL REACHED in episode 973!\n",
      "Path taken: [(0, np.int64(2)), (1, np.int64(1)), (9, np.int64(2)), (10, np.int64(2)), (11, np.int64(2)), (12, np.int64(1)), (20, np.int64(2)), (21, np.int64(2)), (22, np.int64(1)), (30, np.int64(1)), (38, np.int64(2)), (39, np.int64(0)), (38, np.int64(2)), (39, np.int64(1)), (47, np.int64(1)), (55, np.int64(1))]\n",
      "\n",
      "Current Policy:\n",
      "[['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']]\n",
      "\n",
      "GOAL REACHED in episode 974!\n",
      "Path taken: [(0, np.int64(2)), (1, np.int64(2)), (2, np.int64(2)), (3, np.int64(1)), (11, np.int64(2)), (12, np.int64(0)), (11, np.int64(2)), (12, np.int64(1)), (20, np.int64(2)), (21, np.int64(2)), (22, np.int64(1)), (30, np.int64(1)), (38, np.int64(2)), (39, np.int64(1)), (47, np.int64(1)), (55, np.int64(1))]\n",
      "\n",
      "Current Policy:\n",
      "[['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']]\n",
      "\n",
      "GOAL REACHED in episode 976!\n",
      "Path taken: [(0, np.int64(2)), (1, np.int64(0)), (0, np.int64(2)), (1, np.int64(2)), (2, np.int64(2)), (3, np.int64(1)), (11, np.int64(3)), (3, np.int64(2)), (4, np.int64(1)), (12, np.int64(1)), (20, np.int64(2)), (21, np.int64(0)), (20, np.int64(2)), (21, np.int64(2)), (22, np.int64(1)), (30, np.int64(1)), (38, np.int64(2)), (39, np.int64(0)), (38, np.int64(2)), (39, np.int64(1)), (47, np.int64(1)), (55, np.int64(1))]\n",
      "\n",
      "Current Policy:\n",
      "[['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']]\n",
      "\n",
      "GOAL REACHED in episode 979!\n",
      "Path taken: [(0, np.int64(2)), (1, np.int64(2)), (2, np.int64(2)), (3, np.int64(2)), (4, np.int64(1)), (12, np.int64(1)), (20, np.int64(2)), (21, np.int64(2)), (22, np.int64(0)), (21, np.int64(2)), (22, np.int64(1)), (30, np.int64(1)), (38, np.int64(2)), (39, np.int64(1)), (47, np.int64(1)), (55, np.int64(1))]\n",
      "\n",
      "Current Policy:\n",
      "[['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']]\n",
      "\n",
      "GOAL REACHED in episode 981!\n",
      "Path taken: [(0, np.int64(2)), (1, np.int64(1)), (9, np.int64(2)), (10, np.int64(2)), (11, np.int64(2)), (12, np.int64(1)), (20, np.int64(2)), (21, np.int64(2)), (22, np.int64(1)), (30, np.int64(1)), (38, np.int64(2)), (39, np.int64(1)), (47, np.int64(1)), (55, np.int64(1))]\n",
      "\n",
      "Current Policy:\n",
      "[['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']]\n",
      "\n",
      "GOAL REACHED in episode 983!\n",
      "Path taken: [(0, np.int64(2)), (1, np.int64(1)), (9, np.int64(1)), (17, np.int64(2)), (18, np.int64(3)), (10, np.int64(1)), (18, np.int64(3)), (10, np.int64(2)), (11, np.int64(0)), (10, np.int64(2)), (11, np.int64(2)), (12, np.int64(3)), (4, np.int64(1)), (12, np.int64(0)), (11, np.int64(2)), (12, np.int64(0)), (11, np.int64(2)), (12, np.int64(1)), (20, np.int64(2)), (21, np.int64(0)), (20, np.int64(2)), (21, np.int64(2)), (22, np.int64(1)), (30, np.int64(1)), (38, np.int64(2)), (39, np.int64(1)), (47, np.int64(3)), (39, np.int64(1)), (47, np.int64(1)), (55, np.int64(1))]\n",
      "\n",
      "Current Policy:\n",
      "[['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']]\n",
      "\n",
      "GOAL REACHED in episode 985!\n",
      "Path taken: [(0, np.int64(2)), (1, np.int64(0)), (0, np.int64(2)), (1, np.int64(1)), (9, np.int64(2)), (10, np.int64(2)), (11, np.int64(2)), (12, np.int64(1)), (20, np.int64(2)), (21, np.int64(2)), (22, np.int64(2)), (23, np.int64(1)), (31, np.int64(1)), (39, np.int64(2)), (39, np.int64(1)), (47, np.int64(2)), (47, np.int64(1)), (55, np.int64(1))]\n",
      "\n",
      "Current Policy:\n",
      "[['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']]\n",
      "\n",
      "GOAL REACHED in episode 986!\n",
      "Path taken: [(0, np.int64(2)), (1, np.int64(2)), (2, np.int64(2)), (3, np.int64(2)), (4, np.int64(1)), (12, np.int64(1)), (20, np.int64(2)), (21, np.int64(2)), (22, np.int64(2)), (23, np.int64(1)), (31, np.int64(1)), (39, np.int64(3)), (31, np.int64(1)), (39, np.int64(1)), (47, np.int64(1)), (55, np.int64(1))]\n",
      "\n",
      "Current Policy:\n",
      "[['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']]\n",
      "\n",
      "GOAL REACHED in episode 987!\n",
      "Path taken: [(0, np.int64(2)), (1, np.int64(2)), (2, np.int64(2)), (3, np.int64(3)), (3, np.int64(2)), (4, np.int64(1)), (12, np.int64(3)), (4, np.int64(1)), (12, np.int64(1)), (20, np.int64(2)), (21, np.int64(2)), (22, np.int64(1)), (30, np.int64(1)), (38, np.int64(2)), (39, np.int64(1)), (47, np.int64(3)), (39, np.int64(1)), (47, np.int64(1)), (55, np.int64(1))]\n",
      "\n",
      "Current Policy:\n",
      "[['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']]\n",
      "\n",
      "GOAL REACHED in episode 988!\n",
      "Path taken: [(0, np.int64(1)), (8, np.int64(3)), (0, np.int64(2)), (1, np.int64(3)), (1, np.int64(2)), (2, np.int64(3)), (2, np.int64(2)), (3, np.int64(2)), (4, np.int64(1)), (12, np.int64(1)), (20, np.int64(3)), (12, np.int64(1)), (20, np.int64(2)), (21, np.int64(0)), (20, np.int64(3)), (12, np.int64(1)), (20, np.int64(2)), (21, np.int64(2)), (22, np.int64(1)), (30, np.int64(1)), (38, np.int64(2)), (39, np.int64(1)), (47, np.int64(1)), (55, np.int64(1))]\n",
      "\n",
      "Current Policy:\n",
      "[['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']]\n",
      "\n",
      "GOAL REACHED in episode 993!\n",
      "Path taken: [(0, np.int64(2)), (1, np.int64(0)), (0, np.int64(2)), (1, np.int64(2)), (2, np.int64(2)), (3, np.int64(2)), (4, np.int64(1)), (12, np.int64(1)), (20, np.int64(2)), (21, np.int64(0)), (20, np.int64(2)), (21, np.int64(2)), (22, np.int64(1)), (30, np.int64(1)), (38, np.int64(2)), (39, np.int64(1)), (47, np.int64(1)), (55, np.int64(1))]\n",
      "\n",
      "Current Policy:\n",
      "[['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']]\n",
      "\n",
      "GOAL REACHED in episode 996!\n",
      "Path taken: [(0, np.int64(2)), (1, np.int64(3)), (1, np.int64(2)), (2, np.int64(2)), (3, np.int64(2)), (4, np.int64(1)), (12, np.int64(1)), (20, np.int64(2)), (21, np.int64(2)), (22, np.int64(1)), (30, np.int64(1)), (38, np.int64(2)), (39, np.int64(1)), (47, np.int64(1)), (55, np.int64(1))]\n",
      "\n",
      "Current Policy:\n",
      "[['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']]\n",
      "\n",
      "GOAL REACHED in episode 998!\n",
      "Path taken: [(0, np.int64(2)), (1, np.int64(2)), (2, np.int64(2)), (3, np.int64(2)), (4, np.int64(1)), (12, np.int64(1)), (20, np.int64(2)), (21, np.int64(2)), (22, np.int64(1)), (30, np.int64(1)), (38, np.int64(2)), (39, np.int64(1)), (47, np.int64(1)), (55, np.int64(1))]\n",
      "\n",
      "Current Policy:\n",
      "[['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']]\n",
      "\n",
      "GOAL REACHED in episode 999!\n",
      "Path taken: [(0, np.int64(1)), (8, np.int64(3)), (0, np.int64(2)), (1, np.int64(2)), (2, np.int64(2)), (3, np.int64(2)), (4, np.int64(1)), (12, np.int64(1)), (20, np.int64(2)), (21, np.int64(2)), (22, np.int64(1)), (30, np.int64(1)), (38, np.int64(2)), (39, np.int64(0)), (38, np.int64(2)), (39, np.int64(1)), (47, np.int64(1)), (55, np.int64(1))]\n",
      "\n",
      "Current Policy:\n",
      "[['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']]\n",
      "\n",
      "Final Policy:\n",
      "\n",
      "Current Policy:\n",
      "[['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']]\n",
      "\n",
      "Plotting results...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1200x400 with 2 Axes>"
      ],
      "image/png": "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"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 20
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-17T12:26:06.772006Z",
     "start_time": "2025-02-17T12:26:06.763778Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def train(n_training_episodes, min_epsilon, max_epsilon, decay_rate, env, max_steps, Qtable):\n",
    "    # Lists to keep track of rewards\n",
    "    episode_rewards = []\n",
    "    episode_steps = []\n",
    "\n",
    "    for episode in tqdm(range(n_training_episodes)):\n",
    "        # Reduce epsilon (because we need less and less exploration)\n",
    "        epsilon = min_epsilon + (max_epsilon - min_epsilon)*np.exp(-decay_rate*episode)\n",
    "\n",
    "        # Reset the environment\n",
    "        state, info = env.reset()\n",
    "        step = 0\n",
    "        total_reward = 0\n",
    "        terminated = False\n",
    "        truncated = False\n",
    "\n",
    "        # Keep track of Q-value changes for debugging\n",
    "        max_q_change = 0\n",
    "\n",
    "        # repeat\n",
    "        for step in range(max_steps):\n",
    "            # Choose the action At using epsilon greedy policy\n",
    "            action = epsilon_greedy_policy(Qtable, state, epsilon)\n",
    "\n",
    "            # Store old Q-value for debugging\n",
    "            old_q = Qtable[state][action]\n",
    "\n",
    "            # Take action At and observe Rt+1 and St+1\n",
    "            new_state, reward, terminated, truncated, info = env.step(action)\n",
    "\n",
    "            # Update Q(s,a):= Q(s,a) + lr [R(s,a) + gamma * max Q(s',a') - Q(s,a)]\n",
    "            Qtable[state][action] = Qtable[state][action] + learning_rate * (\n",
    "                    reward + gamma * np.max(Qtable[new_state]) - Qtable[state][action]\n",
    "            )\n",
    "\n",
    "            # Track maximum Q-value change in this step\n",
    "            q_change = abs(Qtable[state][action] - old_q)\n",
    "            max_q_change = max(max_q_change, q_change)\n",
    "\n",
    "            total_reward += reward\n",
    "\n",
    "            # If terminated or truncated finish the episode\n",
    "            if terminated or truncated:\n",
    "                break\n",
    "\n",
    "            # Our next state is the new state\n",
    "            state = new_state\n",
    "\n",
    "        # Store episode statistics\n",
    "        episode_rewards.append(total_reward)\n",
    "        episode_steps.append(step)\n",
    "\n",
    "        # Print detailed information every 1000 episodes\n",
    "        if episode % 1000 == 0:\n",
    "            print(f\"\\nEpisode {episode}\")\n",
    "            print(f\"Epsilon: {epsilon:.4f}\")\n",
    "            print(f\"Steps: {step}\")\n",
    "            print(f\"Total reward: {total_reward}\")\n",
    "            print(f\"Max Q-value change: {max_q_change:.4f}\")\n",
    "            print(f\"Average reward over last 1000 episodes: {np.mean(episode_rewards[-1000:]):.4f}\")\n",
    "\n",
    "            # Print some Q-values for key states\n",
    "            print(\"\\nSample Q-values for some states:\")\n",
    "            sample_states = [0, 8, 16, 24, 32, 40, 48, 56]  # Edge states\n",
    "            for s in sample_states:\n",
    "                print(f\"State {s}: {Qtable[s]}\")\n",
    "\n",
    "    return Qtable, episode_rewards, episode_steps"
   ],
   "id": "8c76f142391cf6f2",
   "outputs": [],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-17T12:44:59.529362Z",
     "start_time": "2025-02-17T12:44:59.184993Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#Qtable_frozenlake = train(n_training_episodes, min_epsilon, max_epsilon, decay_rate, env, max_steps, Qtable_frozenlake)\n",
    "# Reset parameters with modifications\n",
    "learning_rate = 0.1\n",
    "gamma = 0.99  # Increased from 0.95 to give more weight to future rewards\n",
    "max_steps = 200\n",
    "max_epsilon = 1.0\n",
    "min_epsilon = 0.1  # Increased from 0.01 to maintain some exploration\n",
    "decay_rate = 0.002  # Increased for faster decay\n",
    "\n",
    "# Initialize fresh Q-table\n",
    "Qtable_frozenlake = initialize_q_table(state_space, action_space)\n",
    "\n",
    "# Run improved training\n",
    "print(\"\\nStarting improved training...\")\n",
    "Qtable_frozenlake, successful_episodes, episode_rewards, state_visits = improved_debug_train_v2(\n",
    "    n_training_episodes=1000,  # Increased episodes\n",
    "    min_epsilon=min_epsilon,\n",
    "    max_epsilon=max_epsilon,\n",
    "    decay_rate=decay_rate,\n",
    "    env=env,\n",
    "    max_steps=max_steps,\n",
    "    Qtable=Qtable_frozenlake\n",
    ")\n",
    "\n",
    "# Final policy visualization\n",
    "print(\"\\nFinal Policy:\")\n",
    "print_policy(Qtable_frozenlake)\n",
    "\n",
    "# Plot training results\n",
    "print(\"\\nPlotting results...\")\n",
    "plt.figure(figsize=(12, 4))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(episode_rewards)\n",
    "plt.title('Rewards per Episode')\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Total Reward')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "window_size = 100\n",
    "moving_avg = [np.mean(episode_rewards[i:i+window_size])\n",
    "              for i in range(0, len(episode_rewards)-window_size+1)]\n",
    "plt.plot(moving_avg)\n",
    "plt.title(f'Moving Average ({window_size} episodes)')\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Average Reward')\n",
    "plt.show()\n",
    "#Qtable_frozenlake = improved_debug_train_v2(n_training_episodes, min_epsilon, max_epsilon, decay_rate, env, max_steps, Qtable_frozenlake)"
   ],
   "id": "f0c2697d3a91ab29",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting improved training...\n",
      "\n",
      "Episode 0\n",
      "Average reward: -5.150\n",
      "Success rate: 0.00%\n",
      "\n",
      "Current Policy:\n",
      "[['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']]\n",
      "\n",
      "Least visited states:\n",
      "State 1: 0 visits\n",
      "State 2: 0 visits\n",
      "State 3: 0 visits\n",
      "State 4: 0 visits\n",
      "State 5: 0 visits\n",
      "\n",
      "Episode 100\n",
      "Average reward: -5.248\n",
      "Success rate: 0.00%\n",
      "\n",
      "Current Policy:\n",
      "[['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']]\n",
      "\n",
      "Least visited states:\n",
      "State 19: 0 visits\n",
      "State 29: 0 visits\n",
      "State 35: 0 visits\n",
      "State 37: 0 visits\n",
      "State 41: 0 visits\n",
      "\n",
      "Episode 200\n",
      "Average reward: -5.180\n",
      "Success rate: 0.00%\n",
      "\n",
      "Current Policy:\n",
      "[['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']]\n",
      "\n",
      "Least visited states:\n",
      "State 19: 0 visits\n",
      "State 29: 0 visits\n",
      "State 35: 0 visits\n",
      "State 41: 0 visits\n",
      "State 42: 0 visits\n",
      "\n",
      "GOAL REACHED in episode 207!\n",
      "Path taken: [(0, np.int64(1)), (8, np.int64(1)), (16, np.int64(3)), (8, np.int64(2)), (9, np.int64(0)), (8, np.int64(1)), (16, np.int64(3)), (8, np.int64(2)), (9, np.int64(3)), (1, np.int64(3)), (1, np.int64(1)), (9, np.int64(2)), (10, np.int64(2)), (11, np.int64(2)), (12, np.int64(2)), (13, np.int64(1)), (21, np.int64(2)), (22, np.int64(2)), (23, np.int64(3)), (15, np.int64(3)), (7, np.int64(1)), (15, np.int64(3)), (7, np.int64(0)), (6, np.int64(2)), (7, np.int64(0)), (6, np.int64(2)), (7, np.int64(2)), (7, np.int64(3)), (7, np.int64(1)), (15, np.int64(0)), (14, np.int64(2)), (15, np.int64(1)), (23, np.int64(2)), (23, np.int64(1)), (31, np.int64(0)), (30, np.int64(2)), (31, np.int64(1)), (39, np.int64(0)), (38, np.int64(2)), (39, np.int64(2)), (39, np.int64(2)), (39, np.int64(1)), (47, np.int64(1)), (55, np.int64(2)), (55, np.int64(1))]\n",
      "\n",
      "Current Policy:\n",
      "[['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']]\n",
      "\n",
      "GOAL REACHED in episode 231!\n",
      "Path taken: [(0, np.int64(2)), (1, np.int64(3)), (1, np.int64(2)), (2, np.int64(2)), (3, np.int64(1)), (11, np.int64(2)), (12, np.int64(2)), (13, np.int64(3)), (5, np.int64(1)), (13, np.int64(3)), (5, np.int64(1)), (13, np.int64(2)), (14, np.int64(1)), (22, np.int64(1)), (30, np.int64(1)), (38, np.int64(2)), (39, np.int64(0)), (38, np.int64(2)), (39, np.int64(2)), (39, np.int64(2)), (39, np.int64(1)), (47, np.int64(1)), (55, np.int64(1))]\n",
      "\n",
      "Current Policy:\n",
      "[['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']]\n",
      "\n",
      "GOAL REACHED in episode 240!\n",
      "Path taken: [(0, np.int64(3)), (0, np.int64(3)), (0, np.int64(2)), (1, np.int64(2)), (2, np.int64(3)), (2, np.int64(0)), (1, np.int64(3)), (1, np.int64(3)), (1, np.int64(3)), (1, np.int64(2)), (2, np.int64(1)), (10, np.int64(2)), (11, np.int64(0)), (10, np.int64(2)), (11, np.int64(2)), (12, np.int64(2)), (13, np.int64(3)), (5, np.int64(1)), (13, np.int64(3)), (5, np.int64(3)), (5, np.int64(0)), (4, np.int64(0)), (3, np.int64(2)), (4, np.int64(0)), (3, np.int64(0)), (2, np.int64(2)), (3, np.int64(3)), (3, np.int64(1)), (11, np.int64(2)), (12, np.int64(0)), (11, np.int64(2)), (12, np.int64(0)), (11, np.int64(2)), (12, np.int64(2)), (13, np.int64(2)), (14, np.int64(2)), (15, np.int64(1)), (23, np.int64(1)), (31, np.int64(1)), (39, np.int64(1)), (47, np.int64(3)), (39, np.int64(3)), (31, np.int64(1)), (39, np.int64(1)), (47, np.int64(2)), (47, np.int64(1)), (55, np.int64(3)), (47, np.int64(3)), (39, np.int64(3)), (31, np.int64(1)), (39, np.int64(2)), (39, np.int64(0)), (38, np.int64(2)), (39, np.int64(3)), (31, np.int64(0)), (30, np.int64(3)), (22, np.int64(2)), (23, np.int64(3)), (15, np.int64(2)), (15, np.int64(1)), (23, np.int64(1)), (31, np.int64(2)), (31, np.int64(1)), (39, np.int64(1)), (47, np.int64(3)), (39, np.int64(1)), (47, np.int64(3)), (39, np.int64(2)), (39, np.int64(1)), (47, np.int64(1)), (55, np.int64(1))]\n",
      "\n",
      "Current Policy:\n",
      "[['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']]\n",
      "\n",
      "GOAL REACHED in episode 271!\n",
      "Path taken: [(0, np.int64(2)), (1, np.int64(2)), (2, np.int64(2)), (3, np.int64(2)), (4, np.int64(0)), (3, np.int64(3)), (3, np.int64(3)), (3, np.int64(1)), (11, np.int64(3)), (3, np.int64(2)), (4, np.int64(1)), (12, np.int64(2)), (13, np.int64(2)), (14, np.int64(0)), (13, np.int64(2)), (14, np.int64(2)), (15, np.int64(0)), (14, np.int64(3)), (6, np.int64(1)), (14, np.int64(1)), (22, np.int64(0)), (21, np.int64(2)), (22, np.int64(1)), (30, np.int64(3)), (22, np.int64(2)), (23, np.int64(3)), (15, np.int64(1)), (23, np.int64(3)), (15, np.int64(2)), (15, np.int64(3)), (7, np.int64(3)), (7, np.int64(3)), (7, np.int64(3)), (7, np.int64(0)), (6, np.int64(3)), (6, np.int64(2)), (7, np.int64(2)), (7, np.int64(0)), (6, np.int64(1)), (14, np.int64(1)), (22, np.int64(1)), (30, np.int64(1)), (38, np.int64(3)), (30, np.int64(1)), (38, np.int64(2)), (39, np.int64(3)), (31, np.int64(0)), (30, np.int64(3)), (22, np.int64(1)), (30, np.int64(1)), (38, np.int64(3)), (30, np.int64(1)), (38, np.int64(2)), (39, np.int64(3)), (31, np.int64(2)), (31, np.int64(3)), (23, np.int64(1)), (31, np.int64(2)), (31, np.int64(3)), (23, np.int64(1)), (31, np.int64(1)), (39, np.int64(2)), (39, np.int64(1)), (47, np.int64(3)), (39, np.int64(1)), (47, np.int64(1)), (55, np.int64(3)), (47, np.int64(1)), (55, np.int64(1))]\n",
      "\n",
      "Current Policy:\n",
      "[['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']]\n",
      "\n",
      "GOAL REACHED in episode 278!\n",
      "Path taken: [(0, np.int64(2)), (1, np.int64(1)), (9, np.int64(3)), (1, np.int64(2)), (2, np.int64(1)), (10, np.int64(3)), (2, np.int64(3)), (2, np.int64(2)), (3, np.int64(1)), (11, np.int64(2)), (12, np.int64(3)), (4, np.int64(1)), (12, np.int64(2)), (13, np.int64(2)), (14, np.int64(1)), (22, np.int64(1)), (30, np.int64(1)), (38, np.int64(0)), (37, np.int64(2)), (38, np.int64(2)), (39, np.int64(3)), (31, np.int64(3)), (23, np.int64(1)), (31, np.int64(1)), (39, np.int64(2)), (39, np.int64(1)), (47, np.int64(1)), (55, np.int64(1))]\n",
      "\n",
      "Current Policy:\n",
      "[['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']]\n",
      "\n",
      "Episode 300\n",
      "Average reward: -4.448\n",
      "Success rate: 5.00%\n",
      "\n",
      "Current Policy:\n",
      "[['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']]\n",
      "\n",
      "Least visited states:\n",
      "State 19: 0 visits\n",
      "State 29: 0 visits\n",
      "State 35: 0 visits\n",
      "State 41: 0 visits\n",
      "State 42: 0 visits\n",
      "\n",
      "GOAL REACHED in episode 306!\n",
      "Path taken: [(0, np.int64(1)), (8, np.int64(1)), (16, np.int64(1)), (24, np.int64(2)), (25, np.int64(1)), (33, np.int64(3)), (25, np.int64(2)), (26, np.int64(0)), (25, np.int64(1)), (33, np.int64(3)), (25, np.int64(2)), (26, np.int64(0)), (25, np.int64(2)), (26, np.int64(2)), (27, np.int64(2)), (28, np.int64(1)), (36, np.int64(1)), (44, np.int64(2)), (45, np.int64(3)), (37, np.int64(2)), (38, np.int64(2)), (39, np.int64(1)), (47, np.int64(1)), (55, np.int64(1))]\n",
      "\n",
      "Current Policy:\n",
      "[['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']]\n",
      "\n",
      "GOAL REACHED in episode 316!\n",
      "Path taken: [(0, np.int64(2)), (1, np.int64(2)), (2, np.int64(3)), (2, np.int64(2)), (3, np.int64(1)), (11, np.int64(3)), (3, np.int64(1)), (11, np.int64(2)), (12, np.int64(0)), (11, np.int64(2)), (12, np.int64(1)), (20, np.int64(2)), (21, np.int64(3)), (13, np.int64(1)), (21, np.int64(2)), (22, np.int64(1)), (30, np.int64(1)), (38, np.int64(0)), (37, np.int64(2)), (38, np.int64(3)), (30, np.int64(1)), (38, np.int64(2)), (39, np.int64(3)), (31, np.int64(0)), (30, np.int64(1)), (38, np.int64(2)), (39, np.int64(1)), (47, np.int64(1)), (55, np.int64(1))]\n",
      "\n",
      "Current Policy:\n",
      "[['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']]\n",
      "\n",
      "GOAL REACHED in episode 317!\n",
      "Path taken: [(0, np.int64(2)), (1, np.int64(0)), (0, np.int64(3)), (0, np.int64(0)), (0, np.int64(1)), (8, np.int64(2)), (9, np.int64(2)), (10, np.int64(0)), (9, np.int64(1)), (17, np.int64(2)), (18, np.int64(3)), (10, np.int64(2)), (11, np.int64(2)), (12, np.int64(3)), (4, np.int64(2)), (5, np.int64(3)), (5, np.int64(0)), (4, np.int64(3)), (4, np.int64(2)), (5, np.int64(1)), (13, np.int64(2)), (14, np.int64(1)), (22, np.int64(2)), (23, np.int64(1)), (31, np.int64(1)), (39, np.int64(1)), (47, np.int64(1)), (55, np.int64(1))]\n",
      "\n",
      "Current Policy:\n",
      "[['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']]\n",
      "\n",
      "GOAL REACHED in episode 322!\n",
      "Path taken: [(0, np.int64(0)), (0, np.int64(1)), (8, np.int64(2)), (9, np.int64(2)), (10, np.int64(2)), (11, np.int64(2)), (12, np.int64(1)), (20, np.int64(2)), (21, np.int64(3)), (13, np.int64(3)), (5, np.int64(1)), (13, np.int64(0)), (12, np.int64(1)), (20, np.int64(1)), (28, np.int64(1)), (36, np.int64(2)), (37, np.int64(2)), (38, np.int64(2)), (39, np.int64(3)), (31, np.int64(0)), (30, np.int64(2)), (31, np.int64(1)), (39, np.int64(1)), (47, np.int64(1)), (55, np.int64(1))]\n",
      "\n",
      "Current Policy:\n",
      "[['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']]\n",
      "\n",
      "GOAL REACHED in episode 340!\n",
      "Path taken: [(0, np.int64(2)), (1, np.int64(3)), (1, np.int64(0)), (0, np.int64(1)), (8, np.int64(2)), (9, np.int64(2)), (10, np.int64(1)), (18, np.int64(1)), (26, np.int64(2)), (27, np.int64(2)), (28, np.int64(3)), (20, np.int64(2)), (21, np.int64(2)), (22, np.int64(1)), (30, np.int64(1)), (38, np.int64(2)), (39, np.int64(1)), (47, np.int64(1)), (55, np.int64(1))]\n",
      "\n",
      "Current Policy:\n",
      "[['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']]\n",
      "\n",
      "GOAL REACHED in episode 342!\n",
      "Path taken: [(0, np.int64(2)), (1, np.int64(2)), (2, np.int64(2)), (3, np.int64(2)), (4, np.int64(3)), (4, np.int64(2)), (5, np.int64(0)), (4, np.int64(2)), (5, np.int64(2)), (6, np.int64(3)), (6, np.int64(2)), (7, np.int64(1)), (15, np.int64(1)), (23, np.int64(2)), (23, np.int64(1)), (31, np.int64(3)), (23, np.int64(1)), (31, np.int64(2)), (31, np.int64(1)), (39, np.int64(1)), (47, np.int64(1)), (55, np.int64(3)), (47, np.int64(2)), (47, np.int64(3)), (39, np.int64(3)), (31, np.int64(3)), (23, np.int64(3)), (15, np.int64(2)), (15, np.int64(1)), (23, np.int64(1)), (31, np.int64(2)), (31, np.int64(0)), (30, np.int64(1)), (38, np.int64(2)), (39, np.int64(1)), (47, np.int64(3)), (39, np.int64(1)), (47, np.int64(1)), (55, np.int64(2)), (55, np.int64(1))]\n",
      "\n",
      "Current Policy:\n",
      "[['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']]\n",
      "\n",
      "GOAL REACHED in episode 346!\n",
      "Path taken: [(0, np.int64(2)), (1, np.int64(2)), (2, np.int64(3)), (2, np.int64(2)), (3, np.int64(0)), (2, np.int64(2)), (3, np.int64(2)), (4, np.int64(3)), (4, np.int64(1)), (12, np.int64(2)), (13, np.int64(2)), (14, np.int64(1)), (22, np.int64(1)), (30, np.int64(1)), (38, np.int64(2)), (39, np.int64(3)), (31, np.int64(1)), (39, np.int64(1)), (47, np.int64(3)), (39, np.int64(1)), (47, np.int64(1)), (55, np.int64(1))]\n",
      "\n",
      "Current Policy:\n",
      "[['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']]\n",
      "\n",
      "GOAL REACHED in episode 352!\n",
      "Path taken: [(0, np.int64(2)), (1, np.int64(3)), (1, np.int64(1)), (9, np.int64(1)), (17, np.int64(3)), (9, np.int64(3)), (1, np.int64(3)), (1, np.int64(2)), (2, np.int64(1)), (10, np.int64(2)), (11, np.int64(3)), (3, np.int64(2)), (4, np.int64(3)), (4, np.int64(2)), (5, np.int64(2)), (6, np.int64(1)), (14, np.int64(1)), (22, np.int64(3)), (14, np.int64(1)), (22, np.int64(2)), (23, np.int64(3)), (15, np.int64(1)), (23, np.int64(3)), (15, np.int64(3)), (7, np.int64(1)), (15, np.int64(3)), (7, np.int64(1)), (15, np.int64(1)), (23, np.int64(2)), (23, np.int64(1)), (31, np.int64(1)), (39, np.int64(1)), (47, np.int64(1)), (55, np.int64(2)), (55, np.int64(1))]\n",
      "\n",
      "Current Policy:\n",
      "[['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']]\n",
      "\n",
      "GOAL REACHED in episode 356!\n",
      "Path taken: [(0, np.int64(1)), (8, np.int64(1)), (16, np.int64(0)), (16, np.int64(1)), (24, np.int64(3)), (16, np.int64(1)), (24, np.int64(0)), (24, np.int64(0)), (24, np.int64(1)), (32, np.int64(0)), (32, np.int64(3)), (24, np.int64(3)), (16, np.int64(0)), (16, np.int64(1)), (24, np.int64(2)), (25, np.int64(3)), (17, np.int64(3)), (9, np.int64(2)), (10, np.int64(2)), (11, np.int64(0)), (10, np.int64(0)), (9, np.int64(2)), (10, np.int64(1)), (18, np.int64(1)), (26, np.int64(2)), (27, np.int64(2)), (28, np.int64(1)), (36, np.int64(3)), (28, np.int64(1)), (36, np.int64(2)), (37, np.int64(1)), (45, np.int64(3)), (37, np.int64(2)), (38, np.int64(2)), (39, np.int64(1)), (47, np.int64(1)), (55, np.int64(3)), (47, np.int64(1)), (55, np.int64(1))]\n",
      "\n",
      "Current Policy:\n",
      "[['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']]\n",
      "\n",
      "GOAL REACHED in episode 373!\n",
      "Path taken: [(0, np.int64(2)), (1, np.int64(2)), (2, np.int64(3)), (2, np.int64(3)), (2, np.int64(1)), (10, np.int64(2)), (11, np.int64(2)), (12, np.int64(1)), (20, np.int64(1)), (28, np.int64(1)), (36, np.int64(1)), (44, np.int64(3)), (36, np.int64(2)), (37, np.int64(2)), (38, np.int64(2)), (39, np.int64(1)), (47, np.int64(1)), (55, np.int64(2)), (55, np.int64(2)), (55, np.int64(1))]\n",
      "\n",
      "Current Policy:\n",
      "[['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']]\n",
      "\n",
      "GOAL REACHED in episode 392!\n",
      "Path taken: [(0, np.int64(0)), (0, np.int64(1)), (8, np.int64(2)), (9, np.int64(1)), (17, np.int64(2)), (18, np.int64(1)), (26, np.int64(2)), (27, np.int64(2)), (28, np.int64(0)), (27, np.int64(2)), (28, np.int64(3)), (20, np.int64(1)), (28, np.int64(1)), (36, np.int64(2)), (37, np.int64(2)), (38, np.int64(3)), (30, np.int64(2)), (31, np.int64(1)), (39, np.int64(0)), (38, np.int64(2)), (39, np.int64(1)), (47, np.int64(1)), (55, np.int64(1))]\n",
      "\n",
      "Current Policy:\n",
      "[['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']]\n",
      "\n",
      "GOAL REACHED in episode 393!\n",
      "Path taken: [(0, np.int64(3)), (0, np.int64(0)), (0, np.int64(2)), (1, np.int64(2)), (2, np.int64(2)), (3, np.int64(2)), (4, np.int64(0)), (3, np.int64(2)), (4, np.int64(3)), (4, np.int64(2)), (5, np.int64(1)), (13, np.int64(1)), (21, np.int64(2)), (22, np.int64(1)), (30, np.int64(1)), (38, np.int64(2)), (39, np.int64(1)), (47, np.int64(1)), (55, np.int64(1))]\n",
      "\n",
      "Current Policy:\n",
      "[['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']]\n",
      "\n",
      "GOAL REACHED in episode 394!\n",
      "Path taken: [(0, np.int64(1)), (8, np.int64(0)), (8, np.int64(1)), (16, np.int64(2)), (17, np.int64(1)), (25, np.int64(2)), (26, np.int64(2)), (27, np.int64(2)), (28, np.int64(3)), (20, np.int64(1)), (28, np.int64(1)), (36, np.int64(2)), (37, np.int64(2)), (38, np.int64(2)), (39, np.int64(2)), (39, np.int64(0)), (38, np.int64(2)), (39, np.int64(3)), (31, np.int64(1)), (39, np.int64(2)), (39, np.int64(3)), (31, np.int64(0)), (30, np.int64(1)), (38, np.int64(2)), (39, np.int64(1)), (47, np.int64(1)), (55, np.int64(1))]\n",
      "\n",
      "Current Policy:\n",
      "[['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']]\n",
      "\n",
      "GOAL REACHED in episode 395!\n",
      "Path taken: [(0, np.int64(2)), (1, np.int64(1)), (9, np.int64(2)), (10, np.int64(1)), (18, np.int64(1)), (26, np.int64(2)), (27, np.int64(2)), (28, np.int64(0)), (27, np.int64(2)), (28, np.int64(1)), (36, np.int64(2)), (37, np.int64(2)), (38, np.int64(3)), (30, np.int64(1)), (38, np.int64(2)), (39, np.int64(3)), (31, np.int64(1)), (39, np.int64(1)), (47, np.int64(2)), (47, np.int64(1)), (55, np.int64(3)), (47, np.int64(1)), (55, np.int64(1))]\n",
      "\n",
      "Current Policy:\n",
      "[['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']]\n",
      "\n",
      "GOAL REACHED in episode 398!\n",
      "Path taken: [(0, np.int64(2)), (1, np.int64(2)), (2, np.int64(2)), (3, np.int64(2)), (4, np.int64(1)), (12, np.int64(2)), (13, np.int64(1)), (21, np.int64(2)), (22, np.int64(1)), (30, np.int64(1)), (38, np.int64(2)), (39, np.int64(1)), (47, np.int64(1)), (55, np.int64(3)), (47, np.int64(3)), (39, np.int64(2)), (39, np.int64(3)), (31, np.int64(1)), (39, np.int64(1)), (47, np.int64(3)), (39, np.int64(1)), (47, np.int64(1)), (55, np.int64(2)), (55, np.int64(1))]\n",
      "\n",
      "Current Policy:\n",
      "[['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']]\n",
      "\n",
      "Episode 400\n",
      "Average reward: -2.918\n",
      "Success rate: 15.00%\n",
      "\n",
      "Current Policy:\n",
      "[['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']]\n",
      "\n",
      "Least visited states:\n",
      "State 19: 0 visits\n",
      "State 29: 0 visits\n",
      "State 35: 0 visits\n",
      "State 41: 0 visits\n",
      "State 42: 0 visits\n",
      "\n",
      "GOAL REACHED in episode 403!\n",
      "Path taken: [(0, np.int64(3)), (0, np.int64(2)), (1, np.int64(3)), (1, np.int64(2)), (2, np.int64(2)), (3, np.int64(0)), (2, np.int64(2)), (3, np.int64(1)), (11, np.int64(2)), (12, np.int64(0)), (11, np.int64(3)), (3, np.int64(2)), (4, np.int64(1)), (12, np.int64(2)), (13, np.int64(1)), (21, np.int64(3)), (13, np.int64(1)), (21, np.int64(3)), (13, np.int64(0)), (12, np.int64(2)), (13, np.int64(1)), (21, np.int64(2)), (22, np.int64(2)), (23, np.int64(1)), (31, np.int64(1)), (39, np.int64(2)), (39, np.int64(1)), (47, np.int64(1)), (55, np.int64(3)), (47, np.int64(1)), (55, np.int64(1))]\n",
      "\n",
      "Current Policy:\n",
      "[['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']]\n",
      "\n",
      "GOAL REACHED in episode 405!\n",
      "Path taken: [(0, np.int64(2)), (1, np.int64(1)), (9, np.int64(2)), (10, np.int64(3)), (2, np.int64(2)), (3, np.int64(2)), (4, np.int64(1)), (12, np.int64(2)), (13, np.int64(1)), (21, np.int64(2)), (22, np.int64(1)), (30, np.int64(2)), (31, np.int64(1)), (39, np.int64(1)), (47, np.int64(1)), (55, np.int64(2)), (55, np.int64(1))]\n",
      "\n",
      "Current Policy:\n",
      "[['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']]\n",
      "\n",
      "GOAL REACHED in episode 409!\n",
      "Path taken: [(0, np.int64(2)), (1, np.int64(2)), (2, np.int64(2)), (3, np.int64(2)), (4, np.int64(0)), (3, np.int64(0)), (2, np.int64(0)), (1, np.int64(2)), (2, np.int64(1)), (10, np.int64(2)), (11, np.int64(0)), (10, np.int64(1)), (18, np.int64(1)), (26, np.int64(2)), (27, np.int64(2)), (28, np.int64(1)), (36, np.int64(2)), (37, np.int64(1)), (45, np.int64(3)), (37, np.int64(0)), (36, np.int64(2)), (37, np.int64(2)), (38, np.int64(0)), (37, np.int64(2)), (38, np.int64(2)), (39, np.int64(3)), (31, np.int64(1)), (39, np.int64(1)), (47, np.int64(3)), (39, np.int64(0)), (38, np.int64(2)), (39, np.int64(1)), (47, np.int64(1)), (55, np.int64(1))]\n",
      "\n",
      "Current Policy:\n",
      "[['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']]\n",
      "\n",
      "GOAL REACHED in episode 414!\n",
      "Path taken: [(0, np.int64(3)), (0, np.int64(1)), (8, np.int64(3)), (0, np.int64(2)), (1, np.int64(2)), (2, np.int64(2)), (3, np.int64(2)), (4, np.int64(2)), (5, np.int64(0)), (4, np.int64(1)), (12, np.int64(2)), (13, np.int64(2)), (14, np.int64(1)), (22, np.int64(1)), (30, np.int64(1)), (38, np.int64(2)), (39, np.int64(0)), (38, np.int64(0)), (37, np.int64(2)), (38, np.int64(2)), (39, np.int64(1)), (47, np.int64(1)), (55, np.int64(1))]\n",
      "\n",
      "Current Policy:\n",
      "[['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']]\n",
      "\n",
      "GOAL REACHED in episode 416!\n",
      "Path taken: [(0, np.int64(2)), (1, np.int64(2)), (2, np.int64(2)), (3, np.int64(0)), (2, np.int64(2)), (3, np.int64(2)), (4, np.int64(1)), (12, np.int64(2)), (13, np.int64(1)), (21, np.int64(2)), (22, np.int64(3)), (14, np.int64(3)), (6, np.int64(1)), (14, np.int64(0)), (13, np.int64(1)), (21, np.int64(2)), (22, np.int64(1)), (30, np.int64(1)), (38, np.int64(2)), (39, np.int64(3)), (31, np.int64(1)), (39, np.int64(2)), (39, np.int64(1)), (47, np.int64(1)), (55, np.int64(1))]\n",
      "\n",
      "Current Policy:\n",
      "[['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']]\n",
      "\n",
      "GOAL REACHED in episode 424!\n",
      "Path taken: [(0, np.int64(2)), (1, np.int64(2)), (2, np.int64(0)), (1, np.int64(3)), (1, np.int64(0)), (0, np.int64(1)), (8, np.int64(1)), (16, np.int64(1)), (24, np.int64(3)), (16, np.int64(1)), (24, np.int64(0)), (24, np.int64(2)), (25, np.int64(1)), (33, np.int64(2)), (34, np.int64(3)), (26, np.int64(2)), (27, np.int64(0)), (26, np.int64(1)), (34, np.int64(3)), (26, np.int64(2)), (27, np.int64(0)), (26, np.int64(0)), (25, np.int64(1)), (33, np.int64(3)), (25, np.int64(2)), (26, np.int64(2)), (27, np.int64(0)), (26, np.int64(3)), (18, np.int64(1)), (26, np.int64(2)), (27, np.int64(2)), (28, np.int64(1)), (36, np.int64(2)), (37, np.int64(2)), (38, np.int64(0)), (37, np.int64(2)), (38, np.int64(2)), (39, np.int64(1)), (47, np.int64(1)), (55, np.int64(2)), (55, np.int64(3)), (47, np.int64(1)), (55, np.int64(2)), (55, np.int64(1))]\n",
      "\n",
      "Current Policy:\n",
      "[['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']]\n",
      "\n",
      "GOAL REACHED in episode 427!\n",
      "Path taken: [(0, np.int64(1)), (8, np.int64(1)), (16, np.int64(1)), (24, np.int64(3)), (16, np.int64(1)), (24, np.int64(2)), (25, np.int64(2)), (26, np.int64(2)), (27, np.int64(2)), (28, np.int64(1)), (36, np.int64(2)), (37, np.int64(2)), (38, np.int64(2)), (39, np.int64(1)), (47, np.int64(1)), (55, np.int64(1))]\n",
      "\n",
      "Current Policy:\n",
      "[['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']]\n",
      "\n",
      "GOAL REACHED in episode 431!\n",
      "Path taken: [(0, np.int64(3)), (0, np.int64(3)), (0, np.int64(1)), (8, np.int64(1)), (16, np.int64(2)), (17, np.int64(1)), (25, np.int64(2)), (26, np.int64(2)), (27, np.int64(2)), (28, np.int64(1)), (36, np.int64(2)), (37, np.int64(0)), (36, np.int64(2)), (37, np.int64(2)), (38, np.int64(2)), (39, np.int64(1)), (47, np.int64(1)), (55, np.int64(2)), (55, np.int64(1))]\n",
      "\n",
      "Current Policy:\n",
      "[['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']]\n",
      "\n",
      "GOAL REACHED in episode 437!\n",
      "Path taken: [(0, np.int64(2)), (1, np.int64(2)), (2, np.int64(2)), (3, np.int64(2)), (4, np.int64(1)), (12, np.int64(2)), (13, np.int64(1)), (21, np.int64(2)), (22, np.int64(2)), (23, np.int64(1)), (31, np.int64(1)), (39, np.int64(1)), (47, np.int64(1)), (55, np.int64(1))]\n",
      "\n",
      "Current Policy:\n",
      "[['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']]\n",
      "\n",
      "GOAL REACHED in episode 442!\n",
      "Path taken: [(0, np.int64(1)), (8, np.int64(1)), (16, np.int64(1)), (24, np.int64(2)), (25, np.int64(2)), (26, np.int64(2)), (27, np.int64(2)), (28, np.int64(0)), (27, np.int64(2)), (28, np.int64(1)), (36, np.int64(2)), (37, np.int64(2)), (38, np.int64(2)), (39, np.int64(1)), (47, np.int64(1)), (55, np.int64(1))]\n",
      "\n",
      "Current Policy:\n",
      "[['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']]\n",
      "\n",
      "GOAL REACHED in episode 446!\n",
      "Path taken: [(0, np.int64(1)), (8, np.int64(1)), (16, np.int64(1)), (24, np.int64(2)), (25, np.int64(2)), (26, np.int64(0)), (25, np.int64(1)), (33, np.int64(2)), (34, np.int64(3)), (26, np.int64(2)), (27, np.int64(2)), (28, np.int64(1)), (36, np.int64(1)), (44, np.int64(3)), (36, np.int64(2)), (37, np.int64(2)), (38, np.int64(2)), (39, np.int64(2)), (39, np.int64(0)), (38, np.int64(2)), (39, np.int64(1)), (47, np.int64(1)), (55, np.int64(1))]\n",
      "\n",
      "Current Policy:\n",
      "[['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']]\n",
      "\n",
      "GOAL REACHED in episode 451!\n",
      "Path taken: [(0, np.int64(2)), (1, np.int64(2)), (2, np.int64(3)), (2, np.int64(3)), (2, np.int64(2)), (3, np.int64(2)), (4, np.int64(2)), (5, np.int64(2)), (6, np.int64(3)), (6, np.int64(3)), (6, np.int64(1)), (14, np.int64(1)), (22, np.int64(1)), (30, np.int64(1)), (38, np.int64(2)), (39, np.int64(1)), (47, np.int64(1)), (55, np.int64(1))]\n",
      "\n",
      "Current Policy:\n",
      "[['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']]\n",
      "\n",
      "GOAL REACHED in episode 453!\n",
      "Path taken: [(0, np.int64(1)), (8, np.int64(3)), (0, np.int64(1)), (8, np.int64(2)), (9, np.int64(3)), (1, np.int64(2)), (2, np.int64(2)), (3, np.int64(2)), (4, np.int64(1)), (12, np.int64(2)), (13, np.int64(0)), (12, np.int64(2)), (13, np.int64(1)), (21, np.int64(2)), (22, np.int64(0)), (21, np.int64(2)), (22, np.int64(1)), (30, np.int64(2)), (31, np.int64(1)), (39, np.int64(1)), (47, np.int64(1)), (55, np.int64(1))]\n",
      "\n",
      "Current Policy:\n",
      "[['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']]\n",
      "\n",
      "GOAL REACHED in episode 459!\n",
      "Path taken: [(0, np.int64(0)), (0, np.int64(1)), (8, np.int64(2)), (9, np.int64(1)), (17, np.int64(1)), (25, np.int64(2)), (26, np.int64(2)), (27, np.int64(2)), (28, np.int64(1)), (36, np.int64(2)), (37, np.int64(0)), (36, np.int64(2)), (37, np.int64(2)), (38, np.int64(2)), (39, np.int64(2)), (39, np.int64(3)), (31, np.int64(1)), (39, np.int64(2)), (39, np.int64(1)), (47, np.int64(1)), (55, np.int64(1))]\n",
      "\n",
      "Current Policy:\n",
      "[['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']]\n",
      "\n",
      "GOAL REACHED in episode 460!\n",
      "Path taken: [(0, np.int64(2)), (1, np.int64(2)), (2, np.int64(2)), (3, np.int64(2)), (4, np.int64(1)), (12, np.int64(2)), (13, np.int64(1)), (21, np.int64(0)), (20, np.int64(1)), (28, np.int64(1)), (36, np.int64(2)), (37, np.int64(2)), (38, np.int64(2)), (39, np.int64(0)), (38, np.int64(2)), (39, np.int64(1)), (47, np.int64(1)), (55, np.int64(1))]\n",
      "\n",
      "Current Policy:\n",
      "[['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']]\n",
      "\n",
      "GOAL REACHED in episode 462!\n",
      "Path taken: [(0, np.int64(1)), (8, np.int64(1)), (16, np.int64(1)), (24, np.int64(2)), (25, np.int64(2)), (26, np.int64(2)), (27, np.int64(2)), (28, np.int64(1)), (36, np.int64(2)), (37, np.int64(2)), (38, np.int64(2)), (39, np.int64(1)), (47, np.int64(1)), (55, np.int64(1))]\n",
      "\n",
      "Current Policy:\n",
      "[['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']]\n",
      "\n",
      "GOAL REACHED in episode 478!\n",
      "Path taken: [(0, np.int64(1)), (8, np.int64(2)), (9, np.int64(1)), (17, np.int64(2)), (18, np.int64(0)), (17, np.int64(1)), (25, np.int64(2)), (26, np.int64(3)), (18, np.int64(1)), (26, np.int64(2)), (27, np.int64(2)), (28, np.int64(1)), (36, np.int64(2)), (37, np.int64(2)), (38, np.int64(2)), (39, np.int64(1)), (47, np.int64(2)), (47, np.int64(1)), (55, np.int64(1))]\n",
      "\n",
      "Current Policy:\n",
      "[['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']]\n",
      "\n",
      "GOAL REACHED in episode 479!\n",
      "Path taken: [(0, np.int64(0)), (0, np.int64(1)), (8, np.int64(1)), (16, np.int64(0)), (16, np.int64(2)), (17, np.int64(1)), (25, np.int64(2)), (26, np.int64(0)), (25, np.int64(3)), (17, np.int64(0)), (16, np.int64(1)), (24, np.int64(2)), (25, np.int64(1)), (33, np.int64(2)), (34, np.int64(3)), (26, np.int64(2)), (27, np.int64(2)), (28, np.int64(3)), (20, np.int64(2)), (21, np.int64(2)), (22, np.int64(1)), (30, np.int64(2)), (31, np.int64(1)), (39, np.int64(3)), (31, np.int64(3)), (23, np.int64(1)), (31, np.int64(3)), (23, np.int64(1)), (31, np.int64(0)), (30, np.int64(1)), (38, np.int64(3)), (30, np.int64(1)), (38, np.int64(2)), (39, np.int64(1)), (47, np.int64(1)), (55, np.int64(1))]\n",
      "\n",
      "Current Policy:\n",
      "[['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']]\n",
      "\n",
      "GOAL REACHED in episode 481!\n",
      "Path taken: [(0, np.int64(0)), (0, np.int64(1)), (8, np.int64(1)), (16, np.int64(1)), (24, np.int64(2)), (25, np.int64(0)), (24, np.int64(2)), (25, np.int64(2)), (26, np.int64(2)), (27, np.int64(2)), (28, np.int64(1)), (36, np.int64(2)), (37, np.int64(2)), (38, np.int64(2)), (39, np.int64(3)), (31, np.int64(3)), (23, np.int64(1)), (31, np.int64(1)), (39, np.int64(1)), (47, np.int64(1)), (55, np.int64(2)), (55, np.int64(1))]\n",
      "\n",
      "Current Policy:\n",
      "[['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']]\n",
      "\n",
      "GOAL REACHED in episode 482!\n",
      "Path taken: [(0, np.int64(1)), (8, np.int64(1)), (16, np.int64(1)), (24, np.int64(2)), (25, np.int64(2)), (26, np.int64(3)), (18, np.int64(1)), (26, np.int64(2)), (27, np.int64(2)), (28, np.int64(0)), (27, np.int64(2)), (28, np.int64(0)), (27, np.int64(2)), (28, np.int64(1)), (36, np.int64(2)), (37, np.int64(1)), (45, np.int64(3)), (37, np.int64(2)), (38, np.int64(2)), (39, np.int64(1)), (47, np.int64(1)), (55, np.int64(2)), (55, np.int64(2)), (55, np.int64(1))]\n",
      "\n",
      "Current Policy:\n",
      "[['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']]\n",
      "\n",
      "GOAL REACHED in episode 483!\n",
      "Path taken: [(0, np.int64(1)), (8, np.int64(0)), (8, np.int64(1)), (16, np.int64(3)), (8, np.int64(0)), (8, np.int64(0)), (8, np.int64(1)), (16, np.int64(1)), (24, np.int64(3)), (16, np.int64(1)), (24, np.int64(2)), (25, np.int64(2)), (26, np.int64(0)), (25, np.int64(2)), (26, np.int64(2)), (27, np.int64(2)), (28, np.int64(1)), (36, np.int64(2)), (37, np.int64(2)), (38, np.int64(2)), (39, np.int64(1)), (47, np.int64(1)), (55, np.int64(1))]\n",
      "\n",
      "Current Policy:\n",
      "[['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']]\n",
      "\n",
      "GOAL REACHED in episode 485!\n",
      "Path taken: [(0, np.int64(2)), (1, np.int64(2)), (2, np.int64(2)), (3, np.int64(2)), (4, np.int64(1)), (12, np.int64(2)), (13, np.int64(1)), (21, np.int64(2)), (22, np.int64(2)), (23, np.int64(0)), (22, np.int64(3)), (14, np.int64(1)), (22, np.int64(1)), (30, np.int64(1)), (38, np.int64(2)), (39, np.int64(1)), (47, np.int64(1)), (55, np.int64(1))]\n",
      "\n",
      "Current Policy:\n",
      "[['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']]\n",
      "\n",
      "GOAL REACHED in episode 486!\n",
      "Path taken: [(0, np.int64(1)), (8, np.int64(2)), (9, np.int64(2)), (10, np.int64(1)), (18, np.int64(0)), (17, np.int64(2)), (18, np.int64(1)), (26, np.int64(1)), (34, np.int64(0)), (33, np.int64(2)), (34, np.int64(3)), (26, np.int64(2)), (27, np.int64(2)), (28, np.int64(1)), (36, np.int64(2)), (37, np.int64(2)), (38, np.int64(2)), (39, np.int64(1)), (47, np.int64(1)), (55, np.int64(1))]\n",
      "\n",
      "Current Policy:\n",
      "[['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']]\n",
      "\n",
      "GOAL REACHED in episode 490!\n",
      "Path taken: [(0, np.int64(1)), (8, np.int64(2)), (9, np.int64(3)), (1, np.int64(1)), (9, np.int64(1)), (17, np.int64(1)), (25, np.int64(1)), (33, np.int64(2)), (34, np.int64(3)), (26, np.int64(2)), (27, np.int64(2)), (28, np.int64(1)), (36, np.int64(2)), (37, np.int64(2)), (38, np.int64(2)), (39, np.int64(3)), (31, np.int64(1)), (39, np.int64(3)), (31, np.int64(0)), (30, np.int64(1)), (38, np.int64(2)), (39, np.int64(1)), (47, np.int64(1)), (55, np.int64(1))]\n",
      "\n",
      "Current Policy:\n",
      "[['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']]\n",
      "\n",
      "GOAL REACHED in episode 493!\n",
      "Path taken: [(0, np.int64(0)), (0, np.int64(1)), (8, np.int64(0)), (8, np.int64(1)), (16, np.int64(1)), (24, np.int64(2)), (25, np.int64(2)), (26, np.int64(2)), (27, np.int64(2)), (28, np.int64(1)), (36, np.int64(2)), (37, np.int64(2)), (38, np.int64(2)), (39, np.int64(1)), (47, np.int64(1)), (55, np.int64(1))]\n",
      "\n",
      "Current Policy:\n",
      "[['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']]\n",
      "\n",
      "GOAL REACHED in episode 495!\n",
      "Path taken: [(0, np.int64(1)), (8, np.int64(1)), (16, np.int64(1)), (24, np.int64(2)), (25, np.int64(2)), (26, np.int64(2)), (27, np.int64(2)), (28, np.int64(1)), (36, np.int64(2)), (37, np.int64(2)), (38, np.int64(0)), (37, np.int64(2)), (38, np.int64(2)), (39, np.int64(1)), (47, np.int64(1)), (55, np.int64(1))]\n",
      "\n",
      "Current Policy:\n",
      "[['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']]\n",
      "\n",
      "GOAL REACHED in episode 496!\n",
      "Path taken: [(0, np.int64(1)), (8, np.int64(1)), (16, np.int64(1)), (24, np.int64(0)), (24, np.int64(0)), (24, np.int64(2)), (25, np.int64(2)), (26, np.int64(3)), (18, np.int64(1)), (26, np.int64(2)), (27, np.int64(0)), (26, np.int64(2)), (27, np.int64(2)), (28, np.int64(1)), (36, np.int64(2)), (37, np.int64(2)), (38, np.int64(2)), (39, np.int64(1)), (47, np.int64(1)), (55, np.int64(2)), (55, np.int64(1))]\n",
      "\n",
      "Current Policy:\n",
      "[['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']]\n",
      "\n",
      "Episode 500\n",
      "Average reward: -1.106\n",
      "Success rate: 27.00%\n",
      "\n",
      "Current Policy:\n",
      "[['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']]\n",
      "\n",
      "Least visited states:\n",
      "State 19: 0 visits\n",
      "State 29: 0 visits\n",
      "State 35: 0 visits\n",
      "State 41: 0 visits\n",
      "State 42: 0 visits\n",
      "\n",
      "GOAL REACHED in episode 502!\n",
      "Path taken: [(0, np.int64(1)), (8, np.int64(1)), (16, np.int64(3)), (8, np.int64(1)), (16, np.int64(1)), (24, np.int64(0)), (24, np.int64(3)), (16, np.int64(1)), (24, np.int64(1)), (32, np.int64(3)), (24, np.int64(2)), (25, np.int64(2)), (26, np.int64(0)), (25, np.int64(0)), (24, np.int64(2)), (25, np.int64(2)), (26, np.int64(2)), (27, np.int64(0)), (26, np.int64(2)), (27, np.int64(0)), (26, np.int64(3)), (18, np.int64(1)), (26, np.int64(2)), (27, np.int64(2)), (28, np.int64(1)), (36, np.int64(3)), (28, np.int64(1)), (36, np.int64(1)), (44, np.int64(3)), (36, np.int64(2)), (37, np.int64(2)), (38, np.int64(2)), (39, np.int64(1)), (47, np.int64(2)), (47, np.int64(1)), (55, np.int64(1))]\n",
      "\n",
      "Current Policy:\n",
      "[['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']]\n",
      "\n",
      "GOAL REACHED in episode 504!\n",
      "Path taken: [(0, np.int64(2)), (1, np.int64(1)), (9, np.int64(2)), (10, np.int64(2)), (11, np.int64(2)), (12, np.int64(3)), (4, np.int64(3)), (4, np.int64(3)), (4, np.int64(2)), (5, np.int64(1)), (13, np.int64(1)), (21, np.int64(2)), (22, np.int64(1)), (30, np.int64(1)), (38, np.int64(2)), (39, np.int64(1)), (47, np.int64(3)), (39, np.int64(1)), (47, np.int64(1)), (55, np.int64(1))]\n",
      "\n",
      "Current Policy:\n",
      "[['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']]\n",
      "\n",
      "GOAL REACHED in episode 509!\n",
      "Path taken: [(0, np.int64(1)), (8, np.int64(0)), (8, np.int64(1)), (16, np.int64(1)), (24, np.int64(2)), (25, np.int64(2)), (26, np.int64(2)), (27, np.int64(2)), (28, np.int64(0)), (27, np.int64(2)), (28, np.int64(1)), (36, np.int64(2)), (37, np.int64(2)), (38, np.int64(2)), (39, np.int64(1)), (47, np.int64(1)), (55, np.int64(1))]\n",
      "\n",
      "Current Policy:\n",
      "[['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']]\n",
      "\n",
      "GOAL REACHED in episode 513!\n",
      "Path taken: [(0, np.int64(3)), (0, np.int64(1)), (8, np.int64(1)), (16, np.int64(2)), (17, np.int64(2)), (18, np.int64(1)), (26, np.int64(2)), (27, np.int64(2)), (28, np.int64(1)), (36, np.int64(1)), (44, np.int64(3)), (36, np.int64(2)), (37, np.int64(2)), (38, np.int64(2)), (39, np.int64(0)), (38, np.int64(0)), (37, np.int64(2)), (38, np.int64(2)), (39, np.int64(3)), (31, np.int64(1)), (39, np.int64(1)), (47, np.int64(1)), (55, np.int64(1))]\n",
      "\n",
      "Current Policy:\n",
      "[['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']]\n",
      "\n",
      "GOAL REACHED in episode 524!\n",
      "Path taken: [(0, np.int64(3)), (0, np.int64(1)), (8, np.int64(1)), (16, np.int64(1)), (24, np.int64(0)), (24, np.int64(3)), (16, np.int64(0)), (16, np.int64(3)), (8, np.int64(2)), (9, np.int64(1)), (17, np.int64(3)), (9, np.int64(0)), (8, np.int64(1)), (16, np.int64(3)), (8, np.int64(1)), (16, np.int64(3)), (8, np.int64(1)), (16, np.int64(3)), (8, np.int64(1)), (16, np.int64(1)), (24, np.int64(2)), (25, np.int64(2)), (26, np.int64(1)), (34, np.int64(3)), (26, np.int64(2)), (27, np.int64(2)), (28, np.int64(1)), (36, np.int64(2)), (37, np.int64(0)), (36, np.int64(2)), (37, np.int64(2)), (38, np.int64(0)), (37, np.int64(2)), (38, np.int64(3)), (30, np.int64(1)), (38, np.int64(3)), (30, np.int64(1)), (38, np.int64(2)), (39, np.int64(1)), (47, np.int64(1)), (55, np.int64(3)), (47, np.int64(2)), (47, np.int64(3)), (39, np.int64(1)), (47, np.int64(1)), (55, np.int64(1))]\n",
      "\n",
      "Current Policy:\n",
      "[['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']]\n",
      "\n",
      "GOAL REACHED in episode 527!\n",
      "Path taken: [(0, np.int64(1)), (8, np.int64(2)), (9, np.int64(1)), (17, np.int64(1)), (25, np.int64(0)), (24, np.int64(2)), (25, np.int64(2)), (26, np.int64(0)), (25, np.int64(0)), (24, np.int64(2)), (25, np.int64(3)), (17, np.int64(3)), (9, np.int64(2)), (10, np.int64(2)), (11, np.int64(2)), (12, np.int64(2)), (13, np.int64(2)), (14, np.int64(1)), (22, np.int64(1)), (30, np.int64(2)), (31, np.int64(0)), (30, np.int64(1)), (38, np.int64(2)), (39, np.int64(1)), (47, np.int64(1)), (55, np.int64(3)), (47, np.int64(3)), (39, np.int64(1)), (47, np.int64(2)), (47, np.int64(1)), (55, np.int64(1))]\n",
      "\n",
      "Current Policy:\n",
      "[['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']]\n",
      "\n",
      "GOAL REACHED in episode 534!\n",
      "Path taken: [(0, np.int64(1)), (8, np.int64(2)), (9, np.int64(2)), (10, np.int64(3)), (2, np.int64(2)), (3, np.int64(2)), (4, np.int64(1)), (12, np.int64(2)), (13, np.int64(2)), (14, np.int64(3)), (6, np.int64(2)), (7, np.int64(2)), (7, np.int64(1)), (15, np.int64(1)), (23, np.int64(1)), (31, np.int64(1)), (39, np.int64(1)), (47, np.int64(3)), (39, np.int64(1)), (47, np.int64(1)), (55, np.int64(3)), (47, np.int64(1)), (55, np.int64(1))]\n",
      "\n",
      "Current Policy:\n",
      "[['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']]\n",
      "\n",
      "GOAL REACHED in episode 537!\n",
      "Path taken: [(0, np.int64(1)), (8, np.int64(1)), (16, np.int64(1)), (24, np.int64(1)), (32, np.int64(3)), (24, np.int64(2)), (25, np.int64(2)), (26, np.int64(2)), (27, np.int64(2)), (28, np.int64(1)), (36, np.int64(2)), (37, np.int64(2)), (38, np.int64(2)), (39, np.int64(0)), (38, np.int64(2)), (39, np.int64(2)), (39, np.int64(1)), (47, np.int64(1)), (55, np.int64(1))]\n",
      "\n",
      "Current Policy:\n",
      "[['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']]\n",
      "\n",
      "GOAL REACHED in episode 540!\n",
      "Path taken: [(0, np.int64(1)), (8, np.int64(1)), (16, np.int64(1)), (24, np.int64(2)), (25, np.int64(2)), (26, np.int64(3)), (18, np.int64(1)), (26, np.int64(2)), (27, np.int64(2)), (28, np.int64(1)), (36, np.int64(2)), (37, np.int64(2)), (38, np.int64(0)), (37, np.int64(2)), (38, np.int64(3)), (30, np.int64(1)), (38, np.int64(2)), (39, np.int64(1)), (47, np.int64(1)), (55, np.int64(1))]\n",
      "\n",
      "Current Policy:\n",
      "[['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']]\n",
      "\n",
      "GOAL REACHED in episode 549!\n",
      "Path taken: [(0, np.int64(3)), (0, np.int64(1)), (8, np.int64(1)), (16, np.int64(1)), (24, np.int64(2)), (25, np.int64(2)), (26, np.int64(2)), (27, np.int64(2)), (28, np.int64(1)), (36, np.int64(2)), (37, np.int64(2)), (38, np.int64(0)), (37, np.int64(2)), (38, np.int64(2)), (39, np.int64(1)), (47, np.int64(1)), (55, np.int64(1))]\n",
      "\n",
      "Current Policy:\n",
      "[['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']]\n",
      "\n",
      "GOAL REACHED in episode 551!\n",
      "Path taken: [(0, np.int64(1)), (8, np.int64(3)), (0, np.int64(3)), (0, np.int64(1)), (8, np.int64(1)), (16, np.int64(1)), (24, np.int64(2)), (25, np.int64(2)), (26, np.int64(2)), (27, np.int64(2)), (28, np.int64(1)), (36, np.int64(3)), (28, np.int64(1)), (36, np.int64(2)), (37, np.int64(2)), (38, np.int64(3)), (30, np.int64(1)), (38, np.int64(0)), (37, np.int64(2)), (38, np.int64(2)), (39, np.int64(0)), (38, np.int64(2)), (39, np.int64(1)), (47, np.int64(1)), (55, np.int64(1))]\n",
      "\n",
      "Current Policy:\n",
      "[['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']]\n",
      "\n",
      "GOAL REACHED in episode 553!\n",
      "Path taken: [(0, np.int64(1)), (8, np.int64(2)), (9, np.int64(1)), (17, np.int64(1)), (25, np.int64(2)), (26, np.int64(2)), (27, np.int64(2)), (28, np.int64(1)), (36, np.int64(1)), (44, np.int64(3)), (36, np.int64(2)), (37, np.int64(2)), (38, np.int64(2)), (39, np.int64(1)), (47, np.int64(1)), (55, np.int64(1))]\n",
      "\n",
      "Current Policy:\n",
      "[['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']]\n",
      "\n",
      "GOAL REACHED in episode 559!\n",
      "Path taken: [(0, np.int64(1)), (8, np.int64(1)), (16, np.int64(1)), (24, np.int64(2)), (25, np.int64(2)), (26, np.int64(0)), (25, np.int64(2)), (26, np.int64(3)), (18, np.int64(1)), (26, np.int64(2)), (27, np.int64(2)), (28, np.int64(1)), (36, np.int64(2)), (37, np.int64(0)), (36, np.int64(3)), (28, np.int64(1)), (36, np.int64(2)), (37, np.int64(2)), (38, np.int64(2)), (39, np.int64(1)), (47, np.int64(1)), (55, np.int64(1))]\n",
      "\n",
      "Current Policy:\n",
      "[['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']]\n",
      "\n",
      "GOAL REACHED in episode 561!\n",
      "Path taken: [(0, np.int64(1)), (8, np.int64(0)), (8, np.int64(1)), (16, np.int64(1)), (24, np.int64(0)), (24, np.int64(2)), (25, np.int64(2)), (26, np.int64(1)), (34, np.int64(3)), (26, np.int64(0)), (25, np.int64(0)), (24, np.int64(2)), (25, np.int64(2)), (26, np.int64(2)), (27, np.int64(2)), (28, np.int64(1)), (36, np.int64(2)), (37, np.int64(2)), (38, np.int64(2)), (39, np.int64(1)), (47, np.int64(1)), (55, np.int64(1))]\n",
      "\n",
      "Current Policy:\n",
      "[['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']]\n",
      "\n",
      "GOAL REACHED in episode 562!\n",
      "Path taken: [(0, np.int64(3)), (0, np.int64(1)), (8, np.int64(0)), (8, np.int64(1)), (16, np.int64(3)), (8, np.int64(1)), (16, np.int64(3)), (8, np.int64(1)), (16, np.int64(1)), (24, np.int64(2)), (25, np.int64(2)), (26, np.int64(1)), (34, np.int64(3)), (26, np.int64(2)), (27, np.int64(2)), (28, np.int64(1)), (36, np.int64(1)), (44, np.int64(3)), (36, np.int64(1)), (44, np.int64(3)), (36, np.int64(1)), (44, np.int64(3)), (36, np.int64(1)), (44, np.int64(2)), (45, np.int64(3)), (37, np.int64(2)), (38, np.int64(2)), (39, np.int64(1)), (47, np.int64(1)), (55, np.int64(1))]\n",
      "\n",
      "Current Policy:\n",
      "[['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']]\n",
      "\n",
      "GOAL REACHED in episode 566!\n",
      "Path taken: [(0, np.int64(2)), (1, np.int64(1)), (9, np.int64(1)), (17, np.int64(1)), (25, np.int64(2)), (26, np.int64(2)), (27, np.int64(2)), (28, np.int64(1)), (36, np.int64(2)), (37, np.int64(2)), (38, np.int64(0)), (37, np.int64(2)), (38, np.int64(0)), (37, np.int64(2)), (38, np.int64(2)), (39, np.int64(1)), (47, np.int64(1)), (55, np.int64(1))]\n",
      "\n",
      "Current Policy:\n",
      "[['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']]\n",
      "\n",
      "GOAL REACHED in episode 568!\n",
      "Path taken: [(0, np.int64(1)), (8, np.int64(1)), (16, np.int64(1)), (24, np.int64(2)), (25, np.int64(2)), (26, np.int64(2)), (27, np.int64(2)), (28, np.int64(1)), (36, np.int64(2)), (37, np.int64(2)), (38, np.int64(0)), (37, np.int64(2)), (38, np.int64(2)), (39, np.int64(1)), (47, np.int64(2)), (47, np.int64(1)), (55, np.int64(2)), (55, np.int64(1))]\n",
      "\n",
      "Current Policy:\n",
      "[['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']]\n",
      "\n",
      "GOAL REACHED in episode 572!\n",
      "Path taken: [(0, np.int64(1)), (8, np.int64(1)), (16, np.int64(0)), (16, np.int64(1)), (24, np.int64(2)), (25, np.int64(2)), (26, np.int64(2)), (27, np.int64(2)), (28, np.int64(1)), (36, np.int64(2)), (37, np.int64(2)), (38, np.int64(2)), (39, np.int64(1)), (47, np.int64(1)), (55, np.int64(1))]\n",
      "\n",
      "Current Policy:\n",
      "[['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']]\n",
      "\n",
      "GOAL REACHED in episode 573!\n",
      "Path taken: [(0, np.int64(1)), (8, np.int64(1)), (16, np.int64(3)), (8, np.int64(1)), (16, np.int64(1)), (24, np.int64(2)), (25, np.int64(1)), (33, np.int64(2)), (34, np.int64(3)), (26, np.int64(2)), (27, np.int64(2)), (28, np.int64(3)), (20, np.int64(1)), (28, np.int64(1)), (36, np.int64(2)), (37, np.int64(2)), (38, np.int64(2)), (39, np.int64(1)), (47, np.int64(1)), (55, np.int64(3)), (47, np.int64(1)), (55, np.int64(1))]\n",
      "\n",
      "Current Policy:\n",
      "[['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']]\n",
      "\n",
      "GOAL REACHED in episode 577!\n",
      "Path taken: [(0, np.int64(1)), (8, np.int64(1)), (16, np.int64(1)), (24, np.int64(2)), (25, np.int64(2)), (26, np.int64(2)), (27, np.int64(2)), (28, np.int64(1)), (36, np.int64(2)), (37, np.int64(2)), (38, np.int64(0)), (37, np.int64(2)), (38, np.int64(2)), (39, np.int64(2)), (39, np.int64(1)), (47, np.int64(1)), (55, np.int64(2)), (55, np.int64(1))]\n",
      "\n",
      "Current Policy:\n",
      "[['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']]\n",
      "\n",
      "GOAL REACHED in episode 579!\n",
      "Path taken: [(0, np.int64(1)), (8, np.int64(3)), (0, np.int64(1)), (8, np.int64(1)), (16, np.int64(2)), (17, np.int64(1)), (25, np.int64(3)), (17, np.int64(2)), (18, np.int64(1)), (26, np.int64(0)), (25, np.int64(2)), (26, np.int64(3)), (18, np.int64(1)), (26, np.int64(2)), (27, np.int64(2)), (28, np.int64(1)), (36, np.int64(2)), (37, np.int64(2)), (38, np.int64(0)), (37, np.int64(0)), (36, np.int64(1)), (44, np.int64(3)), (36, np.int64(2)), (37, np.int64(2)), (38, np.int64(2)), (39, np.int64(0)), (38, np.int64(2)), (39, np.int64(1)), (47, np.int64(1)), (55, np.int64(1))]\n",
      "\n",
      "Current Policy:\n",
      "[['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']]\n",
      "\n",
      "GOAL REACHED in episode 588!\n",
      "Path taken: [(0, np.int64(1)), (8, np.int64(1)), (16, np.int64(1)), (24, np.int64(3)), (16, np.int64(0)), (16, np.int64(1)), (24, np.int64(2)), (25, np.int64(2)), (26, np.int64(2)), (27, np.int64(2)), (28, np.int64(1)), (36, np.int64(2)), (37, np.int64(2)), (38, np.int64(0)), (37, np.int64(0)), (36, np.int64(2)), (37, np.int64(0)), (36, np.int64(2)), (37, np.int64(2)), (38, np.int64(2)), (39, np.int64(1)), (47, np.int64(1)), (55, np.int64(2)), (55, np.int64(3)), (47, np.int64(1)), (55, np.int64(1))]\n",
      "\n",
      "Current Policy:\n",
      "[['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']]\n",
      "\n",
      "GOAL REACHED in episode 591!\n",
      "Path taken: [(0, np.int64(1)), (8, np.int64(1)), (16, np.int64(1)), (24, np.int64(2)), (25, np.int64(2)), (26, np.int64(2)), (27, np.int64(2)), (28, np.int64(1)), (36, np.int64(2)), (37, np.int64(1)), (45, np.int64(3)), (37, np.int64(2)), (38, np.int64(2)), (39, np.int64(1)), (47, np.int64(1)), (55, np.int64(1))]\n",
      "\n",
      "Current Policy:\n",
      "[['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']]\n",
      "\n",
      "GOAL REACHED in episode 600!\n",
      "Path taken: [(0, np.int64(0)), (0, np.int64(1)), (8, np.int64(1)), (16, np.int64(3)), (8, np.int64(1)), (16, np.int64(1)), (24, np.int64(2)), (25, np.int64(0)), (24, np.int64(2)), (25, np.int64(2)), (26, np.int64(2)), (27, np.int64(2)), (28, np.int64(1)), (36, np.int64(3)), (28, np.int64(1)), (36, np.int64(2)), (37, np.int64(2)), (38, np.int64(3)), (30, np.int64(1)), (38, np.int64(3)), (30, np.int64(1)), (38, np.int64(2)), (39, np.int64(1)), (47, np.int64(1)), (55, np.int64(1))]\n",
      "\n",
      "Current Policy:\n",
      "[['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']]\n",
      "\n",
      "Episode 600\n",
      "Average reward: -1.554\n",
      "Success rate: 24.00%\n",
      "\n",
      "Current Policy:\n",
      "[['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']]\n",
      "\n",
      "Least visited states:\n",
      "State 19: 0 visits\n",
      "State 29: 0 visits\n",
      "State 35: 0 visits\n",
      "State 41: 0 visits\n",
      "State 42: 0 visits\n",
      "\n",
      "GOAL REACHED in episode 601!\n",
      "Path taken: [(0, np.int64(1)), (8, np.int64(1)), (16, np.int64(2)), (17, np.int64(1)), (25, np.int64(3)), (17, np.int64(1)), (25, np.int64(2)), (26, np.int64(2)), (27, np.int64(2)), (28, np.int64(1)), (36, np.int64(2)), (37, np.int64(2)), (38, np.int64(2)), (39, np.int64(1)), (47, np.int64(1)), (55, np.int64(1))]\n",
      "\n",
      "Current Policy:\n",
      "[['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']]\n",
      "\n",
      "GOAL REACHED in episode 611!\n",
      "Path taken: [(0, np.int64(1)), (8, np.int64(1)), (16, np.int64(1)), (24, np.int64(2)), (25, np.int64(2)), (26, np.int64(2)), (27, np.int64(2)), (28, np.int64(0)), (27, np.int64(2)), (28, np.int64(1)), (36, np.int64(2)), (37, np.int64(2)), (38, np.int64(2)), (39, np.int64(1)), (47, np.int64(1)), (55, np.int64(1))]\n",
      "\n",
      "Current Policy:\n",
      "[['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']]\n",
      "\n",
      "GOAL REACHED in episode 612!\n",
      "Path taken: [(0, np.int64(1)), (8, np.int64(1)), (16, np.int64(3)), (8, np.int64(1)), (16, np.int64(1)), (24, np.int64(3)), (16, np.int64(1)), (24, np.int64(2)), (25, np.int64(3)), (17, np.int64(1)), (25, np.int64(2)), (26, np.int64(2)), (27, np.int64(2)), (28, np.int64(1)), (36, np.int64(2)), (37, np.int64(2)), (38, np.int64(2)), (39, np.int64(1)), (47, np.int64(1)), (55, np.int64(1))]\n",
      "\n",
      "Current Policy:\n",
      "[['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']]\n",
      "\n",
      "GOAL REACHED in episode 616!\n",
      "Path taken: [(0, np.int64(1)), (8, np.int64(0)), (8, np.int64(1)), (16, np.int64(1)), (24, np.int64(2)), (25, np.int64(2)), (26, np.int64(3)), (18, np.int64(1)), (26, np.int64(2)), (27, np.int64(2)), (28, np.int64(1)), (36, np.int64(2)), (37, np.int64(2)), (38, np.int64(3)), (30, np.int64(1)), (38, np.int64(2)), (39, np.int64(3)), (31, np.int64(2)), (31, np.int64(0)), (30, np.int64(2)), (31, np.int64(3)), (23, np.int64(1)), (31, np.int64(1)), (39, np.int64(0)), (38, np.int64(0)), (37, np.int64(2)), (38, np.int64(0)), (37, np.int64(2)), (38, np.int64(2)), (39, np.int64(1)), (47, np.int64(1)), (55, np.int64(1))]\n",
      "\n",
      "Current Policy:\n",
      "[['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']]\n",
      "\n",
      "GOAL REACHED in episode 617!\n",
      "Path taken: [(0, np.int64(1)), (8, np.int64(1)), (16, np.int64(0)), (16, np.int64(1)), (24, np.int64(0)), (24, np.int64(3)), (16, np.int64(2)), (17, np.int64(1)), (25, np.int64(2)), (26, np.int64(3)), (18, np.int64(1)), (26, np.int64(2)), (27, np.int64(2)), (28, np.int64(1)), (36, np.int64(2)), (37, np.int64(2)), (38, np.int64(2)), (39, np.int64(0)), (38, np.int64(2)), (39, np.int64(1)), (47, np.int64(1)), (55, np.int64(1))]\n",
      "\n",
      "Current Policy:\n",
      "[['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']]\n",
      "\n",
      "GOAL REACHED in episode 618!\n",
      "Path taken: [(0, np.int64(1)), (8, np.int64(1)), (16, np.int64(1)), (24, np.int64(2)), (25, np.int64(2)), (26, np.int64(2)), (27, np.int64(2)), (28, np.int64(1)), (36, np.int64(2)), (37, np.int64(2)), (38, np.int64(2)), (39, np.int64(1)), (47, np.int64(2)), (47, np.int64(2)), (47, np.int64(1)), (55, np.int64(1))]\n",
      "\n",
      "Current Policy:\n",
      "[['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']]\n",
      "\n",
      "GOAL REACHED in episode 619!\n",
      "Path taken: [(0, np.int64(0)), (0, np.int64(1)), (8, np.int64(1)), (16, np.int64(1)), (24, np.int64(1)), (32, np.int64(3)), (24, np.int64(2)), (25, np.int64(3)), (17, np.int64(1)), (25, np.int64(2)), (26, np.int64(2)), (27, np.int64(2)), (28, np.int64(1)), (36, np.int64(2)), (37, np.int64(0)), (36, np.int64(2)), (37, np.int64(2)), (38, np.int64(2)), (39, np.int64(1)), (47, np.int64(3)), (39, np.int64(1)), (47, np.int64(1)), (55, np.int64(1))]\n",
      "\n",
      "Current Policy:\n",
      "[['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']]\n",
      "\n",
      "GOAL REACHED in episode 620!\n",
      "Path taken: [(0, np.int64(1)), (8, np.int64(1)), (16, np.int64(1)), (24, np.int64(2)), (25, np.int64(1)), (33, np.int64(2)), (34, np.int64(3)), (26, np.int64(2)), (27, np.int64(2)), (28, np.int64(1)), (36, np.int64(2)), (37, np.int64(2)), (38, np.int64(2)), (39, np.int64(1)), (47, np.int64(1)), (55, np.int64(1))]\n",
      "\n",
      "Current Policy:\n",
      "[['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']]\n",
      "\n",
      "GOAL REACHED in episode 623!\n",
      "Path taken: [(0, np.int64(1)), (8, np.int64(1)), (16, np.int64(1)), (24, np.int64(2)), (25, np.int64(2)), (26, np.int64(2)), (27, np.int64(2)), (28, np.int64(1)), (36, np.int64(2)), (37, np.int64(1)), (45, np.int64(3)), (37, np.int64(2)), (38, np.int64(2)), (39, np.int64(1)), (47, np.int64(1)), (55, np.int64(1))]\n",
      "\n",
      "Current Policy:\n",
      "[['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']]\n",
      "\n",
      "GOAL REACHED in episode 624!\n",
      "Path taken: [(0, np.int64(1)), (8, np.int64(1)), (16, np.int64(3)), (8, np.int64(1)), (16, np.int64(1)), (24, np.int64(2)), (25, np.int64(1)), (33, np.int64(3)), (25, np.int64(0)), (24, np.int64(2)), (25, np.int64(0)), (24, np.int64(2)), (25, np.int64(1)), (33, np.int64(2)), (34, np.int64(3)), (26, np.int64(2)), (27, np.int64(2)), (28, np.int64(1)), (36, np.int64(2)), (37, np.int64(2)), (38, np.int64(2)), (39, np.int64(1)), (47, np.int64(3)), (39, np.int64(1)), (47, np.int64(1)), (55, np.int64(1))]\n",
      "\n",
      "Current Policy:\n",
      "[['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']]\n",
      "\n",
      "GOAL REACHED in episode 629!\n",
      "Path taken: [(0, np.int64(2)), (1, np.int64(0)), (0, np.int64(1)), (8, np.int64(2)), (9, np.int64(3)), (1, np.int64(3)), (1, np.int64(1)), (9, np.int64(3)), (1, np.int64(1)), (9, np.int64(3)), (1, np.int64(2)), (2, np.int64(1)), (10, np.int64(2)), (11, np.int64(3)), (3, np.int64(2)), (4, np.int64(1)), (12, np.int64(2)), (13, np.int64(1)), (21, np.int64(2)), (22, np.int64(1)), (30, np.int64(1)), (38, np.int64(2)), (39, np.int64(1)), (47, np.int64(1)), (55, np.int64(1))]\n",
      "\n",
      "Current Policy:\n",
      "[['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']]\n",
      "\n",
      "GOAL REACHED in episode 630!\n",
      "Path taken: [(0, np.int64(1)), (8, np.int64(1)), (16, np.int64(0)), (16, np.int64(1)), (24, np.int64(2)), (25, np.int64(1)), (33, np.int64(2)), (34, np.int64(3)), (26, np.int64(2)), (27, np.int64(2)), (28, np.int64(1)), (36, np.int64(3)), (28, np.int64(1)), (36, np.int64(2)), (37, np.int64(2)), (38, np.int64(2)), (39, np.int64(1)), (47, np.int64(1)), (55, np.int64(1))]\n",
      "\n",
      "Current Policy:\n",
      "[['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']]\n",
      "\n",
      "GOAL REACHED in episode 632!\n",
      "Path taken: [(0, np.int64(0)), (0, np.int64(0)), (0, np.int64(1)), (8, np.int64(1)), (16, np.int64(1)), (24, np.int64(2)), (25, np.int64(2)), (26, np.int64(2)), (27, np.int64(2)), (28, np.int64(1)), (36, np.int64(1)), (44, np.int64(3)), (36, np.int64(2)), (37, np.int64(2)), (38, np.int64(2)), (39, np.int64(1)), (47, np.int64(2)), (47, np.int64(1)), (55, np.int64(1))]\n",
      "\n",
      "Current Policy:\n",
      "[['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']]\n",
      "\n",
      "GOAL REACHED in episode 634!\n",
      "Path taken: [(0, np.int64(0)), (0, np.int64(1)), (8, np.int64(1)), (16, np.int64(2)), (17, np.int64(1)), (25, np.int64(3)), (17, np.int64(1)), (25, np.int64(2)), (26, np.int64(2)), (27, np.int64(2)), (28, np.int64(1)), (36, np.int64(2)), (37, np.int64(2)), (38, np.int64(3)), (30, np.int64(1)), (38, np.int64(2)), (39, np.int64(1)), (47, np.int64(1)), (55, np.int64(2)), (55, np.int64(1))]\n",
      "\n",
      "Current Policy:\n",
      "[['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']]\n",
      "\n",
      "GOAL REACHED in episode 641!\n",
      "Path taken: [(0, np.int64(1)), (8, np.int64(1)), (16, np.int64(1)), (24, np.int64(2)), (25, np.int64(2)), (26, np.int64(3)), (18, np.int64(1)), (26, np.int64(2)), (27, np.int64(2)), (28, np.int64(3)), (20, np.int64(2)), (21, np.int64(2)), (22, np.int64(3)), (14, np.int64(2)), (15, np.int64(1)), (23, np.int64(0)), (22, np.int64(1)), (30, np.int64(1)), (38, np.int64(2)), (39, np.int64(1)), (47, np.int64(1)), (55, np.int64(1))]\n",
      "\n",
      "Current Policy:\n",
      "[['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']]\n",
      "\n",
      "GOAL REACHED in episode 642!\n",
      "Path taken: [(0, np.int64(1)), (8, np.int64(1)), (16, np.int64(1)), (24, np.int64(2)), (25, np.int64(3)), (17, np.int64(1)), (25, np.int64(2)), (26, np.int64(2)), (27, np.int64(2)), (28, np.int64(1)), (36, np.int64(2)), (37, np.int64(2)), (38, np.int64(2)), (39, np.int64(2)), (39, np.int64(1)), (47, np.int64(1)), (55, np.int64(1))]\n",
      "\n",
      "Current Policy:\n",
      "[['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']]\n",
      "\n",
      "GOAL REACHED in episode 643!\n",
      "Path taken: [(0, np.int64(1)), (8, np.int64(0)), (8, np.int64(1)), (16, np.int64(1)), (24, np.int64(2)), (25, np.int64(2)), (26, np.int64(2)), (27, np.int64(2)), (28, np.int64(1)), (36, np.int64(2)), (37, np.int64(2)), (38, np.int64(2)), (39, np.int64(1)), (47, np.int64(1)), (55, np.int64(1))]\n",
      "\n",
      "Current Policy:\n",
      "[['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']]\n",
      "\n",
      "GOAL REACHED in episode 650!\n",
      "Path taken: [(0, np.int64(0)), (0, np.int64(0)), (0, np.int64(1)), (8, np.int64(1)), (16, np.int64(1)), (24, np.int64(2)), (25, np.int64(1)), (33, np.int64(2)), (34, np.int64(3)), (26, np.int64(2)), (27, np.int64(2)), (28, np.int64(1)), (36, np.int64(2)), (37, np.int64(2)), (38, np.int64(2)), (39, np.int64(1)), (47, np.int64(1)), (55, np.int64(1))]\n",
      "\n",
      "Current Policy:\n",
      "[['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']]\n",
      "\n",
      "GOAL REACHED in episode 651!\n",
      "Path taken: [(0, np.int64(1)), (8, np.int64(1)), (16, np.int64(0)), (16, np.int64(1)), (24, np.int64(2)), (25, np.int64(2)), (26, np.int64(0)), (25, np.int64(2)), (26, np.int64(2)), (27, np.int64(2)), (28, np.int64(3)), (20, np.int64(1)), (28, np.int64(1)), (36, np.int64(3)), (28, np.int64(1)), (36, np.int64(3)), (28, np.int64(1)), (36, np.int64(2)), (37, np.int64(2)), (38, np.int64(2)), (39, np.int64(1)), (47, np.int64(1)), (55, np.int64(1))]\n",
      "\n",
      "Current Policy:\n",
      "[['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']]\n",
      "\n",
      "GOAL REACHED in episode 653!\n",
      "Path taken: [(0, np.int64(1)), (8, np.int64(1)), (16, np.int64(1)), (24, np.int64(2)), (25, np.int64(0)), (24, np.int64(2)), (25, np.int64(2)), (26, np.int64(2)), (27, np.int64(2)), (28, np.int64(1)), (36, np.int64(2)), (37, np.int64(2)), (38, np.int64(0)), (37, np.int64(2)), (38, np.int64(2)), (39, np.int64(1)), (47, np.int64(1)), (55, np.int64(1))]\n",
      "\n",
      "Current Policy:\n",
      "[['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']]\n",
      "\n",
      "GOAL REACHED in episode 657!\n",
      "Path taken: [(0, np.int64(1)), (8, np.int64(1)), (16, np.int64(1)), (24, np.int64(2)), (25, np.int64(0)), (24, np.int64(2)), (25, np.int64(2)), (26, np.int64(2)), (27, np.int64(2)), (28, np.int64(1)), (36, np.int64(2)), (37, np.int64(2)), (38, np.int64(2)), (39, np.int64(2)), (39, np.int64(0)), (38, np.int64(2)), (39, np.int64(2)), (39, np.int64(1)), (47, np.int64(1)), (55, np.int64(1))]\n",
      "\n",
      "Current Policy:\n",
      "[['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']]\n",
      "\n",
      "GOAL REACHED in episode 658!\n",
      "Path taken: [(0, np.int64(1)), (8, np.int64(1)), (16, np.int64(1)), (24, np.int64(2)), (25, np.int64(0)), (24, np.int64(2)), (25, np.int64(2)), (26, np.int64(1)), (34, np.int64(3)), (26, np.int64(2)), (27, np.int64(2)), (28, np.int64(3)), (20, np.int64(1)), (28, np.int64(1)), (36, np.int64(2)), (37, np.int64(2)), (38, np.int64(0)), (37, np.int64(2)), (38, np.int64(2)), (39, np.int64(1)), (47, np.int64(1)), (55, np.int64(2)), (55, np.int64(1))]\n",
      "\n",
      "Current Policy:\n",
      "[['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']]\n",
      "\n",
      "GOAL REACHED in episode 661!\n",
      "Path taken: [(0, np.int64(1)), (8, np.int64(1)), (16, np.int64(1)), (24, np.int64(2)), (25, np.int64(1)), (33, np.int64(2)), (34, np.int64(3)), (26, np.int64(2)), (27, np.int64(2)), (28, np.int64(1)), (36, np.int64(2)), (37, np.int64(0)), (36, np.int64(2)), (37, np.int64(2)), (38, np.int64(2)), (39, np.int64(0)), (38, np.int64(2)), (39, np.int64(3)), (31, np.int64(1)), (39, np.int64(0)), (38, np.int64(2)), (39, np.int64(3)), (31, np.int64(1)), (39, np.int64(2)), (39, np.int64(0)), (38, np.int64(2)), (39, np.int64(1)), (47, np.int64(1)), (55, np.int64(1))]\n",
      "\n",
      "Current Policy:\n",
      "[['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']]\n",
      "\n",
      "GOAL REACHED in episode 664!\n",
      "Path taken: [(0, np.int64(2)), (1, np.int64(1)), (9, np.int64(1)), (17, np.int64(1)), (25, np.int64(2)), (26, np.int64(2)), (27, np.int64(2)), (28, np.int64(1)), (36, np.int64(2)), (37, np.int64(2)), (38, np.int64(2)), (39, np.int64(1)), (47, np.int64(2)), (47, np.int64(1)), (55, np.int64(1))]\n",
      "\n",
      "Current Policy:\n",
      "[['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']]\n",
      "\n",
      "GOAL REACHED in episode 665!\n",
      "Path taken: [(0, np.int64(1)), (8, np.int64(1)), (16, np.int64(1)), (24, np.int64(2)), (25, np.int64(2)), (26, np.int64(2)), (27, np.int64(2)), (28, np.int64(1)), (36, np.int64(1)), (44, np.int64(3)), (36, np.int64(2)), (37, np.int64(2)), (38, np.int64(2)), (39, np.int64(3)), (31, np.int64(1)), (39, np.int64(0)), (38, np.int64(3)), (30, np.int64(1)), (38, np.int64(2)), (39, np.int64(0)), (38, np.int64(2)), (39, np.int64(0)), (38, np.int64(2)), (39, np.int64(1)), (47, np.int64(1)), (55, np.int64(1))]\n",
      "\n",
      "Current Policy:\n",
      "[['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']]\n",
      "\n",
      "GOAL REACHED in episode 667!\n",
      "Path taken: [(0, np.int64(1)), (8, np.int64(1)), (16, np.int64(1)), (24, np.int64(2)), (25, np.int64(1)), (33, np.int64(2)), (34, np.int64(3)), (26, np.int64(1)), (34, np.int64(3)), (26, np.int64(2)), (27, np.int64(2)), (28, np.int64(1)), (36, np.int64(2)), (37, np.int64(2)), (38, np.int64(2)), (39, np.int64(1)), (47, np.int64(1)), (55, np.int64(3)), (47, np.int64(3)), (39, np.int64(1)), (47, np.int64(1)), (55, np.int64(1))]\n",
      "\n",
      "Current Policy:\n",
      "[['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']]\n",
      "\n",
      "GOAL REACHED in episode 669!\n",
      "Path taken: [(0, np.int64(2)), (1, np.int64(1)), (9, np.int64(1)), (17, np.int64(1)), (25, np.int64(2)), (26, np.int64(2)), (27, np.int64(2)), (28, np.int64(1)), (36, np.int64(2)), (37, np.int64(2)), (38, np.int64(2)), (39, np.int64(1)), (47, np.int64(1)), (55, np.int64(1))]\n",
      "\n",
      "Current Policy:\n",
      "[['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']]\n",
      "\n",
      "GOAL REACHED in episode 670!\n",
      "Path taken: [(0, np.int64(1)), (8, np.int64(1)), (16, np.int64(1)), (24, np.int64(2)), (25, np.int64(2)), (26, np.int64(2)), (27, np.int64(0)), (26, np.int64(2)), (27, np.int64(2)), (28, np.int64(0)), (27, np.int64(2)), (28, np.int64(1)), (36, np.int64(2)), (37, np.int64(2)), (38, np.int64(2)), (39, np.int64(1)), (47, np.int64(1)), (55, np.int64(1))]\n",
      "\n",
      "Current Policy:\n",
      "[['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']]\n",
      "\n",
      "GOAL REACHED in episode 673!\n",
      "Path taken: [(0, np.int64(0)), (0, np.int64(2)), (1, np.int64(1)), (9, np.int64(1)), (17, np.int64(1)), (25, np.int64(1)), (33, np.int64(2)), (34, np.int64(0)), (33, np.int64(0)), (32, np.int64(3)), (24, np.int64(2)), (25, np.int64(1)), (33, np.int64(2)), (34, np.int64(3)), (26, np.int64(2)), (27, np.int64(2)), (28, np.int64(3)), (20, np.int64(3)), (12, np.int64(2)), (13, np.int64(1)), (21, np.int64(2)), (22, np.int64(1)), (30, np.int64(1)), (38, np.int64(2)), (39, np.int64(2)), (39, np.int64(2)), (39, np.int64(1)), (47, np.int64(1)), (55, np.int64(1))]\n",
      "\n",
      "Current Policy:\n",
      "[['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']]\n",
      "\n",
      "GOAL REACHED in episode 674!\n",
      "Path taken: [(0, np.int64(1)), (8, np.int64(1)), (16, np.int64(1)), (24, np.int64(1)), (32, np.int64(3)), (24, np.int64(2)), (25, np.int64(0)), (24, np.int64(1)), (32, np.int64(0)), (32, np.int64(3)), (24, np.int64(2)), (25, np.int64(2)), (26, np.int64(2)), (27, np.int64(2)), (28, np.int64(3)), (20, np.int64(1)), (28, np.int64(1)), (36, np.int64(2)), (37, np.int64(2)), (38, np.int64(2)), (39, np.int64(2)), (39, np.int64(1)), (47, np.int64(1)), (55, np.int64(1))]\n",
      "\n",
      "Current Policy:\n",
      "[['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']]\n",
      "\n",
      "GOAL REACHED in episode 677!\n",
      "Path taken: [(0, np.int64(1)), (8, np.int64(1)), (16, np.int64(1)), (24, np.int64(0)), (24, np.int64(2)), (25, np.int64(0)), (24, np.int64(2)), (25, np.int64(2)), (26, np.int64(2)), (27, np.int64(2)), (28, np.int64(1)), (36, np.int64(2)), (37, np.int64(2)), (38, np.int64(2)), (39, np.int64(1)), (47, np.int64(1)), (55, np.int64(1))]\n",
      "\n",
      "Current Policy:\n",
      "[['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']]\n",
      "\n",
      "GOAL REACHED in episode 680!\n",
      "Path taken: [(0, np.int64(1)), (8, np.int64(1)), (16, np.int64(1)), (24, np.int64(2)), (25, np.int64(2)), (26, np.int64(3)), (18, np.int64(0)), (17, np.int64(1)), (25, np.int64(2)), (26, np.int64(2)), (27, np.int64(2)), (28, np.int64(1)), (36, np.int64(2)), (37, np.int64(2)), (38, np.int64(2)), (39, np.int64(1)), (47, np.int64(1)), (55, np.int64(1))]\n",
      "\n",
      "Current Policy:\n",
      "[['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']]\n",
      "\n",
      "GOAL REACHED in episode 681!\n",
      "Path taken: [(0, np.int64(3)), (0, np.int64(1)), (8, np.int64(1)), (16, np.int64(1)), (24, np.int64(3)), (16, np.int64(2)), (17, np.int64(1)), (25, np.int64(2)), (26, np.int64(2)), (27, np.int64(2)), (28, np.int64(3)), (20, np.int64(1)), (28, np.int64(1)), (36, np.int64(3)), (28, np.int64(1)), (36, np.int64(2)), (37, np.int64(2)), (38, np.int64(2)), (39, np.int64(0)), (38, np.int64(2)), (39, np.int64(1)), (47, np.int64(1)), (55, np.int64(1))]\n",
      "\n",
      "Current Policy:\n",
      "[['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']]\n",
      "\n",
      "GOAL REACHED in episode 689!\n",
      "Path taken: [(0, np.int64(1)), (8, np.int64(1)), (16, np.int64(1)), (24, np.int64(0)), (24, np.int64(2)), (25, np.int64(2)), (26, np.int64(3)), (18, np.int64(1)), (26, np.int64(1)), (34, np.int64(3)), (26, np.int64(2)), (27, np.int64(2)), (28, np.int64(0)), (27, np.int64(2)), (28, np.int64(1)), (36, np.int64(2)), (37, np.int64(2)), (38, np.int64(2)), (39, np.int64(1)), (47, np.int64(1)), (55, np.int64(2)), (55, np.int64(1))]\n",
      "\n",
      "Current Policy:\n",
      "[['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']]\n",
      "\n",
      "GOAL REACHED in episode 692!\n",
      "Path taken: [(0, np.int64(1)), (8, np.int64(2)), (9, np.int64(1)), (17, np.int64(1)), (25, np.int64(2)), (26, np.int64(2)), (27, np.int64(2)), (28, np.int64(1)), (36, np.int64(2)), (37, np.int64(2)), (38, np.int64(2)), (39, np.int64(1)), (47, np.int64(1)), (55, np.int64(1))]\n",
      "\n",
      "Current Policy:\n",
      "[['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']]\n",
      "\n",
      "GOAL REACHED in episode 693!\n",
      "Path taken: [(0, np.int64(1)), (8, np.int64(1)), (16, np.int64(1)), (24, np.int64(2)), (25, np.int64(3)), (17, np.int64(2)), (18, np.int64(1)), (26, np.int64(2)), (27, np.int64(2)), (28, np.int64(1)), (36, np.int64(2)), (37, np.int64(2)), (38, np.int64(2)), (39, np.int64(1)), (47, np.int64(1)), (55, np.int64(1))]\n",
      "\n",
      "Current Policy:\n",
      "[['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']]\n",
      "\n",
      "GOAL REACHED in episode 696!\n",
      "Path taken: [(0, np.int64(1)), (8, np.int64(2)), (9, np.int64(0)), (8, np.int64(1)), (16, np.int64(0)), (16, np.int64(1)), (24, np.int64(2)), (25, np.int64(2)), (26, np.int64(0)), (25, np.int64(0)), (24, np.int64(0)), (24, np.int64(2)), (25, np.int64(2)), (26, np.int64(2)), (27, np.int64(0)), (26, np.int64(3)), (18, np.int64(1)), (26, np.int64(2)), (27, np.int64(2)), (28, np.int64(1)), (36, np.int64(2)), (37, np.int64(2)), (38, np.int64(2)), (39, np.int64(1)), (47, np.int64(2)), (47, np.int64(1)), (55, np.int64(1))]\n",
      "\n",
      "Current Policy:\n",
      "[['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']]\n",
      "\n",
      "Episode 700\n",
      "Average reward: 0.396\n",
      "Success rate: 37.00%\n",
      "\n",
      "Current Policy:\n",
      "[['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']]\n",
      "\n",
      "Least visited states:\n",
      "State 19: 0 visits\n",
      "State 29: 0 visits\n",
      "State 35: 0 visits\n",
      "State 41: 0 visits\n",
      "State 42: 0 visits\n",
      "\n",
      "GOAL REACHED in episode 702!\n",
      "Path taken: [(0, np.int64(1)), (8, np.int64(1)), (16, np.int64(1)), (24, np.int64(2)), (25, np.int64(2)), (26, np.int64(2)), (27, np.int64(2)), (28, np.int64(3)), (20, np.int64(1)), (28, np.int64(1)), (36, np.int64(1)), (44, np.int64(3)), (36, np.int64(2)), (37, np.int64(2)), (38, np.int64(2)), (39, np.int64(1)), (47, np.int64(2)), (47, np.int64(1)), (55, np.int64(3)), (47, np.int64(1)), (55, np.int64(2)), (55, np.int64(1))]\n",
      "\n",
      "Current Policy:\n",
      "[['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']]\n",
      "\n",
      "GOAL REACHED in episode 703!\n",
      "Path taken: [(0, np.int64(0)), (0, np.int64(1)), (8, np.int64(1)), (16, np.int64(1)), (24, np.int64(2)), (25, np.int64(2)), (26, np.int64(2)), (27, np.int64(2)), (28, np.int64(1)), (36, np.int64(2)), (37, np.int64(2)), (38, np.int64(3)), (30, np.int64(1)), (38, np.int64(2)), (39, np.int64(1)), (47, np.int64(1)), (55, np.int64(1))]\n",
      "\n",
      "Current Policy:\n",
      "[['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']]\n",
      "\n",
      "GOAL REACHED in episode 705!\n",
      "Path taken: [(0, np.int64(0)), (0, np.int64(0)), (0, np.int64(1)), (8, np.int64(3)), (0, np.int64(1)), (8, np.int64(1)), (16, np.int64(3)), (8, np.int64(3)), (0, np.int64(1)), (8, np.int64(1)), (16, np.int64(2)), (17, np.int64(0)), (16, np.int64(1)), (24, np.int64(2)), (25, np.int64(2)), (26, np.int64(0)), (25, np.int64(2)), (26, np.int64(3)), (18, np.int64(1)), (26, np.int64(2)), (27, np.int64(2)), (28, np.int64(1)), (36, np.int64(2)), (37, np.int64(2)), (38, np.int64(0)), (37, np.int64(2)), (38, np.int64(2)), (39, np.int64(1)), (47, np.int64(1)), (55, np.int64(2)), (55, np.int64(1))]\n",
      "\n",
      "Current Policy:\n",
      "[['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']]\n",
      "\n",
      "GOAL REACHED in episode 707!\n",
      "Path taken: [(0, np.int64(0)), (0, np.int64(1)), (8, np.int64(1)), (16, np.int64(0)), (16, np.int64(3)), (8, np.int64(2)), (9, np.int64(2)), (10, np.int64(2)), (11, np.int64(2)), (12, np.int64(2)), (13, np.int64(1)), (21, np.int64(2)), (22, np.int64(1)), (30, np.int64(1)), (38, np.int64(2)), (39, np.int64(1)), (47, np.int64(1)), (55, np.int64(1))]\n",
      "\n",
      "Current Policy:\n",
      "[['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']]\n",
      "\n",
      "GOAL REACHED in episode 711!\n",
      "Path taken: [(0, np.int64(2)), (1, np.int64(1)), (9, np.int64(1)), (17, np.int64(1)), (25, np.int64(1)), (33, np.int64(0)), (32, np.int64(2)), (33, np.int64(0)), (32, np.int64(3)), (24, np.int64(3)), (16, np.int64(1)), (24, np.int64(2)), (25, np.int64(0)), (24, np.int64(2)), (25, np.int64(2)), (26, np.int64(0)), (25, np.int64(2)), (26, np.int64(2)), (27, np.int64(2)), (28, np.int64(1)), (36, np.int64(2)), (37, np.int64(2)), (38, np.int64(2)), (39, np.int64(2)), (39, np.int64(2)), (39, np.int64(1)), (47, np.int64(1)), (55, np.int64(1))]\n",
      "\n",
      "Current Policy:\n",
      "[['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']]\n",
      "\n",
      "GOAL REACHED in episode 712!\n",
      "Path taken: [(0, np.int64(2)), (1, np.int64(1)), (9, np.int64(2)), (10, np.int64(1)), (18, np.int64(1)), (26, np.int64(2)), (27, np.int64(2)), (28, np.int64(1)), (36, np.int64(2)), (37, np.int64(2)), (38, np.int64(3)), (30, np.int64(1)), (38, np.int64(2)), (39, np.int64(1)), (47, np.int64(1)), (55, np.int64(1))]\n",
      "\n",
      "Current Policy:\n",
      "[['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']]\n",
      "\n",
      "GOAL REACHED in episode 717!\n",
      "Path taken: [(0, np.int64(2)), (1, np.int64(1)), (9, np.int64(0)), (8, np.int64(1)), (16, np.int64(1)), (24, np.int64(2)), (25, np.int64(2)), (26, np.int64(2)), (27, np.int64(2)), (28, np.int64(1)), (36, np.int64(2)), (37, np.int64(2)), (38, np.int64(2)), (39, np.int64(1)), (47, np.int64(1)), (55, np.int64(1))]\n",
      "\n",
      "Current Policy:\n",
      "[['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']]\n",
      "\n",
      "GOAL REACHED in episode 724!\n",
      "Path taken: [(0, np.int64(1)), (8, np.int64(1)), (16, np.int64(1)), (24, np.int64(2)), (25, np.int64(1)), (33, np.int64(2)), (34, np.int64(3)), (26, np.int64(2)), (27, np.int64(2)), (28, np.int64(1)), (36, np.int64(2)), (37, np.int64(2)), (38, np.int64(2)), (39, np.int64(1)), (47, np.int64(1)), (55, np.int64(1))]\n",
      "\n",
      "Current Policy:\n",
      "[['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']]\n",
      "\n",
      "GOAL REACHED in episode 725!\n",
      "Path taken: [(0, np.int64(2)), (1, np.int64(1)), (9, np.int64(1)), (17, np.int64(1)), (25, np.int64(2)), (26, np.int64(2)), (27, np.int64(2)), (28, np.int64(1)), (36, np.int64(2)), (37, np.int64(1)), (45, np.int64(3)), (37, np.int64(2)), (38, np.int64(2)), (39, np.int64(1)), (47, np.int64(1)), (55, np.int64(1))]\n",
      "\n",
      "Current Policy:\n",
      "[['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']]\n",
      "\n",
      "GOAL REACHED in episode 732!\n",
      "Path taken: [(0, np.int64(1)), (8, np.int64(1)), (16, np.int64(1)), (24, np.int64(2)), (25, np.int64(2)), (26, np.int64(2)), (27, np.int64(2)), (28, np.int64(1)), (36, np.int64(2)), (37, np.int64(2)), (38, np.int64(3)), (30, np.int64(1)), (38, np.int64(3)), (30, np.int64(1)), (38, np.int64(3)), (30, np.int64(1)), (38, np.int64(2)), (39, np.int64(0)), (38, np.int64(2)), (39, np.int64(0)), (38, np.int64(0)), (37, np.int64(2)), (38, np.int64(2)), (39, np.int64(0)), (38, np.int64(2)), (39, np.int64(1)), (47, np.int64(1)), (55, np.int64(1))]\n",
      "\n",
      "Current Policy:\n",
      "[['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']]\n",
      "\n",
      "GOAL REACHED in episode 733!\n",
      "Path taken: [(0, np.int64(2)), (1, np.int64(1)), (9, np.int64(1)), (17, np.int64(1)), (25, np.int64(0)), (24, np.int64(2)), (25, np.int64(2)), (26, np.int64(3)), (18, np.int64(3)), (10, np.int64(0)), (9, np.int64(1)), (17, np.int64(1)), (25, np.int64(2)), (26, np.int64(2)), (27, np.int64(2)), (28, np.int64(1)), (36, np.int64(2)), (37, np.int64(2)), (38, np.int64(2)), (39, np.int64(2)), (39, np.int64(1)), (47, np.int64(1)), (55, np.int64(1))]\n",
      "\n",
      "Current Policy:\n",
      "[['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']]\n",
      "\n",
      "GOAL REACHED in episode 734!\n",
      "Path taken: [(0, np.int64(1)), (8, np.int64(3)), (0, np.int64(1)), (8, np.int64(1)), (16, np.int64(1)), (24, np.int64(2)), (25, np.int64(3)), (17, np.int64(1)), (25, np.int64(3)), (17, np.int64(0)), (16, np.int64(1)), (24, np.int64(3)), (16, np.int64(1)), (24, np.int64(2)), (25, np.int64(2)), (26, np.int64(2)), (27, np.int64(2)), (28, np.int64(1)), (36, np.int64(2)), (37, np.int64(2)), (38, np.int64(2)), (39, np.int64(1)), (47, np.int64(1)), (55, np.int64(1))]\n",
      "\n",
      "Current Policy:\n",
      "[['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']]\n",
      "\n",
      "GOAL REACHED in episode 736!\n",
      "Path taken: [(0, np.int64(0)), (0, np.int64(1)), (8, np.int64(1)), (16, np.int64(1)), (24, np.int64(1)), (32, np.int64(3)), (24, np.int64(2)), (25, np.int64(2)), (26, np.int64(2)), (27, np.int64(2)), (28, np.int64(1)), (36, np.int64(1)), (44, np.int64(2)), (45, np.int64(3)), (37, np.int64(2)), (38, np.int64(2)), (39, np.int64(1)), (47, np.int64(1)), (55, np.int64(1))]\n",
      "\n",
      "Current Policy:\n",
      "[['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']]\n",
      "\n",
      "GOAL REACHED in episode 739!\n",
      "Path taken: [(0, np.int64(1)), (8, np.int64(1)), (16, np.int64(1)), (24, np.int64(2)), (25, np.int64(2)), (26, np.int64(1)), (34, np.int64(3)), (26, np.int64(2)), (27, np.int64(2)), (28, np.int64(1)), (36, np.int64(2)), (37, np.int64(2)), (38, np.int64(2)), (39, np.int64(1)), (47, np.int64(2)), (47, np.int64(1)), (55, np.int64(1))]\n",
      "\n",
      "Current Policy:\n",
      "[['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']]\n",
      "\n",
      "GOAL REACHED in episode 741!\n",
      "Path taken: [(0, np.int64(1)), (8, np.int64(1)), (16, np.int64(1)), (24, np.int64(2)), (25, np.int64(0)), (24, np.int64(2)), (25, np.int64(2)), (26, np.int64(2)), (27, np.int64(2)), (28, np.int64(1)), (36, np.int64(2)), (37, np.int64(2)), (38, np.int64(2)), (39, np.int64(1)), (47, np.int64(1)), (55, np.int64(1))]\n",
      "\n",
      "Current Policy:\n",
      "[['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']]\n",
      "\n",
      "GOAL REACHED in episode 749!\n",
      "Path taken: [(0, np.int64(1)), (8, np.int64(0)), (8, np.int64(1)), (16, np.int64(1)), (24, np.int64(2)), (25, np.int64(2)), (26, np.int64(2)), (27, np.int64(2)), (28, np.int64(1)), (36, np.int64(2)), (37, np.int64(2)), (38, np.int64(2)), (39, np.int64(1)), (47, np.int64(1)), (55, np.int64(2)), (55, np.int64(1))]\n",
      "\n",
      "Current Policy:\n",
      "[['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']]\n",
      "\n",
      "GOAL REACHED in episode 754!\n",
      "Path taken: [(0, np.int64(2)), (1, np.int64(1)), (9, np.int64(1)), (17, np.int64(0)), (16, np.int64(1)), (24, np.int64(2)), (25, np.int64(2)), (26, np.int64(2)), (27, np.int64(2)), (28, np.int64(1)), (36, np.int64(2)), (37, np.int64(2)), (38, np.int64(2)), (39, np.int64(2)), (39, np.int64(0)), (38, np.int64(2)), (39, np.int64(2)), (39, np.int64(1)), (47, np.int64(1)), (55, np.int64(1))]\n",
      "\n",
      "Current Policy:\n",
      "[['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']]\n",
      "\n",
      "GOAL REACHED in episode 758!\n",
      "Path taken: [(0, np.int64(3)), (0, np.int64(1)), (8, np.int64(1)), (16, np.int64(1)), (24, np.int64(2)), (25, np.int64(2)), (26, np.int64(2)), (27, np.int64(2)), (28, np.int64(0)), (27, np.int64(2)), (28, np.int64(1)), (36, np.int64(3)), (28, np.int64(1)), (36, np.int64(2)), (37, np.int64(2)), (38, np.int64(2)), (39, np.int64(1)), (47, np.int64(1)), (55, np.int64(1))]\n",
      "\n",
      "Current Policy:\n",
      "[['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']]\n",
      "\n",
      "GOAL REACHED in episode 762!\n",
      "Path taken: [(0, np.int64(1)), (8, np.int64(1)), (16, np.int64(1)), (24, np.int64(0)), (24, np.int64(2)), (25, np.int64(2)), (26, np.int64(2)), (27, np.int64(2)), (28, np.int64(1)), (36, np.int64(2)), (37, np.int64(2)), (38, np.int64(2)), (39, np.int64(1)), (47, np.int64(1)), (55, np.int64(1))]\n",
      "\n",
      "Current Policy:\n",
      "[['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']]\n",
      "\n",
      "GOAL REACHED in episode 763!\n",
      "Path taken: [(0, np.int64(3)), (0, np.int64(1)), (8, np.int64(1)), (16, np.int64(0)), (16, np.int64(1)), (24, np.int64(2)), (25, np.int64(0)), (24, np.int64(2)), (25, np.int64(2)), (26, np.int64(2)), (27, np.int64(2)), (28, np.int64(1)), (36, np.int64(3)), (28, np.int64(1)), (36, np.int64(2)), (37, np.int64(2)), (38, np.int64(2)), (39, np.int64(1)), (47, np.int64(1)), (55, np.int64(2)), (55, np.int64(1))]\n",
      "\n",
      "Current Policy:\n",
      "[['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']]\n",
      "\n",
      "GOAL REACHED in episode 764!\n",
      "Path taken: [(0, np.int64(2)), (1, np.int64(2)), (2, np.int64(3)), (2, np.int64(1)), (10, np.int64(3)), (2, np.int64(1)), (10, np.int64(1)), (18, np.int64(1)), (26, np.int64(2)), (27, np.int64(2)), (28, np.int64(1)), (36, np.int64(2)), (37, np.int64(2)), (38, np.int64(0)), (37, np.int64(2)), (38, np.int64(0)), (37, np.int64(2)), (38, np.int64(0)), (37, np.int64(2)), (38, np.int64(2)), (39, np.int64(1)), (47, np.int64(1)), (55, np.int64(1))]\n",
      "\n",
      "Current Policy:\n",
      "[['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']]\n",
      "\n",
      "GOAL REACHED in episode 765!\n",
      "Path taken: [(0, np.int64(1)), (8, np.int64(1)), (16, np.int64(3)), (8, np.int64(1)), (16, np.int64(1)), (24, np.int64(3)), (16, np.int64(1)), (24, np.int64(0)), (24, np.int64(2)), (25, np.int64(2)), (26, np.int64(2)), (27, np.int64(2)), (28, np.int64(1)), (36, np.int64(2)), (37, np.int64(2)), (38, np.int64(2)), (39, np.int64(1)), (47, np.int64(1)), (55, np.int64(1))]\n",
      "\n",
      "Current Policy:\n",
      "[['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']]\n",
      "\n",
      "GOAL REACHED in episode 768!\n",
      "Path taken: [(0, np.int64(1)), (8, np.int64(1)), (16, np.int64(1)), (24, np.int64(2)), (25, np.int64(1)), (33, np.int64(2)), (34, np.int64(3)), (26, np.int64(2)), (27, np.int64(2)), (28, np.int64(0)), (27, np.int64(2)), (28, np.int64(1)), (36, np.int64(2)), (37, np.int64(2)), (38, np.int64(2)), (39, np.int64(1)), (47, np.int64(2)), (47, np.int64(1)), (55, np.int64(2)), (55, np.int64(1))]\n",
      "\n",
      "Current Policy:\n",
      "[['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']]\n",
      "\n",
      "GOAL REACHED in episode 774!\n",
      "Path taken: [(0, np.int64(1)), (8, np.int64(1)), (16, np.int64(1)), (24, np.int64(2)), (25, np.int64(2)), (26, np.int64(2)), (27, np.int64(2)), (28, np.int64(1)), (36, np.int64(2)), (37, np.int64(2)), (38, np.int64(2)), (39, np.int64(1)), (47, np.int64(1)), (55, np.int64(1))]\n",
      "\n",
      "Current Policy:\n",
      "[['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']]\n",
      "\n",
      "GOAL REACHED in episode 778!\n",
      "Path taken: [(0, np.int64(1)), (8, np.int64(0)), (8, np.int64(1)), (16, np.int64(1)), (24, np.int64(2)), (25, np.int64(2)), (26, np.int64(2)), (27, np.int64(2)), (28, np.int64(0)), (27, np.int64(2)), (28, np.int64(1)), (36, np.int64(2)), (37, np.int64(1)), (45, np.int64(3)), (37, np.int64(1)), (45, np.int64(3)), (37, np.int64(2)), (38, np.int64(3)), (30, np.int64(1)), (38, np.int64(2)), (39, np.int64(2)), (39, np.int64(1)), (47, np.int64(1)), (55, np.int64(1))]\n",
      "\n",
      "Current Policy:\n",
      "[['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']]\n",
      "\n",
      "GOAL REACHED in episode 782!\n",
      "Path taken: [(0, np.int64(1)), (8, np.int64(1)), (16, np.int64(1)), (24, np.int64(2)), (25, np.int64(2)), (26, np.int64(2)), (27, np.int64(2)), (28, np.int64(1)), (36, np.int64(3)), (28, np.int64(1)), (36, np.int64(2)), (37, np.int64(2)), (38, np.int64(2)), (39, np.int64(2)), (39, np.int64(3)), (31, np.int64(0)), (30, np.int64(1)), (38, np.int64(2)), (39, np.int64(3)), (31, np.int64(1)), (39, np.int64(1)), (47, np.int64(1)), (55, np.int64(3)), (47, np.int64(1)), (55, np.int64(1))]\n",
      "\n",
      "Current Policy:\n",
      "[['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']]\n",
      "\n",
      "GOAL REACHED in episode 785!\n",
      "Path taken: [(0, np.int64(1)), (8, np.int64(1)), (16, np.int64(1)), (24, np.int64(2)), (25, np.int64(2)), (26, np.int64(2)), (27, np.int64(2)), (28, np.int64(1)), (36, np.int64(2)), (37, np.int64(1)), (45, np.int64(3)), (37, np.int64(2)), (38, np.int64(2)), (39, np.int64(1)), (47, np.int64(1)), (55, np.int64(1))]\n",
      "\n",
      "Current Policy:\n",
      "[['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']]\n",
      "\n",
      "GOAL REACHED in episode 788!\n",
      "Path taken: [(0, np.int64(3)), (0, np.int64(2)), (1, np.int64(1)), (9, np.int64(1)), (17, np.int64(0)), (16, np.int64(1)), (24, np.int64(2)), (25, np.int64(0)), (24, np.int64(2)), (25, np.int64(2)), (26, np.int64(2)), (27, np.int64(2)), (28, np.int64(1)), (36, np.int64(2)), (37, np.int64(2)), (38, np.int64(2)), (39, np.int64(1)), (47, np.int64(1)), (55, np.int64(1))]\n",
      "\n",
      "Current Policy:\n",
      "[['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']]\n",
      "\n",
      "GOAL REACHED in episode 789!\n",
      "Path taken: [(0, np.int64(1)), (8, np.int64(1)), (16, np.int64(1)), (24, np.int64(1)), (32, np.int64(3)), (24, np.int64(2)), (25, np.int64(2)), (26, np.int64(2)), (27, np.int64(2)), (28, np.int64(1)), (36, np.int64(2)), (37, np.int64(2)), (38, np.int64(2)), (39, np.int64(1)), (47, np.int64(1)), (55, np.int64(1))]\n",
      "\n",
      "Current Policy:\n",
      "[['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']]\n",
      "\n",
      "GOAL REACHED in episode 791!\n",
      "Path taken: [(0, np.int64(1)), (8, np.int64(1)), (16, np.int64(1)), (24, np.int64(0)), (24, np.int64(2)), (25, np.int64(2)), (26, np.int64(2)), (27, np.int64(2)), (28, np.int64(1)), (36, np.int64(2)), (37, np.int64(2)), (38, np.int64(2)), (39, np.int64(1)), (47, np.int64(1)), (55, np.int64(1))]\n",
      "\n",
      "Current Policy:\n",
      "[['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']]\n",
      "\n",
      "GOAL REACHED in episode 794!\n",
      "Path taken: [(0, np.int64(1)), (8, np.int64(1)), (16, np.int64(0)), (16, np.int64(1)), (24, np.int64(2)), (25, np.int64(2)), (26, np.int64(2)), (27, np.int64(2)), (28, np.int64(3)), (20, np.int64(1)), (28, np.int64(1)), (36, np.int64(2)), (37, np.int64(2)), (38, np.int64(2)), (39, np.int64(1)), (47, np.int64(1)), (55, np.int64(1))]\n",
      "\n",
      "Current Policy:\n",
      "[['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']]\n",
      "\n",
      "GOAL REACHED in episode 795!\n",
      "Path taken: [(0, np.int64(1)), (8, np.int64(1)), (16, np.int64(1)), (24, np.int64(2)), (25, np.int64(1)), (33, np.int64(2)), (34, np.int64(3)), (26, np.int64(2)), (27, np.int64(2)), (28, np.int64(1)), (36, np.int64(2)), (37, np.int64(2)), (38, np.int64(0)), (37, np.int64(2)), (38, np.int64(2)), (39, np.int64(1)), (47, np.int64(2)), (47, np.int64(1)), (55, np.int64(1))]\n",
      "\n",
      "Current Policy:\n",
      "[['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']]\n",
      "\n",
      "GOAL REACHED in episode 797!\n",
      "Path taken: [(0, np.int64(1)), (8, np.int64(1)), (16, np.int64(1)), (24, np.int64(2)), (25, np.int64(2)), (26, np.int64(2)), (27, np.int64(2)), (28, np.int64(1)), (36, np.int64(2)), (37, np.int64(2)), (38, np.int64(2)), (39, np.int64(0)), (38, np.int64(2)), (39, np.int64(1)), (47, np.int64(1)), (55, np.int64(2)), (55, np.int64(1))]\n",
      "\n",
      "Current Policy:\n",
      "[['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']]\n",
      "\n",
      "Episode 800\n",
      "Average reward: -0.195\n",
      "Success rate: 33.00%\n",
      "\n",
      "Current Policy:\n",
      "[['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']]\n",
      "\n",
      "Least visited states:\n",
      "State 19: 0 visits\n",
      "State 29: 0 visits\n",
      "State 35: 0 visits\n",
      "State 41: 0 visits\n",
      "State 42: 0 visits\n",
      "\n",
      "GOAL REACHED in episode 801!\n",
      "Path taken: [(0, np.int64(1)), (8, np.int64(1)), (16, np.int64(1)), (24, np.int64(2)), (25, np.int64(2)), (26, np.int64(2)), (27, np.int64(2)), (28, np.int64(1)), (36, np.int64(2)), (37, np.int64(2)), (38, np.int64(3)), (30, np.int64(1)), (38, np.int64(3)), (30, np.int64(1)), (38, np.int64(2)), (39, np.int64(0)), (38, np.int64(2)), (39, np.int64(1)), (47, np.int64(1)), (55, np.int64(1))]\n",
      "\n",
      "Current Policy:\n",
      "[['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']]\n",
      "\n",
      "GOAL REACHED in episode 803!\n",
      "Path taken: [(0, np.int64(1)), (8, np.int64(1)), (16, np.int64(3)), (8, np.int64(1)), (16, np.int64(1)), (24, np.int64(2)), (25, np.int64(1)), (33, np.int64(2)), (34, np.int64(3)), (26, np.int64(3)), (18, np.int64(1)), (26, np.int64(2)), (27, np.int64(2)), (28, np.int64(1)), (36, np.int64(2)), (37, np.int64(2)), (38, np.int64(3)), (30, np.int64(3)), (22, np.int64(2)), (23, np.int64(1)), (31, np.int64(0)), (30, np.int64(1)), (38, np.int64(2)), (39, np.int64(3)), (31, np.int64(1)), (39, np.int64(1)), (47, np.int64(1)), (55, np.int64(1))]\n",
      "\n",
      "Current Policy:\n",
      "[['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']]\n",
      "\n",
      "GOAL REACHED in episode 807!\n",
      "Path taken: [(0, np.int64(1)), (8, np.int64(1)), (16, np.int64(1)), (24, np.int64(2)), (25, np.int64(2)), (26, np.int64(2)), (27, np.int64(2)), (28, np.int64(1)), (36, np.int64(2)), (37, np.int64(2)), (38, np.int64(2)), (39, np.int64(1)), (47, np.int64(1)), (55, np.int64(1))]\n",
      "\n",
      "Current Policy:\n",
      "[['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']]\n",
      "\n",
      "GOAL REACHED in episode 808!\n",
      "Path taken: [(0, np.int64(1)), (8, np.int64(1)), (16, np.int64(1)), (24, np.int64(2)), (25, np.int64(3)), (17, np.int64(1)), (25, np.int64(2)), (26, np.int64(2)), (27, np.int64(2)), (28, np.int64(1)), (36, np.int64(2)), (37, np.int64(2)), (38, np.int64(2)), (39, np.int64(3)), (31, np.int64(1)), (39, np.int64(1)), (47, np.int64(3)), (39, np.int64(1)), (47, np.int64(3)), (39, np.int64(1)), (47, np.int64(2)), (47, np.int64(1)), (55, np.int64(1))]\n",
      "\n",
      "Current Policy:\n",
      "[['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']]\n",
      "\n",
      "GOAL REACHED in episode 819!\n",
      "Path taken: [(0, np.int64(1)), (8, np.int64(1)), (16, np.int64(3)), (8, np.int64(1)), (16, np.int64(1)), (24, np.int64(2)), (25, np.int64(2)), (26, np.int64(3)), (18, np.int64(1)), (26, np.int64(2)), (27, np.int64(2)), (28, np.int64(0)), (27, np.int64(2)), (28, np.int64(1)), (36, np.int64(2)), (37, np.int64(2)), (38, np.int64(2)), (39, np.int64(1)), (47, np.int64(1)), (55, np.int64(1))]\n",
      "\n",
      "Current Policy:\n",
      "[['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']]\n",
      "\n",
      "GOAL REACHED in episode 824!\n",
      "Path taken: [(0, np.int64(0)), (0, np.int64(2)), (1, np.int64(1)), (9, np.int64(0)), (8, np.int64(1)), (16, np.int64(2)), (17, np.int64(1)), (25, np.int64(2)), (26, np.int64(2)), (27, np.int64(2)), (28, np.int64(1)), (36, np.int64(2)), (37, np.int64(2)), (38, np.int64(2)), (39, np.int64(1)), (47, np.int64(1)), (55, np.int64(1))]\n",
      "\n",
      "Current Policy:\n",
      "[['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']]\n",
      "\n",
      "GOAL REACHED in episode 825!\n",
      "Path taken: [(0, np.int64(1)), (8, np.int64(1)), (16, np.int64(1)), (24, np.int64(2)), (25, np.int64(2)), (26, np.int64(2)), (27, np.int64(2)), (28, np.int64(1)), (36, np.int64(2)), (37, np.int64(0)), (36, np.int64(2)), (37, np.int64(2)), (38, np.int64(2)), (39, np.int64(1)), (47, np.int64(1)), (55, np.int64(1))]\n",
      "\n",
      "Current Policy:\n",
      "[['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']]\n",
      "\n",
      "GOAL REACHED in episode 826!\n",
      "Path taken: [(0, np.int64(1)), (8, np.int64(1)), (16, np.int64(1)), (24, np.int64(2)), (25, np.int64(2)), (26, np.int64(2)), (27, np.int64(2)), (28, np.int64(1)), (36, np.int64(1)), (44, np.int64(3)), (36, np.int64(2)), (37, np.int64(2)), (38, np.int64(3)), (30, np.int64(1)), (38, np.int64(2)), (39, np.int64(1)), (47, np.int64(1)), (55, np.int64(1))]\n",
      "\n",
      "Current Policy:\n",
      "[['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']]\n",
      "\n",
      "GOAL REACHED in episode 828!\n",
      "Path taken: [(0, np.int64(1)), (8, np.int64(1)), (16, np.int64(1)), (24, np.int64(2)), (25, np.int64(2)), (26, np.int64(2)), (27, np.int64(2)), (28, np.int64(0)), (27, np.int64(2)), (28, np.int64(1)), (36, np.int64(2)), (37, np.int64(1)), (45, np.int64(3)), (37, np.int64(2)), (38, np.int64(2)), (39, np.int64(1)), (47, np.int64(1)), (55, np.int64(3)), (47, np.int64(1)), (55, np.int64(2)), (55, np.int64(1))]\n",
      "\n",
      "Current Policy:\n",
      "[['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']]\n",
      "\n",
      "GOAL REACHED in episode 835!\n",
      "Path taken: [(0, np.int64(1)), (8, np.int64(3)), (0, np.int64(1)), (8, np.int64(1)), (16, np.int64(1)), (24, np.int64(2)), (25, np.int64(1)), (33, np.int64(2)), (34, np.int64(3)), (26, np.int64(2)), (27, np.int64(2)), (28, np.int64(1)), (36, np.int64(2)), (37, np.int64(2)), (38, np.int64(2)), (39, np.int64(1)), (47, np.int64(3)), (39, np.int64(3)), (31, np.int64(1)), (39, np.int64(1)), (47, np.int64(1)), (55, np.int64(1))]\n",
      "\n",
      "Current Policy:\n",
      "[['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']]\n",
      "\n",
      "GOAL REACHED in episode 836!\n",
      "Path taken: [(0, np.int64(1)), (8, np.int64(2)), (9, np.int64(1)), (17, np.int64(1)), (25, np.int64(2)), (26, np.int64(2)), (27, np.int64(2)), (28, np.int64(1)), (36, np.int64(1)), (44, np.int64(3)), (36, np.int64(2)), (37, np.int64(2)), (38, np.int64(2)), (39, np.int64(2)), (39, np.int64(2)), (39, np.int64(2)), (39, np.int64(1)), (47, np.int64(1)), (55, np.int64(1))]\n",
      "\n",
      "Current Policy:\n",
      "[['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']]\n",
      "\n",
      "GOAL REACHED in episode 837!\n",
      "Path taken: [(0, np.int64(1)), (8, np.int64(2)), (9, np.int64(1)), (17, np.int64(1)), (25, np.int64(2)), (26, np.int64(2)), (27, np.int64(2)), (28, np.int64(1)), (36, np.int64(2)), (37, np.int64(2)), (38, np.int64(2)), (39, np.int64(1)), (47, np.int64(1)), (55, np.int64(1))]\n",
      "\n",
      "Current Policy:\n",
      "[['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']]\n",
      "\n",
      "GOAL REACHED in episode 838!\n",
      "Path taken: [(0, np.int64(3)), (0, np.int64(3)), (0, np.int64(1)), (8, np.int64(1)), (16, np.int64(1)), (24, np.int64(2)), (25, np.int64(2)), (26, np.int64(2)), (27, np.int64(2)), (28, np.int64(1)), (36, np.int64(2)), (37, np.int64(2)), (38, np.int64(3)), (30, np.int64(1)), (38, np.int64(2)), (39, np.int64(1)), (47, np.int64(1)), (55, np.int64(1))]\n",
      "\n",
      "Current Policy:\n",
      "[['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']]\n",
      "\n",
      "GOAL REACHED in episode 839!\n",
      "Path taken: [(0, np.int64(1)), (8, np.int64(1)), (16, np.int64(1)), (24, np.int64(2)), (25, np.int64(2)), (26, np.int64(2)), (27, np.int64(2)), (28, np.int64(1)), (36, np.int64(1)), (44, np.int64(0)), (43, np.int64(1)), (51, np.int64(3)), (43, np.int64(1)), (51, np.int64(0)), (50, np.int64(2)), (51, np.int64(0)), (50, np.int64(1)), (58, np.int64(3)), (50, np.int64(2)), (51, np.int64(3)), (43, np.int64(1)), (51, np.int64(0)), (50, np.int64(2)), (51, np.int64(3)), (43, np.int64(2)), (44, np.int64(3)), (36, np.int64(2)), (37, np.int64(2)), (38, np.int64(0)), (37, np.int64(2)), (38, np.int64(2)), (39, np.int64(2)), (39, np.int64(1)), (47, np.int64(1)), (55, np.int64(1))]\n",
      "\n",
      "Current Policy:\n",
      "[['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']]\n",
      "\n",
      "GOAL REACHED in episode 840!\n",
      "Path taken: [(0, np.int64(2)), (1, np.int64(1)), (9, np.int64(1)), (17, np.int64(1)), (25, np.int64(2)), (26, np.int64(2)), (27, np.int64(2)), (28, np.int64(1)), (36, np.int64(3)), (28, np.int64(0)), (27, np.int64(2)), (28, np.int64(1)), (36, np.int64(2)), (37, np.int64(2)), (38, np.int64(2)), (39, np.int64(1)), (47, np.int64(1)), (55, np.int64(1))]\n",
      "\n",
      "Current Policy:\n",
      "[['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']]\n",
      "\n",
      "GOAL REACHED in episode 841!\n",
      "Path taken: [(0, np.int64(1)), (8, np.int64(0)), (8, np.int64(1)), (16, np.int64(1)), (24, np.int64(2)), (25, np.int64(2)), (26, np.int64(2)), (27, np.int64(2)), (28, np.int64(1)), (36, np.int64(2)), (37, np.int64(0)), (36, np.int64(2)), (37, np.int64(2)), (38, np.int64(2)), (39, np.int64(1)), (47, np.int64(1)), (55, np.int64(1))]\n",
      "\n",
      "Current Policy:\n",
      "[['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']]\n",
      "\n",
      "GOAL REACHED in episode 848!\n",
      "Path taken: [(0, np.int64(1)), (8, np.int64(1)), (16, np.int64(1)), (24, np.int64(1)), (32, np.int64(3)), (24, np.int64(2)), (25, np.int64(2)), (26, np.int64(2)), (27, np.int64(2)), (28, np.int64(1)), (36, np.int64(2)), (37, np.int64(2)), (38, np.int64(2)), (39, np.int64(1)), (47, np.int64(2)), (47, np.int64(1)), (55, np.int64(1))]\n",
      "\n",
      "Current Policy:\n",
      "[['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']]\n",
      "\n",
      "GOAL REACHED in episode 850!\n",
      "Path taken: [(0, np.int64(0)), (0, np.int64(3)), (0, np.int64(3)), (0, np.int64(2)), (1, np.int64(3)), (1, np.int64(1)), (9, np.int64(1)), (17, np.int64(0)), (16, np.int64(1)), (24, np.int64(2)), (25, np.int64(2)), (26, np.int64(2)), (27, np.int64(2)), (28, np.int64(1)), (36, np.int64(2)), (37, np.int64(1)), (45, np.int64(3)), (37, np.int64(2)), (38, np.int64(2)), (39, np.int64(1)), (47, np.int64(1)), (55, np.int64(1))]\n",
      "\n",
      "Current Policy:\n",
      "[['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']]\n",
      "\n",
      "GOAL REACHED in episode 851!\n",
      "Path taken: [(0, np.int64(1)), (8, np.int64(1)), (16, np.int64(1)), (24, np.int64(2)), (25, np.int64(2)), (26, np.int64(2)), (27, np.int64(2)), (28, np.int64(1)), (36, np.int64(2)), (37, np.int64(2)), (38, np.int64(2)), (39, np.int64(1)), (47, np.int64(1)), (55, np.int64(1))]\n",
      "\n",
      "Current Policy:\n",
      "[['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']]\n",
      "\n",
      "GOAL REACHED in episode 854!\n",
      "Path taken: [(0, np.int64(1)), (8, np.int64(1)), (16, np.int64(1)), (24, np.int64(2)), (25, np.int64(2)), (26, np.int64(2)), (27, np.int64(2)), (28, np.int64(1)), (36, np.int64(2)), (37, np.int64(2)), (38, np.int64(2)), (39, np.int64(1)), (47, np.int64(3)), (39, np.int64(1)), (47, np.int64(3)), (39, np.int64(1)), (47, np.int64(1)), (55, np.int64(1))]\n",
      "\n",
      "Current Policy:\n",
      "[['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']]\n",
      "\n",
      "GOAL REACHED in episode 856!\n",
      "Path taken: [(0, np.int64(1)), (8, np.int64(1)), (16, np.int64(1)), (24, np.int64(1)), (32, np.int64(3)), (24, np.int64(2)), (25, np.int64(2)), (26, np.int64(2)), (27, np.int64(2)), (28, np.int64(1)), (36, np.int64(2)), (37, np.int64(2)), (38, np.int64(2)), (39, np.int64(1)), (47, np.int64(1)), (55, np.int64(1))]\n",
      "\n",
      "Current Policy:\n",
      "[['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']]\n",
      "\n",
      "GOAL REACHED in episode 857!\n",
      "Path taken: [(0, np.int64(1)), (8, np.int64(1)), (16, np.int64(1)), (24, np.int64(0)), (24, np.int64(2)), (25, np.int64(2)), (26, np.int64(2)), (27, np.int64(2)), (28, np.int64(1)), (36, np.int64(2)), (37, np.int64(2)), (38, np.int64(2)), (39, np.int64(1)), (47, np.int64(1)), (55, np.int64(1))]\n",
      "\n",
      "Current Policy:\n",
      "[['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']]\n",
      "\n",
      "GOAL REACHED in episode 859!\n",
      "Path taken: [(0, np.int64(1)), (8, np.int64(1)), (16, np.int64(1)), (24, np.int64(1)), (32, np.int64(3)), (24, np.int64(3)), (16, np.int64(1)), (24, np.int64(2)), (25, np.int64(2)), (26, np.int64(2)), (27, np.int64(2)), (28, np.int64(1)), (36, np.int64(2)), (37, np.int64(2)), (38, np.int64(2)), (39, np.int64(1)), (47, np.int64(1)), (55, np.int64(1))]\n",
      "\n",
      "Current Policy:\n",
      "[['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']]\n",
      "\n",
      "GOAL REACHED in episode 860!\n",
      "Path taken: [(0, np.int64(1)), (8, np.int64(2)), (9, np.int64(1)), (17, np.int64(1)), (25, np.int64(1)), (33, np.int64(2)), (34, np.int64(3)), (26, np.int64(2)), (27, np.int64(2)), (28, np.int64(1)), (36, np.int64(2)), (37, np.int64(2)), (38, np.int64(2)), (39, np.int64(1)), (47, np.int64(1)), (55, np.int64(2)), (55, np.int64(1))]\n",
      "\n",
      "Current Policy:\n",
      "[['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']]\n",
      "\n",
      "GOAL REACHED in episode 861!\n",
      "Path taken: [(0, np.int64(2)), (1, np.int64(1)), (9, np.int64(1)), (17, np.int64(1)), (25, np.int64(2)), (26, np.int64(2)), (27, np.int64(2)), (28, np.int64(1)), (36, np.int64(2)), (37, np.int64(2)), (38, np.int64(2)), (39, np.int64(1)), (47, np.int64(1)), (55, np.int64(3)), (47, np.int64(2)), (47, np.int64(1)), (55, np.int64(3)), (47, np.int64(1)), (55, np.int64(2)), (55, np.int64(1))]\n",
      "\n",
      "Current Policy:\n",
      "[['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']]\n",
      "\n",
      "GOAL REACHED in episode 862!\n",
      "Path taken: [(0, np.int64(1)), (8, np.int64(1)), (16, np.int64(3)), (8, np.int64(1)), (16, np.int64(1)), (24, np.int64(2)), (25, np.int64(2)), (26, np.int64(2)), (27, np.int64(2)), (28, np.int64(1)), (36, np.int64(2)), (37, np.int64(2)), (38, np.int64(2)), (39, np.int64(3)), (31, np.int64(1)), (39, np.int64(1)), (47, np.int64(1)), (55, np.int64(1))]\n",
      "\n",
      "Current Policy:\n",
      "[['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']]\n",
      "\n",
      "GOAL REACHED in episode 863!\n",
      "Path taken: [(0, np.int64(1)), (8, np.int64(1)), (16, np.int64(1)), (24, np.int64(2)), (25, np.int64(2)), (26, np.int64(2)), (27, np.int64(2)), (28, np.int64(1)), (36, np.int64(2)), (37, np.int64(2)), (38, np.int64(2)), (39, np.int64(1)), (47, np.int64(1)), (55, np.int64(1))]\n",
      "\n",
      "Current Policy:\n",
      "[['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']]\n",
      "\n",
      "GOAL REACHED in episode 865!\n",
      "Path taken: [(0, np.int64(3)), (0, np.int64(1)), (8, np.int64(1)), (16, np.int64(1)), (24, np.int64(2)), (25, np.int64(2)), (26, np.int64(2)), (27, np.int64(2)), (28, np.int64(1)), (36, np.int64(2)), (37, np.int64(2)), (38, np.int64(2)), (39, np.int64(1)), (47, np.int64(1)), (55, np.int64(1))]\n",
      "\n",
      "Current Policy:\n",
      "[['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']]\n",
      "\n",
      "GOAL REACHED in episode 868!\n",
      "Path taken: [(0, np.int64(2)), (1, np.int64(1)), (9, np.int64(3)), (1, np.int64(1)), (9, np.int64(1)), (17, np.int64(1)), (25, np.int64(2)), (26, np.int64(2)), (27, np.int64(2)), (28, np.int64(1)), (36, np.int64(2)), (37, np.int64(2)), (38, np.int64(2)), (39, np.int64(1)), (47, np.int64(1)), (55, np.int64(1))]\n",
      "\n",
      "Current Policy:\n",
      "[['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']]\n",
      "\n",
      "GOAL REACHED in episode 870!\n",
      "Path taken: [(0, np.int64(1)), (8, np.int64(1)), (16, np.int64(1)), (24, np.int64(1)), (32, np.int64(3)), (24, np.int64(1)), (32, np.int64(3)), (24, np.int64(0)), (24, np.int64(2)), (25, np.int64(2)), (26, np.int64(2)), (27, np.int64(2)), (28, np.int64(1)), (36, np.int64(2)), (37, np.int64(1)), (45, np.int64(0)), (44, np.int64(3)), (36, np.int64(2)), (37, np.int64(2)), (38, np.int64(2)), (39, np.int64(1)), (47, np.int64(3)), (39, np.int64(1)), (47, np.int64(1)), (55, np.int64(1))]\n",
      "\n",
      "Current Policy:\n",
      "[['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']]\n",
      "\n",
      "GOAL REACHED in episode 872!\n",
      "Path taken: [(0, np.int64(1)), (8, np.int64(1)), (16, np.int64(1)), (24, np.int64(2)), (25, np.int64(2)), (26, np.int64(2)), (27, np.int64(2)), (28, np.int64(1)), (36, np.int64(2)), (37, np.int64(0)), (36, np.int64(2)), (37, np.int64(0)), (36, np.int64(2)), (37, np.int64(2)), (38, np.int64(3)), (30, np.int64(1)), (38, np.int64(2)), (39, np.int64(1)), (47, np.int64(1)), (55, np.int64(2)), (55, np.int64(2)), (55, np.int64(1))]\n",
      "\n",
      "Current Policy:\n",
      "[['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']]\n",
      "\n",
      "GOAL REACHED in episode 873!\n",
      "Path taken: [(0, np.int64(2)), (1, np.int64(3)), (1, np.int64(2)), (2, np.int64(1)), (10, np.int64(1)), (18, np.int64(1)), (26, np.int64(2)), (27, np.int64(2)), (28, np.int64(1)), (36, np.int64(1)), (44, np.int64(3)), (36, np.int64(3)), (28, np.int64(1)), (36, np.int64(2)), (37, np.int64(2)), (38, np.int64(2)), (39, np.int64(1)), (47, np.int64(1)), (55, np.int64(1))]\n",
      "\n",
      "Current Policy:\n",
      "[['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']]\n",
      "\n",
      "GOAL REACHED in episode 874!\n",
      "Path taken: [(0, np.int64(1)), (8, np.int64(1)), (16, np.int64(1)), (24, np.int64(2)), (25, np.int64(2)), (26, np.int64(0)), (25, np.int64(2)), (26, np.int64(2)), (27, np.int64(2)), (28, np.int64(1)), (36, np.int64(2)), (37, np.int64(2)), (38, np.int64(2)), (39, np.int64(1)), (47, np.int64(1)), (55, np.int64(1))]\n",
      "\n",
      "Current Policy:\n",
      "[['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']]\n",
      "\n",
      "GOAL REACHED in episode 875!\n",
      "Path taken: [(0, np.int64(1)), (8, np.int64(1)), (16, np.int64(1)), (24, np.int64(2)), (25, np.int64(2)), (26, np.int64(2)), (27, np.int64(2)), (28, np.int64(1)), (36, np.int64(2)), (37, np.int64(2)), (38, np.int64(2)), (39, np.int64(1)), (47, np.int64(1)), (55, np.int64(1))]\n",
      "\n",
      "Current Policy:\n",
      "[['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']]\n",
      "\n",
      "GOAL REACHED in episode 878!\n",
      "Path taken: [(0, np.int64(1)), (8, np.int64(1)), (16, np.int64(1)), (24, np.int64(0)), (24, np.int64(2)), (25, np.int64(2)), (26, np.int64(2)), (27, np.int64(0)), (26, np.int64(2)), (27, np.int64(2)), (28, np.int64(1)), (36, np.int64(2)), (37, np.int64(2)), (38, np.int64(0)), (37, np.int64(2)), (38, np.int64(2)), (39, np.int64(1)), (47, np.int64(1)), (55, np.int64(1))]\n",
      "\n",
      "Current Policy:\n",
      "[['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']]\n",
      "\n",
      "GOAL REACHED in episode 879!\n",
      "Path taken: [(0, np.int64(1)), (8, np.int64(1)), (16, np.int64(1)), (24, np.int64(2)), (25, np.int64(2)), (26, np.int64(2)), (27, np.int64(2)), (28, np.int64(1)), (36, np.int64(2)), (37, np.int64(2)), (38, np.int64(2)), (39, np.int64(1)), (47, np.int64(1)), (55, np.int64(1))]\n",
      "\n",
      "Current Policy:\n",
      "[['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']]\n",
      "\n",
      "GOAL REACHED in episode 880!\n",
      "Path taken: [(0, np.int64(1)), (8, np.int64(1)), (16, np.int64(1)), (24, np.int64(2)), (25, np.int64(2)), (26, np.int64(3)), (18, np.int64(1)), (26, np.int64(2)), (27, np.int64(2)), (28, np.int64(1)), (36, np.int64(2)), (37, np.int64(2)), (38, np.int64(2)), (39, np.int64(3)), (31, np.int64(1)), (39, np.int64(1)), (47, np.int64(1)), (55, np.int64(1))]\n",
      "\n",
      "Current Policy:\n",
      "[['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']]\n",
      "\n",
      "GOAL REACHED in episode 883!\n",
      "Path taken: [(0, np.int64(1)), (8, np.int64(1)), (16, np.int64(1)), (24, np.int64(2)), (25, np.int64(2)), (26, np.int64(2)), (27, np.int64(2)), (28, np.int64(1)), (36, np.int64(2)), (37, np.int64(2)), (38, np.int64(2)), (39, np.int64(2)), (39, np.int64(1)), (47, np.int64(1)), (55, np.int64(1))]\n",
      "\n",
      "Current Policy:\n",
      "[['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']]\n",
      "\n",
      "GOAL REACHED in episode 885!\n",
      "Path taken: [(0, np.int64(2)), (1, np.int64(1)), (9, np.int64(1)), (17, np.int64(0)), (16, np.int64(1)), (24, np.int64(2)), (25, np.int64(2)), (26, np.int64(2)), (27, np.int64(2)), (28, np.int64(1)), (36, np.int64(2)), (37, np.int64(0)), (36, np.int64(3)), (28, np.int64(1)), (36, np.int64(2)), (37, np.int64(2)), (38, np.int64(2)), (39, np.int64(2)), (39, np.int64(1)), (47, np.int64(1)), (55, np.int64(1))]\n",
      "\n",
      "Current Policy:\n",
      "[['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']]\n",
      "\n",
      "GOAL REACHED in episode 886!\n",
      "Path taken: [(0, np.int64(1)), (8, np.int64(2)), (9, np.int64(1)), (17, np.int64(1)), (25, np.int64(2)), (26, np.int64(2)), (27, np.int64(2)), (28, np.int64(1)), (36, np.int64(2)), (37, np.int64(2)), (38, np.int64(2)), (39, np.int64(0)), (38, np.int64(2)), (39, np.int64(1)), (47, np.int64(1)), (55, np.int64(1))]\n",
      "\n",
      "Current Policy:\n",
      "[['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']]\n",
      "\n",
      "GOAL REACHED in episode 888!\n",
      "Path taken: [(0, np.int64(2)), (1, np.int64(1)), (9, np.int64(1)), (17, np.int64(1)), (25, np.int64(2)), (26, np.int64(2)), (27, np.int64(2)), (28, np.int64(1)), (36, np.int64(2)), (37, np.int64(2)), (38, np.int64(2)), (39, np.int64(1)), (47, np.int64(1)), (55, np.int64(2)), (55, np.int64(1))]\n",
      "\n",
      "Current Policy:\n",
      "[['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']]\n",
      "\n",
      "GOAL REACHED in episode 891!\n",
      "Path taken: [(0, np.int64(3)), (0, np.int64(3)), (0, np.int64(1)), (8, np.int64(2)), (9, np.int64(1)), (17, np.int64(1)), (25, np.int64(2)), (26, np.int64(2)), (27, np.int64(2)), (28, np.int64(1)), (36, np.int64(2)), (37, np.int64(2)), (38, np.int64(2)), (39, np.int64(2)), (39, np.int64(3)), (31, np.int64(1)), (39, np.int64(1)), (47, np.int64(2)), (47, np.int64(2)), (47, np.int64(3)), (39, np.int64(1)), (47, np.int64(1)), (55, np.int64(1))]\n",
      "\n",
      "Current Policy:\n",
      "[['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']]\n",
      "\n",
      "GOAL REACHED in episode 893!\n",
      "Path taken: [(0, np.int64(2)), (1, np.int64(1)), (9, np.int64(0)), (8, np.int64(1)), (16, np.int64(1)), (24, np.int64(0)), (24, np.int64(2)), (25, np.int64(2)), (26, np.int64(2)), (27, np.int64(2)), (28, np.int64(1)), (36, np.int64(2)), (37, np.int64(2)), (38, np.int64(2)), (39, np.int64(0)), (38, np.int64(2)), (39, np.int64(1)), (47, np.int64(1)), (55, np.int64(1))]\n",
      "\n",
      "Current Policy:\n",
      "[['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']]\n",
      "\n",
      "GOAL REACHED in episode 894!\n",
      "Path taken: [(0, np.int64(1)), (8, np.int64(1)), (16, np.int64(1)), (24, np.int64(2)), (25, np.int64(2)), (26, np.int64(2)), (27, np.int64(2)), (28, np.int64(0)), (27, np.int64(2)), (28, np.int64(1)), (36, np.int64(2)), (37, np.int64(2)), (38, np.int64(2)), (39, np.int64(3)), (31, np.int64(3)), (23, np.int64(2)), (23, np.int64(1)), (31, np.int64(1)), (39, np.int64(2)), (39, np.int64(1)), (47, np.int64(1)), (55, np.int64(1))]\n",
      "\n",
      "Current Policy:\n",
      "[['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']]\n",
      "\n",
      "GOAL REACHED in episode 896!\n",
      "Path taken: [(0, np.int64(2)), (1, np.int64(1)), (9, np.int64(1)), (17, np.int64(1)), (25, np.int64(2)), (26, np.int64(2)), (27, np.int64(2)), (28, np.int64(1)), (36, np.int64(2)), (37, np.int64(2)), (38, np.int64(2)), (39, np.int64(1)), (47, np.int64(1)), (55, np.int64(3)), (47, np.int64(1)), (55, np.int64(1))]\n",
      "\n",
      "Current Policy:\n",
      "[['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']]\n",
      "\n",
      "GOAL REACHED in episode 897!\n",
      "Path taken: [(0, np.int64(1)), (8, np.int64(1)), (16, np.int64(1)), (24, np.int64(2)), (25, np.int64(2)), (26, np.int64(2)), (27, np.int64(2)), (28, np.int64(1)), (36, np.int64(1)), (44, np.int64(3)), (36, np.int64(2)), (37, np.int64(2)), (38, np.int64(2)), (39, np.int64(0)), (38, np.int64(2)), (39, np.int64(1)), (47, np.int64(1)), (55, np.int64(1))]\n",
      "\n",
      "Current Policy:\n",
      "[['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']]\n",
      "\n",
      "GOAL REACHED in episode 899!\n",
      "Path taken: [(0, np.int64(0)), (0, np.int64(1)), (8, np.int64(3)), (0, np.int64(1)), (8, np.int64(1)), (16, np.int64(1)), (24, np.int64(2)), (25, np.int64(1)), (33, np.int64(2)), (34, np.int64(3)), (26, np.int64(2)), (27, np.int64(2)), (28, np.int64(1)), (36, np.int64(2)), (37, np.int64(2)), (38, np.int64(2)), (39, np.int64(1)), (47, np.int64(1)), (55, np.int64(2)), (55, np.int64(1))]\n",
      "\n",
      "Current Policy:\n",
      "[['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']]\n",
      "\n",
      "GOAL REACHED in episode 900!\n",
      "Path taken: [(0, np.int64(2)), (1, np.int64(1)), (9, np.int64(1)), (17, np.int64(1)), (25, np.int64(2)), (26, np.int64(1)), (34, np.int64(3)), (26, np.int64(2)), (27, np.int64(2)), (28, np.int64(1)), (36, np.int64(2)), (37, np.int64(2)), (38, np.int64(2)), (39, np.int64(2)), (39, np.int64(1)), (47, np.int64(1)), (55, np.int64(1))]\n",
      "\n",
      "Current Policy:\n",
      "[['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']]\n",
      "\n",
      "Episode 900\n",
      "Average reward: 2.059\n",
      "Success rate: 48.00%\n",
      "\n",
      "Current Policy:\n",
      "[['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']]\n",
      "\n",
      "Least visited states:\n",
      "State 19: 0 visits\n",
      "State 29: 0 visits\n",
      "State 35: 0 visits\n",
      "State 41: 0 visits\n",
      "State 42: 0 visits\n",
      "\n",
      "GOAL REACHED in episode 905!\n",
      "Path taken: [(0, np.int64(1)), (8, np.int64(1)), (16, np.int64(1)), (24, np.int64(0)), (24, np.int64(1)), (32, np.int64(3)), (24, np.int64(3)), (16, np.int64(1)), (24, np.int64(3)), (16, np.int64(1)), (24, np.int64(2)), (25, np.int64(2)), (26, np.int64(0)), (25, np.int64(2)), (26, np.int64(2)), (27, np.int64(2)), (28, np.int64(1)), (36, np.int64(3)), (28, np.int64(1)), (36, np.int64(2)), (37, np.int64(2)), (38, np.int64(2)), (39, np.int64(1)), (47, np.int64(1)), (55, np.int64(1))]\n",
      "\n",
      "Current Policy:\n",
      "[['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']]\n",
      "\n",
      "GOAL REACHED in episode 907!\n",
      "Path taken: [(0, np.int64(3)), (0, np.int64(3)), (0, np.int64(0)), (0, np.int64(1)), (8, np.int64(1)), (16, np.int64(0)), (16, np.int64(1)), (24, np.int64(2)), (25, np.int64(2)), (26, np.int64(2)), (27, np.int64(2)), (28, np.int64(1)), (36, np.int64(2)), (37, np.int64(0)), (36, np.int64(2)), (37, np.int64(2)), (38, np.int64(2)), (39, np.int64(1)), (47, np.int64(2)), (47, np.int64(1)), (55, np.int64(1))]\n",
      "\n",
      "Current Policy:\n",
      "[['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']]\n",
      "\n",
      "GOAL REACHED in episode 909!\n",
      "Path taken: [(0, np.int64(1)), (8, np.int64(1)), (16, np.int64(2)), (17, np.int64(1)), (25, np.int64(2)), (26, np.int64(2)), (27, np.int64(2)), (28, np.int64(1)), (36, np.int64(2)), (37, np.int64(2)), (38, np.int64(2)), (39, np.int64(1)), (47, np.int64(1)), (55, np.int64(3)), (47, np.int64(1)), (55, np.int64(1))]\n",
      "\n",
      "Current Policy:\n",
      "[['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']]\n",
      "\n",
      "GOAL REACHED in episode 910!\n",
      "Path taken: [(0, np.int64(1)), (8, np.int64(1)), (16, np.int64(1)), (24, np.int64(0)), (24, np.int64(2)), (25, np.int64(2)), (26, np.int64(0)), (25, np.int64(2)), (26, np.int64(2)), (27, np.int64(2)), (28, np.int64(1)), (36, np.int64(2)), (37, np.int64(2)), (38, np.int64(2)), (39, np.int64(1)), (47, np.int64(1)), (55, np.int64(1))]\n",
      "\n",
      "Current Policy:\n",
      "[['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']]\n",
      "\n",
      "GOAL REACHED in episode 911!\n",
      "Path taken: [(0, np.int64(1)), (8, np.int64(1)), (16, np.int64(1)), (24, np.int64(1)), (32, np.int64(3)), (24, np.int64(2)), (25, np.int64(2)), (26, np.int64(2)), (27, np.int64(2)), (28, np.int64(0)), (27, np.int64(2)), (28, np.int64(1)), (36, np.int64(2)), (37, np.int64(2)), (38, np.int64(2)), (39, np.int64(1)), (47, np.int64(1)), (55, np.int64(1))]\n",
      "\n",
      "Current Policy:\n",
      "[['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']]\n",
      "\n",
      "GOAL REACHED in episode 914!\n",
      "Path taken: [(0, np.int64(1)), (8, np.int64(2)), (9, np.int64(2)), (10, np.int64(1)), (18, np.int64(1)), (26, np.int64(2)), (27, np.int64(2)), (28, np.int64(1)), (36, np.int64(2)), (37, np.int64(0)), (36, np.int64(2)), (37, np.int64(2)), (38, np.int64(2)), (39, np.int64(1)), (47, np.int64(1)), (55, np.int64(1))]\n",
      "\n",
      "Current Policy:\n",
      "[['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']]\n",
      "\n",
      "GOAL REACHED in episode 915!\n",
      "Path taken: [(0, np.int64(1)), (8, np.int64(2)), (9, np.int64(1)), (17, np.int64(1)), (25, np.int64(2)), (26, np.int64(2)), (27, np.int64(2)), (28, np.int64(1)), (36, np.int64(2)), (37, np.int64(2)), (38, np.int64(2)), (39, np.int64(1)), (47, np.int64(1)), (55, np.int64(1))]\n",
      "\n",
      "Current Policy:\n",
      "[['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']]\n",
      "\n",
      "GOAL REACHED in episode 919!\n",
      "Path taken: [(0, np.int64(1)), (8, np.int64(3)), (0, np.int64(1)), (8, np.int64(2)), (9, np.int64(3)), (1, np.int64(1)), (9, np.int64(1)), (17, np.int64(1)), (25, np.int64(2)), (26, np.int64(2)), (27, np.int64(2)), (28, np.int64(1)), (36, np.int64(2)), (37, np.int64(2)), (38, np.int64(2)), (39, np.int64(1)), (47, np.int64(1)), (55, np.int64(1))]\n",
      "\n",
      "Current Policy:\n",
      "[['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']]\n",
      "\n",
      "GOAL REACHED in episode 920!\n",
      "Path taken: [(0, np.int64(1)), (8, np.int64(1)), (16, np.int64(1)), (24, np.int64(2)), (25, np.int64(2)), (26, np.int64(2)), (27, np.int64(2)), (28, np.int64(1)), (36, np.int64(2)), (37, np.int64(2)), (38, np.int64(2)), (39, np.int64(0)), (38, np.int64(2)), (39, np.int64(1)), (47, np.int64(3)), (39, np.int64(1)), (47, np.int64(1)), (55, np.int64(1))]\n",
      "\n",
      "Current Policy:\n",
      "[['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']]\n",
      "\n",
      "GOAL REACHED in episode 923!\n",
      "Path taken: [(0, np.int64(1)), (8, np.int64(1)), (16, np.int64(3)), (8, np.int64(1)), (16, np.int64(1)), (24, np.int64(2)), (25, np.int64(2)), (26, np.int64(2)), (27, np.int64(2)), (28, np.int64(3)), (20, np.int64(1)), (28, np.int64(3)), (20, np.int64(1)), (28, np.int64(1)), (36, np.int64(2)), (37, np.int64(2)), (38, np.int64(2)), (39, np.int64(1)), (47, np.int64(1)), (55, np.int64(1))]\n",
      "\n",
      "Current Policy:\n",
      "[['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']]\n",
      "\n",
      "GOAL REACHED in episode 925!\n",
      "Path taken: [(0, np.int64(1)), (8, np.int64(1)), (16, np.int64(1)), (24, np.int64(2)), (25, np.int64(2)), (26, np.int64(2)), (27, np.int64(2)), (28, np.int64(1)), (36, np.int64(2)), (37, np.int64(2)), (38, np.int64(2)), (39, np.int64(1)), (47, np.int64(1)), (55, np.int64(1))]\n",
      "\n",
      "Current Policy:\n",
      "[['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']]\n",
      "\n",
      "GOAL REACHED in episode 927!\n",
      "Path taken: [(0, np.int64(2)), (1, np.int64(1)), (9, np.int64(0)), (8, np.int64(0)), (8, np.int64(1)), (16, np.int64(1)), (24, np.int64(2)), (25, np.int64(1)), (33, np.int64(2)), (34, np.int64(3)), (26, np.int64(2)), (27, np.int64(2)), (28, np.int64(1)), (36, np.int64(2)), (37, np.int64(1)), (45, np.int64(3)), (37, np.int64(2)), (38, np.int64(2)), (39, np.int64(1)), (47, np.int64(1)), (55, np.int64(2)), (55, np.int64(1))]\n",
      "\n",
      "Current Policy:\n",
      "[['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']]\n",
      "\n",
      "GOAL REACHED in episode 930!\n",
      "Path taken: [(0, np.int64(1)), (8, np.int64(1)), (16, np.int64(1)), (24, np.int64(2)), (25, np.int64(2)), (26, np.int64(2)), (27, np.int64(2)), (28, np.int64(0)), (27, np.int64(2)), (28, np.int64(1)), (36, np.int64(2)), (37, np.int64(2)), (38, np.int64(2)), (39, np.int64(1)), (47, np.int64(1)), (55, np.int64(1))]\n",
      "\n",
      "Current Policy:\n",
      "[['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']]\n",
      "\n",
      "GOAL REACHED in episode 931!\n",
      "Path taken: [(0, np.int64(2)), (1, np.int64(3)), (1, np.int64(1)), (9, np.int64(1)), (17, np.int64(1)), (25, np.int64(2)), (26, np.int64(0)), (25, np.int64(3)), (17, np.int64(0)), (16, np.int64(3)), (8, np.int64(0)), (8, np.int64(1)), (16, np.int64(0)), (16, np.int64(1)), (24, np.int64(3)), (16, np.int64(1)), (24, np.int64(1)), (32, np.int64(3)), (24, np.int64(3)), (16, np.int64(1)), (24, np.int64(2)), (25, np.int64(3)), (17, np.int64(1)), (25, np.int64(2)), (26, np.int64(2)), (27, np.int64(2)), (28, np.int64(1)), (36, np.int64(2)), (37, np.int64(2)), (38, np.int64(2)), (39, np.int64(1)), (47, np.int64(1)), (55, np.int64(1))]\n",
      "\n",
      "Current Policy:\n",
      "[['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']]\n",
      "\n",
      "GOAL REACHED in episode 932!\n",
      "Path taken: [(0, np.int64(3)), (0, np.int64(1)), (8, np.int64(1)), (16, np.int64(1)), (24, np.int64(2)), (25, np.int64(2)), (26, np.int64(2)), (27, np.int64(2)), (28, np.int64(1)), (36, np.int64(2)), (37, np.int64(2)), (38, np.int64(0)), (37, np.int64(2)), (38, np.int64(2)), (39, np.int64(1)), (47, np.int64(1)), (55, np.int64(2)), (55, np.int64(1))]\n",
      "\n",
      "Current Policy:\n",
      "[['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']]\n",
      "\n",
      "GOAL REACHED in episode 935!\n",
      "Path taken: [(0, np.int64(1)), (8, np.int64(1)), (16, np.int64(1)), (24, np.int64(2)), (25, np.int64(2)), (26, np.int64(2)), (27, np.int64(2)), (28, np.int64(1)), (36, np.int64(2)), (37, np.int64(2)), (38, np.int64(2)), (39, np.int64(1)), (47, np.int64(1)), (55, np.int64(1))]\n",
      "\n",
      "Current Policy:\n",
      "[['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']]\n",
      "\n",
      "GOAL REACHED in episode 938!\n",
      "Path taken: [(0, np.int64(1)), (8, np.int64(3)), (0, np.int64(3)), (0, np.int64(1)), (8, np.int64(1)), (16, np.int64(1)), (24, np.int64(2)), (25, np.int64(2)), (26, np.int64(2)), (27, np.int64(2)), (28, np.int64(1)), (36, np.int64(2)), (37, np.int64(2)), (38, np.int64(2)), (39, np.int64(1)), (47, np.int64(1)), (55, np.int64(1))]\n",
      "\n",
      "Current Policy:\n",
      "[['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']]\n",
      "\n",
      "GOAL REACHED in episode 941!\n",
      "Path taken: [(0, np.int64(1)), (8, np.int64(1)), (16, np.int64(2)), (17, np.int64(1)), (25, np.int64(2)), (26, np.int64(0)), (25, np.int64(2)), (26, np.int64(2)), (27, np.int64(0)), (26, np.int64(2)), (27, np.int64(2)), (28, np.int64(1)), (36, np.int64(2)), (37, np.int64(2)), (38, np.int64(2)), (39, np.int64(1)), (47, np.int64(1)), (55, np.int64(1))]\n",
      "\n",
      "Current Policy:\n",
      "[['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']]\n",
      "\n",
      "GOAL REACHED in episode 942!\n",
      "Path taken: [(0, np.int64(1)), (8, np.int64(1)), (16, np.int64(1)), (24, np.int64(2)), (25, np.int64(2)), (26, np.int64(0)), (25, np.int64(2)), (26, np.int64(1)), (34, np.int64(3)), (26, np.int64(2)), (27, np.int64(0)), (26, np.int64(1)), (34, np.int64(3)), (26, np.int64(0)), (25, np.int64(2)), (26, np.int64(0)), (25, np.int64(2)), (26, np.int64(2)), (27, np.int64(2)), (28, np.int64(1)), (36, np.int64(2)), (37, np.int64(2)), (38, np.int64(2)), (39, np.int64(1)), (47, np.int64(1)), (55, np.int64(1))]\n",
      "\n",
      "Current Policy:\n",
      "[['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']]\n",
      "\n",
      "GOAL REACHED in episode 943!\n",
      "Path taken: [(0, np.int64(1)), (8, np.int64(1)), (16, np.int64(1)), (24, np.int64(3)), (16, np.int64(1)), (24, np.int64(1)), (32, np.int64(3)), (24, np.int64(2)), (25, np.int64(2)), (26, np.int64(2)), (27, np.int64(2)), (28, np.int64(1)), (36, np.int64(2)), (37, np.int64(2)), (38, np.int64(0)), (37, np.int64(1)), (45, np.int64(3)), (37, np.int64(2)), (38, np.int64(0)), (37, np.int64(2)), (38, np.int64(2)), (39, np.int64(2)), (39, np.int64(2)), (39, np.int64(3)), (31, np.int64(1)), (39, np.int64(0)), (38, np.int64(2)), (39, np.int64(1)), (47, np.int64(2)), (47, np.int64(3)), (39, np.int64(1)), (47, np.int64(1)), (55, np.int64(1))]\n",
      "\n",
      "Current Policy:\n",
      "[['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']]\n",
      "\n",
      "GOAL REACHED in episode 945!\n",
      "Path taken: [(0, np.int64(1)), (8, np.int64(1)), (16, np.int64(1)), (24, np.int64(2)), (25, np.int64(2)), (26, np.int64(2)), (27, np.int64(2)), (28, np.int64(1)), (36, np.int64(2)), (37, np.int64(2)), (38, np.int64(2)), (39, np.int64(1)), (47, np.int64(1)), (55, np.int64(1))]\n",
      "\n",
      "Current Policy:\n",
      "[['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']]\n",
      "\n",
      "GOAL REACHED in episode 947!\n",
      "Path taken: [(0, np.int64(1)), (8, np.int64(1)), (16, np.int64(1)), (24, np.int64(1)), (32, np.int64(2)), (33, np.int64(2)), (34, np.int64(3)), (26, np.int64(2)), (27, np.int64(0)), (26, np.int64(2)), (27, np.int64(2)), (28, np.int64(1)), (36, np.int64(2)), (37, np.int64(2)), (38, np.int64(2)), (39, np.int64(1)), (47, np.int64(1)), (55, np.int64(1))]\n",
      "\n",
      "Current Policy:\n",
      "[['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']]\n",
      "\n",
      "GOAL REACHED in episode 949!\n",
      "Path taken: [(0, np.int64(1)), (8, np.int64(1)), (16, np.int64(1)), (24, np.int64(2)), (25, np.int64(2)), (26, np.int64(3)), (18, np.int64(1)), (26, np.int64(2)), (27, np.int64(0)), (26, np.int64(2)), (27, np.int64(2)), (28, np.int64(0)), (27, np.int64(2)), (28, np.int64(1)), (36, np.int64(2)), (37, np.int64(2)), (38, np.int64(3)), (30, np.int64(1)), (38, np.int64(0)), (37, np.int64(2)), (38, np.int64(2)), (39, np.int64(0)), (38, np.int64(0)), (37, np.int64(2)), (38, np.int64(0)), (37, np.int64(2)), (38, np.int64(2)), (39, np.int64(1)), (47, np.int64(3)), (39, np.int64(2)), (39, np.int64(1)), (47, np.int64(1)), (55, np.int64(1))]\n",
      "\n",
      "Current Policy:\n",
      "[['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']]\n",
      "\n",
      "GOAL REACHED in episode 950!\n",
      "Path taken: [(0, np.int64(1)), (8, np.int64(1)), (16, np.int64(1)), (24, np.int64(2)), (25, np.int64(2)), (26, np.int64(2)), (27, np.int64(2)), (28, np.int64(1)), (36, np.int64(2)), (37, np.int64(2)), (38, np.int64(2)), (39, np.int64(1)), (47, np.int64(1)), (55, np.int64(1))]\n",
      "\n",
      "Current Policy:\n",
      "[['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']]\n",
      "\n",
      "GOAL REACHED in episode 951!\n",
      "Path taken: [(0, np.int64(2)), (1, np.int64(1)), (9, np.int64(1)), (17, np.int64(1)), (25, np.int64(2)), (26, np.int64(2)), (27, np.int64(2)), (28, np.int64(1)), (36, np.int64(2)), (37, np.int64(0)), (36, np.int64(2)), (37, np.int64(2)), (38, np.int64(2)), (39, np.int64(1)), (47, np.int64(1)), (55, np.int64(1))]\n",
      "\n",
      "Current Policy:\n",
      "[['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']]\n",
      "\n",
      "GOAL REACHED in episode 953!\n",
      "Path taken: [(0, np.int64(1)), (8, np.int64(1)), (16, np.int64(1)), (24, np.int64(2)), (25, np.int64(2)), (26, np.int64(1)), (34, np.int64(3)), (26, np.int64(2)), (27, np.int64(2)), (28, np.int64(3)), (20, np.int64(1)), (28, np.int64(3)), (20, np.int64(1)), (28, np.int64(1)), (36, np.int64(2)), (37, np.int64(2)), (38, np.int64(2)), (39, np.int64(1)), (47, np.int64(1)), (55, np.int64(1))]\n",
      "\n",
      "Current Policy:\n",
      "[['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']]\n",
      "\n",
      "GOAL REACHED in episode 955!\n",
      "Path taken: [(0, np.int64(1)), (8, np.int64(1)), (16, np.int64(1)), (24, np.int64(3)), (16, np.int64(1)), (24, np.int64(3)), (16, np.int64(1)), (24, np.int64(2)), (25, np.int64(2)), (26, np.int64(2)), (27, np.int64(2)), (28, np.int64(1)), (36, np.int64(2)), (37, np.int64(2)), (38, np.int64(2)), (39, np.int64(1)), (47, np.int64(1)), (55, np.int64(1))]\n",
      "\n",
      "Current Policy:\n",
      "[['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']]\n",
      "\n",
      "GOAL REACHED in episode 956!\n",
      "Path taken: [(0, np.int64(1)), (8, np.int64(1)), (16, np.int64(1)), (24, np.int64(1)), (32, np.int64(3)), (24, np.int64(2)), (25, np.int64(2)), (26, np.int64(2)), (27, np.int64(2)), (28, np.int64(1)), (36, np.int64(2)), (37, np.int64(2)), (38, np.int64(2)), (39, np.int64(1)), (47, np.int64(1)), (55, np.int64(3)), (47, np.int64(1)), (55, np.int64(1))]\n",
      "\n",
      "Current Policy:\n",
      "[['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']]\n",
      "\n",
      "GOAL REACHED in episode 960!\n",
      "Path taken: [(0, np.int64(1)), (8, np.int64(1)), (16, np.int64(1)), (24, np.int64(0)), (24, np.int64(3)), (16, np.int64(1)), (24, np.int64(2)), (25, np.int64(2)), (26, np.int64(2)), (27, np.int64(0)), (26, np.int64(2)), (27, np.int64(2)), (28, np.int64(1)), (36, np.int64(2)), (37, np.int64(2)), (38, np.int64(0)), (37, np.int64(2)), (38, np.int64(0)), (37, np.int64(2)), (38, np.int64(3)), (30, np.int64(1)), (38, np.int64(2)), (39, np.int64(0)), (38, np.int64(3)), (30, np.int64(3)), (22, np.int64(1)), (30, np.int64(1)), (38, np.int64(2)), (39, np.int64(2)), (39, np.int64(3)), (31, np.int64(1)), (39, np.int64(3)), (31, np.int64(1)), (39, np.int64(1)), (47, np.int64(1)), (55, np.int64(1))]\n",
      "\n",
      "Current Policy:\n",
      "[['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']]\n",
      "\n",
      "GOAL REACHED in episode 962!\n",
      "Path taken: [(0, np.int64(1)), (8, np.int64(2)), (9, np.int64(1)), (17, np.int64(1)), (25, np.int64(2)), (26, np.int64(2)), (27, np.int64(2)), (28, np.int64(1)), (36, np.int64(2)), (37, np.int64(2)), (38, np.int64(2)), (39, np.int64(1)), (47, np.int64(1)), (55, np.int64(1))]\n",
      "\n",
      "Current Policy:\n",
      "[['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']]\n",
      "\n",
      "GOAL REACHED in episode 963!\n",
      "Path taken: [(0, np.int64(1)), (8, np.int64(1)), (16, np.int64(1)), (24, np.int64(2)), (25, np.int64(0)), (24, np.int64(2)), (25, np.int64(2)), (26, np.int64(2)), (27, np.int64(2)), (28, np.int64(1)), (36, np.int64(2)), (37, np.int64(2)), (38, np.int64(2)), (39, np.int64(1)), (47, np.int64(1)), (55, np.int64(1))]\n",
      "\n",
      "Current Policy:\n",
      "[['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']]\n",
      "\n",
      "GOAL REACHED in episode 965!\n",
      "Path taken: [(0, np.int64(1)), (8, np.int64(1)), (16, np.int64(1)), (24, np.int64(1)), (32, np.int64(3)), (24, np.int64(2)), (25, np.int64(2)), (26, np.int64(2)), (27, np.int64(2)), (28, np.int64(1)), (36, np.int64(2)), (37, np.int64(2)), (38, np.int64(2)), (39, np.int64(1)), (47, np.int64(1)), (55, np.int64(3)), (47, np.int64(1)), (55, np.int64(1))]\n",
      "\n",
      "Current Policy:\n",
      "[['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']]\n",
      "\n",
      "GOAL REACHED in episode 967!\n",
      "Path taken: [(0, np.int64(0)), (0, np.int64(1)), (8, np.int64(1)), (16, np.int64(1)), (24, np.int64(2)), (25, np.int64(2)), (26, np.int64(2)), (27, np.int64(2)), (28, np.int64(1)), (36, np.int64(2)), (37, np.int64(2)), (38, np.int64(2)), (39, np.int64(1)), (47, np.int64(1)), (55, np.int64(1))]\n",
      "\n",
      "Current Policy:\n",
      "[['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']]\n",
      "\n",
      "GOAL REACHED in episode 969!\n",
      "Path taken: [(0, np.int64(1)), (8, np.int64(1)), (16, np.int64(1)), (24, np.int64(2)), (25, np.int64(2)), (26, np.int64(2)), (27, np.int64(2)), (28, np.int64(1)), (36, np.int64(2)), (37, np.int64(2)), (38, np.int64(2)), (39, np.int64(1)), (47, np.int64(1)), (55, np.int64(1))]\n",
      "\n",
      "Current Policy:\n",
      "[['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']]\n",
      "\n",
      "GOAL REACHED in episode 971!\n",
      "Path taken: [(0, np.int64(1)), (8, np.int64(1)), (16, np.int64(1)), (24, np.int64(3)), (16, np.int64(1)), (24, np.int64(2)), (25, np.int64(2)), (26, np.int64(2)), (27, np.int64(2)), (28, np.int64(1)), (36, np.int64(2)), (37, np.int64(2)), (38, np.int64(2)), (39, np.int64(1)), (47, np.int64(1)), (55, np.int64(1))]\n",
      "\n",
      "Current Policy:\n",
      "[['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']]\n",
      "\n",
      "GOAL REACHED in episode 972!\n",
      "Path taken: [(0, np.int64(3)), (0, np.int64(1)), (8, np.int64(1)), (16, np.int64(3)), (8, np.int64(1)), (16, np.int64(0)), (16, np.int64(1)), (24, np.int64(2)), (25, np.int64(2)), (26, np.int64(2)), (27, np.int64(2)), (28, np.int64(1)), (36, np.int64(2)), (37, np.int64(2)), (38, np.int64(2)), (39, np.int64(1)), (47, np.int64(1)), (55, np.int64(1))]\n",
      "\n",
      "Current Policy:\n",
      "[['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']]\n",
      "\n",
      "GOAL REACHED in episode 973!\n",
      "Path taken: [(0, np.int64(3)), (0, np.int64(1)), (8, np.int64(1)), (16, np.int64(2)), (17, np.int64(1)), (25, np.int64(2)), (26, np.int64(2)), (27, np.int64(0)), (26, np.int64(2)), (27, np.int64(2)), (28, np.int64(1)), (36, np.int64(2)), (37, np.int64(2)), (38, np.int64(2)), (39, np.int64(1)), (47, np.int64(2)), (47, np.int64(1)), (55, np.int64(1))]\n",
      "\n",
      "Current Policy:\n",
      "[['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']]\n",
      "\n",
      "GOAL REACHED in episode 976!\n",
      "Path taken: [(0, np.int64(1)), (8, np.int64(1)), (16, np.int64(1)), (24, np.int64(2)), (25, np.int64(2)), (26, np.int64(2)), (27, np.int64(2)), (28, np.int64(1)), (36, np.int64(2)), (37, np.int64(2)), (38, np.int64(2)), (39, np.int64(1)), (47, np.int64(1)), (55, np.int64(1))]\n",
      "\n",
      "Current Policy:\n",
      "[['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']]\n",
      "\n",
      "GOAL REACHED in episode 977!\n",
      "Path taken: [(0, np.int64(1)), (8, np.int64(1)), (16, np.int64(1)), (24, np.int64(2)), (25, np.int64(2)), (26, np.int64(2)), (27, np.int64(2)), (28, np.int64(1)), (36, np.int64(2)), (37, np.int64(2)), (38, np.int64(2)), (39, np.int64(2)), (39, np.int64(1)), (47, np.int64(1)), (55, np.int64(1))]\n",
      "\n",
      "Current Policy:\n",
      "[['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']]\n",
      "\n",
      "GOAL REACHED in episode 978!\n",
      "Path taken: [(0, np.int64(1)), (8, np.int64(3)), (0, np.int64(3)), (0, np.int64(1)), (8, np.int64(1)), (16, np.int64(1)), (24, np.int64(2)), (25, np.int64(2)), (26, np.int64(1)), (34, np.int64(3)), (26, np.int64(2)), (27, np.int64(2)), (28, np.int64(0)), (27, np.int64(2)), (28, np.int64(1)), (36, np.int64(2)), (37, np.int64(2)), (38, np.int64(2)), (39, np.int64(1)), (47, np.int64(1)), (55, np.int64(1))]\n",
      "\n",
      "Current Policy:\n",
      "[['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']]\n",
      "\n",
      "GOAL REACHED in episode 979!\n",
      "Path taken: [(0, np.int64(1)), (8, np.int64(1)), (16, np.int64(0)), (16, np.int64(1)), (24, np.int64(2)), (25, np.int64(2)), (26, np.int64(2)), (27, np.int64(2)), (28, np.int64(1)), (36, np.int64(2)), (37, np.int64(2)), (38, np.int64(2)), (39, np.int64(2)), (39, np.int64(1)), (47, np.int64(1)), (55, np.int64(1))]\n",
      "\n",
      "Current Policy:\n",
      "[['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']]\n",
      "\n",
      "GOAL REACHED in episode 980!\n",
      "Path taken: [(0, np.int64(1)), (8, np.int64(1)), (16, np.int64(1)), (24, np.int64(2)), (25, np.int64(1)), (33, np.int64(2)), (34, np.int64(3)), (26, np.int64(2)), (27, np.int64(2)), (28, np.int64(1)), (36, np.int64(2)), (37, np.int64(2)), (38, np.int64(2)), (39, np.int64(1)), (47, np.int64(1)), (55, np.int64(1))]\n",
      "\n",
      "Current Policy:\n",
      "[['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']]\n",
      "\n",
      "GOAL REACHED in episode 982!\n",
      "Path taken: [(0, np.int64(3)), (0, np.int64(1)), (8, np.int64(1)), (16, np.int64(1)), (24, np.int64(1)), (32, np.int64(3)), (24, np.int64(2)), (25, np.int64(2)), (26, np.int64(2)), (27, np.int64(2)), (28, np.int64(3)), (20, np.int64(1)), (28, np.int64(1)), (36, np.int64(2)), (37, np.int64(1)), (45, np.int64(3)), (37, np.int64(2)), (38, np.int64(2)), (39, np.int64(1)), (47, np.int64(2)), (47, np.int64(1)), (55, np.int64(1))]\n",
      "\n",
      "Current Policy:\n",
      "[['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']]\n",
      "\n",
      "GOAL REACHED in episode 983!\n",
      "Path taken: [(0, np.int64(1)), (8, np.int64(1)), (16, np.int64(1)), (24, np.int64(2)), (25, np.int64(2)), (26, np.int64(2)), (27, np.int64(2)), (28, np.int64(1)), (36, np.int64(2)), (37, np.int64(2)), (38, np.int64(0)), (37, np.int64(2)), (38, np.int64(2)), (39, np.int64(1)), (47, np.int64(1)), (55, np.int64(1))]\n",
      "\n",
      "Current Policy:\n",
      "[['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']]\n",
      "\n",
      "GOAL REACHED in episode 985!\n",
      "Path taken: [(0, np.int64(1)), (8, np.int64(3)), (0, np.int64(1)), (8, np.int64(1)), (16, np.int64(1)), (24, np.int64(2)), (25, np.int64(2)), (26, np.int64(2)), (27, np.int64(2)), (28, np.int64(1)), (36, np.int64(2)), (37, np.int64(2)), (38, np.int64(2)), (39, np.int64(3)), (31, np.int64(1)), (39, np.int64(1)), (47, np.int64(3)), (39, np.int64(1)), (47, np.int64(1)), (55, np.int64(1))]\n",
      "\n",
      "Current Policy:\n",
      "[['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']]\n",
      "\n",
      "GOAL REACHED in episode 986!\n",
      "Path taken: [(0, np.int64(1)), (8, np.int64(1)), (16, np.int64(1)), (24, np.int64(2)), (25, np.int64(3)), (17, np.int64(1)), (25, np.int64(1)), (33, np.int64(2)), (34, np.int64(3)), (26, np.int64(2)), (27, np.int64(2)), (28, np.int64(1)), (36, np.int64(2)), (37, np.int64(2)), (38, np.int64(2)), (39, np.int64(1)), (47, np.int64(1)), (55, np.int64(1))]\n",
      "\n",
      "Current Policy:\n",
      "[['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']]\n",
      "\n",
      "GOAL REACHED in episode 987!\n",
      "Path taken: [(0, np.int64(1)), (8, np.int64(1)), (16, np.int64(1)), (24, np.int64(2)), (25, np.int64(2)), (26, np.int64(1)), (34, np.int64(3)), (26, np.int64(2)), (27, np.int64(2)), (28, np.int64(1)), (36, np.int64(2)), (37, np.int64(2)), (38, np.int64(2)), (39, np.int64(1)), (47, np.int64(1)), (55, np.int64(1))]\n",
      "\n",
      "Current Policy:\n",
      "[['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']]\n",
      "\n",
      "GOAL REACHED in episode 989!\n",
      "Path taken: [(0, np.int64(1)), (8, np.int64(1)), (16, np.int64(2)), (17, np.int64(2)), (18, np.int64(1)), (26, np.int64(2)), (27, np.int64(2)), (28, np.int64(1)), (36, np.int64(2)), (37, np.int64(2)), (38, np.int64(2)), (39, np.int64(1)), (47, np.int64(1)), (55, np.int64(1))]\n",
      "\n",
      "Current Policy:\n",
      "[['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']]\n",
      "\n",
      "GOAL REACHED in episode 990!\n",
      "Path taken: [(0, np.int64(2)), (1, np.int64(1)), (9, np.int64(1)), (17, np.int64(1)), (25, np.int64(2)), (26, np.int64(2)), (27, np.int64(2)), (28, np.int64(1)), (36, np.int64(2)), (37, np.int64(2)), (38, np.int64(2)), (39, np.int64(1)), (47, np.int64(1)), (55, np.int64(1))]\n",
      "\n",
      "Current Policy:\n",
      "[['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']]\n",
      "\n",
      "GOAL REACHED in episode 991!\n",
      "Path taken: [(0, np.int64(1)), (8, np.int64(1)), (16, np.int64(3)), (8, np.int64(1)), (16, np.int64(1)), (24, np.int64(2)), (25, np.int64(2)), (26, np.int64(2)), (27, np.int64(2)), (28, np.int64(1)), (36, np.int64(2)), (37, np.int64(2)), (38, np.int64(2)), (39, np.int64(1)), (47, np.int64(1)), (55, np.int64(1))]\n",
      "\n",
      "Current Policy:\n",
      "[['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']]\n",
      "\n",
      "GOAL REACHED in episode 993!\n",
      "Path taken: [(0, np.int64(1)), (8, np.int64(1)), (16, np.int64(3)), (8, np.int64(1)), (16, np.int64(1)), (24, np.int64(2)), (25, np.int64(2)), (26, np.int64(2)), (27, np.int64(2)), (28, np.int64(1)), (36, np.int64(2)), (37, np.int64(2)), (38, np.int64(2)), (39, np.int64(1)), (47, np.int64(1)), (55, np.int64(1))]\n",
      "\n",
      "Current Policy:\n",
      "[['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']]\n",
      "\n",
      "GOAL REACHED in episode 995!\n",
      "Path taken: [(0, np.int64(3)), (0, np.int64(1)), (8, np.int64(1)), (16, np.int64(1)), (24, np.int64(2)), (25, np.int64(2)), (26, np.int64(2)), (27, np.int64(2)), (28, np.int64(1)), (36, np.int64(2)), (37, np.int64(2)), (38, np.int64(2)), (39, np.int64(3)), (31, np.int64(1)), (39, np.int64(1)), (47, np.int64(2)), (47, np.int64(1)), (55, np.int64(2)), (55, np.int64(1))]\n",
      "\n",
      "Current Policy:\n",
      "[['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']]\n",
      "\n",
      "GOAL REACHED in episode 998!\n",
      "Path taken: [(0, np.int64(1)), (8, np.int64(1)), (16, np.int64(1)), (24, np.int64(3)), (16, np.int64(1)), (24, np.int64(1)), (32, np.int64(3)), (24, np.int64(2)), (25, np.int64(2)), (26, np.int64(2)), (27, np.int64(2)), (28, np.int64(1)), (36, np.int64(2)), (37, np.int64(2)), (38, np.int64(2)), (39, np.int64(1)), (47, np.int64(1)), (55, np.int64(1))]\n",
      "\n",
      "Current Policy:\n",
      "[['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']]\n",
      "\n",
      "GOAL REACHED in episode 999!\n",
      "Path taken: [(0, np.int64(1)), (8, np.int64(1)), (16, np.int64(1)), (24, np.int64(2)), (25, np.int64(2)), (26, np.int64(2)), (27, np.int64(2)), (28, np.int64(1)), (36, np.int64(2)), (37, np.int64(2)), (38, np.int64(2)), (39, np.int64(1)), (47, np.int64(1)), (55, np.int64(1))]\n",
      "\n",
      "Current Policy:\n",
      "[['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']]\n",
      "\n",
      "Final Policy:\n",
      "\n",
      "Current Policy:\n",
      "[['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '']]\n",
      "\n",
      "Plotting results...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1200x400 with 2 Axes>"
      ],
      "image/png": "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"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 23
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-17T12:43:37.612308Z",
     "start_time": "2025-02-17T12:43:37.608425Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def evaluate_agent(env, max_steps, n_eval_episodes, Q, seed):\n",
    "    \"\"\"\n",
    "    Evaluate the agent for ``n_eval_episodes`` episodes and returns average reward and std of reward.\n",
    "    :param env: The evaluation environment\n",
    "    :param max_steps: Maximum number of steps per episode\n",
    "    :param n_eval_episodes: Number of episode to evaluate the agent\n",
    "    :param Q: The Q-table\n",
    "    :param seed: The evaluation seed array (for taxi-v3)\n",
    "    \"\"\"\n",
    "    episode_rewards = []\n",
    "    for episode in tqdm(range(n_eval_episodes)):\n",
    "        if seed:\n",
    "            state, info = env.reset(seed=seed[episode])\n",
    "        else:\n",
    "            state, info = env.reset()\n",
    "        step = 0\n",
    "        truncated = False\n",
    "        terminated = False\n",
    "        total_rewards_ep = 0\n",
    "\n",
    "        for step in range(max_steps):\n",
    "            # Take the action (index) that have the maximum expected future reward given that state\n",
    "            action = greedy_policy(Q, state)\n",
    "            new_state, reward, terminated, truncated, info = env.step(action)\n",
    "            total_rewards_ep += reward\n",
    "\n",
    "            if terminated or truncated:\n",
    "                break\n",
    "            state = new_state\n",
    "        episode_rewards.append(total_rewards_ep)\n",
    "    mean_reward = np.mean(episode_rewards)\n",
    "    std_reward = np.std(episode_rewards)\n",
    "\n",
    "    return mean_reward, std_reward"
   ],
   "id": "d96cd3c7843d9713",
   "outputs": [],
   "execution_count": 22
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-17T12:45:09.137043Z",
     "start_time": "2025-02-17T12:45:09.105138Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Evaluate our Agent\n",
    "mean_reward, std_reward = evaluate_agent(env, max_steps, n_eval_episodes, Qtable_frozenlake, eval_seed)\n",
    "print(f\"Mean_reward={mean_reward:.2f} +/- {std_reward:.2f}\")"
   ],
   "id": "79348ef9e59cc641",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 100/100 [00:00<00:00, 9137.32it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean_reward=1.00 +/- 0.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "execution_count": 24
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "from IPython.display import clear_output\n",
    "import time\n",
    "\n",
    "def print_frames(frames):\n",
    "    for i, frame in enumerate(frames):\n",
    "        clear_output(wait=True)\n",
    "        print(frame['frame'])\n",
    "        print(f\"Timestep: {i + 1}\")\n",
    "        print(f\"State: {frame['state']}\")\n",
    "        print(f\"Action: {frame['action']}\")\n",
    "        print(f\"Reward: {frame['reward']}\")\n",
    "        time.sleep(0.5)  # Half second between frames\n",
    "\n",
    "def run_episode(env, Qtable, render=True):\n",
    "    frames = []\n",
    "    state, info = env.reset()\n",
    "    terminated = False\n",
    "    truncated = False\n",
    "\n",
    "    while not terminated and not truncated:\n",
    "        # Take the action (index) that have the maximum expected future reward given that state\n",
    "        action = np.argmax(Qtable[state][:])\n",
    "        new_state, reward, terminated, truncated, info = env.step(action)\n",
    "\n",
    "        if render:\n",
    "            frame = env.render()\n",
    "            frames.append({\n",
    "                'frame': frame,\n",
    "                'state': state,\n",
    "                'action': action,\n",
    "                'reward': reward\n",
    "            })\n",
    "\n",
    "        state = new_state\n",
    "\n",
    "    return frames\n",
    "\n",
    "# Create environment\n",
    "env = gym.make(\"FrozenLake-v1\", map_name=\"8x8\", is_slippery=False, render_mode=\"human\")\n",
    "\n",
    "# Use your trained Q-table here\n",
    "frames = run_episode(env, Qtable_frozenlake)\n",
    "print_frames(frames)\n",
    "\n",
    "# Don't forget to close the environment\n",
    "env.close()"
   ],
   "id": "6fab4b6d081ab8b8",
   "execution_count": 25,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-17T10:46:19.603998Z",
     "start_time": "2025-02-17T10:46:19.601943Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "570ccce95c125480",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-17T10:46:19.616183Z",
     "start_time": "2025-02-17T10:46:19.614389Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "e242ddeac8c725fd",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-17T10:46:19.659750Z",
     "start_time": "2025-02-17T10:46:19.658276Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "674be089d6c01784",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-17T10:46:19.802346Z",
     "start_time": "2025-02-17T10:46:19.800694Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "e46369ecb2ac34f5",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-17T10:46:19.959455Z",
     "start_time": "2025-02-17T10:46:19.955680Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "38f3b09c41d8b4d4",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-17T10:46:20.062402Z",
     "start_time": "2025-02-17T10:46:20.060626Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "2a3d8feb2e85e69c",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
